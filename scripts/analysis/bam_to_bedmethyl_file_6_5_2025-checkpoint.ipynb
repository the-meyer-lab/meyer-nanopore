{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7f7519699219a1c",
   "metadata": {},
   "source": [
    "# Notebook for compiling and analyzing fiber-seq peaks and chip-seq peaks together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d861f4a602b9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool, cpu_count # used for parallel processing\n",
    "import subprocess\n",
    "import os\n",
    "import multiprocessing\n",
    "import tempfile\n",
    "from plotly.subplots import make_subplots\n",
    "from qnorm import quantile_normalize\n",
    "import numpy as np\n",
    "# import go library\n",
    "import plotly.graph_objects as go\n",
    "from pathlib import Path\n",
    "import nanotools\n",
    "importlib.reload(nanotools) # reload nanotools module\n",
    "# import numpy as np\n",
    "# import plotly.io as pio\n",
    "# import plotly\n",
    "# import plotly.express as px # Used for plotting\n",
    "# import plotly.graph_objects as go # Used for plotting\n",
    "# from plotly.subplots import make_subplots\n",
    "# import pywt # for wavelet transform\n",
    "# import matplotlib.pyplot as plt # Use for plotting m6A frac and coverage plot\n",
    "# from matplotlib import cm # Use for plotting m6A frac and coverage plot\n",
    "# from qnorm import quantile_normalize\n",
    "# #import tqdm\n",
    "# #import pysam\n",
    "# #import pyBigWig\n",
    "\n",
    "# import other standard libraries\n",
    "\n",
    "# print date and time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00cb69369fddf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    " ### BAM Configurations\n",
    "R9_m6A_thresh_percent = 0.7\n",
    "R10_m6A_thresh_percent = 0.8\n",
    "R10_5mC_thresh_percent = 0.8  # Note: 0.7 in R9 ~ 0.9 in R10\n",
    "R9_m6A_thresh = int(\n",
    "    round(R9_m6A_thresh_percent * 258, 0))  #default is 129 = 50%; 181=70%; 194=75%; 207 = 80%; 232 = 90%\n",
    "m6A_thresh = int(round(R10_m6A_thresh_percent * 258, 0))\n",
    "mC_thresh = int(round(R10_5mC_thresh_percent * 258, 0))\n",
    "\n",
    "# modkit is used for aggregating methylation data from .bam files\n",
    "# https://nanoporetech.github.io/modkit/quick_start.html\n",
    "modkit_path = \"/Data1/software/modkit_v0.3/modkit\"\n",
    "bedgraphtobigwig_path = \"/Data1/software/ucsc_genome_browser/bedGraphToBigWig\"\n",
    "danpos_path = \"/Data1/software/DANPOS3/danpos.py\"\n",
    "chrom_sizes = \"/Data1/reference/chrom.sizes.ce11.txt\"\n",
    "\n",
    "# analysis_cond = [\n",
    "#     \"N2_mixed_endogenous_R10\",\n",
    "#     \"N2_old_fiber_R10\",\n",
    "#     \"SDC2degron_old_R10\",\n",
    "#     \"DPY27_degron_old_R10\",\n",
    "#     \"DPY21null_old_fiber_R10\",\n",
    "# ]\n",
    "\n",
    "compare_type = \"COND\" # \"COND\" or \"BATCH\"\n",
    "\n",
    "analysis_cond = [\n",
    "    \"N2_comb_R10\",\n",
    "    \"SDC2deg_comb_R10\",\n",
    "    \"DPY27deg_comb_R10\",\n",
    "    \"DPY21null_comb_R10\",\n",
    "\n",
    "    #\"N2_rep1\",\n",
    "    #\"N2_rep2\",\n",
    "    #\"N2_rep3\",\n",
    "    #\"SDC2deg_rep1\",\n",
    "    #\"SDC2deg_rep2\",\n",
    "    #\"SDC2deg_rep3\",\n",
    "\n",
    "    # \"N2_young_SMACseq_R10\",\n",
    "    #\"rex1_MEXIICtoG_R10\",\n",
    "    #\"rex1_MEXIIscramble_R10\"\n",
    "    # \"rex1_MEXIIscramble_R10\",\n",
    "    # \"rex1_MEXIICtoG_R10\"\n",
    "    #\"DPY27deg_rep1\",\n",
    "    #\"DPY27deg_rep2\",\n",
    "    #\"DPY27deg_rep3\",\n",
    "    #\"rex1_MEXIIscramble_biorep0_fiber_old_R10_04_2025\",\n",
    "    # \"rex1_MEXIIscramble_biorep1_fiber_old_R10_04_2025\",\n",
    "    #\"rex1_MEXIIscramble_biorep2_fiber_old_R10_04_2025\",\n",
    "    #\"rex1_MEXIIscramble_biorep3_fiber_old_R10_04_2025\",\n",
    "    #\"rex1_4thCtoG_biorep0_fiber_old_R10_04_2025\",\n",
    "    # \"rex1_4thCtoG_biorep1_fiber_old_R10_04_2025\"]\n",
    "#     \"N2_rep1\",\n",
    "#     \"SDC2deg_rep1\",\n",
    "#     \"SDC2_degron_mid_T0_rep2_R10\",\n",
    "#     \"SDC2_degron_mid_T0_rep3_R10\",\n",
    "#     \"SDC2_degron_mid_T0p5_rep3_R10\",\n",
    "#     \"SDC2_degron_mid_T1_rep3_R10\",\n",
    "#     \"SDC2_degron_mid_T1p5_rep3_R10\",\n",
    "#     \"SDC2_degron_mid_T2_rep2_R10\",\n",
    "#     \"SDC2_degron_mid_T3_rep2_R10\",\n",
    "#     \"SDC2_degron_mid_T4_rep3_R10\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "# analysis_cond = [\n",
    "#     \"N2_mixed_endogenous_R10\",\n",
    "#     \"N2_old_R10\",\n",
    "#     \"SDC2deg_old_R10\",\n",
    "#     \"DPY27deg_old_R10\",\n",
    "#     \"DPY21null_old_R10\",\n",
    "# ]\n",
    "\n",
    "# analysis_cond = [\n",
    "#     #\"N2_mixed_endogenous_R10\",\n",
    "#     \"N2_fiber_old_R10_04_2025\",\n",
    "#     \"SDC2deg_fiber_old_R10_04_2025\",\n",
    "#     \"SDC3deg_fiber_old_R10_04_2025\",\n",
    "#     \"SDC2_3deg_fiber_old_R10_04_2025\",\n",
    "#     \"DPY27deg_fiber_old_R10_04_2025\",\n",
    "#     \"DPY21null_fiber_old_R10_04_2025\",\n",
    "#     \"96_DPY27degron_rep1_fiber_old_R10_052424\",\n",
    "# ]\n",
    "\n",
    "### IMPORT BAM FILES AND METADATA FROM CSV FILE\n",
    "if compare_type == \"COND\":\n",
    "    input_metadata = pd.read_csv(\"/Data1/git/meyer-nanopore/scripts/bam_input_metadata_4_28_2025_COND.txt\", sep=\"\\t\", header=0)\n",
    "else:\n",
    "    input_metadata = pd.read_csv(\"/Data1/git/meyer-nanopore/scripts/bam_input_metadata_4_28_2025_BATCH.txt\", sep=\"\\t\", header=0)\n",
    "# Set bam_files equal to list of items in column bam_files where conditions == N2_fiber\n",
    "bam_files = input_metadata[input_metadata[\"conditions\"].isin(analysis_cond)][\"bam_files\"].tolist()\n",
    "ft_files = [x.replace(\".bam\", \"_ftools0p8.bed\") for x in bam_files]\n",
    "\n",
    "conditions = input_metadata[input_metadata[\"conditions\"].isin(analysis_cond)][\"conditions\"].tolist()\n",
    "exp_ids = input_metadata[input_metadata[\"conditions\"].isin(analysis_cond)][\"exp_id_date\"].tolist()\n",
    "flowcells = input_metadata[input_metadata[\"conditions\"].isin(analysis_cond)][\"flowcell\"].tolist()\n",
    "bam_fracs = len(bam_files) * [1]  # For full .bam set to = 1\n",
    "sample_indices = list(range(len(bam_files)))\n",
    "\n",
    "output_stem = \"/Data1/git/meyer-nanopore/scripts/analysis/temp_files/04292025/\"\n",
    "thresh_list = len(bam_files) * [m6A_thresh / 258]  # For R10 flow cells use 0.5; for R9 flow cells use 0.9\n",
    "# for position in flowcells == R9 set item with same index in thresh_list to R9_m6A_thresh/258\n",
    "for i in range(len(flowcells)):\n",
    "    if \"R9\" in flowcells[i]:\n",
    "        thresh_list[i] = R9_m6A_thresh / 258\n",
    "\n",
    "file_prefix = \"04292025\"\n",
    "\n",
    "# for backwards compatibility\n",
    "exp_ids = input_metadata[input_metadata[\"conditions\"].isin(analysis_cond)][\"exp_id_date\"].tolist()\n",
    "ext_exp_ids = [\"None\"] * len(exp_ids)\n",
    "\n",
    "# Subsample bam based on bam_frac, used to accelerate testing\n",
    "# if bam_frac = 1 will use original bam files, otherwise will save new subsampled bam files to output_stem.\n",
    "\n",
    "new_bam_files = []\n",
    "new_bam_files = nanotools.parallel_subsample_bam(bam_files, conditions, bam_fracs, sample_indices, output_stem)\n",
    "\n",
    "\n",
    "# args_list = [(bam_file, condition, bam_frac, sample_index, output_stem, exp_id) for\n",
    "#              bam_file, condition, bam_frac, sample_index, exp_id in\n",
    "#              zip(bam_files, conditions, bam_fracs, sample_indices, ext_exp_ids)]\n",
    "\n",
    "args_list = [(bam_file, condition, bam_frac, sample_index, output_stem,exp_id) for bam_file, condition, bam_frac, sample_index, exp_id in zip(ft_files,conditions,bam_fracs,sample_indices,exp_ids)]\n",
    "\n",
    "print(\"Program finished!\")\n",
    "print(\"new_bam_files: \", new_bam_files)\n",
    "print(\"exp_ids: \", exp_ids)\n",
    "print(\"conditions: \", conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a829149f5fa6782",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Bed file configurations:\n",
    "sample_source = \"chr_type\" # \"chr_type\" or \"type\" or \"chromosome\"\n",
    "chr_type_selected = [\"X\", \"Autosome\"]\n",
    "# type_selected = \"rex1\" all the way through \"rex48\"\n",
    "\n",
    "type_selected = ['nDCC1', 'nDCC10', 'nDCC11', 'nDCC12', 'nDCC13', 'nDCC14', 'nDCC15', 'nDCC16', 'nDCC17', 'nDCC18', 'nDCC19', 'nDCC2', 'nDCC20', 'nDCC21', 'nDCC22', 'nDCC23', 'nDCC3', 'nDCC4', 'nDCC5', 'nDCC6', 'nDCC7', 'nDCC8', 'nDCC9', 'Prex1', 'Prex11', 'Prex14', 'Prex15', 'Prex16', 'Prex2', 'Prex20', 'Prex21', 'Prex22', 'Prex23', 'Prex25', 'Prex26', 'Prex27', 'Prex3', 'Prex30', 'Prex31', 'Prex6', 'Prex7', 'rex1', 'rex14', 'rex16', 'rex17', 'rex18', 'rex19', 'rex2', 'rex20', 'rex21', 'rex23', 'rex24', 'rex25', 'rex26', 'rex27', 'rex28', 'rex29', 'rex3', 'rex31', 'rex32', 'rex33', 'rex34', 'rex35', 'rex36', 'rex37', 'rex38', 'rex39', 'rex4', 'rex40', 'rex41', 'rex42', 'rex43', 'rex44', 'rex45', 'rex46', 'rex47', 'rex48', 'rex5', 'rex6', 'rex7', 'rex8',\"intergenic_control\"]\n",
    "\n",
    "\n",
    "\n",
    "#\"LMN1_only_damID\",\"EMR1_only_damID\",\"her-1_FULL\",\"sex-1_FULL\"\n",
    "#,\"DPY27_chip_q2\",\"DPY27_chip_q3\",\"DPY27_chip_q4\",\"intergenic_control\"\n",
    "#,\"her-1_TSS\",\"fem-1_TSS\",\"fem-2_TSS\",\"fem-3_TSS\",\"sex-1_TSS\"\n",
    "# her-1_TSS/TES/FULL | TES_q1-4 | #TSS_q1-4 | strong/weak rex | whole_chr | 200kb_region | 50kb_region | center_DPY27_chip_albretton | gene_q1-q4 | MEX_motif | center_SDC2_chip_albretton | center_SDC3_chip_albretton |\n",
    "# ATAC_seq_EXCL_dpy27_ol100 | ATAC_seq;DPY27_ol100\n",
    "# \"motif_weak_dcc_1\",\"motif_weak_dcc_2\",\"motif_weak_dcc_3\",\"motif_weak_dcc_4\",\"motif_weak_dcc_5\",\"motif_weak_dcc_6\",\"motif_weak_dcc_7\",\"motif_weak_dcc_8\",\"motif_weak_dcc_9\",\"motif_weak_dcc_10\"\n",
    "\n",
    "max_regions = 100 # max regions to consider; 0 = full set;\n",
    "chromosome_selected = [\"CHROMOSOME_X\",\"CHROMOSOME_I\", \"CHROMOSOME_II\", \"CHROMOSOME_III\", \"CHROMOSOME_IV\",\"CHROMOSOME_V\"]\n",
    "strand_selected = [\"+\",\"-\"] #+ and/or -\n",
    "select_opp_strand = True #If you want to select both + and - strands for all regions set to True\n",
    "down_sample_autosome = False # If you want to downsample autosome genes to match number of X genes set to True\n",
    "if chr_type_selected == [\"X\"]:\n",
    "    down_sample_autosome = False\n",
    "bed_file = \"/Data1/reference/tss_tes_rex_combined_v27_WS235.bed\"\n",
    "bed_window = 3000   # +/- around bed elements.\n",
    "intergenic_window = 3000 # +/- around intergenic regions\n",
    "num_bins = 1000 #bins for metagene plot\n",
    "mods = \"a\" # {A,CG,A+CG}\n",
    "if sample_source == \"chr_type\":\n",
    "    selection = chr_type_selected\n",
    "if sample_source == \"type\":\n",
    "    selection = type_selected\n",
    "if sample_source == \"chromosome\":\n",
    "    selection = chromosome_selected\n",
    "\n",
    "# Filter input bed_file based on input parameters (e.g. chromosome, type, strand, etc.)\n",
    "# Function saves a new filtered bed file to the same folder as the original bed file\n",
    "# called temp_do_not_use_\"type\".bed\n",
    "importlib.reload(nanotools)\n",
    "new_bed_files=nanotools.filter_bed_file(\n",
    "    bed_file,\n",
    "    sample_source,\n",
    "    selection,\n",
    "    chromosome_selected,\n",
    "    chr_type_selected,\n",
    "    type_selected,\n",
    "    strand_selected,\n",
    "    max_regions,\n",
    "    bed_window,\n",
    "    intergenic_window\n",
    ")\n",
    "\n",
    "modkit_bed_name_ext = \"modkit_temp_ext.bed\"\n",
    "modkit_bed_df_ext = nanotools.generate_modkit_bed(new_bed_files, down_sample_autosome, select_opp_strand,modkit_bed_name_ext)\n",
    "nanotools.display_sample_rows(modkit_bed_df_ext, 5)\n",
    "\n",
    "print(\"Program finished!\")\n",
    "print(\"exp_ids: \", exp_ids)\n",
    "\n",
    "combined_bed_df_ext = nanotools.create_lookup_bed(new_bed_files)\n",
    "\n",
    "nanotools.display_sample_rows(combined_bed_df_ext, 5)\n",
    "\n",
    "# print count by type\n",
    "print(\"Count by type: \", combined_bed_df_ext[\"type\"].value_counts())\n",
    "\n",
    "\n",
    "# for intergneic control for background methylation by condition plots\n",
    "bed_path = Path(output_stem) / \"intergenic_control.bed\"\n",
    "\n",
    "\n",
    "# ────────────────────  BUILD include-bed FROM DataFrame  ───────────────────\n",
    "\n",
    "if not bed_path.exists():\n",
    "    if 'combined_bed_df_ext' not in globals():\n",
    "        abort(\"combined_bed_df_ext DataFrame not found in workspace.\")\n",
    "    bed_df = (\n",
    "        combined_bed_df_ext\n",
    "        .loc[combined_bed_df_ext[\"type\"] == \"intergenic_control\",\n",
    "             [\"chrom\", \"bed_start\", \"bed_end\"]]\n",
    "        .copy()\n",
    "    )\n",
    "    bed_df[\"bed_start\"] = bed_df[\"bed_start\"].astype(int)\n",
    "    bed_df[\"bed_end\"]   = bed_df[\"bed_end\"].astype(int)\n",
    "    bed_df.to_csv(bed_path, sep=\"\\t\", header=False, index=False)\n",
    "\n",
    "# print all unique types\n",
    "print(\"Unique types in combined_bed_df_ext: \", combined_bed_df_ext[\"type\"].unique().tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f83da661a367750",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ╔══════════════════════════════════════════════════════════════════════════╗\n",
    "# ║  modkit sample-probs  ▸  per-sample cut-offs using include-bed filter    ║\n",
    "# ║                                                                          ║\n",
    "# ║  Outputs two dicts:                                                      ║\n",
    "# ║    • m6A_thresh_dict  = { bam_path: threshold, … }                       ║\n",
    "# ║    • m5mC_thresh_dict = { bam_path: threshold, … }                       ║\n",
    "# ║                                                                          ║\n",
    "# ║  All other logic (skip-if-exists, ascending search, verbose DEBUG)       ║\n",
    "# ║  remains unchanged.                                                      ║\n",
    "# ╚══════════════════════════════════════════════════════════════════════════╝\n",
    "import os, subprocess, logging, sys\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "force_replace = False  # skip modkit run if TSV already exists and this is False\n",
    "\n",
    "# ─────────────────────────  LOGGING CONFIG  ────────────────────────────────\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s [%(levelname)7s] %(message)s\",\n",
    "    level=logging.DEBUG,\n",
    "    datefmt=\"%H:%M:%S\",\n",
    ")\n",
    "abort = lambda msg: (logging.critical(msg), sys.exit(1))\n",
    "\n",
    "# ──────────────────────────  INPUT CHECKS  ────────────────────────────────\n",
    "if not (modkit_path and os.path.isfile(modkit_path) and os.access(modkit_path, os.X_OK)):\n",
    "    abort(f\"modkit executable not found: {modkit_path}\")\n",
    "if not new_bam_files:\n",
    "    abort(\"new_bam_files is empty.\")\n",
    "for p in new_bam_files:\n",
    "    if not os.path.isfile(p):\n",
    "        abort(f\"BAM not found: {p}\")\n",
    "\n",
    "# ─────────────────────  CONSTANTS & PATHS  ────────────────────────────────\n",
    "MAX_CPUS        = 64\n",
    "THREADS_PER_JOB = min(12, MAX_CPUS, cpu_count())\n",
    "POOL_SIZE       = max(1, min(len(new_bam_files), MAX_CPUS // THREADS_PER_JOB))\n",
    "SAMPLE_FRAC     = 0.5\n",
    "PROB_FLOOR      = 0.8\n",
    "\n",
    "sample_probs_root = Path(output_stem) / \"sample_probs\"\n",
    "sample_probs_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# ────────────────────────  PREFIX GENERATION  ─────────────────────────────\n",
    "def make_prefix(bam_path: str, idx: int) -> str:\n",
    "    return f\"{idx:02d}_{Path(bam_path).stem}\"\n",
    "prefixes = [make_prefix(p, i) for i, p in enumerate(new_bam_files)]\n",
    "\n",
    "# ─────────────────────────  RUN sample-probs  ─────────────────────────────\n",
    "def run_sample_probs(bam, prefix):\n",
    "    out_dir = sample_probs_root / prefix\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    cached = list(out_dir.glob(\"*_probabilities.tsv\"))\n",
    "    if cached and not force_replace:\n",
    "        logging.debug(\"%s: using cached TSV %s\", prefix, cached[0].name)\n",
    "        return str(cached[0])\n",
    "\n",
    "    cmd = [\n",
    "        modkit_path, \"sample-probs\", bam,\n",
    "        \"-f\", str(SAMPLE_FRAC),\n",
    "        \"-t\", str(THREADS_PER_JOB),\n",
    "        \"--include-bed\", str(bed_path),\n",
    "        \"-o\", str(out_dir),\n",
    "        \"--prefix\", prefix,\n",
    "        \"--hist\", \"--force\", \"--suppress-progress\",\n",
    "    ]\n",
    "    logging.debug(\"CMD: %s\", \" \".join(cmd))\n",
    "\n",
    "    try:\n",
    "        subprocess.run(cmd, check=True, stderr=subprocess.PIPE, text=True)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        logging.error(\"modkit failed with exit code %d\", e.returncode)\n",
    "        logging.error(\"modkit stderr:\\n%s\", e.stderr.strip())\n",
    "        # re-raise so the pool still sees the failure\n",
    "        raise\n",
    "\n",
    "    tsvs = list(out_dir.glob(\"*_probabilities.tsv\"))\n",
    "    if len(tsvs) != 1:\n",
    "        abort(f\"Expected one *_probabilities.tsv in {out_dir}, found {len(tsvs)}\")\n",
    "    return str(tsvs[0])\n",
    "\n",
    "\n",
    "with Pool(POOL_SIZE) as pool:\n",
    "    tsv_paths = list(\n",
    "        tqdm(pool.starmap(run_sample_probs, zip(new_bam_files, prefixes)),\n",
    "             total=len(new_bam_files), desc=\"sample-probs\")\n",
    "    )\n",
    "\n",
    "# ─────────────────────────  LOAD TABLES  ───────────────────────────────────\n",
    "tables = [pd.read_csv(p, sep=r\"\\s+\") for p in tsv_paths]\n",
    "\n",
    "# ────────────────────  CANONICAL COUNTS ABOVE 0.80  ────────────────────────\n",
    "def canonical_count(df, base):\n",
    "    return int(df.loc[(df[\"code\"] == base) & (df[\"range_end\"] >= PROB_FLOOR), \"count\"].sum())\n",
    "denom_A = [canonical_count(df, \"A\") for df in tables]\n",
    "denom_C = [canonical_count(df, \"C\") for df in tables]\n",
    "\n",
    "# ────────────────────  MODIFIED COUNTS & RATIOS @ 0.80  ────────────────────\n",
    "def mod_above(df, mod_code, thresh):\n",
    "    return int(df.loc[(df[\"code\"] == mod_code) & (df[\"range_end\"] >= thresh), \"count\"].sum())\n",
    "ratio_floor_A = [mod_above(df, \"a\", PROB_FLOOR)/den if den else 0.0\n",
    "                 for df, den in zip(tables, denom_A)]\n",
    "ratio_floor_C = [mod_above(df, \"m\", PROB_FLOOR)/den if den else 0.0\n",
    "                 for df, den in zip(tables, denom_C)]\n",
    "\n",
    "baseline_ratio_m6A = min(ratio_floor_A)\n",
    "baseline_ratio_5mC = min(ratio_floor_C)\n",
    "logging.info(\"Baseline ratios  m6A=%.5f  5mC=%.5f\",\n",
    "             baseline_ratio_m6A, baseline_ratio_5mC)\n",
    "\n",
    "# ───────────────────  ASCENDING THRESHOLD SEARCH  ──────────────────────────\n",
    "def ascending_thresh(df, mod_code, denom, baseline):\n",
    "    if denom == 0:\n",
    "        logging.debug(\"  denom=0 → %.2f\", PROB_FLOOR)\n",
    "        return PROB_FLOOR\n",
    "    bins = df[(df[\"code\"] == mod_code) & (df[\"range_end\"] >= PROB_FLOOR)].copy()\n",
    "    bins.sort_values(\"range_start\", inplace=True)\n",
    "    total = bins[\"count\"].sum()\n",
    "    if total/denom <= baseline:\n",
    "        return PROB_FLOOR\n",
    "    for _, row in bins.iterrows():\n",
    "        total -= row[\"count\"]\n",
    "        if total/denom <= baseline:\n",
    "            return max(row[\"range_end\"], PROB_FLOOR)\n",
    "    return PROB_FLOOR\n",
    "\n",
    "m6A_thresh = []\n",
    "m5mC_thresh = []\n",
    "for df, denA, denC in zip(tables, denom_A, denom_C):\n",
    "    m6A_thresh.append(round(ascending_thresh(df, \"a\", denA, baseline_ratio_m6A), 2))\n",
    "    m5mC_thresh.append(round(ascending_thresh(df, \"m\", denC, baseline_ratio_5mC), 2))\n",
    "\n",
    "# ─────────────────────────  BUILD OUTPUT DICTS ─────────────────────────────\n",
    "m6A_thresh_dict  = dict(zip(new_bam_files, m6A_thresh))\n",
    "m5mC_thresh_dict = dict(zip(new_bam_files, m5mC_thresh))\n",
    "\n",
    "# ─────────────────────────────  LOG & RETURN  ─────────────────────────────\n",
    "logging.info(\"m6A thresholds per BAM:\")\n",
    "for bam, thr in m6A_thresh_dict.items():\n",
    "    logging.info(\"  %s → %.2f\", bam, thr)\n",
    "\n",
    "logging.info(\"5mC thresholds per BAM:\")\n",
    "for bam, thr in m5mC_thresh_dict.items():\n",
    "    logging.info(\"  %s → %.2f\", bam, thr)\n",
    "\n",
    "# Now m6A_thresh_dict and m5mC_thresh_dict are ready for downstream use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f39b6ab9a9ada35",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from previous run:\n",
    "# Hardcoded m6A thresholds per BAM file\n",
    "# ───── New cell ─────\n",
    "print(new_bam_files)\n",
    "# Build a list of m6A thresholds in the same order as new_bam_files\n",
    "thresh_list = [m6A_thresh_dict[bam] for bam in new_bam_files]\n",
    "print(thresh_list)\n",
    "print(m6A_thresh_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "2629edda0164bc8b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### START CODE FOR PLOTTING PILEUP\n",
    "\n",
    "regenerate_bit = True # SEt to true to force regenerate, otherwise load if available.\n",
    "\n",
    "### Generate modkit pileup file, used for plotting m6A/A in a given region.\n",
    "# Generating the list of output_file_names based on the given structure\n",
    "out_file_names = [output_stem + \"modkit-pileup-\" + each_condition +\"_\"+ str(round(each_thresh,2))+\"_\"+str(each_index)+\"_\"+str(each_bamfrac)+ \"_\".join([each_type[-5:] for each_type in type_selected[-3:]]) + \"_\".join([each_type[0:5] for each_type in type_selected[-3]]) + str(bed_window)+\".bed\" for each_condition,each_thresh,each_index, each_bamfrac in zip(conditions,thresh_list,sample_indices,bam_fracs)]\n",
    "\n",
    "# Function to run a single command\n",
    "def modkit_pileup_extract(args, index):\n",
    "    each_bam, each_thresh, each_condition, each_index, each_bamfrac, each_type,modkit_path, output_stem, modkit_bed_name = args\n",
    "\n",
    "    # Use the index to get the correct file name from out_file_names\n",
    "    each_output = out_file_names[index]\n",
    "\n",
    "    # Check if the output file exists\n",
    "    if not regenerate_bit:\n",
    "        if os.path.exists(each_output):\n",
    "            print(f\"File already exists: {each_output}\")\n",
    "            # Read in output file and check if empty\n",
    "            modkit_qc = pd.DataFrame()\n",
    "            try:\n",
    "                modkit_qc = pd.read_csv(each_output, sep=\"\\t\", header=None, nrows=10)\n",
    "            except:\n",
    "                if modkit_qc.empty:\n",
    "                    print(f\"File is empty: {each_output}\")\n",
    "                    return\n",
    "            return\n",
    "    print(f\"Starting on: {each_output}\", \"with bam file: \", each_bam,\"and bedfile:\", modkit_bed_name)\n",
    "    command = [\n",
    "        modkit_path,\n",
    "        \"pileup\",\n",
    "        \"--only-tabs\",\n",
    "        #\"--ignore\",\n",
    "        #\"m\",\n",
    "        \"--threads\",\n",
    "        \"10\",\n",
    "        #\"--filter-threshold\",\n",
    "        #f\"A:{1-each_thresh}\",\n",
    "        #f\"A:{1-each_thresh}\",\n",
    "        \"--mod-thresholds\",\n",
    "        f\"a:{each_thresh}\",\n",
    "        \"--mod-thresholds\",\n",
    "        f\"m:{each_thresh}\",\n",
    "        \"--ref\",\n",
    "        \"/Data1/reference/c_elegans.WS235.genomic.fa\",\n",
    "        \"--filter-threshold\",\n",
    "        f\"A:{0.8}\",\n",
    "        \"--filter-threshold\",\n",
    "        f\"C:{0.8}\",\n",
    "        \"--motif\",\n",
    "        \"GC\",\n",
    "        \"1\",\n",
    "        \"--motif\",\n",
    "        \"A\",\n",
    "        \"0\",\n",
    "        # \"--max-depth\",\n",
    "        # \"100\",\n",
    "        \"--log-filepath\",\n",
    "        output_stem + each_condition + str(each_index) + \"_modkit-pileup.log\",\n",
    "        \"--include-bed\",\n",
    "        modkit_bed_name,\n",
    "        each_bam,\n",
    "        each_output\n",
    "    ]\n",
    "    subprocess.run(command, text=True)\n",
    "\n",
    "\n",
    "print(\"modkit_bed_name\",modkit_bed_name)\n",
    "print(\"temp_file_path\",temp_file_path)\n",
    "# Now you need to adjust the task_args to include the index\n",
    "# Instead of directly zipping, enumerate one of the lists to get the index\n",
    "task_args_with_index = [(args, index) for index, args in enumerate(zip(\n",
    "    new_bam_files,\n",
    "    thresh_list,\n",
    "    conditions,\n",
    "    sample_indices,\n",
    "    bam_fracs,\n",
    "    [type_selected]*len(new_bam_files),\n",
    "    [modkit_path]*len(new_bam_files), #modkit_path or temp_file_path\n",
    "    [output_stem]*len(new_bam_files),\n",
    "    [temp_file_path]*len(new_bam_files), # modkit_bed_name or temp_file_path\n",
    "))]\n",
    "\n",
    "# Execute commands in parallel, unpacking the arguments and index within the map call\n",
    "with Pool(\n",
    "    processes=16\n",
    ") as pool:\n",
    "    pool.starmap(modkit_pileup_extract, task_args_with_index)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
                                                                                                                                                                                                                                                                                   