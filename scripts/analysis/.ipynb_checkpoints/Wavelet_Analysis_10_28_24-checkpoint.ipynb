{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "608d7544",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T20:55:37.554677Z",
     "start_time": "2024-08-28T20:55:37.552503Z"
    }
   },
   "outputs": [],
   "source": [
    "# Purpose of this script is to perform an unbiased analysis of Fiber-seq and Chip-seq data\n",
    "\n",
    "__author__ = \"Yuri Malina\"\n",
    "__contact__ = \"ymalina@berkeley.edu\"\n",
    "__copyright__ = \"The Meyer Lab, UC Berkeley\"\n",
    "__credits__ = [\"\"]\n",
    "__date__ = \"6/23/2024\"\n",
    "__deprecated__ = False\n",
    "__status__ = \"In development\"\n",
    "__version__ = \"0.0.1\"\n",
    "\n",
    "### TO DOS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5cd4207",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T20:55:38.392674Z",
     "start_time": "2024-08-28T20:55:37.555389Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.25.2.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import importlib\n",
    "import nanotools\n",
    "importlib.reload(nanotools) # reload nanotools module\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.io as pio\n",
    "import plotly\n",
    "import plotly.express as px # Used for plotting\n",
    "import plotly.graph_objects as go # Used for plotting\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool, cpu_count # used for parallel processing\n",
    "import subprocess\n",
    "import os\n",
    "import pywt # for wavelet transform\n",
    "import matplotlib.pyplot as plt # Use for plotting m6A frac and coverage plot\n",
    "from matplotlib import cm # Use for plotting m6A frac and coverage plot\n",
    "\n",
    "#import tqdm\n",
    "#import pysam\n",
    "#import pyBigWig\n",
    "\n",
    "# set renderer to vscode\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "pio.renderers.default = 'plotly_mimetype+notebook' #'plotly_mimetype+notebook'\n",
    "pd.options.display.max_rows = None\n",
    "pd.set_option('display.max_columns', None)\n",
    "# display count_df with no limits on rows\n",
    "pd.set_option('display.max_rows', None)\n",
    "# left align print\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf833661",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Bed file configurations:\n",
    "sample_source = \"type\" # \"chr_type\" or \"type\" or \"chromosome\"\n",
    "chr_type_selected = [\"X\",\"Autosome\"] # 'X' or \"Autosome\"\n",
    "type_selected = [\"gene_q4\",\"gene_q1\"]\n",
    "max_regions = 1000 # max regions to consider; 0 = full set;\n",
    "chromosome_selected = [\"CHROMOSOME_V\",\"CHROMOSOME_X\"] #\"CHROMOSOME_I\", \"CHROMOSOME_II\", \"CHROMOSOME_III\", \"CHROMOSOME_IV\",\"CHROMOSOME_V\",\n",
    "strand_selected = [\"+\",\"-\"] #+ and/or -\n",
    "select_opp_strand = True #If you want to select both + and - strands for all regions set to True\n",
    "down_sample_autosome = False # If you want to downsample autosome genes to match number of X genes set to True\n",
    "if chr_type_selected == [\"X\"]:\n",
    "    down_sample_autosome = False\n",
    "bed_file = \"/Data1/reference/tss_tes_rex_combined_v20_WS235.bed\"\n",
    "bed_window = 1000   # +/- around bed elements.\n",
    "intergenic_window = 2000 # +/- around intergenic regions\n",
    "num_bins = 1000 #bins for metagene plot\n",
    "mods = \"a\" # {A,CG,A+CG}\n",
    "if sample_source == \"chr_type\":\n",
    "    selection = chr_type_selected\n",
    "if sample_source == \"type\":\n",
    "    selection = type_selected\n",
    "if sample_source == \"chromosome\":\n",
    "    selection = chromosome_selected\n",
    "\n",
    "# Filter input bed_file based on input parameters (e.g. chromosome, type, strand, etc.)\n",
    "# Function saves a new filtered bed file to the same folder as the original bed file\n",
    "# called temp_do_not_use_\"type\".bed\n",
    "importlib.reload(nanotools)\n",
    "new_bed_files=nanotools.filter_bed_file(\n",
    "    bed_file,\n",
    "    sample_source,\n",
    "    selection,\n",
    "    chromosome_selected,\n",
    "    chr_type_selected,\n",
    "    type_selected,\n",
    "    strand_selected,\n",
    "    max_regions,\n",
    "    bed_window,\n",
    "    intergenic_window\n",
    ")\n",
    "\n",
    "modkit_bed_name = \"modkit_temp.bed\"\n",
    "modkit_bed_df = nanotools.generate_modkit_bed(new_bed_files, down_sample_autosome, select_opp_strand,modkit_bed_name)\n",
    "nanotools.display_sample_rows(modkit_bed_df, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2e6ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### BAM Configurations\n",
    "R9_m6A_thresh_percent = 0.8\n",
    "R10_m6A_thresh_percent = 0.8\n",
    "R10_5mC_thresh_percent = 0.8 # Note: 0.7 in R9 ~ 0.9 in R10\n",
    "R9_m6A_thresh = int(round(R9_m6A_thresh_percent*258,0)) #default is 129 = 50%; 181=70%; 194=75%; 207 = 80%; 232 = 90%\n",
    "m6A_thresh = int(round(R10_m6A_thresh_percent*258,0))\n",
    "mC_thresh = int(round(R10_5mC_thresh_percent*258,0))\n",
    "print(\"R9_m6A_thresh: \", R9_m6A_thresh)\n",
    "print(\"m6A_thresh: \", m6A_thresh)\n",
    "print(\"mC_thresh: \", mC_thresh)\n",
    "\n",
    "# modkit is used for aggregating methylation data from .bam files\n",
    "# https://nanoporetech.github.io/modkit/quick_start.html\n",
    "modkit_path = \"/Data1/software/modkit_v0.3/modkit\"\n",
    "bedgraphtobigwig_path = \"/Data1/software/ucsc_genome_browser/bedGraphToBigWig\"\n",
    "danpos_path = \"/Data1/software/DANPOS3/danpos.py\"\n",
    "chrom_sizes = \"/Data1/reference/chrom.sizes.ce11.txt\"\n",
    "\n",
    "analysis_cond = [\"N2_old_SMACseq_R10\",\"N2_old_fiber_R10\",\"N2_young_SMACseq_R10\",\"96_old_DPY27degron_SMACseq_R10\",\"SDC2_degron_mixed_fiber_R10\"]\n",
    "    #,\"N2_mixed_fiber_R10\",\"N2_mixed_fiber_R9\",\"SDC2_degron_mixed_fiber_R9\",\"N2_young_fiber_R9\",\"N2_mixed_endogenous_R9\",\"N2_old_fiber_R10\",\"51_old_dpy21null_fiber_R10\",\"52_old_dpy21jmjc_fiber_R10\",\"SDC2_degron_mixed_fiber_R10\"]\n",
    "\n",
    "### IMPORT BAM FILES AND METADATA FROM CSV FILE\n",
    "\n",
    "input_metadata = pd.read_csv(\"/Data1/git/meyer-nanopore/scripts/bam_input_metadata_3_25_2024.txt\", sep=\"\\t\", header=0)\n",
    "# Set bam_files equal to list of items in column bam_files where conditions == N2_fiber\n",
    "bam_files = input_metadata[input_metadata[\"conditions\"].isin(analysis_cond)][\"bam_files\"].tolist()\n",
    "conditions = input_metadata[input_metadata[\"conditions\"].isin(analysis_cond)][\"conditions\"].tolist()\n",
    "exp_ids = input_metadata[input_metadata[\"conditions\"].isin(analysis_cond)][\"exp_id_date\"].tolist()\n",
    "flowcells = input_metadata[input_metadata[\"conditions\"].isin(analysis_cond)][\"flowcell\"].tolist()\n",
    "bam_fracs = len(bam_files)*[1] # For full .bam set to = 1\n",
    "sample_indices = list(range(len(bam_files)))\n",
    "\n",
    "### Import CHIP SEQ / EXTERNAL DATA\n",
    "ext_target = [] # h3_chip, sdc2_chip, sdc3_chip, dhs, gro, mnase, h4k20me1_chip, ama1_chip, \"dpy27_chip\",\"ama1_chip\",\"mnase\",\"gro\"\n",
    "#\"sdc2_chip_albritton\",\"sdc3_chip_albritton\",\"sdc3_chip_anderson\",\"dpy27_chip_anderson\"\n",
    "ext_metadata = pd.read_csv(\"/Data1/git/meyer-nanopore/scripts/bw_input_metadata_2_21_2024.txt\", sep=\"\\t\", header=0)\n",
    "ext_exp_ids = ext_metadata[ext_metadata[\"target\"].isin(ext_target)][\"exp_id_date\"].tolist()\n",
    "ext_files = ext_metadata[ext_metadata[\"target\"].isin(ext_target)][\"bw_files\"].tolist()\n",
    "ext_targets = ext_metadata[ext_metadata[\"target\"].isin(ext_target)][\"target\"].tolist()\n",
    "\n",
    "output_stem = \"/Data1/git/meyer-nanopore/scripts/analysis/result/\"\n",
    "# for dimelo: [181/258,194/258]\n",
    "thresh_list=len(bam_files)*[m6A_thresh/258] # For R10 flow cells use 0.5; for R9 flow cells use 0.9\n",
    "# for position in flowcells == R9 set item with same index in thresh_list to R9_m6A_thresh/258\n",
    "for i in range(len(flowcells)):\n",
    "    if \"R9\" in flowcells[i]:\n",
    "        thresh_list[i] = R9_m6A_thresh/258\n",
    "\n",
    "file_prefix = \"jul_2024\"\n",
    "\n",
    "# Subsample bam based on bam_frac, used to accelerate testing\n",
    "# if bam_frac = 1 will use original bam files, otherwise will save new subsampled bam files to output_stem.\n",
    "args_list = [(bam_file, condition, bam_frac, sample_index, output_stem, exp_id) for bam_file, condition, bam_frac, sample_index, exp_id in zip(bam_files,conditions,bam_fracs,sample_indices, ext_exp_ids)]\n",
    "new_bam_files=[]\n",
    "new_bam_files = nanotools.parallel_subsample_bam(bam_files, conditions, bam_fracs, sample_indices, output_stem)\n",
    "\n",
    "print(\"Program finished!\")\n",
    "print(\"new_bam_files: \", new_bam_files)\n",
    "print(\"exp_ids: \", exp_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c15a3fef5cef8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate modkit sample-probs\n",
    "\n",
    "# for each bam file, calculate the sample-probability distribution using modkit sample-probs command\n",
    "# and save the output to a file in the output_stem directory\n",
    "# if the file already exists, it will not be recalculated\n",
    "# if force_replace is set to True, the file will be recalculated\n",
    "force_replace = False\n",
    "# Define a function to encapsulate the task you want parallelized\n",
    "def sample_probs_bam(arg, force_replace=False):\n",
    "    each_bam, each_condition, each_bamfrac, each_index, each_output_stem, each_exp_id = arg\n",
    "    # Define the output file name as output_stem + each_condition + each_exp_id + \"_sample_probs.txt\"\n",
    "    each_output = each_output_stem + \"/sample_probs/\" + each_condition + \"-\" + each_exp_id + \"_thresholds.tsv\"\n",
    "    # Check if the output file exists\n",
    "    if not force_replace and os.path.exists(each_output):\n",
    "        print(f\"File already exists: {each_output}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Starting on: {each_output}\")\n",
    "    # Define the command to run as modkit_path \"sample-probs\" each_bam -t 10 -o output_stem --prefix each_condition + each_exp_id\n",
    "    command = [\n",
    "        modkit_path,\n",
    "        \"sample-probs\",\n",
    "        each_bam,\n",
    "        \"-t\",\n",
    "        \"10\",\n",
    "        \"-o\",\n",
    "        each_output_stem,\n",
    "        \"--prefix\",\n",
    "        each_condition + \"-\" + each_exp_id,\n",
    "        \"--percentiles\",\n",
    "        \"0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9\",\n",
    "        \"--force\",\n",
    "        \"--hist\"\n",
    "    ]\n",
    "    subprocess.run(command, text=True)\n",
    "\n",
    "# Now you need to adjust the task_args to include the index\n",
    "# Instead of directly zipping, enumerate one of the lists to get the index\n",
    "task_args_with_index = [(args, index) for index, args in enumerate(zip(\n",
    "    new_bam_files,\n",
    "    conditions,\n",
    "    bam_fracs,\n",
    "    sample_indices,\n",
    "    [output_stem]*len(new_bam_files),\n",
    "    exp_ids\n",
    "))]\n",
    "# Execute commands in parallel, unpacking the arguments and index within the map call\n",
    "with Pool(\n",
    "    processes=15\n",
    ") as pool:\n",
    "    pool.starmap(sample_probs_bam, task_args_with_index, force_replace)\n",
    "\n",
    "# Read in all files into a single dataframe for downstream analysis\n",
    "sample_probs_df = pd.DataFrame()\n",
    "for each_condition, each_exp_id in zip(conditions, exp_ids):\n",
    "    each_output = output_stem + \"/sample_probs/\"+ each_condition + \"-\" + each_exp_id + \"_thresholds.tsv\"\n",
    "\n",
    "    # Read the file with whitespace as the delimiter\n",
    "    each_df = pd.read_csv(each_output, delim_whitespace=True, header=0)\n",
    "\n",
    "    # Debugging: Print columns of each dataframe\n",
    "    print(f\"Columns in {each_output}: {each_df.columns.tolist()}\")\n",
    "\n",
    "    each_df['condition'] = each_condition\n",
    "    each_df['exp_id'] = each_exp_id\n",
    "    sample_probs_df = pd.concat([sample_probs_df, each_df], ignore_index=True)\n",
    "# Reset the index\n",
    "sample_probs_df = sample_probs_df.reset_index(drop=True)\n",
    "nanotools.display_sample_rows(sample_probs_df, 10)\n",
    "\n",
    "# Print column names\n",
    "print(sample_probs_df.columns)\n",
    "\n",
    "# convert percentile column to integer using round\n",
    "sample_probs_df['percentile'] = sample_probs_df['percentile'].apply(lambda x: round(x,0))\n",
    "\n",
    "\n",
    "#Plot threshold by percentile for all conditions + exp_ids using plotly where base == \"C\"\n",
    "\n",
    "fig = px.line(sample_probs_df[\n",
    "    sample_probs_df['base'] == 'C'\n",
    "              ], x=\"percentile\", y=\"threshold\", color=\"condition\", line_group=\"exp_id\", hover_name=\"exp_id\", title=\"Threshold by Percentile for all conditions and exp_ids\")\n",
    "fig.show(renderer='plotly_mimetype+notebook')\n",
    "\n",
    "# Plot threshold by percentile for all conditions + exp_ids using plotly where base == \"A\"\n",
    "fig = px.line(sample_probs_df[\n",
    "    sample_probs_df['base'] == 'A'\n",
    "                ], x=\"percentile\", y=\"threshold\", color=\"condition\", line_group=\"exp_id\", hover_name=\"exp_id\", title=\"Threshold by Percentile for all conditions and exp_ids\")\n",
    "fig.show(renderer='plotly_mimetype+notebook')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ecf5f536199c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the folder path\n",
    "FOLDER_PATH = \"/Data1/git/meyer-nanopore/scripts/analysis/result\"\n",
    "import glob\n",
    "import re\n",
    "\n",
    "# Set the default theme to white\n",
    "pio.templates.default = \"plotly_white\"\n",
    "\n",
    "# Specify the folder path\n",
    "FOLDER_PATH = \"/Data1/git/meyer-nanopore/scripts/analysis/result/sample_probs\"\n",
    "\n",
    "def parse_probability_file(file_path):\n",
    "    print(f\"Parsing file: {file_path}\")\n",
    "    with open(file_path, 'r') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    data = []\n",
    "    current_code = None\n",
    "    number_of_samples = None\n",
    "\n",
    "    for section in re.split(r'# code', content)[1:]:  # Split by code sections\n",
    "        lines = section.strip().split('\\n')\n",
    "        current_code = lines[0].strip()\n",
    "        print(f\"Processing code: {current_code}\")\n",
    "\n",
    "        for line in lines:\n",
    "            if \"Number of samples =\" in line:\n",
    "                number_of_samples = int(line.split('=')[1].strip())\n",
    "                print(f\"Number of samples: {number_of_samples}\")\n",
    "            elif '..' in line and '[' in line and ']' in line:\n",
    "                try:\n",
    "                    parts = line.split()\n",
    "                    threshold = float(parts[0])\n",
    "                    # Extract count directly from brackets\n",
    "                    count_match = re.search(r'\\[(.*?)\\]', line)\n",
    "                    if count_match:\n",
    "                        count = int(count_match.group(1).strip())\n",
    "                    else:\n",
    "                        print(f\"Warning: Could not find count in line: {line}\")\n",
    "                        continue\n",
    "\n",
    "                    # Normalize the count\n",
    "                    if number_of_samples:\n",
    "                        normalized_count = count / number_of_samples\n",
    "                    else:\n",
    "                        print(f\"Warning: No number of samples found for code {current_code}\")\n",
    "                        normalized_count = count\n",
    "\n",
    "                    data.append({'code': current_code, 'threshold': threshold, 'count': normalized_count})\n",
    "                    print(f\"Parsed data point: threshold={threshold}, normalized_count={normalized_count}\")\n",
    "                except (IndexError, ValueError) as e:\n",
    "                    print(f\"Warning: Error parsing line: {line}\")\n",
    "                    print(f\"Error details: {str(e)}\")\n",
    "\n",
    "    # Extract condition and exp_ID from filename\n",
    "    filename = os.path.basename(file_path)\n",
    "    parts = filename.split('_probabilities.txt')[0].split('-')\n",
    "    condition = '-'.join(parts[:-1])  # Everything before the last part\n",
    "    exp_id = parts[-1]  # The last part\n",
    "    condition_exp_id = f\"{condition}-{exp_id}\"\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    if not df.empty:\n",
    "        df['condition_exp_id'] = condition_exp_id\n",
    "    else:\n",
    "        print(f\"Warning: No valid data extracted from file {file_path}\")\n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    # Find all files ending with \"_probabilities.txt\" in the specified folder\n",
    "    file_pattern = os.path.join(FOLDER_PATH, \"*_probabilities.txt\")\n",
    "    files = glob.glob(file_pattern)\n",
    "\n",
    "    if not files:\n",
    "        print(f\"No '*_probabilities.txt' files found in {FOLDER_PATH}\")\n",
    "        return\n",
    "\n",
    "    # Combine all data into a single DataFrame\n",
    "    all_data = pd.concat([parse_probability_file(f) for f in files], ignore_index=True)\n",
    "\n",
    "    if all_data.empty:\n",
    "        print(\"No valid data found in the files. Please check the file contents and the console output for detailed warnings.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Total data points collected: {len(all_data)}\")\n",
    "\n",
    "    # Create individual plots\n",
    "    codes = ['a', 'C', 'm', 'A']\n",
    "    for code in codes:\n",
    "        code_data = all_data[all_data['code'] == code]\n",
    "        if code_data.empty:\n",
    "            print(f\"No data found for code {code}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Plotting data for code {code}\")\n",
    "        fig = go.Figure()\n",
    "\n",
    "        for condition in code_data['condition_exp_id'].unique():\n",
    "            condition_data = code_data[code_data['condition_exp_id'] == condition]\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=condition_data['threshold'],\n",
    "                    y=condition_data['count'],\n",
    "                    mode='lines',\n",
    "                    name=f'{condition}',\n",
    "                )\n",
    "            )\n",
    "\n",
    "        fig.update_layout(\n",
    "            title=f\"Probability Analysis - Code {code}\",\n",
    "            xaxis_title=\"Threshold\",\n",
    "            yaxis_title=\"Normalized Count\",\n",
    "            legend_title=\"Condition + Exp ID\",\n",
    "            height=600,\n",
    "            width=1000,\n",
    "        )\n",
    "\n",
    "        # Display the plot\n",
    "        fig.show()\n",
    "\n",
    "    print(\"Analysis complete. Plots displayed in the notebook.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2326adbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate bam summary statistics\n",
    "importlib.reload(nanotools)\n",
    "force_replace = False\n",
    "sampling_frac = 0.1 # fraction of bam to sample for summary statistics\n",
    "\n",
    "summary_bam_df = pd.DataFrame()\n",
    "\n",
    "### Define filename for summary table based on selected conditions\n",
    "# We'll start by defining a function to encapsulate the task you want parallelized\n",
    "def process_bam(args):\n",
    "    each_bam, each_condition, each_thresh, each_exp_id = args\n",
    "    print(\"starting on bam:\", each_bam,\" | condition:\", each_condition,\"| each_exp_id:\",each_exp_id, \"with thresh:\", each_thresh)\n",
    "    return nanotools.get_summary_from_bam(sampling_frac, each_thresh, modkit_path, each_bam, each_condition,each_exp_id, thread_ct = 50)\n",
    "\n",
    "# Define filename for summary table based on selected conditions\n",
    "summary_table_name = \"temp_files/\" + \"_\" + conditions[0] + conditions[-1] + \"_\" + str(sampling_frac) + \"_thresh\" + str(thresh_list[0]) + \"_summary_table.csv\"\n",
    "\n",
    "# Check if summary table exists\n",
    "if not force_replace and os.path.exists(summary_table_name):\n",
    "    print(\"Summary table exists, importing...\")\n",
    "    summary_bam_df = pd.read_csv(summary_table_name, sep=\"\\t\", header=0)\n",
    "else:\n",
    "    print(\"Summary table does not exist, creating...\")\n",
    "    #\n",
    "    # Create a pool of worker processes\n",
    "    pool = multiprocessing.Pool(3)\n",
    "\n",
    "    # Map the function to the arguments\n",
    "    results = pool.map(process_bam, zip(new_bam_files, conditions, thresh_list, exp_ids))\n",
    "\n",
    "    # Close the pool\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    # Append the results to the summary dataframe\n",
    "    summary_bam_df = pd.concat(results, ignore_index=True)\n",
    "    # Reset the index\n",
    "    summary_bam_df = summary_bam_df.reset_index(drop=True)\n",
    "\n",
    "    # Use display as per the instructions\n",
    "    display(summary_bam_df.head(3))\n",
    "\n",
    "    # Save the dataframe to a CSV file\n",
    "    summary_bam_df.to_csv(summary_table_name, sep=\"\\t\", header=True, index=False)\n",
    "\n",
    "### Plot m6A frac and coverage by condition\n",
    "\n",
    "### Create coverage_df file name (similar to summary_table_name)\n",
    "coverage_df_name =  \"temp_files/\"+\"_\"+conditions[0]+conditions[-1] + \"_\"+str(sampling_frac)+\"_thresh\"+str(thresh_list[0])+\"_coverage_df.csv\"\n",
    "# if coverage_df exists, import it otherwise create it\n",
    "if not force_replace and os.path.exists(coverage_df_name):\n",
    "    print(\"Coverage table exists, importing...\")\n",
    "    coverage_df = pd.read_csv(coverage_df_name, sep=\"\\t\", header=0)\n",
    "else:\n",
    "    # Calculate total_m6a and total_A\n",
    "    nanotools.display_sample_rows(summary_bam_df,2)\n",
    "    # Call the function to create and export the coverage DataFrame\n",
    "    total_m6a = summary_bam_df.loc[summary_bam_df['code'] == 'a'].groupby(['exp_id', 'condition'])['pass_count'].sum().reset_index()\n",
    "    total_m6a.rename(columns={'pass_count': 'total_m6a'}, inplace=True)\n",
    "    total_5mc = summary_bam_df.loc[summary_bam_df['code'] == 'm'].groupby(['exp_id', 'condition'])['pass_count'].sum().reset_index()\n",
    "    total_5mc.rename(columns={'pass_count': 'total_5mc'}, inplace=True)\n",
    "\n",
    "    total_A = summary_bam_df.loc[(summary_bam_df['base'] == 'A') & (summary_bam_df['code'] == '-')].groupby(['exp_id', 'condition'])['pass_count'].sum().reset_index()\n",
    "    total_A.rename(columns={'pass_count': 'total_A'}, inplace=True)\n",
    "\n",
    "    total_C = summary_bam_df.loc[(summary_bam_df['base'] == 'C') & (summary_bam_df['code'] == '-')].groupby(['exp_id', 'condition'])['pass_count'].sum().reset_index()\n",
    "    total_C.rename(columns={'pass_count': 'total_C'}, inplace=True)\n",
    "\n",
    "    # Merge total_m6a and total_A DataFrames\n",
    "    coverage_df_A = pd.merge(total_m6a, total_A,on=['exp_id', 'condition'], how='outer').fillna(0)\n",
    "    coverage_df_C = pd.merge(total_5mc, total_C,on=['exp_id', 'condition'], how='outer').fillna(0)\n",
    "    coverage_df = pd.merge(coverage_df_A, coverage_df_C,on=['exp_id', 'condition'], how='outer').fillna(0)\n",
    "\n",
    "    # Calculate coverage (ce genome size = 100,272,763)\n",
    "    coverage_df['coverage'] = ((coverage_df['total_A'] + coverage_df['total_m6a']) * (1/sampling_frac)) / 100000000 * 4 # * 4 since As are 1/4 of genome\n",
    "\n",
    "    coverage_df['total_A_m6a'] = coverage_df['total_A'] + coverage_df['total_m6a']\n",
    "    coverage_df['total_C_5mc'] = coverage_df['total_C'] + coverage_df['total_5mc']\n",
    "\n",
    "    # Calculate m6A_frac\n",
    "    coverage_df['m6A_frac'] = coverage_df['total_m6a'] / (coverage_df['total_A_m6a'])\n",
    "    coverage_df['5mC_frac'] = coverage_df['total_5mc'] / (coverage_df['total_C_5mc'])\n",
    "\n",
    "    # Drop rows where exp_id == AD1-nb_06_13_23\n",
    "    #coverage_df = coverage_df[coverage_df.exp_id != 'AD1-nb_06_13_23']\n",
    "\n",
    "    display(coverage_df)\n",
    "    #Save coverage df\n",
    "    coverage_df.to_csv(coverage_df_name, sep=\"\\t\", header=True, index=False)\n",
    "\n",
    "# Create the bar plot\n",
    "fig = go.Figure()\n",
    "\n",
    "# Function to generate color map for n unique items\n",
    "def generate_color_map(n, cmap):\n",
    "    norm = plt.Normalize(-1, n)\n",
    "    return [cm.colors.to_hex(cmap(norm(i))) for i in range(n)]\n",
    "\n",
    "# Find unique conditions\n",
    "unique_conditions = coverage_df['condition'].unique()\n",
    "\n",
    "# Different color maps for each condition\n",
    "condition_colormaps = {\n",
    "    'N2_fiber': cm.Blues,\n",
    "    'SDC2_degron_fiber': cm.Greens,\n",
    "    'N2_bg': cm.Reds,\n",
    "    'N2_fiber_R10': cm.viridis,\n",
    "    'N2-DPY27_dimelo_pAHia5': cm.plasma,\n",
    "    'N2-DPY27_dimelo_RbNbHia5': cm.viridis,\n",
    "    '50_dpy27dimelo_mcvipi': cm.inferno\n",
    "    # Add more conditions and their corresponding colormaps here\n",
    "}\n",
    "\n",
    "# Generate color families for each condition\n",
    "color_families = {}\n",
    "for condition in unique_conditions:\n",
    "    n_exp_ids = len(coverage_df[coverage_df['condition'] == condition]['exp_id'].unique())\n",
    "    color_families[condition] = generate_color_map(n_exp_ids, condition_colormaps.get(condition, cm.viridis))\n",
    "\n",
    "# Map each exp_id to its color\n",
    "coverage_df['custom_color'] = coverage_df.groupby('condition')['exp_id'].transform(lambda x: x.astype('category').cat.codes)\n",
    "coverage_df['custom_color'] = coverage_df.apply(lambda row: color_families[row['condition']][row['custom_color']], axis=1)\n",
    "coverage_df['exp_id'] = coverage_df['exp_id'].str.strip()\n",
    "\n",
    "display(coverage_df)\n",
    "# Create the bar plot using Plotly Graph Objects for more customization\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add bars for each condition and exp_id\n",
    "for condition, condition_df in coverage_df.groupby('condition'):\n",
    "    for exp_id, exp_df in condition_df.groupby('exp_id'):\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=[condition],\n",
    "                y=[exp_df['coverage'].iloc[0]],  # Assuming only one row per exp_id per condition\n",
    "                name=exp_id,\n",
    "                marker=dict(color=exp_df['custom_color'].iloc[0]),\n",
    "                legendgroup=exp_id\n",
    "            )\n",
    "        )\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Stacked Bar Plot of Coverage by Condition',\n",
    "    xaxis_title='Condition',\n",
    "    yaxis_title='Coverage',\n",
    "    barmode='stack',\n",
    "    legend_title=\"exp_id\"\n",
    ")\n",
    "\n",
    "# auto set height\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    #width=600,\n",
    "    #height=700,\n",
    "    template=\"simple_white\"\n",
    ")\n",
    "\n",
    "# Group data by 'condition'\n",
    "grouped = coverage_df.groupby('condition')\n",
    "\n",
    "\n",
    "# Initialize the figure\n",
    "fig2 = go.Figure()\n",
    "\n",
    "# Add weighted box plot and points for each condition\n",
    "for name, group in grouped:\n",
    "    weighted_points = np.repeat(group['m6A_frac'], np.ceil(group['coverage'].astype(int)))\n",
    "    weighted_avg = np.average(group['m6A_frac'], weights=(group['coverage'].astype(int)*100)+1)# * 10))\n",
    "    # calculate median\n",
    "    weighted_median= np.median(weighted_points)\n",
    "    fig2.add_trace(go.Box(\n",
    "        y=weighted_points,\n",
    "        name=name,\n",
    "        boxmean=True,\n",
    "        boxpoints=\"all\",  # No points on the box plot itself\n",
    "        jitter=0.3,       # Add some jitter for visibility\n",
    "        pointpos=0,     # Position of points relative to box\n",
    "        marker_size=2,\n",
    "        marker_opacity=0.5,\n",
    "        fillcolor='rgba(0,0,0,0)'  # Transparent fill\n",
    "    ))\n",
    "        # Add annotation for weighted average\n",
    "    fig2.add_annotation(\n",
    "        x=name,\n",
    "        y=weighted_median,#weighted_avg,\n",
    "        text=f\"Median: {weighted_median:.2%}\",\n",
    "        arrowhead=1,\n",
    "        ax=0,\n",
    "        ay=-10\n",
    "    )\n",
    "\n",
    "# Customize the layout\n",
    "fig2.update_layout(\n",
    "    title='Distribution of m6A by Condition',\n",
    "    xaxis_title='Condition',\n",
    "    yaxis_title='m6A frac',\n",
    "    template=\"simple_white\",\n",
    "    #width=600,\n",
    "    # set y axis range\n",
    "    yaxis=dict(\n",
    "        range=[0, 0.4]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Set y axis to %\n",
    "fig2.update_yaxes(tickformat='.0%')\n",
    "\n",
    "# Add weighted box plot and points for each condition\n",
    "# Initialize the figure\n",
    "fig3 = go.Figure()\n",
    "\n",
    "for name, group in grouped:\n",
    "    weighted_points = np.repeat(group['5mC_frac'], np.ceil(group['coverage'].astype(int)))\n",
    "    weighted_avg = np.average(group['5mC_frac'], weights=(group['coverage'].astype(int)*100)+1)# * 10))\n",
    "    # calculate median\n",
    "    weighted_median= np.median(weighted_points)\n",
    "    fig3.add_trace(go.Box(\n",
    "        y=weighted_points,\n",
    "        name=name,\n",
    "        boxmean=True,\n",
    "        boxpoints=\"all\",  # No points on the box plot itself\n",
    "        jitter=0.3,       # Add some jitter for visibility\n",
    "        pointpos=0,     # Position of points relative to box\n",
    "        marker_size=2,\n",
    "        marker_opacity=0.5,\n",
    "        fillcolor='rgba(0,0,0,0)'  # Transparent fill\n",
    "    ))\n",
    "        # Add annotation for weighted average\n",
    "    fig3.add_annotation(\n",
    "        x=name,\n",
    "        y=weighted_median,#weighted_avg,\n",
    "        text=f\"Median: {weighted_median:.2%}\",\n",
    "        arrowhead=1,\n",
    "        ax=0,\n",
    "        ay=-10\n",
    "    )\n",
    "\n",
    "# Customize the layout\n",
    "fig3.update_layout(\n",
    "    title='Distribution of 5mC frac by Condition',\n",
    "    xaxis_title='Condition',\n",
    "    yaxis_title='5mC frac',\n",
    "    template=\"simple_white\",\n",
    "    #width=600,\n",
    "    # set y axis range\n",
    "    yaxis=dict(\n",
    "        range=[0, 0.4]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Set y axis to %\n",
    "fig3.update_yaxes(tickformat='.0%')\n",
    "\n",
    "fig3.show(renderer='plotly_mimetype+notebook')\n",
    "fig2.show(renderer='plotly_mimetype+notebook')\n",
    "fig.show(renderer='plotly_mimetype+notebook')\n",
    "\"\"\"\n",
    "fig.write_image(\"images_11_14_23/bulk_m6Afrac_n2_sdc2degron_0p1sample.svg\")\n",
    "fig.write_image(\"images_11_14_23/bulk_m6Afrac_n2_sdc2degron_0p1sample.png\")\n",
    "fig2.write_image(\"images_11_14_23/coverage_n2_sdc2degron_0p1sample.svg\")\n",
    "fig2.write_image(\"images_11_14_23/coverage_n2_sdc2degron_0p1sample.png\")\n",
    "\n",
    "# Function call example\n",
    "### Calculate N50s SKIP, NOT NECESSARY FOR ANY FOLLOWING STEPS\n",
    "n50_fig = nanotools.calculate_and_plot_n50(new_bam_files, conditions, exp_ids)\n",
    "n50_fig.show(renderer='plotly_mimetype+notebook')\n",
    "n50_fig.write_image(\"images_11_14_23/n50_fig_n2_sdc2degron_0p1sample.svg\")\n",
    "n50_fig.write_image(\"images_11_14_23/n50_fig_n2_sdc2degron_0p1sample.png\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e17694c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate bedgraph from bam files\n",
    "\n",
    "regenerate_bit = False # SEt to true to force regenerate, otherwise load if available.\n",
    "num_processors = 15\n",
    "\n",
    "# Generating the list of input bam folder paths from new_bam_files\n",
    "input_bam_paths = [os.path.dirname(bam) for bam in new_bam_files]\n",
    "\n",
    "# Function to run a single command\n",
    "def modkit_pileup_extract(args):\n",
    "    (each_bam, each_thresh, each_condition, each_index, each_bamfrac, each_expid, \n",
    "     each_type, modkit_path, output_stem, modkit_bed_name,num_processors) = args\n",
    "    \n",
    "    # if regenerate_bit is True delete all files ending in .bedgraph in output_stem\n",
    "    if regenerate_bit:\n",
    "        for file in os.listdir(output_stem):\n",
    "            if file.endswith(\".bedgraph\"):\n",
    "                print(\"Deleting file: \", os.path.join(output_stem, file))\n",
    "                os.remove(os.path.join(output_stem, file))\n",
    "                \n",
    "    # Check if the output file exists\n",
    "    if not regenerate_bit:\n",
    "        print(\"Checking if file exists: \", output_stem + \"/\"+each_expid + \"-\" + each_condition + \"_a_A0_m_GC1.bedgraph\")\n",
    "        if os.path.exists(output_stem + \"/\"+each_expid + \"-\" + each_condition + \"_a_A0_m_GC1.bedgraph\"):\n",
    "            print(f\"File already exists: {output_stem}/{each_expid}-{each_condition}_a_A0_m_GC1.bedgraph\")\n",
    "            # Read in output file and check if empty\n",
    "            return\n",
    "        else:\n",
    "            for file in os.listdir(output_stem):\n",
    "                # if file contains {each_expid}-{each_condition} and ends with .bedgraph, delete it\n",
    "                if each_expid in file and each_condition in file and file.endswith(\".bedgraph\"):\n",
    "                    print(\"Deleting file: \", os.path.join(output_stem, file))\n",
    "\n",
    "    \n",
    "    print(f\"Starting on bam file: \", each_bam,\"and bedfile:\", modkit_bed_name)\n",
    "    command = [\n",
    "        modkit_path,\n",
    "        \"pileup\",\n",
    "        #\"--only-tabs\",\n",
    "        #\"--ignore\",\n",
    "        #\"m\",\n",
    "        \"--threads\",\n",
    "        f\"{num_processors}\",\n",
    "        \"--bedgraph\",\n",
    "        #\"--combine-strands\",\n",
    "        #\"--filter-threshold\",\n",
    "        #f\"A:{1-each_thresh}\",\n",
    "        #f\"A:{1-each_thresh}\",\n",
    "        \"--mod-thresholds\",\n",
    "        f\"a:{each_thresh}\",\n",
    "        \"--mod-thresholds\",\n",
    "        f\"m:{each_thresh}\",\n",
    "        \"--ref\",\n",
    "        \"/Data1/reference/c_elegans.WS235.genomic.fa\",\n",
    "        #\"--filter-threshold\",\n",
    "        #f\"A:{1-each_thresh}\",\n",
    "        #\"--filter-threshold\",\n",
    "        #f\"C:{1-each_thresh}\",\n",
    "        \"--motif\",\n",
    "        \"GC\",\n",
    "        \"1\",\n",
    "        #\"--motif\",\n",
    "        #\"CC\",\n",
    "        #\"0\",\n",
    "        \"--motif\",\n",
    "        \"A\",\n",
    "        \"0\",\n",
    "        \"--prefix\",\n",
    "        f\"{each_expid}-{each_condition}\",\n",
    "\n",
    "        #\"--include-bed\",\n",
    "        #modkit_bed_name,\n",
    "        each_bam,\n",
    "        output_stem\n",
    "    ]\n",
    "    subprocess.run(command, text=True)\n",
    "    \n",
    "    # delete any files in output_stem that contain any of the following strings: \"a_CG0\" or \"m_A0\"\n",
    "    for file in os.listdir(output_stem):\n",
    "        if \"a_GC1\" in file or \"m_A0\" in file or \"a_CC0\" in file:\n",
    "            print(\"Deleting file: \", os.path.join(output_stem, file))\n",
    "            os.remove(os.path.join(output_stem, file))\n",
    "\n",
    "    # if m_GC1_positive and m_GC1_negative files not exist, due to missing mods in bam file, create file with empty row\n",
    "    if not os.path.exists(f\"{output_stem}/{each_expid}-{each_condition}_m_GC1_positive.bedgraph\"):\n",
    "        with open(f\"{output_stem}/{each_expid}-{each_condition}_m_GC1_positive.bedgraph\", \"w\") as f:\n",
    "            f.write(\"\\n\")\n",
    "    if not os.path.exists(f\"{output_stem}/{each_expid}-{each_condition}_m_GC1_negative.bedgraph\"):\n",
    "        with open(f\"{output_stem}/{each_expid}-{each_condition}_m_GC1_negative.bedgraph\", \"w\") as f:\n",
    "            f.write(\"\\n\")\n",
    "    \n",
    "    # Merge A0_negative and A0_positive files by concatenating them, and then sorting by chromosome and start position in bash\n",
    "    # and saving the output to a new file, then deleting the old files\n",
    "    def merge_and_sort_bedgraph_files(output_stem, each_expid, each_condition, file_suffixes, num_processors=8):\n",
    "        for suffix_pair in file_suffixes:\n",
    "            negative_suffix, positive_suffix, output_suffix = suffix_pair\n",
    "    \n",
    "            negative_file = f\"{output_stem}/{each_expid}-{each_condition}_{negative_suffix}.bedgraph\"\n",
    "            positive_file = f\"{output_stem}/{each_expid}-{each_condition}_{positive_suffix}.bedgraph\"\n",
    "            merged_file = f\"{output_stem}/{each_expid}-{each_condition}_{output_suffix}.bedgraph\"\n",
    "    \n",
    "            command = f\"cat {negative_file} {positive_file} | sort -k1,1 -k2,2n --parallel={num_processors} > {merged_file}\"\n",
    "            subprocess.run(command, shell=True)\n",
    "            \n",
    "            \n",
    "            # if either suffix contains \"positive\" or \"negative, Delete the old files\n",
    "            if \"positive\" in negative_suffix or \"negative\" in negative_suffix or \"positive\" in positive_suffix or \"negative\" in positive_suffix:\n",
    "                os.remove(negative_file)\n",
    "                os.remove(positive_file)\n",
    "    \n",
    "    file_suffixes = [\n",
    "        (\"a_A0_negative\", \"a_A0_positive\", \"a_A0\"),\n",
    "        (\"m_GC1_negative\", \"m_GC1_positive\", \"m_GC1\"),\n",
    "        (\"a_A0\", \"m_GC1\", \"a_A0_m_GC1\"),\n",
    "    ]\n",
    "    \n",
    "    merge_and_sort_bedgraph_files(output_stem, each_expid, each_condition, file_suffixes, num_processors)\n",
    "\n",
    "    \n",
    "    \n",
    "# Now you need to adjust the task_args to include the index\n",
    "# Prepare the arguments for each task\n",
    "task_args = list(zip(\n",
    "    new_bam_files,\n",
    "    thresh_list,\n",
    "    conditions,\n",
    "    sample_indices,\n",
    "    bam_fracs,\n",
    "    exp_ids,\n",
    "    [type_selected]*len(new_bam_files),\n",
    "    [modkit_path]*len(new_bam_files),\n",
    "    input_bam_paths,\n",
    "    [modkit_bed_name]*len(new_bam_files),\n",
    "    [num_processors] * len(new_bam_files)\n",
    "))\n",
    "\n",
    "# Select task_args where new_bam_files contains \"AG1\"\n",
    "#task_args = [task for task in task_args if \"AG1\" in task[0]]\n",
    "\n",
    "# Print bam paths for debugging\n",
    "print(\"new_bam_files: \", input_bam_paths)\n",
    "\n",
    "# Execute commands in parallel w\n",
    "with Pool(processes=4) as pool:\n",
    "    pool.map(modkit_pileup_extract, task_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7941cb9fd2352d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Convert bam to bedgraph and bw files.\n",
    "importlib.reload(nanotools)\n",
    "import tempfile\n",
    "\n",
    "#initiate list of bw file names\n",
    "bw_files = []\n",
    "\n",
    "def process_bedgraph(args):\n",
    "    each_bam, each_condition, each_expid, smoothing_window, imputation_window, bedgraphtobigwig_path = args\n",
    "    \n",
    "    # Define raw output file names\n",
    "    bedgraph_fn = os.path.join(os.path.dirname(each_bam), f\"{each_expid}-{each_condition}_a_A0.bedgraph\")\n",
    "    raw_bw_fn = os.path.join(os.path.dirname(each_bam), f\"{each_expid}-{each_condition}_a_A0.bw\")\n",
    "    \n",
    "    ### Convert raw file directly to bigwig, without filling or smoothing or normalizing.\n",
    "    if not os.path.exists(raw_bw_fn) and False:\n",
    "        print(\"Converting raw bedgraph directly to raw bigwig...\")\n",
    "\n",
    "        with tempfile.NamedTemporaryFile(mode='w+t', delete=False, suffix='.bedgraph') as temp_file:\n",
    "            temp_filename = temp_file.name\n",
    "            cut_command = f\"cut -f 1-4 {bedgraph_fn}\"\n",
    "\n",
    "            try:\n",
    "                subprocess.run(cut_command, shell=True, check=True, stdout=temp_file)\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                print(f\"An error occurred while cutting the bedgraph file: {e}\")\n",
    "                os.unlink(temp_filename)\n",
    "                raise\n",
    "\n",
    "        try:\n",
    "            bigwig_command = [\n",
    "                bedgraphtobigwig_path,\n",
    "                temp_filename,\n",
    "                chrom_sizes,\n",
    "                raw_bw_fn\n",
    "            ]\n",
    "            subprocess.run(bigwig_command, check=True)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"An error occurred during bedgraph to bigwig conversion: {e}\")\n",
    "            raise\n",
    "        finally:\n",
    "            print(\"Saved filled bigwig file: \", raw_bw_fn)\n",
    "            # add filled bigwig file to list\n",
    "            os.unlink(temp_filename)\n",
    "    else:\n",
    "        print(\"Bigwig file already exists, skipping conversion.\")\n",
    "    \n",
    "    # Imputation and smoothing (currently skipped as per original code)\n",
    "    \n",
    "    if True:\n",
    "        ## Define output file name path\n",
    "        # Files where missing positions are filled with 0\n",
    "        filled_bedgraph_fn = os.path.join(os.path.dirname(each_bam), f\"{each_expid}-{each_condition}_a_A0_raw_filled.bedgraph\")\n",
    "        filled_bw_fn = filled_bedgraph_fn.replace(\".bedgraph\", \".bw\")\n",
    "\n",
    "        nafilled_bedgraph_fn = os.path.join(os.path.dirname(each_bam), f\"{each_expid}-{each_condition}_a_A0_nafilled.bedgraph\")\n",
    "        nafilled_bw_fn = nafilled_bedgraph_fn.replace(\".bedgraph\", \".bw\")\n",
    "        \n",
    "        # File where all positions are imputed and smoothe.\n",
    "        smoothed_bedgraph_fn = os.path.join(os.path.dirname(each_bam), f\"{each_expid}-{each_condition}_a_A0_smoothed-{smoothing_window}-{imputation_window}.bedgraph\")\n",
    "        # Convert smoothed bedgraph to bigwig\n",
    "        smoothed_bigwig_fn = smoothed_bedgraph_fn.replace(\".bedgraph\", \".bw\")\n",
    "        \n",
    "        bedgraph_df = pd.DataFrame()\n",
    "        \n",
    "        if not os.path.exists(filled_bedgraph_fn):\n",
    "            print(\"Starting to fill raw bedgraph...\")\n",
    "            print(\"Loading bedgraph file: \", bedgraph_fn)\n",
    "            if bedgraph_df.empty:\n",
    "                bedgraph_df = nanotools.load_bedgraph_file(bedgraph_fn)\n",
    "                bedgraph_df['score'] = bedgraph_df['score'].fillna(0)\n",
    "\n",
    "            # write filled bedgraph file to disk\n",
    "            bedgraph_df[['chromosome', 'start', 'end', 'score']].to_csv(\n",
    "                filled_bedgraph_fn,\n",
    "                sep=\"\\t\", header=False, index=False)\n",
    "        else:\n",
    "            print(f\"Raw filled bedgraph file already exists, skipping: {filled_bedgraph_fn}\")\n",
    "        \n",
    "        if not os.path.exists(filled_bw_fn):\n",
    "            print(\"Converting filled bedgraph to bigwig...\")\n",
    "            try:\n",
    "                bigwig_command = [\n",
    "                    bedgraphtobigwig_path,\n",
    "                    filled_bedgraph_fn,\n",
    "                    chrom_sizes,\n",
    "                    filled_bw_fn\n",
    "                ]\n",
    "                subprocess.run(bigwig_command, check=True)\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                print(f\"An error occurred during bedgraph to bigwig conversion: {e}\")\n",
    "                raise\n",
    "        else:\n",
    "            print(f\"Raw filled bigwig file already exists, skipping: {filled_bw_fn}\")\n",
    "\n",
    "        if not os.path.exists(nafilled_bedgraph_fn):\n",
    "            print(\"Starting to fill raw bedgraph with NAs...\")\n",
    "            print(\"Loading bedgraph file: \", bedgraph_fn)\n",
    "            if bedgraph_df.empty:\n",
    "                bedgraph_df = nanotools.load_bedgraph_file(bedgraph_fn)\n",
    "                #bedgraph_df['score'] = bedgraph_df['score'].fillna(0)\n",
    "\n",
    "            # write filled bedgraph file to disk\n",
    "            bedgraph_df[['chromosome', 'start', 'end', 'score']].to_csv(\n",
    "                nafilled_bedgraph_fn,\n",
    "                sep=\"\\t\", header=False, index=False)\n",
    "        else:\n",
    "            print(f\"Raw NA filled bedgraph file already exists, skipping: {filled_bedgraph_fn}\")\n",
    "\n",
    "\n",
    "        if not os.path.exists(smoothed_bedgraph_fn): \n",
    "            if bedgraph_df.empty:\n",
    "                print(\"Loading bedgraph file: \", bedgraph_fn)\n",
    "                bedgraph_df = nanotools.load_bedgraph_file(bedgraph_fn)\n",
    "                bedgraph_df['score'] = bedgraph_df['score'].fillna(0)\n",
    "                \n",
    "            print(f\"Imputing and smoothing bedgraph file: {smoothed_bedgraph_fn}\")\n",
    "            bedgraph_df['imputed_score'], bedgraph_df['imputed_coverage'], bedgraph_df['smoothed_score'], bedgraph_df['smoothed_coverage'] = nanotools.parallel_impute_and_smooth(\n",
    "                bedgraph_df,\n",
    "                impute_window=imputation_window,\n",
    "                smooth_window=smoothing_window,\n",
    "                fill_value=0\n",
    "            )\n",
    "            \n",
    "            print(f\"Saving smoothed bedgraph file: {smoothed_bedgraph_fn}\")\n",
    "            bedgraph_df[['chromosome', 'start', 'end', 'smoothed_score']].to_csv(\n",
    "                smoothed_bedgraph_fn,\n",
    "                sep=\"\\t\", header=False, index=False)\n",
    "        else:\n",
    "            print(f\"Imputed and smoothed bedgraph file already exists, skipping: {smoothed_bedgraph_fn}\")\n",
    "        \n",
    "        if not os.path.exists(smoothed_bigwig_fn):\n",
    "            command = [\n",
    "                bedgraphtobigwig_path,\n",
    "                smoothed_bedgraph_fn,\n",
    "                chrom_sizes,\n",
    "                smoothed_bigwig_fn\n",
    "            ]\n",
    "            #print(\"Command:\")\n",
    "            #print(' '.join(command))\n",
    "            print(f\"Converting smoothed bedgraph to bigwig: {smoothed_bigwig_fn}\")\n",
    "            subprocess.run(command, text=True, capture_output=True)    \n",
    "        else:\n",
    "            print(f\"Imputed and smoothed bigwig file already exists, skipping: {smoothed_bigwig_fn}\")   \n",
    "    else:\n",
    "        print(\"Skipping everything for now.\")\n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    importlib.reload(nanotools)\n",
    "    \n",
    "    # Configurable parameters\n",
    "    smoothing_window = 20\n",
    "    imputation_window = 0\n",
    "    \n",
    "    # Prepare arguments for multiprocessing\n",
    "    args_list = [\n",
    "        (each_bam, each_condition, each_expid, smoothing_window, imputation_window, bedgraphtobigwig_path)\n",
    "        for each_bam, each_condition, each_expid in zip(new_bam_files, conditions, exp_ids)\n",
    "    ]\n",
    "    \n",
    "    # Use multiprocessing to process bedgraph files in parallel\n",
    "    with multiprocessing.Pool(processes=10) as pool:\n",
    "        pool.map(process_bedgraph, args_list)\n",
    "\n",
    "    print(\"All processing completed.\")\n",
    "    \n",
    "    for each_bam, each_condition, each_expid in zip(new_bam_files, conditions, exp_ids):\n",
    "        filled_bw_fn = os.path.join(os.path.dirname(each_bam), f\"{each_expid}-{each_condition}_a_A0_filled.bw\")\n",
    "        #append to bw_files list\n",
    "        bw_files.append(filled_bw_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6faf8aeb2585bf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot size of all files\n",
    "def get_file_info(bam_files, conditions, exp_ids):\n",
    "    file_info = []\n",
    "    for bam_file, condition, exp_id in zip(bam_files, conditions, exp_ids):\n",
    "        output_dir = os.path.dirname(bam_file)\n",
    "\n",
    "        # Define the specific file patterns\n",
    "        patterns = [\n",
    "            f\"{exp_id}-{condition}_a_A0.bedgraph\",\n",
    "            f\"{exp_id}-{condition}_a_A0_raw.bw\",\n",
    "            f\"{exp_id}-{condition}_a_A0_raw_filled.bedgraph\",\n",
    "            f\"{exp_id}-{condition}_a_A0_raw_filled.bw\",\n",
    "            f\"{exp_id}-{condition}_a_A0_smoothed-{smoothing_window}-{imputation_window}.bedgraph\",\n",
    "            f\"{exp_id}-{condition}_a_A0_smoothed-{smoothing_window}-{imputation_window}.bw\"\n",
    "        ]\n",
    "\n",
    "        for pattern in patterns:\n",
    "            file_path = os.path.join(output_dir, pattern)\n",
    "            if os.path.exists(file_path):\n",
    "                file_size = os.path.getsize(file_path) # in bytes\n",
    "                # conert to Gb\n",
    "                file_size = file_size / (1024**3)\n",
    "\n",
    "                if file_path.endswith('.bedgraph'):\n",
    "                    file_type = 'Bedgraph'\n",
    "                elif file_path.endswith('.bw'):\n",
    "                    file_type = 'Bigwig'\n",
    "\n",
    "                if 'raw' in file_path and not 'filled' in file_path:\n",
    "                    processing = 'Raw'\n",
    "                elif 'filled' in file_path:\n",
    "                    processing = 'Filled'\n",
    "                elif 'smoothed' in file_path:\n",
    "                    processing = 'Smoothed'\n",
    "                else:\n",
    "                    processing = 'Original'\n",
    "\n",
    "                file_info.append({\n",
    "                    'File Name': os.path.basename(file_path),\n",
    "                    'File Path': file_path,\n",
    "                    'File Size (bytes)': file_size,\n",
    "                    'File Type': file_type,\n",
    "                    'Processing': processing,\n",
    "                    'Experiment ID': exp_id,\n",
    "                    'Condition': condition\n",
    "                })\n",
    "\n",
    "    return file_info\n",
    "\n",
    "# Use the variables from your original script\n",
    "new_bam_files = [each_bam for each_bam, each_condition, each_expid in zip(new_bam_files, conditions, exp_ids)]\n",
    "conditions = [each_condition for each_bam, each_condition, each_expid in zip(new_bam_files, conditions, exp_ids)]\n",
    "exp_ids = [each_expid for each_bam, each_condition, each_expid in zip(new_bam_files, conditions, exp_ids)]\n",
    "\n",
    "# Define smoothing_window and imputation_window as in your original script\n",
    "smoothing_window = 20\n",
    "imputation_window = 0\n",
    "\n",
    "# Get the file information\n",
    "file_info = get_file_info(new_bam_files, conditions, exp_ids)\n",
    "\n",
    "if file_info:\n",
    "    df_file_info = pd.DataFrame(file_info)\n",
    "\n",
    "    # Sort the DataFrame by file size (largest to smallest)\n",
    "    df_file_info = df_file_info.sort_values('File Size (bytes)', ascending=False)\n",
    "\n",
    "    # Display the DataFrame\n",
    "    #display(df_file_info)\n",
    "\n",
    "    # Create plots using Plotly\n",
    "    file_types = df_file_info['File Type'].unique()\n",
    "    fig = make_subplots(rows=len(file_types), cols=1,\n",
    "                        subplot_titles=[f\"{ftype} File Sizes\" for ftype in file_types],\n",
    "                        vertical_spacing=0.1)\n",
    "\n",
    "    for i, file_type in enumerate(file_types, start=1):\n",
    "        df_subset = df_file_info[df_file_info['File Type'] == file_type]\n",
    "\n",
    "        trace = go.Bar(\n",
    "            x=df_subset['Experiment ID'],\n",
    "            y=df_subset['File Size (bytes)'],\n",
    "            name=file_type,\n",
    "            text=df_subset['Processing'],\n",
    "            hoverinfo='text+y',\n",
    "            hovertext=[f\"Exp ID: {exp}<br>Size: {size:.2f} GB<br>Processing: {proc}\"\n",
    "                       for exp, size, proc in zip(df_subset['Experiment ID'],\n",
    "                                                  df_subset['File Size (bytes)'],\n",
    "                                                  df_subset['Processing'])]\n",
    "        )\n",
    "\n",
    "        fig.add_trace(trace, row=i, col=1)\n",
    "\n",
    "        fig.update_xaxes(title_text=\"Experiment ID\", row=i, col=1)\n",
    "        fig.update_yaxes(title_text=\"File Size (GB)\", row=i, col=1)\n",
    "\n",
    "    fig.update_layout(\n",
    "        height=300 * len(file_types),\n",
    "        title_text=\"File Sizes by Experiment ID and File Type\",\n",
    "        showlegend=False,\n",
    "        template=\"plotly_white\"\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    # Optionally, you can save this DataFrame to a CSV file\n",
    "    # df_file_info.to_csv('file_analysis_results.csv', index=False)\n",
    "else:\n",
    "    print(\"No matching files found in the specified directories.\")\n",
    "\n",
    "# Print total number of files found\n",
    "print(f\"\\nTotal number of files found: {len(file_info)}\")\n",
    "\n",
    "# Print total size of all files\n",
    "if file_info:\n",
    "    total_size = sum(file['File Size (bytes)'] for file in file_info)\n",
    "    print(f\"Total size of all files: {total_size:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7d1228749e7c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create merged bedgraph file for qnormalization\n",
    "import os\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_optimal_dtypes(chunk):\n",
    "    dtypes = {\n",
    "        'chromosome': 'category',\n",
    "        'start': 'uint32',\n",
    "    }\n",
    "    # Determine the best numeric type for the score column\n",
    "    score_col = chunk.columns[-1]\n",
    "    if pd.api.types.is_float_dtype(chunk[score_col]):\n",
    "        if chunk[score_col].apply(lambda x: x.is_integer()).all():\n",
    "            dtypes[score_col] = 'float32'\n",
    "        else:\n",
    "            dtypes[score_col] = 'float32'\n",
    "    elif pd.api.types.is_integer_dtype(chunk[score_col]):\n",
    "        dtypes[score_col] = 'float32'\n",
    "    else:\n",
    "        dtypes[score_col] = 'float32' #chunk[score_col].dtype\n",
    "    return dtypes\n",
    "\n",
    "def process_chunk(args):\n",
    "    chunk, file_name = args\n",
    "    chunk.columns = ['chromosome', 'start', file_name]\n",
    "    dtypes = get_optimal_dtypes(chunk)\n",
    "    return chunk.astype(dtypes)\n",
    "\n",
    "def process_bedgraph_in_chunks(file_path, chunk_size=1000000,rows_to_process=None, suffix = None):\n",
    "    file_name = os.path.basename(file_path).replace(suffix, '')\n",
    "\n",
    "    ### TEMP LIMITTING INPUT ROWS\n",
    "    chunks = pd.read_csv(file_path, sep='\\t', header=None, chunksize=chunk_size, usecols = [0, 1, 3],nrows = rows_to_process)\n",
    "\n",
    "    with Pool(\n",
    "        processes=50\n",
    "    ) as pool:\n",
    "        processed_chunks = list(pool.imap(process_chunk, ((chunk, file_name) for chunk in chunks)))\n",
    "\n",
    "    return pd.concat(processed_chunks)\n",
    "\n",
    "def check_and_concat_dataframes(base_df, new_df):\n",
    "    if base_df is None:\n",
    "        return new_df\n",
    "\n",
    "    if not np.array_equal(base_df[['chromosome', 'start']].values, new_df[['chromosome', 'start']].values):\n",
    "        raise ValueError(\"chromosome, start, and end columns are not identical across all files.\")\n",
    "\n",
    "    base_df[new_df.columns[-1]] = new_df.iloc[:, -1]\n",
    "    return base_df\n",
    "\n",
    "def find_bedgraph_files(directory, suffix=None):\n",
    "    ## DEFAULTS TO RAW UNFILLED FILE\n",
    "    if suffix == None:\n",
    "        print(\"Requires Suffix to find files\")\n",
    "        return\n",
    "    return [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith(suffix)]\n",
    "\n",
    "def main(new_bam_files):\n",
    "    unique_directories = list(set(os.path.dirname(path) for path in new_bam_files))\n",
    "    print(f\"Found {len(unique_directories)} unique directories.\")\n",
    "    suffix = \"_nafilled.bedgraph\"\n",
    "    all_bedgraph_files = []\n",
    "    for directory in unique_directories:\n",
    "        all_bedgraph_files.extend(find_bedgraph_files(directory, suffix = suffix))\n",
    "\n",
    "\n",
    "    # keep only bedgraph files that have the required substrings (BM_, BK_, BN_, AG2_merged)\n",
    "    all_bedgraph_files = [file for file in all_bedgraph_files if \"BM_\" in file or \"BK_\" in file or \"BN_\" in file or \"AG-22\" in file or \"AH-\" in file]\n",
    "\n",
    "    print(f\"Found {len(all_bedgraph_files)} bedgraph files.\")\n",
    "    for file in all_bedgraph_files:\n",
    "        print(os.path.basename(file))\n",
    "\n",
    "    final_df = None\n",
    "    temp_folder = \"/Data1/git/meyer-nanopore/scripts/analysis/temp_files/\"+ file_prefix + \"/\"\n",
    "    input_file = temp_folder + \"/merged_bedgraph_test.csv\"\n",
    "    # if input_file does not exist, create it\n",
    "    if not os.path.exists(input_file):\n",
    "        for file in tqdm(all_bedgraph_files, desc=\"Processing files\"):\n",
    "            try:\n",
    "                df = process_bedgraph_in_chunks(file,rows_to_process=None, suffix = suffix)\n",
    "                final_df = check_and_concat_dataframes(final_df, df)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file}: {str(e)}\")\n",
    "                continue  # Skip to the next file if there's an error\n",
    "\n",
    "        if final_df is not None:\n",
    "            print(\"Saving merged dataframe...\")\n",
    "            final_df.to_csv('/Data1/git/meyer-nanopore/scripts/analysis/temp_files/merged_bedgraph_test.csv', index=False)\n",
    "\n",
    "            print(f\"Processed {len(all_bedgraph_files)} files from {len(unique_directories)} directories. Merged dataframe saved as 'merged_bedgraph_test.csv'\")\n",
    "        else:\n",
    "            print(\"No data was processed successfully. Please check the errors above.\")\n",
    "    else:\n",
    "        print(\"Merged dataframe already exists, skipping.\")\n",
    "\n",
    "print(new_bam_files)\n",
    "if __name__ == '__main__':\n",
    "    main(\n",
    "        # select all elements in new_bam_files that contain \"BM_\" \"BK_\" or \"BN_\"\n",
    "        [bam for bam in new_bam_files if \"BM_\" in bam or \"BK_\" in bam or \"BN_\" in bam or \"AG1_\" in bam or \"AH_\" in bam]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35338787d1650bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Qnromalization\n",
    "import stat\n",
    "\n",
    "# https://pypi.org/project/qnorm/\n",
    "from qnorm import quantile_normalize\n",
    "# reimport nanotools\n",
    "importlib.reload(nanotools)\n",
    "import pyarrow.csv as pv\n",
    "\n",
    "def import_normalize_smooth_and_convert(bedgraphtobigwig_path, chrom_sizes, imputation_window, smoothing_window):\n",
    "    temp_folder = \"/Data1/git/meyer-nanopore/scripts/analysis/temp_files/\"+ file_prefix + \"/\"\n",
    "    input_file = temp_folder + \"/merged_bedgraph_test.csv\"\n",
    "\n",
    "    # Check if the input file exists\n",
    "    if not os.path.exists(input_file):\n",
    "        raise FileNotFoundError(f\"Input file not found: {input_file}\")\n",
    "\n",
    "    # Read the CSV file\n",
    "    print(\"Importing data...\")\n",
    "    # Read the CSV file using pyarrow with parallel processing\n",
    "    read_options = pv.ReadOptions(use_threads=True)\n",
    "    table = pv.read_csv(input_file, read_options=read_options)\n",
    "\n",
    "    # Convert the pyarrow Table to a pandas DataFrame\n",
    "    df = table.to_pandas()\n",
    "\n",
    "    #df = pd.read_csv(input_file,sep=',', header=0)\n",
    "    print(f\"Imported data shape: {df.shape}\")\n",
    "\n",
    "    # Check the structure of the dataframe\n",
    "    if 'chromosome' not in df.columns or 'start' not in df.columns:\n",
    "        raise ValueError(\"Input file must have 'chromosome' and 'start' columns\")\n",
    "\n",
    "    if df.shape[1] < 3:\n",
    "        raise ValueError(\"Input file must have at least one data column besides 'chromosome' and 'start'\")\n",
    "\n",
    "    # Identify columns to normalize (all except 'chrom' and 'start')\n",
    "    columns_to_normalize = df.columns[2:]\n",
    "\n",
    "    # Convert 'na' to NaN and columns to float\n",
    "    df[columns_to_normalize] = df[columns_to_normalize].replace('na', np.nan).astype(float)\n",
    "\n",
    "    # Prepare data for normalization\n",
    "    data_to_normalize = df[columns_to_normalize].values\n",
    "\n",
    "    # Apply qnorm with parallel processing\n",
    "    print(\"Applying quantile normalization using 10 CPU cores...\")\n",
    "    normalized_data = quantile_normalize(data_to_normalize, ncpus=10)\n",
    "\n",
    "    # Replace original data with normalized data\n",
    "    df[columns_to_normalize] = normalized_data\n",
    "\n",
    "    print(\"Normalization complete.\")\n",
    "    print(f\"Final data shape: {df.shape}\")\n",
    "\n",
    "    # Save the normalized dataframe\n",
    "    if (False):\n",
    "        qnorm_output_file = os.path.join(temp_folder, \"qnormalized_bedgraph.tsv\")\n",
    "        df.to_csv(qnorm_output_file, sep='\\t', index=False)\n",
    "        print(f\"Normalized data saved to: {qnorm_output_file}\")\n",
    "\n",
    "    # Process each column\n",
    "    for column in columns_to_normalize:\n",
    "        # Create a new dataframe for this column\n",
    "        bedgraph_df = df[['chromosome', 'start']].copy()\n",
    "        bedgraph_df['end'] = bedgraph_df['start'] + 1  # Assuming 1-base positions\n",
    "        bedgraph_df['score'] = df[column]\n",
    "        bedgraph_df['coverage'] = 1  # Placeholder for 'coverage' column\n",
    "\n",
    "        # Generate output filenames\n",
    "        original_filename = f\"{column}_filled.bedgraph\"\n",
    "        qnorm_bedgraph_filename = original_filename.replace(\"filled.bedgraph\", \"qnorm.bedgraph\")\n",
    "        qnorm_bigwig_filename = qnorm_bedgraph_filename.replace(\".bedgraph\", \".bw\")\n",
    "        smoothed_bedgraph_filename = original_filename.replace(\"filled.bedgraph\", \"qnorm_smoothed.bedgraph\")\n",
    "        smoothed_bigwig_filename = smoothed_bedgraph_filename.replace(\".bedgraph\", \".bw\")\n",
    "        \n",
    "        qnorm_bedgraph_path = os.path.join(os.path.dirname(input_file), qnorm_bedgraph_filename)\n",
    "        qnorm_bigwig_path = os.path.join(os.path.dirname(input_file), qnorm_bigwig_filename)\n",
    "        smoothed_bedgraph_path = os.path.join(os.path.dirname(input_file), smoothed_bedgraph_filename)\n",
    "        smoothed_bigwig_path = os.path.join(os.path.dirname(input_file), smoothed_bigwig_filename)\n",
    "\n",
    "        # Save the qnorm bedgraph file from the first 4 columns of bedgraph_df\n",
    "        if(False):\n",
    "            bedgraph_df[['chromosome', 'start', 'end', 'score']].to_csv(qnorm_bedgraph_path, sep='\\t', index=False, header=False, na_rep='na')\n",
    "            print(f\"Saved qnorm bedgraph: {qnorm_bedgraph_path}\")\n",
    "\n",
    "            # Convert qnorm bedgraph to bigwig\n",
    "            if not os.path.exists(qnorm_bigwig_path):\n",
    "                print(f\"Converting {qnorm_bedgraph_filename} to bigwig...\")\n",
    "                try:\n",
    "                    bigwig_command = [bedgraphtobigwig_path, qnorm_bedgraph_path, chrom_sizes, qnorm_bigwig_path]\n",
    "                    subprocess.run(bigwig_command, check=True)\n",
    "                    print(f\"Qnorm bigwig file created: {qnorm_bigwig_path}\")\n",
    "                except subprocess.CalledProcessError as e:\n",
    "                    print(f\"An error occurred during bedgraph to bigwig conversion: {e}\")\n",
    "                    raise\n",
    "            else:\n",
    "                print(f\"Qnorm bigwig file already exists, skipping: {qnorm_bigwig_path}\")\n",
    "\n",
    "        # Apply smoothing\n",
    "        if not os.path.exists(smoothed_bedgraph_path):\n",
    "            print(f\"Imputing and smoothing bedgraph file: {smoothed_bedgraph_path}\")\n",
    "            bedgraph_df['score'] = bedgraph_df['score'].fillna(0)\n",
    "            bedgraph_df['imputed_score'], bedgraph_df['imputed_coverage'], bedgraph_df['smoothed_score'], bedgraph_df['smoothed_coverage'] = nanotools.parallel_impute_and_smooth(\n",
    "                bedgraph_df,\n",
    "                impute_window=imputation_window,\n",
    "                smooth_window=smoothing_window,\n",
    "                fill_value=0\n",
    "            )\n",
    "            \n",
    "            print(f\"Saving smoothed bedgraph file: {smoothed_bedgraph_path}\")\n",
    "            bedgraph_df[['chromosome', 'start', 'end', 'smoothed_score']].to_csv(\n",
    "                smoothed_bedgraph_path,\n",
    "                sep=\"\\t\", header=False, index=False)\n",
    "        else:\n",
    "            print(f\"Imputed and smoothed bedgraph file already exists, skipping: {smoothed_bedgraph_path}\")\n",
    "\n",
    "        # Convert smoothed bedgraph to bigwig\n",
    "        if not os.path.exists(smoothed_bigwig_path):\n",
    "            print(f\"Converting smoothed bedgraph to bigwig: {smoothed_bigwig_path}\")\n",
    "            command = [bedgraphtobigwig_path, smoothed_bedgraph_path, chrom_sizes, smoothed_bigwig_path]\n",
    "            subprocess.run(command, text=True, capture_output=True)\n",
    "        else:\n",
    "            print(f\"Imputed and smoothed bigwig file already exists, skipping: {smoothed_bigwig_path}\")\n",
    "\n",
    "    print(\"All files have been processed, normalized, smoothed, and converted.\")\n",
    "    \n",
    "imputation_window = 0  # Set this to your desired value\n",
    "smoothing_window = 50  # Set this to your desired value\n",
    "import_normalize_smooth_and_convert(bedgraphtobigwig_path, chrom_sizes, imputation_window, smoothing_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0036a91c2fe8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "### BINNING AND PREP FOR WAVELET ANALYSIS\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.csv as csv\n",
    "import pyarrow.compute as pc\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def import_bedgraph_files(file_paths, num_lines, binsize=0, chromosomes=None,temp_folder = None):\n",
    "    dataframes = []\n",
    "    if chromosomes is not None:\n",
    "        chromosomes = [f\"CHROMOSOME_{chrom}\" for chrom in chromosomes]\n",
    "        print(chromosomes)\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        base_name = os.path.basename(file_path)\n",
    "        # shortened name as \"_\".join(base_name.split(\"_\")[:-1]) + chromosome and bin size\n",
    "        shortened_name =\"_\".join(base_name.split(\"_\")[:-1]) + f\"_chr_{chromosomes[0]}_{binsize}\"\n",
    "        # Output fn as temp path, shortened name, chromosome and bin size\n",
    "        output_fn = os.path.join(temp_folder, shortened_name + f\"_chr_{chromosomes[0]}_{binsize}\" + \".csv\")\n",
    "\n",
    "\n",
    "        # if file is already binned, skip\n",
    "        if os.path.exists(output_fn):\n",
    "            # load file\n",
    "            print(\"file already exists, loading file...\")\n",
    "            df = pd.read_csv(output_fn)\n",
    "            df.shortened_name = shortened_name\n",
    "        else:\n",
    "            # Define PyArrow read options\n",
    "            read_options = csv.ReadOptions(use_threads=True)\n",
    "            parse_options = csv.ParseOptions(delimiter='\\t')\n",
    "\n",
    "            print(f\"Reading file: {file_path}\")\n",
    "            # Read the CSV file using PyArrow\n",
    "            table = csv.read_csv(file_path, read_options=read_options, parse_options=parse_options)\n",
    "\n",
    "            # Convert to pandas DataFrame\n",
    "            df = table.to_pandas()\n",
    "            df.columns = ['chromosome', 'start', 'end', 'score']\n",
    "\n",
    "            print(\"Filtering chromosomes...\")\n",
    "            if chromosomes is not None:\n",
    "                df = df[df['chromosome'].isin(chromosomes)]\n",
    "\n",
    "            if num_lines > 0:\n",
    "                df = df.head(num_lines)\n",
    "\n",
    "            print(\"binning...\")\n",
    "            if binsize > 0:\n",
    "                grouped = df.groupby('chromosome')\n",
    "                binned_dfs = []\n",
    "\n",
    "                for chrom, chrom_df in grouped:\n",
    "                    bin_edges = np.arange(int(chrom_df['start'].min()), int(chrom_df['end'].max()) + int(binsize), int(binsize))\n",
    "                    chrom_df['bin'] = pd.cut(chrom_df['start'], bins=bin_edges, labels=bin_edges[:-1], include_lowest=True)\n",
    "\n",
    "                    binned = chrom_df.groupby('bin').agg({\n",
    "                        'chromosome': 'first',\n",
    "                        'start': 'min',\n",
    "                        'end': 'max',\n",
    "                        'score': 'mean'\n",
    "                    }).reset_index(drop=True)\n",
    "\n",
    "                    binned_dfs.append(binned)\n",
    "\n",
    "                df = pd.concat(binned_dfs, ignore_index=True)\n",
    "\n",
    "            df.to_csv(output_fn, index=False)\n",
    "            df.shortened_name = shortened_name\n",
    "\n",
    "        dataframes.append(df)\n",
    "    return dataframes\n",
    "\n",
    "# Usage remains the same\n",
    "# file_paths = [\n",
    "#     \"/Data1/git/meyer-nanopore/scripts/analysis/temp_files/jun_2024/BK_05_30_24-N2_young_SMACseq_R10_a_A0_qnorm_smoothed.bedgraph\",\n",
    "#     \"/Data1/git/meyer-nanopore/scripts/analysis/temp_files/jun_2024/BM_05_30_24-N2_old_SMACseq_R10_a_A0_qnorm_smoothed.bedgraph\",\n",
    "#     \"/Data1/git/meyer-nanopore/scripts/analysis/temp_files/jun_2024/BN_05_24_24-96_old_DPY27degron_SMACseq_R10_a_A0_qnorm_smoothed.bedgraph\"\n",
    "# ]\n",
    "\n",
    "temp_folder = \"/Data1/git/meyer-nanopore/scripts/analysis/temp_files/\"+ file_prefix + \"/\"\n",
    "# Find all files ending in .bedgraph in temp_folder\n",
    "extracted_elements = [file for file in os.listdir(temp_folder) if file.endswith(\"smoothed.bedgraph\")]\n",
    "# Create the file paths\n",
    "file_paths = [os.path.join(temp_folder, f\"{element}\") for element in extracted_elements]\n",
    "\n",
    "num_lines = 0  # Change this to the number of lines you want to read from each file\n",
    "binsize = 100\n",
    "bg_dfs = import_bedgraph_files(file_paths, num_lines, binsize=binsize, chromosomes=[\"X\"], temp_folder = temp_folder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516aac3a388b4104",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wavelet(time, signal,\n",
    "                 waveletname='cmor',\n",
    "                 min_period=None,\n",
    "                 max_period=None,\n",
    "                 num_scales=100,\n",
    "                 cmap=plt.cm.magma,\n",
    "                 title='Wavelet Transform (Power Spectrum) of signal',\n",
    "                 ylabel='Period (bp)',\n",
    "                 xlabel='Position (bp)',\n",
    "                 normalize=True):\n",
    "\n",
    "    dt = time[1] - time[0]\n",
    "\n",
    "    if normalize:\n",
    "        signal = (signal - np.mean(signal)) / np.std(signal)\n",
    "\n",
    "    # Convert periods to scales\n",
    "    # Convert periods to scales\n",
    "    def period_to_scale(period, waveletname, dt):\n",
    "        if waveletname == 'cgau6':\n",
    "            fc = 0.6678700143418761  # Center frequency for cgau6\n",
    "            return (period * fc) / dt\n",
    "        else:\n",
    "            return period / (4 * dt)  # General approximation for other wavelets\n",
    "\n",
    "    if min_period is None:\n",
    "        min_period = 2 * dt  # Nyquist limit\n",
    "    if max_period is None:\n",
    "        max_period = len(time) * dt / 2  # Half the signal length\n",
    "\n",
    "    min_scale = period_to_scale(min_period, waveletname, dt)\n",
    "    max_scale = period_to_scale(max_period, waveletname, dt)\n",
    "\n",
    "    # Generate logarithmically spaced scales\n",
    "    scales = np.geomspace(min_scale, max_scale, num=num_scales)\n",
    "\n",
    "    [coefficients, frequencies] = pywt.cwt(signal, scales, waveletname, dt)\n",
    "    power = (abs(coefficients)) ** 2\n",
    "    period = 1. / frequencies\n",
    "    \n",
    "    # Compute the range of log2(power) values\n",
    "    log2_power = np.log2(power)\n",
    "    vmin, vmax = np.nanpercentile(log2_power, [10, 95])\n",
    "    vmin = -4\n",
    "    vmax = 10\n",
    "    # Create levels that span the entire range of the data\n",
    "    #num_levels = 9  # You can adjust this for more or fewer contour levels\n",
    "    #contourlevels = np.linspace(vmin, vmax, num_levels)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 4))\n",
    "\n",
    "    #im = ax.contourf(time, np.log2(period), np.log2(power), contourlevels, extend='both', cmap=cmap)\n",
    "    im = ax.pcolormesh(time, np.log2(period), np.log2(power), cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "    \n",
    "    ax.set_title(title, fontsize=20)\n",
    "    ax.set_ylabel(ylabel, fontsize=18)\n",
    "    ax.set_xlabel(xlabel, fontsize=18)\n",
    "    # set min and max x-axis limits\n",
    "    ax.set_xlim(9000000, 11000000)\n",
    "\n",
    "    # Set y-axis ticks to show actual period values\n",
    "    yticks = 2**np.arange(np.ceil(np.log2(period.min())), np.ceil(np.log2(period.max())))\n",
    "    ax.set_yticks(np.log2(yticks))\n",
    "    ax.set_yticklabels(yticks)\n",
    "\n",
    "    ax.invert_yaxis()\n",
    "    #ylim = ax.get_ylim()\n",
    "    #ax.set_ylim(ylim[0], -1)  # Adjust upper limit if needed\n",
    "\n",
    "    # Add thin semi-transparent horizontal and vertical gridlines\n",
    "    ax.grid(which='both', color='white', linestyle='-', linewidth=0.5, alpha=0.3)\n",
    "\n",
    "    # Add colorbar\n",
    "    cbar_ax = fig.add_axes([0.95, 0.5, 0.03, 0.25])\n",
    "    cbar = fig.colorbar(im, cax=cbar_ax, orientation=\"vertical\")\n",
    "    cbar.ax.set_ylabel('Power', rotation=270, labelpad=20)\n",
    "\n",
    "    # Set colorbar ticks to show log2 values\n",
    "    tick_locations = np.linspace(vmin, vmax, 3)\n",
    "    # increase size of ticks\n",
    "    cbar.ax.yaxis.set_tick_params(width=2)\n",
    "    cbar.set_ticks(tick_locations)\n",
    "    cbar.set_ticklabels([f'{x:.0f}' for x in tick_locations])\n",
    "\n",
    "    #plt.tight_layout()\n",
    "    return fig, power, period, time\n",
    "\n",
    "# import fft from scipy\n",
    "import numpy as np\n",
    "from scipy.fft import fft\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "def get_fft_values(y_values, T, N, f_s):\n",
    "    f_values = np.linspace(0.0, 1.0/(2.0*T), N//2)\n",
    "    fft_values_ = fft(y_values)\n",
    "    fft_values = 2.0/N * np.abs(fft_values_[0:N//2])\n",
    "    return f_values, fft_values\n",
    "\n",
    "def plot_fft_plus_power(time, signal, min_period=10000, max_period=200000):\n",
    "    dt = time[1] - time[0]\n",
    "    N = len(signal)\n",
    "    fs = 1/dt\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(15, 5))\n",
    "    variance = np.std(signal)**2\n",
    "    f_values, fft_values = get_fft_values(signal, dt, N, fs)\n",
    "    fft_power = variance * abs(fft_values) ** 2     # FFT power spectrum\n",
    "    \n",
    "    # Convert frequency to period (in basepairs)\n",
    "    periods = np.where(f_values != 0, 1 / f_values, np.inf)\n",
    "    \n",
    "    # Filter out periods outside the specified range\n",
    "    mask = (periods >= min_period) & (periods <= max_period)\n",
    "    periods = periods[mask]\n",
    "    fft_power = fft_power[mask]\n",
    "    \n",
    "    # Sort the arrays by period for proper plotting\n",
    "    sort_idx = np.argsort(periods)\n",
    "    periods = periods[sort_idx]\n",
    "    fft_power = fft_power[sort_idx]\n",
    "    \n",
    "    # Plot power spectrum\n",
    "    ax.plot(periods, fft_power, 'k-', linewidth=1, label='FFT Power Spectrum')\n",
    "    \n",
    "    # Calculate and plot rolling average (trendline)\n",
    "    window = 50\n",
    "    trendline = gaussian_filter1d(fft_power, sigma=window/5)\n",
    "    ax.plot(periods, trendline, 'r-', linewidth=2, label='Trendline (Rolling Average)')\n",
    "    \n",
    "    #ax.set_xscale('log')\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_xlabel('Period [basepairs]', fontsize=18)\n",
    "    ax.set_ylabel('Power', fontsize=18)\n",
    "    ax.set_xlim(min_period, max_period)\n",
    "    ax.legend()\n",
    "    plt.title('FFT Power Spectrum', fontsize=20)\n",
    "    plt.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n",
    "    plt.show()\n",
    "\n",
    "# Usage\n",
    "wavelet_results = []\n",
    "\n",
    "for i, df in enumerate(bg_dfs):\n",
    "    print(f\"Dataframe {i}: {df.shortened_name}\")\n",
    "\n",
    "    N = df.shape[0]\n",
    "    t0 = 0\n",
    "    dt = binsize\n",
    "    time = np.arange(0, N) * dt + t0\n",
    "    signal = df[\"score\"].values.squeeze()\n",
    "    \n",
    "    plot_fft_plus_power(time, signal,min_period=10000, max_period=200000)\n",
    "    \n",
    "    plt_obj, log_power, period, time = plot_wavelet(time, signal,cmap=plt.cm.coolwarm, waveletname=\"cgau6\", min_period=100, max_period=10000, num_scales=100, title = df.shortened_name)#cgau6 mexh\n",
    "    wavelet_results.append((plt_obj, log_power, period, time, df.shortened_name, signal))\n",
    "    \n",
    "# Combine all plt_obj into a single figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58114c8ba6492f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all plt_obj into a single figure\n",
    "n_plots = len(wavelet_results)\n",
    "fig, axs = plt.subplots(n_plots, 1, figsize=(15, 4 * n_plots))\n",
    "\n",
    "if n_plots == 1:\n",
    "    axs = [axs]\n",
    "\n",
    "for i, (plt_obj, log_power, period, time, title, signal) in enumerate(wavelet_results):\n",
    "    fig_tmp, ax_tmp = plt_obj.gca(), axs[i]\n",
    "    im = ax_tmp.pcolormesh(time, np.log2(period), np.log2(log_power), cmap=plt.cm.coolwarm, vmin=-2, vmax=8)\n",
    "    # Split subtitle based on \"-\" and \"R10\"\n",
    "    subtitle = title.split('-')[-1]\n",
    "    subtitle = subtitle.split('R10')[0]\n",
    "\n",
    "    ax_tmp.set_title(subtitle, fontsize=20)\n",
    "    ax_tmp.set_ylabel('Period (bp)', fontsize=18)\n",
    "    ax_tmp.set_xlabel('Position (bp)', fontsize=18)\n",
    "    \n",
    "    # Set y-axis ticks to show actual period values\n",
    "    yticks = 2**np.arange(np.ceil(np.log2(period.min())), np.ceil(np.log2(period.max())))\n",
    "    ax_tmp.set_yticks(np.log2(yticks))\n",
    "    ax_tmp.set_yticklabels(yticks)\n",
    "    \n",
    "    ax_tmp.invert_yaxis()\n",
    "    ax_tmp.grid(which='both', color='white', linestyle='-', linewidth=0.5, alpha=0.3)\n",
    "\n",
    "# Add a single colorbar for all subplots\n",
    "fig.subplots_adjust(right=0.9, top=0.85)\n",
    "cbar_ax = fig.add_axes([0.92, 0.7, 0.02, 0.2])  # Adjust size and position\n",
    "cbar = fig.colorbar(im, cax=cbar_ax)\n",
    "cbar.ax.set_ylabel('Power', rotation=270, labelpad=25, fontsize=21)  # Increase fontsize by 50% from 14\n",
    "\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 0.9, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc5b1a08b11faef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_combined_power_plotly_smooth(wavelet_results, period_of_interest=100000, window_size=100):\n",
    "    fig = go.Figure()\n",
    "\n",
    "    for plt_obj, log_power, period, time, condition_name, signal in wavelet_results:\n",
    "\n",
    "        # Find the index of the period closest to 100000\n",
    "        idx = np.argmin(np.abs(period - period_of_interest))\n",
    "\n",
    "        # Extract the power for this period\n",
    "        power_at_period = 2**log_power[idx, :]  # Convert back from log2\n",
    "        #print(\"log_power\",log_power)\n",
    "        #print(\"power_at_period\",power_at_period)\n",
    "\n",
    "        # Create a pandas Series and apply rolling mean\n",
    "        power_series = pd.Series(power_at_period)\n",
    "        power_smooth = power_series.rolling(window=window_size, center=True, min_periods=1).mean()\n",
    "\n",
    "        # Add trace for this condition\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=time,\n",
    "            y=power_smooth,\n",
    "            mode='lines',\n",
    "            name=condition_name,\n",
    "            line=dict(shape='spline', smoothing=1.3)  # This makes the line even smoother\n",
    "        ))\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=f'Smoothed Power at period {period_of_interest} bp across conditions (Rolling mean, window={window_size})',\n",
    "        xaxis_title='Position (bp)',\n",
    "        yaxis_title='Smoothed Power',\n",
    "        legend_title='Conditions',\n",
    "        font=dict(size=14),\n",
    "        hovermode='x unified',\n",
    "        template='plotly_white',  # Use Plotly white template\n",
    "        legend=dict(\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=-1,\n",
    "            xanchor=\"center\",\n",
    "            x=0.5\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Update y axis to span from 2^-4 to 2^8 on logarithmic of 2 scale\n",
    "    fig.update_yaxes(type=\"log\", range=[-4, 30])\n",
    "\n",
    "    # Adjust margins to accommodate legend below the chart\n",
    "    fig.update_layout(margin=dict(l=50, r=50, t=80, b=120))\n",
    "\n",
    "    return fig\n",
    "\n",
    "# After running your existing code to get wavelet_results, use this function:\n",
    "fig = plot_combined_power_plotly_smooth(wavelet_results, period_of_interest=10000, window_size=1000)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569025c900b684b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pycwt import wavelet\n",
    "import pywt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "def get_pycwt_wavelet(waveletname):\n",
    "    \"\"\"\n",
    "    Map PyWavelets names to pyCWT wavelet objects where possible,\n",
    "    or return a pyCWT wavelet object directly.\n",
    "    \"\"\"\n",
    "    pycwt_wavelets = {\n",
    "        'morlet': wavelet.Morlet(),\n",
    "        'paul': wavelet.Paul(),\n",
    "        'dog': wavelet.DOG(),\n",
    "        'mexican_hat': wavelet.DOG(m=2),  # Mexican hat is DOG of order 2\n",
    "        'cgau6': wavelet.DOG(m=6)  # 6th order DOG to approximate cgau6\n",
    "    }\n",
    "\n",
    "    pywt_to_pycwt = {\n",
    "        'morl': 'morlet',\n",
    "        'paul': 'paul',\n",
    "        'mexh': 'mexican_hat',\n",
    "        'cgau6': 'cgau6'\n",
    "    }\n",
    "\n",
    "    if waveletname in pycwt_wavelets:\n",
    "        return pycwt_wavelets[waveletname]\n",
    "    elif waveletname in pywt_to_pycwt:\n",
    "        return pycwt_wavelets[pywt_to_pycwt[waveletname]]\n",
    "    else:\n",
    "        raise ValueError(f\"Wavelet '{waveletname}' is not supported. \"\n",
    "                         f\"Supported wavelets are: {list(pycwt_wavelets.keys()) + list(pywt_to_pycwt.keys())}\")\n",
    "\n",
    "def plot_mother_wavelet(waveletname):\n",
    "    \"\"\"\n",
    "    Plot the mother wavelet.\n",
    "    \"\"\"\n",
    "    mother = get_pycwt_wavelet(waveletname)\n",
    "\n",
    "    # Generate time array\n",
    "    t = np.linspace(-20, 20, 1000)\n",
    "\n",
    "    # Compute wavelet\n",
    "    psi = mother.psi(t)\n",
    "\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    ax.plot(t, psi.real, label='Real part')\n",
    "    if np.iscomplexobj(psi):\n",
    "        ax.plot(t, psi.imag, label='Imaginary part')\n",
    "    ax.set_title(f'Mother Wavelet: {waveletname}')\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Amplitude')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "    return fig\n",
    "\n",
    "def wavelet_coherence(time, signal1, signal2,\n",
    "                      waveletname='morlet',\n",
    "                      min_scale=None,\n",
    "                      max_scale=None,\n",
    "                      num_scales=100,\n",
    "                      cmap=plt.cm.viridis,\n",
    "                      title='Wavelet Coherence',\n",
    "                      ylabel='Period (bp)',\n",
    "                      xlabel='Position (bp)'):\n",
    "\n",
    "    dt = time[1] - time[0]\n",
    "\n",
    "    # Normalize the signals\n",
    "    signal1 = (signal1 - np.mean(signal1)) / np.std(signal1)\n",
    "    signal2 = (signal2 - np.mean(signal2)) / np.std(signal2)\n",
    "\n",
    "    # Automatically determine min_scale and max_scale if not provided\n",
    "    if min_scale is None:\n",
    "        min_scale = 2 * dt  # Nyquist frequency\n",
    "    if max_scale is None:\n",
    "        max_scale = (time[-1] - time[0]) / 2  # Half the time series length\n",
    "\n",
    "    # Generate logarithmically spaced scales\n",
    "    scales = np.logspace(np.log2(min_scale), np.log2(max_scale), num_scales)\n",
    "\n",
    "    # Get the appropriate wavelet\n",
    "    mother = get_pycwt_wavelet(waveletname)\n",
    "\n",
    "    # Perform wavelet coherence analysis\n",
    "    wct, aWCT, coi, freq, sig = wavelet.wct(signal1, signal2, dt,\n",
    "                                            dj=1/12, s0=-1, J=-1,\n",
    "                                            sig=False, sig_test=0,\n",
    "                                            significance_level=0.95,\n",
    "                                            wavelet=mother,\n",
    "                                            normalize=True)\n",
    "\n",
    "    period = 1 / freq\n",
    "\n",
    "    # Create figure\n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "    gs = GridSpec(2, 2, width_ratios=[1, 0.05], height_ratios=[0.1, 1], wspace=0.02, hspace=0.2)\n",
    "\n",
    "    # Coherence plot\n",
    "    ax = fig.add_subplot(gs[1, 0])\n",
    "    im = ax.contourf(time, np.log2(period), wct, cmap=cmap, levels=np.linspace(0, 1, 11))\n",
    "    ax.set_title(title, fontsize=16)\n",
    "    ax.set_ylabel(ylabel, fontsize=14)\n",
    "    ax.set_xlabel(xlabel, fontsize=14)\n",
    "\n",
    "    # Dynamically set y-axis ticks based on actual period range\n",
    "    min_period = period.min()\n",
    "    max_period = period.max()\n",
    "\n",
    "    # Determine appropriate tick locations\n",
    "    log_min_period = np.log2(min_period)\n",
    "    log_max_period = np.log2(max_period)\n",
    "\n",
    "    # Generate tick locations at powers of 2\n",
    "    tick_locations = 2**np.arange(np.floor(log_min_period), np.ceil(log_max_period) + 1)\n",
    "\n",
    "    # Filter tick locations to be within the actual period range\n",
    "    tick_locations = tick_locations[(tick_locations >= min_period) & (tick_locations <= max_period)]\n",
    "\n",
    "    ax.set_yticks(np.log2(tick_locations))\n",
    "    ax.set_yticklabels([f'{x:.0f}' for x in tick_locations])\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    # Add colorbar\n",
    "    cax = fig.add_subplot(gs[1, 1])\n",
    "    cbar = fig.colorbar(im, cax=cax)\n",
    "    cbar.set_label('Coherence', rotation=270, labelpad=15)\n",
    "\n",
    "    # Add thin semi-transparent horizontal and vertical gridlines\n",
    "    ax.grid(which='both', color='white', linestyle='-', linewidth=0.5, alpha=0.3)\n",
    "\n",
    "    # Plot time series\n",
    "    ax_ts = fig.add_subplot(gs[0, 0])\n",
    "    ax_ts.plot(time, signal1, 'b-', label='Signal 1')\n",
    "    ax_ts.plot(time, signal2, 'r-', label='Signal 2')\n",
    "    ax_ts.set_ylabel('Amplitude')\n",
    "    ax_ts.legend(loc='upper right')\n",
    "    ax_ts.set_xlim(time.min(), time.max())\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig, wct, period, time\n",
    "\n",
    "# Example usage:\n",
    "# Plot the mother wavelet (this can be any supported wavelet, including 'cgau6')\n",
    "wavelet_name_plot = \"cgau6\"\n",
    "mother_wavelet_fig = plot_mother_wavelet(wavelet_name_plot)\n",
    "mother_wavelet_fig.savefig(f\"mother_wavelet_{wavelet_name_plot}.png\")\n",
    "plt.close(mother_wavelet_fig)\n",
    "\n",
    "# Perform pairwise coherence analysis (use a fully supported pyCWT wavelet)\n",
    "wavelet_name_coherence = \"morlet\"\n",
    "coherence_results = []\n",
    "\n",
    "for i in range(len(bg_dfs)):\n",
    "    for j in range(i+1, len(bg_dfs)):\n",
    "        _, _, period, time, name1, signal1 = wavelet_results[i]\n",
    "        _, _, _, _, name2, signal2 = wavelet_results[j]\n",
    "\n",
    "        print(f\"Calculating coherence between {name1} and {name2}\")\n",
    "        # Keep only first 5000000 items of signal1 and signal2\n",
    "        nrows = 20000\n",
    "        signal1_sub = signal1[:nrows]\n",
    "        signal2_sub = signal2[:nrows]\n",
    "        period_sub = period\n",
    "        time_sub = time[:nrows]\n",
    "\n",
    "        fig, wct, period_sub, time_sub = wavelet_coherence(\n",
    "            time_sub, signal1_sub, signal2_sub,\n",
    "            waveletname=wavelet_name_coherence,\n",
    "            min_scale=100,\n",
    "            max_scale=5000,\n",
    "            num_scales=100,\n",
    "            title=f'Wavelet Coherence: {name1} vs {name2}'\n",
    "        )\n",
    "\n",
    "        coherence_results.append((fig, wct, period_sub, time_sub, name1, name2))\n",
    "        fig.savefig(f\"coherence_{name1}_vs_{name2}.png\")\n",
    "        plt.close(fig)\n",
    "\n",
    "# You can now analyze the coherence results\n",
    "for fig, wct, period, time, name1, name2 in coherence_results:\n",
    "    # Perform any additional analysis on the coherence results\n",
    "    # For example, you could identify regions of high coherence:\n",
    "    high_coherence = wct > 0.8  # Adjust threshold as needed\n",
    "    print(f\"High coherence regions between {name1} and {name2}:\")\n",
    "    print(f\"Total area of high coherence: {high_coherence.sum() / high_coherence.size:.2%}\")\n",
    "    # show plot\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28cef3333500560",
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN MACS3 Peak Caller, in terminal\n",
    "'''e.g.\n",
    "# To determine cutoffs:\n",
    "macs3 bdgpeakcall --ifile /Data1/git/meyer-nanopore/scripts/analysis/temp_files/BK_05_30_24-N2_young_SMACseq_R10_a_A0_qnorm_smoothed.bedgraph --cutoff-analysis --verbose 2 --o-prefix N2_young\n",
    "# Then to call peaks:\n",
    "macs3 bdgpeakcall --ifile /Data1/git/meyer-nanopore/scripts/analysis/temp_files/BK_05_30_24-N2_young_SMACseq_R10_a_A0_qnorm_smoothed.bedgraph --cutoff 0.085 --verbose 2 --o-prefix N2_young\n",
    "'''\n",
    "\n",
    "### Now MACS3 peaks analysis:\n",
    "from venn import venn\n",
    "\n",
    "\n",
    "def import_narrowpeak_files(directory):\n",
    "    dfs = {}\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.narrowPeak'):\n",
    "            if \"7p5\" in filename:\n",
    "                condition = filename.split('.')[0]\n",
    "                condition = condition.replace(\"_7p5_c0.\", \"\")\n",
    "                file_path = os.path.join(directory, filename)\n",
    "                df = pd.read_csv(file_path, sep='\\t', header=None,\n",
    "                                 names=['chromosome', 'start', 'end', 'name', 'score',\n",
    "                                        'strand', 'signalValue', 'pValue', 'qValue', 'peak'])\n",
    "                print(f\"Imported {condition}: {len(df)} peaks\")\n",
    "                dfs[condition] = df\n",
    "    return dfs\n",
    "\n",
    "def create_venn_diagram(dfs):\n",
    "    peak_sets = {condition: set(zip(df['chromosome'], df['start'], df['end'])) for condition, df in dfs.items()}\n",
    "\n",
    "    # Debug: Print peak set sizes\n",
    "    for condition, peaks in peak_sets.items():\n",
    "        print(f\"{condition} peak set size: {len(peaks)}\")\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    venn(peak_sets)\n",
    "    plt.title(\"Shared and Unique Peaks Across Conditions\")\n",
    "    plt.show()\n",
    "\n",
    "def create_boxplot(dfs):\n",
    "    fig = go.Figure()\n",
    "    for condition, df in dfs.items():\n",
    "        widths = df['end'] - df['start']\n",
    "        fig.add_trace(go.Box(y=widths, name=condition))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=\"Peak Width Distribution\",\n",
    "        yaxis_title=\"Width (bp)\",\n",
    "        width=600,\n",
    "        height=400,\n",
    "        template=\"plotly_white\"\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "def create_histogram(dfs):\n",
    "    peak_counts = {condition: len(df) for condition, df in dfs.items()}\n",
    "\n",
    "    fig = go.Figure(data=[go.Bar(\n",
    "        x=list(peak_counts.keys()),\n",
    "        y=list(peak_counts.values()),\n",
    "    )])\n",
    "    for i, count in enumerate(peak_counts.values()):\n",
    "        fig.add_annotation(\n",
    "            x=list(peak_counts.keys())[i],\n",
    "            y=count,\n",
    "            text=str(count),\n",
    "            showarrow=False,\n",
    "            yshift=10\n",
    "        )\n",
    "    fig.update_traces(marker_color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'])\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=\"Number of Peaks per Condition\",\n",
    "        xaxis_title=\"Condition\",\n",
    "        yaxis_title=\"Peak Count\",\n",
    "        width=600,\n",
    "        height=400,\n",
    "        template=\"plotly_white\",\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    directory = \"/Data1/git/meyer-nanopore/scripts/analysis/temp_files/\"\n",
    "    dfs = import_narrowpeak_files(directory)\n",
    "\n",
    "    create_venn_diagram(dfs)\n",
    "    create_boxplot(dfs)\n",
    "    create_histogram(dfs)\n",
    "\n",
    "    # Export to CSV\n",
    "    comprehensive_df = pd.concat(dfs.values(), keys=dfs.keys())\n",
    "    output_file = \"/Data1/git/meyer-nanopore/scripts/analysis/comprehensive_peaks_7p5.csv\"\n",
    "    #comprehensive_df.to_csv(output_file, index=False)\n",
    "    print(f\"Comprehensive peak data exported to {output_file}\")\n",
    "\n",
    "    # Display the first few rows of the DataFrame\n",
    "    print(comprehensive_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f256ad328634eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_narrowpeak_files(directory, debug=False):\n",
    "    print(\"Importing narrowPeak files...\")\n",
    "    dfs = {}\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.narrowPeak'):\n",
    "            if \"8p5\" in filename:\n",
    "                condition = filename.split('.')[0]\n",
    "                condition = condition.replace(\"_8p5_c0\", \"\")\n",
    "                file_path = os.path.join(directory, filename)\n",
    "                if debug:\n",
    "                    print(f\"Reading file: {file_path}\")\n",
    "                df = pd.read_csv(file_path, sep='\\t', header=None,skiprows=1,\n",
    "                                 names=['Chromosome', 'Start', 'End', 'Name', 'Score',\n",
    "                                        'Strand', 'SignalValue', 'PValue', 'QValue', 'Peak'])\n",
    "                df['Condition'] = condition\n",
    "                dfs[condition] = df\n",
    "                if debug:\n",
    "                    print(f\"Imported {condition}: {df.shape[0]} rows\")\n",
    "    print(f\"Imported {len(dfs)} narrowPeak files.\")\n",
    "    return dfs\n",
    "\n",
    "\n",
    "\n",
    "def concatenate_conditions_on_overlap(dfs, progress=True, debug=False):\n",
    "    pyranges_dict = {}\n",
    "    total_files = len(dfs)\n",
    "    for i, (condition, df) in enumerate(dfs.items(), 1):\n",
    "        df = df[['Chromosome', 'Start', 'End', 'Condition']]\n",
    "\n",
    "        # Convert Start and End to integers\n",
    "        df['Start'] = pd.to_numeric(df['Start'], errors='coerce').fillna(0).astype(int)\n",
    "        df['End'] = pd.to_numeric(df['End'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "        pyr = pr.PyRanges(df)\n",
    "        pyranges_dict[condition] = pyr\n",
    "        if progress:\n",
    "            print(f\"Processed {i}/{total_files} files: {condition}\")\n",
    "        if debug and i <= 5:  # Limit the debug output to the first 5 files\n",
    "            print(f\"{condition} PyRanges:\\n{pyr.df.head()}\")\n",
    "\n",
    "    combined_df_list = []\n",
    "    conditions = list(pyranges_dict.keys())\n",
    "\n",
    "    # Include all original peaks\n",
    "    for condition in conditions:\n",
    "        original_df = pyranges_dict[condition].df.copy()\n",
    "        original_df['CombinedCondition'] = condition\n",
    "        combined_df_list.append(original_df)\n",
    "\n",
    "    # Find pairwise intersections\n",
    "    for i in range(len(conditions)):\n",
    "        for j in range(i+1, len(conditions)):\n",
    "            combined = pyranges_dict[conditions[i]].intersect(pyranges_dict[conditions[j]])\n",
    "            combined_df = combined.df.copy()\n",
    "            combined_df['CombinedCondition'] = '-'.join(sorted([conditions[i], conditions[j]]))\n",
    "            combined_df_list.append(combined_df)\n",
    "            if debug:\n",
    "                print(f\"Intersection of {conditions[i]}, {conditions[j]}:\\n{combined_df.head(5)}\")  # Limit to first 5 rows\n",
    "\n",
    "    # Find three-way intersections\n",
    "    for i in range(len(conditions)):\n",
    "        for j in range(i+1, len(conditions)):\n",
    "            for k in range(j+1, len(conditions)):\n",
    "                combined = pyranges_dict[conditions[i]].intersect(pyranges_dict[conditions[j]])\n",
    "                combined = combined.intersect(pyranges_dict[conditions[k]])\n",
    "                combined_df = combined.df.copy()\n",
    "                combined_df['CombinedCondition'] = '-'.join(sorted([conditions[i], conditions[j], conditions[k]]))\n",
    "                combined_df_list.append(combined_df)\n",
    "                if debug:\n",
    "                    print(f\"Intersection of {conditions[i]}, {conditions[j]}, {conditions[k]}:\\n{combined_df.head(5)}\")  # Limit to first 5 rows\n",
    "\n",
    "    # Concatenate all combined dataframes\n",
    "    if combined_df_list:\n",
    "        final_df = pd.concat(combined_df_list, ignore_index=True)\n",
    "    else:\n",
    "        final_df = pd.DataFrame(columns=['Chromosome', 'Start', 'End', 'Condition', 'CombinedCondition'])\n",
    "\n",
    "    # Count the number of conditions each peak overlaps with\n",
    "    final_df['ConditionCount'] = final_df['CombinedCondition'].apply(lambda x: len(x.split('-')))\n",
    "\n",
    "    # Filter to retain peaks with the highest overlap count\n",
    "    final_df.sort_values(by=['Chromosome', 'Start', 'End', 'ConditionCount'], ascending=[True, True, True, False], inplace=True)\n",
    "    final_df.drop_duplicates(subset=['Chromosome', 'Start', 'End'], keep='first', inplace=True)\n",
    "\n",
    "    # Reformat the DataFrame\n",
    "    final_result = final_df[['Chromosome', 'Start', 'End', 'CombinedCondition']].copy()\n",
    "    final_result['Strand'] = '.'\n",
    "\n",
    "    # Reorder columns to place 'Strand' after 'End'\n",
    "    final_result = final_result[['Chromosome', 'Start', 'End', 'Strand', 'CombinedCondition']]\n",
    "\n",
    "    # Reindex the DataFrame\n",
    "    final_result.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    if debug:\n",
    "        print(\"Final result sample:\\n\", final_result.head(5))  # Limit to first 5 rows\n",
    "\n",
    "    return final_result\n",
    "\n",
    "\n",
    "\n",
    "# Usage\n",
    "directory = \"/Data1/git/meyer-nanopore/scripts/analysis/temp_files\"  # Replace with your directory\n",
    "dfs = import_narrowpeak_files(directory, debug=False)\n",
    "result_df = concatenate_conditions_on_overlap(dfs, progress=False, debug=True)\n",
    "\n",
    "# write to file\n",
    "result_df.to_csv(\"/Data1/git/meyer-nanopore/scripts/analysis/temp_files/combined_peaks_8p5.bed\", sep='\\t', header=False, index=False)\n",
    "\n",
    "# Print only the head of the final result to avoid excessive output\n",
    "print(result_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ad013471f46cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib_venn import venn3\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_venn_diagram(df, condition1, condition2, condition3):\n",
    "    # filter df to only contain chromosome_X\n",
    "    df = df[df['Chromosome'] == 'CHROMOSOME_X']\n",
    "    # Filter the dataframe for each condition\n",
    "    condition1_regions = df[df['CombinedCondition'].str.contains(condition1)]\n",
    "    condition2_regions = df[df['CombinedCondition'].str.contains(condition2)]\n",
    "    condition3_regions = df[df['CombinedCondition'].str.contains(condition3)]\n",
    "\n",
    "    # Get the unique identifiers for each region\n",
    "    condition1_set = set(condition1_regions.apply(lambda row: (row['Chromosome'], row['Start'], row['End']), axis=1))\n",
    "    condition2_set = set(condition2_regions.apply(lambda row: (row['Chromosome'], row['Start'], row['End']), axis=1))\n",
    "    condition3_set = set(condition3_regions.apply(lambda row: (row['Chromosome'], row['Start'], row['End']), axis=1))\n",
    "\n",
    "    # Create the Venn diagram\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    venn = venn3([condition1_set, condition2_set, condition3_set], (condition1, condition2, condition3))\n",
    "\n",
    "    # Display the Venn diagram\n",
    "    plt.title(f\"Venn Diagram of {condition1}, {condition2}, and {condition3}\")\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "# Assuming result_df is your DataFrame containing the regions and CombinedCondition\n",
    "plot_venn_diagram(result_df, 'N2_young', 'N2_old', 'DPY27_degron')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6999262151b80a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# Function to merge overlapping regions with debugging\n",
    "# Function to merge overlapping regions with debugging\n",
    "def merge_overlapping_regions(df, debug=False):\n",
    "    if debug:\n",
    "        print(\"Initial DataFrame:\")\n",
    "        print(df.head())\n",
    "\n",
    "    merged_regions = []\n",
    "\n",
    "    # Ensure correct data types\n",
    "    df['start'] = df['start'].astype(int)\n",
    "    df['end'] = df['end'].astype(int)\n",
    "\n",
    "    # Sort dataframe by chromosome and start\n",
    "    df = df.sort_values(by=['chromosome', 'start'])\n",
    "\n",
    "    if debug:\n",
    "        print(\"\\nSorted DataFrame:\")\n",
    "        print(df.head())\n",
    "\n",
    "    current_region = None\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        if current_region is None:\n",
    "            current_region = row\n",
    "        else:\n",
    "            if debug:\n",
    "                print(\"\\nCurrent Region:\", current_region)\n",
    "                print(\"Next Row:\", row)\n",
    "\n",
    "            if row['start'] <= current_region['end']:  # Overlapping regions\n",
    "                if debug:\n",
    "                    print(\"Overlapping detected.\")\n",
    "                current_types = set(current_region['type'].split('-'))\n",
    "                new_types = set(row['type'].split('-'))\n",
    "                combined_types = sorted(current_types.union(new_types))\n",
    "                current_region['type'] = '-'.join(combined_types)\n",
    "\n",
    "                # Check for \"qy\" in types\n",
    "                contains_qy_current = any(\"qy\" in t.lower() for t in current_types)\n",
    "                contains_qy_new = any(\"qy\" in t.lower() for t in new_types)\n",
    "\n",
    "                if contains_qy_current and contains_qy_new:\n",
    "                    current_region['start'] = min(current_region['start'], row['start'])\n",
    "                    current_region['end'] = max(current_region['end'], row['end'])\n",
    "                elif not contains_qy_current and not contains_qy_new:\n",
    "                    current_region['start'] = min(current_region['start'], row['start'])\n",
    "                    current_region['end'] = max(current_region['end'], row['end'])\n",
    "                else:\n",
    "                    # If merging one with \"qy\" and one without, preserve the non-\"qy\" start/end\n",
    "                    if contains_qy_current and not contains_qy_new:\n",
    "                        current_region['start'] = row['start']\n",
    "                        current_region['end'] = row['end']\n",
    "                    elif not contains_qy_current and contains_qy_new:\n",
    "                        current_region['start'] = current_region['start']\n",
    "                        current_region['end'] = current_region['end']\n",
    "            else:  # No overlap\n",
    "                merged_regions.append(current_region)\n",
    "                current_region = row\n",
    "\n",
    "    # Append the last region\n",
    "    if current_region is not None:\n",
    "        merged_regions.append(current_region)\n",
    "\n",
    "    merged_df = pd.DataFrame(merged_regions)\n",
    "\n",
    "    # Keep the first region start and end values if neither contains \"qy\"\n",
    "    def adjust_start_end(row):\n",
    "        current_types = set(row['type'].split('-'))\n",
    "        contains_qy = any(\"qy\" in t.lower() for t in current_types)\n",
    "        if not contains_qy:\n",
    "            row['start'] = row['start']\n",
    "            row['end'] = row['end']\n",
    "        return row\n",
    "\n",
    "    merged_df = merged_df.apply(adjust_start_end, axis=1)\n",
    "    \n",
    "    if debug:\n",
    "        print(\"\\nMerged DataFrame with updated start and end:\")\n",
    "        print(merged_df.head())\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "# Import csv to collapse\n",
    "all_peaks_df = pd.read_csv(\"/Data1/git/meyer-nanopore/scripts/analysis/temp_files/QY_chip_fiber.csv\", sep=',', header=None,\n",
    "                           index_col=False,\n",
    "                           names=['chromosome', 'start', 'end', 'strand', 'type'])\n",
    "display(all_peaks_df.head())\n",
    "\n",
    "# Merge overlapping regions\n",
    "merged_df = merge_overlapping_regions(all_peaks_df, debug=False)\n",
    "display(merged_df.head())\n",
    "\n",
    "# Write to csv\n",
    "merged_df.to_csv(\"/Data1/git/meyer-nanopore/scripts/analysis/temp_files/QY_fiber_merged.csv\", sep=',', header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d76a327a92cb670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct deeptools multibigwigsummary command on all filled.bw files\n",
    "\n",
    "# Zip up conditions and exp_ids together\n",
    "conditions_exp_ids = [f\"{condition}-{exp_id}\" for condition, exp_id in zip(conditions, exp_ids)]\n",
    "\n",
    "#bw_files is equal to every file ending in \"filled.bw\" in the base folder of each_bam in new_bam_files\n",
    "#bw_files = [os.path.join(os.path.dirname(each_bam), f\"{each_expid}-{each_condition}_a_A0_filled.bigwig\") for each_bam, each_condition, each_expid in zip(new_bam_files, conditions, exp_ids)]\n",
    "\n",
    "#bw_files is equal to every file containing \"smoothed\" and ending in .bw in the base folder of each_bam in new_bam_files\n",
    "#bw_files = [os.path.join(os.path.dirname(each_bam), f\"{each_expid}-{each_condition}_a_A0_smoothed-20-0.bw\") for each_bam, each_condition, each_expid in zip(new_bam_files, conditions, exp_ids)]\n",
    "\n",
    "# Select only conditions_exp_ids and bw_files that contain \"R10\"\n",
    "#conditions_exp_ids = [condition_exp_id for condition_exp_id in conditions_exp_ids if \"SMAC\" in condition_exp_id]\n",
    "#bw_files = [bw_file for bw_file in bw_files if \"SMAC\" in bw_file]\n",
    "\n",
    "# select files inding in qnorm_smoothed.bw in temp folder\n",
    "bw_files = [os.path.join(\"/Data1/git/meyer-nanopore/scripts/analysis/temp_files/\", f) for f in os.listdir(\"/Data1/git/meyer-nanopore/scripts/analysis/temp_files/\") if f.endswith(\"A0_qnorm_smoothed.bw\")]\n",
    "# set conditions_exp_ids\n",
    "conditions_exp_ids = [f.split(\"_R10\")[0] for f in bw_files]\n",
    "# replace \"/Data1/git/meyer-nanopore/scripts/analysis/temp_files/\" with \"\" in conditions_exp_ids\n",
    "conditions_exp_ids = [f.replace(\"/Data1/git/meyer-nanopore/scripts/analysis/temp_files/\", \"\") for f in conditions_exp_ids]\n",
    "\n",
    "print(\"bw_files:\",  len(bw_files))\n",
    "print(bw_files)\n",
    "\n",
    "print(\"Conditions and exp_ids:\", len(conditions_exp_ids))\n",
    "print(conditions_exp_ids)\n",
    "\n",
    "# Select only conditions_exp_ids and bw_files that do not contain \"AH-0\"\n",
    "#conditions_exp_ids = [condition_exp_id for condition_exp_id in conditions_exp_ids if \"AH-0\" not in condition_exp_id]\n",
    "#bw_files = [bw_file for bw_file in bw_files if \"AH-0\" not in bw_file]\n",
    "\n",
    "bin_size = 100\n",
    "command = [\n",
    "    \"multiBigwigSummary\",\n",
    "    \"bins\",\n",
    "    \"--verbose\",\n",
    "    # \"--chromosomesToSkip\",\n",
    "    # \"CHROMOSOME_I\",\n",
    "    # \"CHROMOSOME_II\",\n",
    "    # \"CHROMOSOME_III\",\n",
    "    # \"CHROMOSOME_IV\",\n",
    "    # \"CHROMOSOME_V\",\n",
    "    \"--numberOfProcessors\",\n",
    "    str(50),\n",
    "    \"--binSize\",\n",
    "    str(bin_size),\n",
    "    #\"--BED\",\n",
    "    #\"/Data1/git/meyer-nanopore/scripts/analysis/temp_files/ce11_first_100kb.bed\",\n",
    "    \"--outFileName\",\n",
    "    f\"/Data1/git/meyer-nanopore/scripts/analysis/temp_files/multiBigwigSummary_results-na-allSMAC-{bin_size}.npz\",\n",
    "    \"--bwfiles\",\n",
    "] + bw_files + [\n",
    "    \"--labels\",\n",
    "] + conditions_exp_ids\n",
    "\n",
    "# print command as it will appear to console\n",
    "print(\"Executing command:\")\n",
    "print(' '.join(command))\n",
    "# Execute command:\n",
    "subprocess.run(command, text=True, check=True)\n",
    "\n",
    "# Execute plotting command: plotCorrelation -in multiBigwigSummary_results-1000.npz -c spearman -p heatmap -o multiBigWigSummary_Correlation_R10_1000bp.png\n",
    "command = [\n",
    "    \"plotCorrelation\",\n",
    "    \"-in\",\n",
    "    f\"/Data1/git/meyer-nanopore/scripts/analysis/temp_files/multiBigwigSummary_results-na-allSMAC-{bin_size}.npz\",\n",
    "    \"-c\",\n",
    "    \"pearson\",\n",
    "    \"-p\",\n",
    "    \"scatterplot\",\n",
    "    \"--labels\",\n",
    "] + conditions_exp_ids + [\n",
    "    #\"--plotNumbers\",\n",
    "    \"-o\",\n",
    "    f\"/Data1/git/meyer-nanopore/scripts/analysis/temp_files/multiBigWigSummary_Correlation-na-allSMAC_{bin_size}.png\"\n",
    "]\n",
    "subprocess.run(command, text=True, check=True)\n",
    "incl_PCs = [1,2]\n",
    "# Execute plotting command: plotCorrelation -in multiBigwigSummary_results-1000.npz -c spearman -p heatmap -o multiBigWigSummary_Correlation_R10_1000bp.png\n",
    "command = [\n",
    "    \"plotPCA\",\n",
    "    \"-in\",\n",
    "    f\"/Data1/git/meyer-nanopore/scripts/analysis/temp_files/multiBigwigSummary_results-na-allSMAC-{bin_size}.npz\",\n",
    "    \"--labels\",\n",
    "] + conditions_exp_ids + [\n",
    "    \"--PCs\",\n",
    "    \"2\",\n",
    "    \"3\",\n",
    "    \"-o\",\n",
    "    f\"/Data1/git/meyer-nanopore/scripts/analysis/temp_files/multiBigWigSummary_PCA-na-allSMAC_{bin_size}.png\"\n",
    "]\n",
    "subprocess.run(command, text=True, check=True)\n",
    "\n",
    "# Display the correlation plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d35dcd72687cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Convert Qiming's .bw files to bedgraph:\n",
    "# for each file ending in .bw in /Data1/ext_data/qiming_2024/ convert to bedgraph\n",
    "for f in os.listdir(\"/Data1/ext_data/qiming_2024/\"):\n",
    "    if f.endswith(\".bw\"):\n",
    "        # define path\n",
    "        f = os.path.join(\"/Data1/ext_data/qiming_2024/\", f)\n",
    "        print(\"Converting to bedgraph: \", f)\n",
    "        try:\n",
    "            output_bw_fn = f.replace(\".bw\", \".bedgraph\")\n",
    "            bigwig_command = [\n",
    "                bedgraphtobigwig_path,\n",
    "                f,\n",
    "                chrom_sizes,\n",
    "                output_bw_fn\n",
    "            ]\n",
    "            subprocess.run(bigwig_command, check=True)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"An error occurred during bedgraph to bigwig conversion: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed96465ff4b5032",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c4f532f6339cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter1d\n",
    "# Reimport nanotools\n",
    "importlib.reload(nanotools)\n",
    "\n",
    "# Base folder path as temp folder\n",
    "base_folder = \"/Data1/git/meyer-nanopore/scripts/analysis/temp_files/\"\n",
    "\n",
    "# File names as all files ending in \"R10_a_A0_raw_qnorm_smoothed.bedgraph\" in temp folder, with keys as the file names without the extension\n",
    "files = {f.split(\".\")[0]: f for f in os.listdir(base_folder) if f.endswith(\"R10_a_A0_qnorm_smoothed.bedgraph\")}\n",
    "\n",
    "# ### PREP DF FOR PLOTTING NON NORMALIZED FILES\n",
    "# # Base folder path\n",
    "# base_folder = \"/Data1/seq_data/BM_N2_old_Fiber_Hia5_MCVIPI_05_30_24/no_sample/20240530_1930_X2_FAX32001_56dbbc37/basecalls/\"\n",
    "# # File names\n",
    "# files = {\n",
    "#     'a_A0': 'BM_05_30_24-N2_old_SMACseq_R10_a_A0.bedgraph',\n",
    "#     'm_GC1': 'BM_05_30_24-N2_old_SMACseq_R10_m_GC1.bedgraph',\n",
    "#     'a_A0_m_GC1': 'BM_05_30_24-N2_old_SMACseq_R10_a_A0_m_GC1.bedgraph',\n",
    "# }\n",
    "\n",
    "\n",
    "# set chr to NULL\n",
    "\n",
    "# Function to apply centered Gaussian smoothing with weights\n",
    "# def apply_weighted_gaussian_smoothing(scores, weights, sigma):\n",
    "#     weighted_scores = scores * weights\n",
    "#     smoothed_weighted_scores = gaussian_filter1d(weighted_scores, sigma=sigma)\n",
    "#     smoothed_weights = gaussian_filter1d(weights, sigma=sigma)\n",
    "#     smoothed_scores = smoothed_weighted_scores / smoothed_weights\n",
    "#     return smoothed_scores\n",
    "\n",
    "# Load and process all files\n",
    "data = {}\n",
    "for key, file_name in files.items():\n",
    "    file_path = os.path.join(base_folder, file_name)\n",
    "    data[key] = nanotools.load_bedgraph_file(file_path,'CHROMOSOME_I',500000,505000, False)\n",
    "\n",
    "# Combine all data into one DataFrame with type column = key\n",
    "combined_data = pd.concat([df.assign(type=key) for key, df in data.items()], ignore_index=True)\n",
    "nanotools.display_sample_rows(combined_data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2e0663227b34dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reimport nanotools local library\n",
    "importlib.reload(nanotools)\n",
    "\n",
    "# Configurable window for exponential smoothing\n",
    "smoothing_window = 0\n",
    "imputation_window = 0\n",
    "\n",
    "# Create scatter plot\n",
    "fig = go.Figure()\n",
    "\n",
    "# Select a color scale\n",
    "color_scale = px.colors.qualitative.Vivid\n",
    "\n",
    "# Make sure we have enough colors in the color scale for all types\n",
    "assert len(files) <= len(color_scale), \"Not enough colors in the color scale for all types\"\n",
    "\n",
    "# Assign a color from the color scale to each type\n",
    "color_dict = {file_key: color_scale[i] for i, file_key in enumerate(files.keys())}\n",
    "\n",
    "smoothed_data = {}\n",
    "\n",
    "for type in files.keys():\n",
    "    df_type = combined_data[combined_data['type'] == type].sort_values('start')\n",
    "\n",
    "    if imputation_window > 0 or smoothing_window > 0:\n",
    "        # Apply combined imputation and smoothing\n",
    "        df_type['imputed_score'], df_type['imputed_coverage'], df_type['smoothed_score'], df_type['smoothed_coverage'] = nanotools.impute_and_smooth(\n",
    "            df_type['score'],\n",
    "            df_type['coverage'],\n",
    "            impute_window=imputation_window,  # Adjust as needed\n",
    "            smooth_window=smoothing_window,  # Adjust as needed\n",
    "            fill_value=0  # Use 0 if you prefer to fill with zeros\n",
    "        )\n",
    "    else:\n",
    "        df_type['smoothed_score'] = df_type['score']\n",
    "\n",
    "    # Store smoothed data for correlation calculation\n",
    "    smoothed_data[type] = df_type['smoothed_score'].values\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=df_type['start'],\n",
    "        y=df_type['score'],\n",
    "        mode='markers',\n",
    "        name=f\"{type} (raw)\",\n",
    "        marker=dict(color=color_dict[type]),\n",
    "    ))\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=df_type['start'],\n",
    "        y=df_type['smoothed_score'],\n",
    "        mode='lines',\n",
    "        name=f\"{type} (smoothed)\",\n",
    "        line=dict(color=color_dict[type], width=2)\n",
    "    ))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Bedgraph Scatter Plot with Raw and Smoothed Data',\n",
    "    xaxis_title='Position',\n",
    "    yaxis_title='Score',\n",
    "    legend_title='Type',\n",
    "    hovermode='closest',\n",
    "    template=\"plotly_white\"\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n",
    "\n",
    "# Output boxplots for each type of data\n",
    "fig2 = go.Figure()\n",
    "\n",
    "# Add boxplot for each type\n",
    "for type in files.keys():\n",
    "    df_type = combined_data[combined_data['type'] == type]\n",
    "    fig2.add_trace(go.Box(\n",
    "        y=df_type['score'],\n",
    "        name=type,\n",
    "        boxpoints='all',  # show all points\n",
    "        jitter=0.3,  # spread them out so they all appear\n",
    "        pointpos=-1.8,  # offset them to the left of the box\n",
    "        marker_color=color_dict[type],  # Use color_dict instead of colors\n",
    "    ))\n",
    "\n",
    "# Update layout\n",
    "fig2.update_layout(\n",
    "    title='Box Plot of Scores by Type',\n",
    "    yaxis_title='Score',\n",
    "    legend_title='Type',\n",
    "    template=\"plotly_white\"\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig2.show()\n",
    "\n",
    "# Calculate correlation matrices\n",
    "# Ensure all arrays are of the same length\n",
    "min_length = min(len(arr) for arr in smoothed_data.values())\n",
    "selected_keys = list(smoothed_data.keys())[:7]\n",
    "\n",
    "for key in selected_keys:\n",
    "    smoothed_data[key] = smoothed_data[key][:min_length]\n",
    "\n",
    "smoothed_df = pd.DataFrame({key: smoothed_data[key] for key in selected_keys})\n",
    "pearson_corr_matrix = smoothed_df.corr()\n",
    "spearman_corr_matrix = smoothed_df.corr(method='spearman')\n",
    "\n",
    "# Plot Pearson correlation matrix as a heatmap\n",
    "fig3 = px.imshow(pearson_corr_matrix, text_auto=True, color_continuous_scale='Viridis')\n",
    "\n",
    "fig3.update_layout(\n",
    "    title='Pearson Correlation Matrix of Smoothed Scores (First 4 Keys)',\n",
    "    xaxis_title='Type',\n",
    "    yaxis_title='Type',\n",
    "    template=\"plotly_white\",\n",
    "    #increase plot height\n",
    "    height=800\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig3.show()\n",
    "\n",
    "# Plot Spearman correlation matrix as a heatmap\n",
    "fig4 = px.imshow(spearman_corr_matrix, text_auto=True, color_continuous_scale='Viridis')\n",
    "\n",
    "fig4.update_layout(\n",
    "    title='Spearman Correlation Matrix of Smoothed Scores (First 4 Keys)',\n",
    "    xaxis_title='Type',\n",
    "    yaxis_title='Type',\n",
    "    template=\"plotly_white\",\n",
    "    #increase plot height\n",
    "    height=800\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b595bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Add bed and condition details to modkit output for plotting\n",
    "# Using DataFrame merging to achieve the task without explicit loops\n",
    "## Looks up the bed_start and bed_end values for each row in bedmethyl_df\n",
    "def add_bed_columns_no_loops(bedmethyl_df_loc, combined_bed_df):\n",
    "    # Calculate midpoint in combined_bed_df\n",
    "    combined_bed_df['midpoint'] = (combined_bed_df['bed_start'] + combined_bed_df['bed_end']) / 2\n",
    "    # Convert midpoint to the same type as start_position (int, in this case)\n",
    "    combined_bed_df['midpoint'] = combined_bed_df['midpoint'].astype(int)\n",
    "\n",
    "    combined_bed_df = combined_bed_df.sort_values(by='midpoint')\n",
    "\n",
    "    # Ensure that start_position is of type int (if it's not already)\n",
    "    bedmethyl_df_loc['start_position'] = bedmethyl_df_loc['start_position'].astype(int)\n",
    "\n",
    "    # Merge bedmethyl_df with combined_bed_df based on the nearest midpoint\n",
    "    merged_df = pd.merge_asof(bedmethyl_df_loc.sort_values('start_position'),\n",
    "                              combined_bed_df,\n",
    "                              by='chrom',\n",
    "                              left_on='start_position',\n",
    "                              right_on='midpoint',\n",
    "                              direction='nearest')\n",
    "\n",
    "    # Filter out rows where the start_position is not within the bed_start and bed_end range\n",
    "    merged_df = merged_df.loc[(merged_df['start_position'] >= merged_df['bed_start']) &\n",
    "                                (merged_df['start_position'] <= merged_df['bed_end'])]\n",
    "\n",
    "    #reset index\n",
    "    merged_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    # Create the final DataFrame by merging the merged DataFrame back to the original bedmethyl_df\n",
    "    final_df = pd.merge(bedmethyl_df_loc,\n",
    "                        merged_df[['chrom', 'start_position', 'bed_start', 'bed_end', 'bed_strand', 'type', 'chr_type']],\n",
    "                        on=['chrom', 'start_position'],\n",
    "                        how='left')\n",
    "\n",
    "    # Drop all final_df rows where type == NaN\n",
    "    final_df = final_df[final_df['type'].notna()]\n",
    "\n",
    "    return final_df\n",
    "\n",
    "combined_bed_df = nanotools.create_lookup_bed(new_bed_files)\n",
    "\n",
    "# Initialize comb_bedmethyl_plot_df\n",
    "comb_bedmethyl_df = pd.DataFrame()\n",
    "\n",
    "# Create combined plotting dataframe\n",
    "for each_output,each_condition,each_exp_id in zip(out_file_names,conditions,exp_ids):\n",
    "    #print(\"Starting on:\",each_output)\n",
    "    # Define bed methyl columns and import bedmethyl file\n",
    "    bedmethyl_df = pd.DataFrame()\n",
    "    bedmethyl_cols = ['chrom','start_position','end_position','modified_base_code','score','strand','start_position_compat','end_position_compat','color','Nvalid_cov','fraction_modified','Nmod','Ncanonical','Nother_mod','Ndelete','Nfail','Ndiff','Nnocall']\n",
    "    bedmethyl_df=pd.read_csv(each_output, sep=\"\\t\", header=None, names=bedmethyl_cols)\n",
    "    # if bedmethyl_df is empty\n",
    "    # drop all rows where modified_base_code is not equal to \"a,A,0\" or \"m,GC,1\"\n",
    "    bedmethyl_df = bedmethyl_df[bedmethyl_df['modified_base_code'].isin(['a,A,0','m,GC,1'])]\n",
    "    if bedmethyl_df.empty:\n",
    "        print(\"!Read in empty csv!!\")\n",
    "        print(\"Tried to select:\",each_output,\" \",each_condition,\" \",each_exp_id, \"and failed...\")\n",
    "        continue\n",
    "\n",
    "\n",
    "    # sort bedmethyl_df by chrom and start_position\n",
    "    bedmethyl_df = bedmethyl_df.sort_values(['start_position'], ascending=[True])\n",
    "    # drop any rows with a nan\n",
    "    bedmethyl_df = bedmethyl_df.dropna()\n",
    "    bedmethyl_df.drop_duplicates(inplace=True)\n",
    "    bedmethyl_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    bedmethyl_df = add_bed_columns_no_loops(bedmethyl_df, combined_bed_df)\n",
    "    # Add rel_start and rel_end columns equal to start-bed_start and end-bed_start\n",
    "\n",
    "    # if type_selected contains 'gene', map to metagene bins\n",
    "    if 'gene' in type_selected[0] or 'damID' in type_selected[0]:\n",
    "        print(\"Mapping to metagene bins...\")\n",
    "        # Define a function to process each group\n",
    "        def process_group(group_tuple, num_bins, edge_window_size, sum_columns):\n",
    "            _, group = group_tuple\n",
    "            min_pos, max_pos = group['start_position'].min(), group['start_position'].max()\n",
    "            bed_start, bed_end = group['bed_start'].iloc[0], group['bed_end'].iloc[0]\n",
    "\n",
    "            group['rel_start'] = np.where(group['start_position'] < bed_start + edge_window_size, # if start position is less than or equal to bed_start + edge_window_size then\n",
    "                                          group['start_position'] - bed_start - edge_window_size, # shift rel_pos by bed_start and window size. Otherwise\n",
    "                                          np.where(group['start_position'] > bed_end - edge_window_size, #if start position is greater than bed_end - edge_window_size then\n",
    "                                                   num_bins + edge_window_size - (bed_end - group['start_position']), #assign to bin otherwise\n",
    "                                                   100000)) # assign to nan\n",
    "\n",
    "            # delete any points outside of window\n",
    "            if (max_pos - min_pos) > (num_bins + 2 * edge_window_size):\n",
    "                binning_mask = (group['start_position'] >= bed_start + edge_window_size) & (group['start_position'] <= bed_end - edge_window_size)\n",
    "                bin_edges = np.linspace(bed_start + edge_window_size, bed_end - edge_window_size, num_bins + 1)\n",
    "                group.loc[binning_mask, 'rel_start'] = np.digitize(group.loc[binning_mask, 'start_position'], bins=bin_edges, right=True)\n",
    "\n",
    "            return group\n",
    "\n",
    "        def map_to_metagene_bins_and_sum(df, num_bins=1000, edge_window_size=500):\n",
    "            # Columns for summing within bins\n",
    "            sum_columns = ['Nmod', 'Ncanonical', 'Nother_mod', 'Ndelete', 'Nfail', 'Ndiff', 'Nnocall','Nvalid_cov']\n",
    "            # Columns to retain in the final DataFrame\n",
    "            retain_columns = ['bed_strand', 'chr_type', 'strand', 'bed_end','type']\n",
    "            # Adjust group columns based on the updated request\n",
    "            group_columns = ['bed_start', 'chrom' ,'modified_base_code']\n",
    "\n",
    "            # Splitting the DataFrame into groups\n",
    "            groups = list(df.groupby(group_columns))\n",
    "\n",
    "            # Using multiprocessing to process groups in parallel\n",
    "            with Pool(500) as pool:\n",
    "                processed_groups = pool.starmap(process_group, [(group, num_bins, edge_window_size, sum_columns) for group in groups])\n",
    "\n",
    "            # Combine the processed groups into a single DataFrame\n",
    "            result_df = pd.concat(processed_groups, ignore_index=True)\n",
    "\n",
    "            # Summing within bins and merging\n",
    "            sum_group_columns = group_columns + ['rel_start']\n",
    "            summed_df = result_df.groupby(sum_group_columns)[sum_columns].sum()\n",
    "            merged_df = pd.merge(result_df[sum_group_columns + retain_columns].drop_duplicates(), summed_df, on=sum_group_columns, how='left')\n",
    "\n",
    "            return merged_df\n",
    "\n",
    "        bedmethyl_df = map_to_metagene_bins_and_sum(bedmethyl_df, num_bins=num_bins, edge_window_size=bed_window)\n",
    "\n",
    "    else:\n",
    "        bedmethyl_df['rel_start'] = bedmethyl_df['start_position'] - bedmethyl_df['bed_start'] - bed_window +1\n",
    "\n",
    "    # set rel_start to int\n",
    "    bedmethyl_df['rel_start'] = bedmethyl_df['rel_start'].astype(int)\n",
    "\n",
    "    #print(\"2. bedmethyl_df\")\n",
    "    #display(bedmethyl_df)\n",
    "    bedmethyl_df['condition'] = each_condition\n",
    "    bedmethyl_df['exp_id'] = each_exp_id\n",
    "    # eliminate levels in dataframe\n",
    "\n",
    "    # if bedmethyl_df is empty\n",
    "    if bedmethyl_df.empty:\n",
    "        print(\"!Bedmethyl_df is empty!\")\n",
    "        print(\"Tried to select:\",each_output,\" \",each_condition,\" \",each_exp_id, \"and failed...\")\n",
    "        continue\n",
    "\n",
    "    # if comb_bedmethyl_plot_df is null, set it equal to bedmethyl_plot\n",
    "    if comb_bedmethyl_df.empty:\n",
    "        print(\"comb_bedmethyl_plot_df is empty, setting it equal to bedmethyl_plot...\")\n",
    "        comb_bedmethyl_df = bedmethyl_df\n",
    "        #print(\"comb_bedmethyl_plot_df:\",comb_bedmethyl_plot_df)\n",
    "    # else append bedmethyl_plot to comb_bedmethyl_plot_df\n",
    "    else:\n",
    "        print(\"comb_bedmethyl_plot_df is not empty, appending bedmethyl_plot...\")\n",
    "        comb_bedmethyl_df = comb_bedmethyl_df.append(bedmethyl_df)\n",
    "        #print(\"comb_bedmethyl_plot_df:\",comb_bedmethyl_plot_df)\n",
    "\n",
    "comb_bedmethyl_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "#print(\"head\")\n",
    "#display(comb_bedmethyl_df.head(100))\n",
    "print(\"sample\")\n",
    "display(nanotools.display_sample_rows(comb_bedmethyl_df,10))\n",
    "#print(\"tail\")\n",
    "#display(comb_bedmethyl_df.tail(100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6e1c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "### SHIFT AND TRANSFORM (OPTIONAL)\n",
    "align_zero_bool = False\n",
    "flip_bool = False\n",
    "def compute_lag_for_maximum_alignment(series1, bed_start1): #,series2, bed_start2):\n",
    "    \"\"\"\n",
    "    Decides flipping based on maximum cross-correlation, and then computes the lag\n",
    "    required to align the maximum values of two series. Returns both the lag and the decision to flip.\n",
    "    \"\"\"\n",
    "    \"\"\" if flip_bool:\n",
    "         # Calculate the correlations without any shift\n",
    "         correlation_original = np.correlate(series1, series2, mode='valid')\n",
    "         correlation_flipped = np.correlate(series1, series2[::-1], mode='valid')\n",
    "\n",
    "         # Decide the flip based on the correlation values\n",
    "         original_max_correlation = correlation_original.max()\n",
    "         flipped_max_correlation = correlation_flipped.max()\n",
    "         # set flip to 1 if flipped_max_correlation > original_max_correlation otherwise set to 0\n",
    "         flip = 1 if flipped_max_correlation > original_max_correlation else 0\n",
    "     else:\n",
    "         flip = 0\n",
    "\n",
    "     # Depending on the flip decision, align based on max values in the series\n",
    "     if flip == 1:\n",
    "         pos_max_series1 = -np.argmax(series1)\n",
    "     else:\"\"\"\n",
    "    flip=0\n",
    "    # print max value in the series\n",
    "    # set all values > 9 to 0\n",
    "    #series1[series1 > 9] = 0\n",
    "    pos_max_series1 = np.argmax(series1)\n",
    "    #print(\"len(series1):\",len(series1))\n",
    "    #print(\"max(series1):\",max(series1))\n",
    "    #print(\"argmax:\",pos_max_series1)\n",
    "    # print value at argmax\n",
    "    #print(\"series1[pos_max_series1]:\",series1[pos_max_series1])\n",
    "\n",
    "    lag = (round(len(series1)/2))-pos_max_series1\n",
    "    #print(\"lag:\",lag)\n",
    "\n",
    "    return (lag, flip)\n",
    "\n",
    "def get_continuous_series(df_subset):\n",
    "    # Create a Series with rel_start as the index and norm_mod_frac_weighted as the values\n",
    "    series_filled = df_subset.set_index('rel_start')['weighted_norm_mod_frac']\n",
    "\n",
    "    #print(\"series before filling:\",series)\n",
    "    # Fill NaNs using a rolling average\n",
    "    #series_filled = series.rolling(50, min_periods=1,center=True).mean()\n",
    "    # Fill any remaining NaNs at the start or end of the series using ffill or bfill\n",
    "    series_filled = series_filled.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "    # Ensure it's a continuous series by filling any gaps in rel_start\n",
    "    try:\n",
    "        series_filled = series_filled.reindex(range(int(series_filled.index.min()), int(series_filled.index.max()) + 1), fill_value=0)\n",
    "    except:\n",
    "        print(\"Failed series_filled:\",series_filled)\n",
    "        print(\"Duplicate indexes:\",series_filled.index[series_filled.index.duplicated()])\n",
    "    # For the newly introduced NaNs due to reindexing, we fill them again using a rolling average\n",
    "    #rolling_avg_reindexed = series_filled.rolling(window=50, center=True, min_periods=1).mean()\n",
    "    #series_filled = series_filled.fillna(rolling_avg_reindexed)\n",
    "    #print(\"series after filling:\",series_filled)\n",
    "    return series_filled.values\n",
    "\n",
    "def align_profiles(df):\n",
    "    df = df.sort_values(['bed_start', 'rel_start']).copy()\n",
    "    bed_starts = df['bed_start'].unique()\n",
    "\n",
    "    # Determine the reference bed_start\n",
    "    summed_Nvalid_cov = df.groupby('bed_start')['Nvalid_cov'].sum()\n",
    "    reference_bed_start = summed_Nvalid_cov.idxmax()\n",
    "    series_reference = get_continuous_series(df[df['bed_start'] == reference_bed_start])\n",
    "\n",
    "    # Calculate the number of positions to shift\n",
    "    shift_positions = int(round(len(series_reference)/2)) - np.argmax(series_reference)\n",
    "\n",
    "    # Shift the entire series_reference by shift_positions to the left or right depending on the sign\n",
    "    if shift_positions > 0:  # shift to the left\n",
    "        series_reference = np.concatenate(([0]*shift_positions, series_reference))\n",
    "    else:\n",
    "        series_reference = np.concatenate((series_reference,[0]*shift_positions))\n",
    "\n",
    "    #df[\"shift\"] = 0\n",
    "    df[\"flipped\"] = 0\n",
    "\n",
    "    for other_bed_start in bed_starts:\n",
    "        #if other_bed_start == reference_bed_start:\n",
    "        #    continue\n",
    "\n",
    "        series_to_shift = get_continuous_series(df[df['bed_start'] == other_bed_start])\n",
    "        # print every item in series_to_shift\n",
    "        #for item in series_to_shift:\n",
    "        lag, flip = compute_lag_for_maximum_alignment(series_to_shift, other_bed_start)#,series_reference, reference_bed_start)\n",
    "\n",
    "        df.loc[df['bed_start'] == other_bed_start, 'shift'] = lag\n",
    "        df.loc[df['bed_start'] == other_bed_start, 'flipped'] = 1 if flip else 0\n",
    "\n",
    "        #print(f\"Decision for bed_start {other_bed_start}: flipped={flip}, shift={lag}\\n\")\n",
    "\n",
    "    # Calculate statistics using the 'flipped' and 'shift' columns\n",
    "    total_flipped = df[df['flipped'] == 1]['bed_start'].nunique()\n",
    "    lag_distribution = df['shift'].describe()\n",
    "\n",
    "    print(f\"Total bed_starts flipped: {total_flipped} out of {len(bed_starts) - 1}\")\n",
    "    print(\"Lag Distribution:\")\n",
    "    print(lag_distribution)\n",
    "\n",
    "    return df\n",
    "\n",
    "print(\"Copying and dropping rows...\")\n",
    "comb_bedmethyl_plot_df = comb_bedmethyl_df.copy()\n",
    "\n",
    "# If aligning to 0\n",
    "if align_zero_bool:\n",
    "    final_df = comb_bedmethyl_plot_df.copy()\n",
    "\n",
    "    final_df = comb_bedmethyl_plot_df.groupby(\n",
    "    ['chrom', 'chr_type', 'rel_start', 'exp_id', 'condition', 'type', 'bed_start']).agg({\n",
    "    'Nvalid_cov': 'sum',\n",
    "    'Nmod': 'sum',\n",
    "    'Ncanonical': 'sum',\n",
    "    'Nother_mod': 'sum'\n",
    "    }).reset_index()\n",
    "\n",
    "    # Calculate normalized m6A\n",
    "    final_df['raw_mod_frac'] = final_df['Nmod'] / (final_df['Nmod'] + final_df['Ncanonical'])\n",
    "\n",
    "    # Merge operation\n",
    "    final_df = pd.merge(\n",
    "        final_df,\n",
    "        coverage_df[['exp_id', 'm6A_frac']],\n",
    "        on=['exp_id'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # rename m6A_frac column to exp_id_m6A_frac\n",
    "    final_df.rename(columns={'m6A_frac': 'exp_id_m6A_frac'}, inplace=True)\n",
    "\n",
    "    # Calculate norm_mod_frac\n",
    "    final_df['norm_mod_frac_init'] = final_df['raw_mod_frac'] / final_df['exp_id_m6A_frac']\n",
    "\n",
    "    # 2. Reuse DataFrame\n",
    "    final_df['norm_mod_frac_weighted'] = final_df['norm_mod_frac_init'] * (final_df['Nmod'] + final_df['Ncanonical'])\n",
    "\n",
    "    # set bed_start to bed_start as a string + chrom as a string\n",
    "    final_df['bed_start'] = final_df['bed_start'].astype(str) + \"_\" + final_df['chrom'].astype(str)\n",
    "\n",
    "    # Group by and aggregation\n",
    "    final_df = final_df.groupby(\n",
    "        ['rel_start', 'condition', 'type', 'chr_type','bed_start']\n",
    "    )[['Nvalid_cov', 'Ncanonical', 'Nmod', 'norm_mod_frac_weighted']].sum().reset_index()\n",
    "\n",
    "\n",
    "    # Additional calculations\n",
    "    final_df['weighted_norm_mod_frac'] = final_df['norm_mod_frac_weighted'] / (final_df['Nmod'] + final_df['Ncanonical'])\n",
    "    final_df['raw_mod_frac'] = final_df['Nmod'] / (final_df['Nmod'] + final_df['Ncanonical'])\n",
    "\n",
    "    # drop rows where Nvalid_cov is lower than lower quartile\n",
    "    final_df = final_df[final_df['Nvalid_cov'] > final_df['Nvalid_cov'].quantile(0.1)]\n",
    "\n",
    "    # Sorting and re-indexing\n",
    "    final_df.sort_values(['bed_start', 'rel_start'], inplace=True)\n",
    "    final_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    # print rows with duplicate rel_pos, bed_start, condition and type\n",
    "    print(\"Duplicate rows:\")\n",
    "    display(final_df[final_df.duplicated(['rel_start','bed_start','condition','type'])].head(10))\n",
    "\n",
    "    # Displaying the first 100 rows\n",
    "    display(final_df.head(100))\n",
    "    center_iter = 0\n",
    "    for each_condition in final_df['condition'].unique():\n",
    "        for each_type in final_df['type'].unique():\n",
    "            # Filtering the data\n",
    "            print(\"Starting on:\",each_condition,each_type)\n",
    "            final_df_cluster = final_df[(final_df['condition'] == each_condition) & (final_df['type'] == each_type)]\n",
    "            #print(\"final_df_cluster:\")\n",
    "            #display(final_df_cluster.head(100))\n",
    "\n",
    "            aligned_df = align_profiles(final_df_cluster)\n",
    "            aligned_df.drop_duplicates(subset=['bed_start'], inplace=True)\n",
    "\n",
    "            final_df_cluster = comb_bedmethyl_df[(comb_bedmethyl_df['condition'] == each_condition) & (comb_bedmethyl_df['type'] == each_type)].groupby(\n",
    "                ['chrom', 'rel_start', 'exp_id', 'modified_base_code', 'condition', 'type', 'chr_type', 'bed_start']\n",
    "            ).agg({\n",
    "                'Nvalid_cov': 'sum',\n",
    "                'Nmod': 'sum',\n",
    "                'Ncanonical': 'sum',\n",
    "                'Nother_mod': 'sum'\n",
    "            }).reset_index()\n",
    "\n",
    "            # set bed_start to bed_start as a string + chrom as a string\n",
    "            final_df_cluster['bed_start'] = final_df_cluster['bed_start'].astype(str) + \"_\" + final_df_cluster['chrom'].astype(str)\n",
    "\n",
    "            # Calculate normalized m6A\n",
    "            final_df_cluster['raw_mod_frac'] = final_df_cluster['Nmod'] / (final_df_cluster['Nmod'] + final_df_cluster['Ncanonical'])\n",
    "\n",
    "            # Merge operation\n",
    "            final_df_cluster = pd.merge(\n",
    "                final_df_cluster,\n",
    "                coverage_df[['exp_id', 'm6A_frac']],\n",
    "                on=['exp_id'],\n",
    "                how='left'\n",
    "            )\n",
    "\n",
    "            # rename m6A_frac column to exp_id_m6A_frac\n",
    "            final_df_cluster.rename(columns={'m6A_frac': 'exp_id_m6A_frac'}, inplace=True)\n",
    "\n",
    "            # Calculate norm_mod_frac\n",
    "            final_df_cluster['norm_mod_frac_init'] = final_df_cluster['raw_mod_frac'] / final_df_cluster['exp_id_m6A_frac']\n",
    "\n",
    "            # 2. Reuse DataFrame\n",
    "            final_df_cluster['norm_mod_frac_weighted'] = final_df_cluster['norm_mod_frac_init'] * (final_df_cluster['Nmod'] + final_df_cluster['Ncanonical'])\n",
    "\n",
    "            ### Since multiple samples have same condition:\n",
    "            #merge final_df with aligned_df on bed_start adding shift and flipped columns\n",
    "            print(\"Merging final_df_cluster with aligned_df...\")\n",
    "            #display(final_df_cluster.head(10))\n",
    "            #display(aligned_df.head(10))\n",
    "\n",
    "            final_df_cluster = pd.merge(final_df_cluster, aligned_df[['bed_start','shift','flipped']], on=['bed_start'], how='left')\n",
    "            final_df_cluster.loc[final_df_cluster['flipped'] == 1, 'rel_start'] *= -1\n",
    "            # add shift to rel_pos\n",
    "            final_df_cluster['rel_start'] += final_df_cluster['shift']\n",
    "            final_df_cluster = final_df_cluster.groupby(['rel_start','modified_base_code','condition','type','chr_type'])[['Nvalid_cov','Ncanonical','Nmod','norm_mod_frac_weighted']].sum() #,'strand'\n",
    "            final_df_cluster.reset_index(inplace=True)\n",
    "\n",
    "            # set norm_mod_frac to norm_mod_frac_weighted / Nvalid_cov\n",
    "            final_df_cluster['weighted_norm_mod_frac'] = final_df_cluster['norm_mod_frac_weighted']/(final_df_cluster['Nmod']+final_df_cluster['Ncanonical'])\n",
    "            final_df_cluster['raw_mod_frac'] = final_df_cluster['Nmod']/(final_df_cluster['Nmod']+final_df_cluster['Ncanonical'])\n",
    "            #sort by rel_start\n",
    "            final_df_cluster.sort_values(['rel_start'], inplace=True)\n",
    "            final_df_cluster.reset_index(inplace=True, drop=True)\n",
    "\n",
    "            if center_iter == 0:\n",
    "                plot_df = final_df_cluster.copy()\n",
    "            else:\n",
    "                plot_df = plot_df.append(final_df_cluster)\n",
    "            center_iter += 1\n",
    "\n",
    "else:\n",
    "    # FOR GENES If bed_strand is -, multiply rel_start by -1, and sort by rel_start resetting index afterwards\n",
    "    for each_type in type_selected:\n",
    "        # if each_type contains substring \"TSS\", \"TES\", \"MEX\", then flip only those genes\n",
    "        if any(x in each_type for x in [\"TSS\", \"TES\", \"MEX\", \"MEXII\", \"gene\"]):\n",
    "            print(f\"Strand orientation sensitive {each_type} type selected, multiplying rel_start by -1 for '-' strand genes...\")\n",
    "            if 'gene' in each_type:\n",
    "                # subtract 1/2 of num_bins from rel_start for metagene profiles, so they are centered\n",
    "                comb_bedmethyl_plot_df['rel_start'] -= num_bins/2\n",
    "            # Mask comb_bedmethyl_plot_df by type == each_type and strand == '-'\n",
    "            mask = (comb_bedmethyl_plot_df['type'] == each_type) & (comb_bedmethyl_plot_df['bed_strand'] == '-')\n",
    "            # Multiply rel_start by -1  for all rows where mask is true\n",
    "            comb_bedmethyl_plot_df.loc[mask, 'rel_start'] *= -1\n",
    "\n",
    "            # DO the same for bigwig lines:\n",
    "            #if ext_target is not empty list []:\n",
    "            if ext_target != []:\n",
    "                mask = (bw_df['type'] == each_type) & (bw_df['bed_strand'] == '-')\n",
    "                # Multiply rel_start by -1  for all rows where mask is true\n",
    "                bw_df.loc[mask, 'rel_start'] *= -1\n",
    "            if 'gene' in each_type:\n",
    "                # subtract 1/2 of num_bins from rel_start\n",
    "                comb_bedmethyl_plot_df['rel_start'] += num_bins/2\n",
    "\n",
    "    print(\"Grouping by chrom, rel_start, exp_id, modified_base_code, condition, type, chr_type, bed_start...\")\n",
    "    # Group comb_bedmethyl_plot_df and sum specific columns\n",
    "    grouped_df = comb_bedmethyl_plot_df.groupby(['chrom', 'rel_start','exp_id','modified_base_code','condition','type','chr_type']).agg({ #,'strand'\n",
    "        'Nvalid_cov': 'sum',\n",
    "        'Nmod': 'sum',\n",
    "        'Ncanonical': 'sum',\n",
    "        'Nother_mod': 'sum'\n",
    "    }).reset_index()\n",
    "\n",
    "    print(\"Calculating normalized m6A...\")\n",
    "    ### Calculate normalized m6A\n",
    "    #grouped_df['mod_frac'] = grouped_df['Nmod'] / grouped_df['Nvalid_cov']\n",
    "    grouped_df['raw_mod_frac'] = grouped_df['Nmod'] / (grouped_df['Nmod'] + grouped_df['Ncanonical'])\n",
    "    # Merge the two dataframes based on 'exp_id' and 'condition'\n",
    "\n",
    "    coverage_df['exp_id'] = coverage_df['exp_id'].str.strip()\n",
    "    grouped_df['exp_id'] = grouped_df['exp_id'].str.strip()\n",
    "    nanotools.display_sample_rows(coverage_df,10)\n",
    "    nanotools.display_sample_rows(grouped_df,10)\n",
    "    merged_df = pd.merge(grouped_df, coverage_df[['exp_id', 'm6A_frac']],\n",
    "                         on=['exp_id'], how='left')\n",
    "    nanotools.display_sample_rows(merged_df,10)\n",
    "    # rename m6A_frac column to exp_id_m6A_frac\n",
    "    merged_df.rename(columns={'m6A_frac': 'exp_id_m6A_frac'}, inplace=True)\n",
    "    # Calculate norm_mod_frac\n",
    "    merged_df['norm_mod_frac_init'] = merged_df['raw_mod_frac'] / merged_df['exp_id_m6A_frac']\n",
    "    # If you want to keep only the original columns plus the new 'norm_mod_frac'\n",
    "    plot_df = merged_df[grouped_df.columns.tolist() + ['norm_mod_frac_init']]\n",
    "    # Calculate norm_mod_frac_weighted\n",
    "    plot_df['norm_mod_weighted'] = plot_df['norm_mod_frac_init'] * (grouped_df['Nmod'] + grouped_df['Ncanonical'])\n",
    "\n",
    "    ### Since multiple samples have same condition:\n",
    "    plot_df = plot_df.groupby(['rel_start','modified_base_code','condition','type','chr_type'])[['Nvalid_cov','Ncanonical','Nmod','norm_mod_weighted']].sum() #,'strand'\n",
    "    plot_df.reset_index(inplace=True)\n",
    "\n",
    "    if ext_target != []:\n",
    "        plot_comb_bigwig_df = bw_df.groupby(['rel_start','chrom','condition','type','chr_type'])['value'].mean().reset_index()\n",
    "\n",
    "    else:\n",
    "        plot_comb_bigwig_df = pd.DataFrame()\n",
    "    # set norm_mod_frac to norm_mod_frac_weighted / Nvalid_cov\n",
    "    plot_df['weighted_norm_mod_frac'] = plot_df['norm_mod_weighted']/(plot_df['Nmod']+plot_df['Ncanonical'])\n",
    "    plot_df['raw_mod_frac'] = plot_df['Nmod']/(plot_df['Nmod']+plot_df['Ncanonical'])\n",
    "    #sort by rel_start\n",
    "    plot_df.sort_values(['rel_start'], inplace=True)\n",
    "    plot_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "print(\"plot_df:\")\n",
    "# display random 100 rows\n",
    "nanotools.display_sample_rows(plot_df,10)\n",
    "if ext_target != []:\n",
    "    nanotools.display_sample_rows(plot_comb_bigwig_df,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037e6481",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot correlation plot between replicates\n",
    "merged_df_correlation = merged_df.copy()\n",
    "merged_df_correlation[\"exp_condition_id\"] = merged_df_correlation[\"exp_id\"] + \"_\" + merged_df_correlation[\"condition\"]\n",
    "# Define a binning function to bin every 10 bases\n",
    "def binning_func(x):\n",
    "    return np.floor(x / 50) * 50\n",
    "\n",
    "# Apply the binning function to the start_position to create binned_start_position\n",
    "merged_df_correlation['binned_start_position'] = merged_df_correlation['start_position'].apply(binning_func)\n",
    "nanotools.display_sample_rows(merged_df_correlation)\n",
    "# Group by 'chrom', 'binned_start_position', and 'exp_condition_id', and then sum up 'Nvalid_cov' and 'Nmod'\n",
    "binned_df = merged_df_correlation.groupby(['chrom', 'binned_start_position', 'exp_condition_id','exp_id_m6A_frac']).agg({\n",
    "    'Nvalid_cov': 'sum',\n",
    "    'Nmod': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Calculate the new 'm6A_frac' as the ratio of 'Nmod' to 'Nvalid_cov'\n",
    "binned_df['m6A_frac'] = binned_df['Nmod'] / binned_df['Nvalid_cov']\n",
    "\n",
    "# add norm_mod_frac column\n",
    "binned_df['norm_mod_frac'] = binned_df['m6A_frac'] / binned_df['exp_id_m6A_frac']\n",
    "nanotools.display_sample_rows(binned_df)\n",
    "binned_df['transformed_mod_frac'] = np.arcsin(np.sqrt(binned_df['m6A_frac']))\n",
    "\n",
    "# Pivot the DataFrame with the new binned positions and calculated 'm6A_frac'\n",
    "pivoted_df = binned_df.pivot_table(\n",
    "    index=['chrom', 'binned_start_position'],\n",
    "    columns='exp_condition_id',\n",
    "    values='transformed_mod_frac'\n",
    ")\n",
    "\n",
    "nanotools.display_sample_rows(pivoted_df)\n",
    "\n",
    "# Step 3: Calculate the Pearson correlation coefficient matrix\n",
    "correlation_matrix = pivoted_df.corr(method='pearson')\n",
    "\n",
    "nanotools.display_sample_rows(correlation_matrix)\n",
    "\n",
    "# Step 4: Square the correlation coefficients to obtain r² values\n",
    "r_squared_matrix = correlation_matrix ** 2\n",
    "\n",
    "nanotools.display_sample_rows(r_squared_matrix)\n",
    "\n",
    "# Create a heatmap using plotly.graph_objects\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    z=r_squared_matrix.values,\n",
    "    x=r_squared_matrix.columns,\n",
    "    y=r_squared_matrix.index,\n",
    "    colorscale='Oranges'))\n",
    "\n",
    "# Update the layout to use the plotly_white template and adjust the title\n",
    "fig.update_layout(\n",
    "    template='plotly_white',\n",
    "    title='Pearson r² Values Heatmap'\n",
    ")\n",
    "\n",
    "# Show the figure in a Jupyter environment or it can be saved to an HTML file using fig.write_html('heatmap.html')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae92b40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "force_replace = True\n",
    "# save final_df to /temp folder as csv, with all configurations in file name if it does not exist. If it does exist, import it.\n",
    "final_fn = \"temp_files/\" + \"final_df_\" + \"_\".join([each_type for each_type in type_selected]) + str(round(thresh_list[0],2)) + \"_\"+str(bam_fracs[0])+str(bed_window)+\".csv\"\n",
    "final_fn_chip = \"temp_files/\" + \"final_df_chip\" + \"_\".join([each_type for each_type in type_selected]) + str(round(thresh_list[0],2)) + \"_\"+str(bam_fracs[0])+str(bed_window)+\".csv\"\n",
    "\n",
    "if not force_replace and os.path.exists(final_fn):\n",
    "    print(\"final_df already exists, importing it...\")\n",
    "    plot_df = pd.read_csv(final_fn)\n",
    "    nanotools.display_sample_rows(plot_df,5)\n",
    "else:\n",
    "    print(\"final_df does not exist, saving it...\")\n",
    "    plot_df.to_csv(final_fn, index=False)\n",
    "\n",
    "# if plot_comb_bigwig_df dataframe does not exist:\n",
    "try:\n",
    "    if not force_replace and os.path.exists(final_fn_chip):\n",
    "        print(\"final_df_chip already exists, importing it...\")\n",
    "        plot_comb_bigwig_df = pd.read_csv(final_fn_chip)\n",
    "        nanotools.display_sample_rows(plot_comb_bigwig_df,5)\n",
    "    else:\n",
    "        print(\"final_df_chip does not exist, saving it...\")\n",
    "        plot_comb_bigwig_df.to_csv(final_fn_chip, index=False)\n",
    "except:\n",
    "    print(\"plot_comb_bigwig_df does not exist, skipping...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03d8839",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Base folder path\n",
    "base_folder = \"/Data1/seq_data/BM_N2_old_Fiber_Hia5_MCVIPI_05_30_24/no_sample/20240530_1930_X2_FAX32001_56dbbc37/basecalls/\"\n",
    "\n",
    "# File names\n",
    "files = {\n",
    "    'a_positive': 'a_A0_positive.bedgraph',\n",
    "    'a_negative': 'a_A0_negative.bedgraph',\n",
    "    'm_positive': 'm_GC1_positive.bedgraph',\n",
    "    'm_negative': 'm_GC1_negative.bedgraph'\n",
    "}\n",
    "\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path, sep='\\t', header=None, names=['chromosome', 'start', 'end', 'score', 'coverage'])\n",
    "    df = df[(df['chromosome'] == 'CHROMOSOME_I') & (df['start'] >= 1000000) & (df['start'] < 1010000)]\n",
    "    # Ensure 'start' is unique by aggregating scores if necessary\n",
    "    df = df.groupby('start').agg({'score': 'mean', 'coverage': 'sum'}).reset_index()\n",
    "    return df\n",
    "\n",
    "def weighted_average(x):\n",
    "    # Handle numpy array\n",
    "    x = x[~np.isnan(x)]  # Remove NA values\n",
    "    if len(x) == 0:\n",
    "        return np.nan\n",
    "    weights = np.arange(1, len(x) + 1)\n",
    "    return np.average(x, weights=weights)\n",
    "\n",
    "def apply_weighted_rolling_average(series, window=10):\n",
    "    return series.rolling(window=window, center=True, min_periods=1).apply(weighted_average, raw=True)\n",
    "\n",
    "# Load and process all files\n",
    "data = {}\n",
    "for key, file_name in files.items():\n",
    "    file_path = os.path.join(base_folder, file_name)\n",
    "    data[key] = load_data(file_path)\n",
    "    print(f\"Loaded {key}: {len(data[key])} rows\")  # Debug print\n",
    "\n",
    "# Create a complete range of start positions\n",
    "all_starts = pd.DataFrame({'start': range(1000000, 1010000)})\n",
    "\n",
    "# Combine all data into one DataFrame, keeping NaN values\n",
    "combined_data = all_starts.copy()\n",
    "for key, df in data.items():\n",
    "    type, strand = key.split('_')\n",
    "    combined_data = combined_data.merge(\n",
    "        df[['start', 'score']].rename(columns={'score': f'score_{type}_{strand}'}),\n",
    "        on='start', how='left'\n",
    "    )\n",
    "\n",
    "print(\"Combined data shape:\", combined_data.shape)  # Debug print\n",
    "print(\"Combined data non-null counts:\\n\", combined_data.notnull().sum())  # Debug print\n",
    "\n",
    "# Melt the DataFrame to long format\n",
    "combined_data_long = pd.melt(\n",
    "    combined_data,\n",
    "    id_vars=['start'],\n",
    "    value_vars=[col for col in combined_data.columns if col.startswith('score')],\n",
    "    var_name='type_strand',\n",
    "    value_name='score'\n",
    ")\n",
    "combined_data_long[['type', 'strand']] = combined_data_long['type_strand'].str.split('_', expand=True).iloc[:, 1:]\n",
    "combined_data_long = combined_data_long.drop('type_strand', axis=1)\n",
    "\n",
    "print(\"Long data shape:\", combined_data_long.shape)  # Debug print\n",
    "print(\"Long data non-null counts:\\n\", combined_data_long.notnull().sum())  # Debug print\n",
    "\n",
    "# Create scatter plot\n",
    "fig = go.Figure()\n",
    "\n",
    "colors = {'a': 'red', 'm': 'blue'}\n",
    "\n",
    "for type in ['a', 'm']:\n",
    "    df_type = combined_data_long[combined_data_long['type'] == type]\n",
    "\n",
    "    # Sort dataframe by 'start' to ensure correct line connections\n",
    "    df_type = df_type.sort_values('start')\n",
    "\n",
    "    # Set 'start' as index for smoothing\n",
    "    df_type = df_type.set_index('start')\n",
    "\n",
    "    # Apply weighted rolling average\n",
    "    df_type['smoothed'] = apply_weighted_rolling_average(df_type['score'])\n",
    "\n",
    "    print(f\"Type {type} data points: {len(df_type)}\")  # Debug print\n",
    "    print(f\"Type {type} non-null counts:\\n\", df_type.notnull().sum())  # Debug print\n",
    "\n",
    "    # Plot raw data line\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=df_type.index,\n",
    "        y=df_type['score'],\n",
    "        mode='markers+lines',\n",
    "        name=f\"{type} (raw)\",\n",
    "        line=dict(color=colors[type], width=1),\n",
    "        legendgroup=type,\n",
    "        showlegend=True\n",
    "    ))\n",
    "\n",
    "    # Plot smoothed line\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=df_type.index,\n",
    "        y=df_type['smoothed'],\n",
    "        mode='lines',\n",
    "        name=f\"{type} (smoothed)\",\n",
    "        line=dict(color=colors[type], width=2, dash='dash'),\n",
    "        legendgroup=type,\n",
    "        showlegend=True\n",
    "    ))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Bedgraph Scatter Plot with Raw and Weighted Rolling Average Smoothed Lines',\n",
    "    xaxis_title='Position',\n",
    "    yaxis_title='Score',\n",
    "    legend_title='Type',\n",
    "    hovermode='closest',\n",
    "    template=\"simple_white\"\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n",
    "\n",
    "# Print statistics to verify zero and NA handling\n",
    "for type in ['a', 'm']:\n",
    "    df_type = combined_data_long[combined_data_long['type'] == type]\n",
    "    total_count = len(df_type)\n",
    "    na_count = df_type['score'].isna().sum()\n",
    "    zero_count = (df_type['score'] == 0).sum()\n",
    "    non_zero_count = ((df_type['score'] != 0) & (~df_type['score'].isna())).sum()\n",
    "\n",
    "    print(f\"Type {type}:\")\n",
    "    print(f\"  Total data points: {total_count}\")\n",
    "    print(f\"  NA values: {na_count} ({na_count/total_count*100:.2f}%)\")\n",
    "    print(f\"  Zero values: {zero_count} ({zero_count/total_count*100:.2f}%)\")\n",
    "    print(f\"  Non-zero values: {non_zero_count} ({non_zero_count/total_count*100:.2f}%)\")\n",
    "    print(f\"  Min non-zero value: {df_type['score'][df_type['score'] != 0].min()}\")\n",
    "    print(f\"  Max value: {df_type['score'].max()}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfa4d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def filter_data(df, condition, chr_type, type_,strand):\n",
    "    filters = []\n",
    "    if condition:\n",
    "        filters.append(df['condition'] == condition)\n",
    "    if chr_type != \"all\":\n",
    "        filters.append(df['chr_type'] == chr_type)\n",
    "    if type_ != \"all\":\n",
    "        filters.append(df['type'] == type_)\n",
    "    if strand != \"all\":\n",
    "        filters.append(df['strand'] == strand)\n",
    "\n",
    "    base_filter = pd.concat(filters).groupby(level=0).all()\n",
    "    return df.loc[base_filter]\n",
    "\n",
    "def plot_bedmethyl_diff(bed_window,final_df, conditions, window_size=25, *args):\n",
    "    fig = go.Figure()\n",
    "    fig.update_layout(\n",
    "        title='m6A Fraction Difference vs Genomic Position N2 SDC3 - N2 intergenic control',\n",
    "        xaxis_title='Genomic Position',\n",
    "        yaxis_title='% change in norm m6A/A',\n",
    "        template=\"plotly_white\",\n",
    "        width=1000,\n",
    "        height=600,\n",
    "        #increase font size\n",
    "        font=dict(\n",
    "            size=14\n",
    "        ),\n",
    "        # set y axis to % with rounded to nearest int\n",
    "        yaxis_tickformat = '.0%'\n",
    "\n",
    "    )\n",
    "    # Update to place legend at the bottom\n",
    "    fig.update_layout(legend=dict(\n",
    "        y=-0.4,\n",
    "        x=0.25\n",
    "    ))\n",
    "\n",
    "    #Shift y axis labels left\n",
    "    # Skip the first tick label\n",
    "    \"\"\"x_min = -20\n",
    "    x_max = 20\n",
    "    tickvals = list([(x)/100 for x in range(x_min,x_max,5)])\n",
    "    ticktext = [(str(round(x*100))+\"%\") for x in tickvals]  # Empty string for the first tick label\n",
    "    fig.update_yaxes(tickvals=tickvals, ticktext=ticktext)\n",
    "    # set y axis min and max\n",
    "    fig.update_yaxes(range=[min(tickvals)-0.025,max(tickvals)+0.025])\n",
    "\n",
    "    # Add vertical dashed line at x=0\n",
    "    fig.add_shape(\n",
    "        type=\"line\", line=dict(dash=\"dash\"),\n",
    "        x0=0, x1=0, y0=min(tickvals)-0.025, y1=max(tickvals), line_color=\"Grey\"\n",
    "    )\"\"\"\n",
    "\n",
    "\n",
    "    fig.update_xaxes(range=[-bed_window, bed_window])\n",
    "    #fig.update_yaxes(range=[-0.1,0.4])\n",
    "\n",
    "    for (selection_index1, chr_type1, type1,strand, selection_index2, chr_type2, type2,strand) in args:\n",
    "        condition1 = conditions[selection_index1]\n",
    "        condition2 = conditions[selection_index2]\n",
    "\n",
    "        df1 = filter_data(final_df, condition1, chr_type1, type1,strand)\n",
    "        df2 = filter_data(final_df, condition2, chr_type2, type2,strand)\n",
    "\n",
    "        df1.reset_index(drop=True, inplace=True)\n",
    "        df2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        \"\"\"#Drop outlier weighted_norm_mod_frac datapoints from df1 and df2 more than 3 standard deviations away from the mean\n",
    "        # Calculate mean and standard deviation for the column 'weighted_norm_mod_frac' in df1\n",
    "        mean1 = df1['weighted_norm_mod_frac'].mean()\n",
    "        std1 = df1['weighted_norm_mod_frac'].std()\n",
    "\n",
    "        # Drop outliers in df1\n",
    "        df1 = df1[(df1['weighted_norm_mod_frac'] >= mean1 - 6 * std1) &\n",
    "                  (df1['weighted_norm_mod_frac'] <= mean1 + 6 * std1)]\n",
    "\n",
    "        # Calculate mean and standard deviation for the column 'weighted_norm_mod_frac' in df2\n",
    "        mean2 = df2['weighted_norm_mod_frac'].mean()\n",
    "        std2 = df2['weighted_norm_mod_frac'].std()\n",
    "\n",
    "        # Drop outliers in df2\n",
    "        df2 = df2[(df2['weighted_norm_mod_frac'] >= mean2 - 6 * std2) &\n",
    "                  (df2['weighted_norm_mod_frac'] <= mean2 + 6 * std2)]\"\"\"\n",
    "\n",
    "        def weighted_average(sub_df):\n",
    "            weights = sub_df['Nvalid_cov']\n",
    "            values = sub_df['weighted_norm_mod_frac']\n",
    "            if weights.sum() == 0:\n",
    "                return np.nan\n",
    "            return np.average(values, weights=weights)\n",
    "        # smooth df1 weighted_norm_mod_frac using a rolling average centered, weighted on Nvalid_cov column\n",
    "        df1['weighted_norm_mod_frac_smooth'] = df1.apply(lambda row: weighted_average(df1.loc[row.name - window_size // 2 : row.name + window_size // 2]), axis=1)\n",
    "\n",
    "        df2['weighted_norm_mod_frac_smooth'] = df2.apply(lambda row: weighted_average(df2.loc[row.name - window_size // 2 : row.name + window_size // 2]), axis=1)\n",
    "        diff_data = (df1['weighted_norm_mod_frac_smooth'] - df2['weighted_norm_mod_frac_smooth'])/df1['weighted_norm_mod_frac_smooth']\n",
    "        diff_data_xaxis = df1['rel_start']\n",
    "\n",
    "        # combine diff_data and diff_data_xaxis into a dataframe and display\n",
    "        diff_df = pd.concat([diff_data_xaxis, diff_data], axis=1)\n",
    "        diff_df.columns = ['rel_start', 'diff_data']\n",
    "        # display diff_df between -100 and 100\n",
    "        #display(diff_df[(diff_df['rel_start'] >= -50) & (diff_df['rel_start'] <= 50)])\n",
    "\n",
    "        #smoothed_data = diff_data.rolling(window=window_size, center=True).mean()\n",
    "\n",
    "        label = f\"Diff_{condition1}_{chr_type1}_{type1} - {condition2}_{chr_type2}_{type2}\"\n",
    "        print(label)\n",
    "\n",
    "        fig.add_trace(go.Scatter(\n",
    "            name=label,\n",
    "            x=diff_data_xaxis.values,\n",
    "            y=diff_data.values,\n",
    "            mode='lines',\n",
    "            # make color of lines shades of grey\n",
    "            #line=dict(color='grey', width=2)\n",
    "        ))\n",
    "        # set x axis min and max using bed_window\n",
    "        fig.update_xaxes(range=[-bed_window, bed_window])\n",
    "\n",
    "    fig.show(renderer='plotly_mimetype+notebook')\n",
    "    return fig,label\n",
    "\n",
    "# capture fig and label\n",
    "\n",
    "diff_fig = plot_bedmethyl_diff(1000, plot_df, conditions, 200,\n",
    "                               (8,\"X\",type_selected[0],\"all\",1,\"X\",type_selected[0],\"all\"))\n",
    "                               #(8,\"X\",type_selected[0],\"all\",8,\"X\",type_selected[1],\"all\"),\n",
    "                               #(8,\"X\",type_selected[0],\"all\",1,\"X\",type_selected[0],\"all\"),\n",
    "                               #(8,\"X\",type_selected[1],\"all\",1,\"X\",type_selected[1],\"all\"))\n",
    "                               #(8,\"X\",type_selected[1],1,\"X\",type_selected[1]))\n",
    "                    #(8,\"X\",\"all\",1,\"X\",\"all\"))\n",
    "                    #(8,\"X\",\"strong_rex\",1,\"X\",\"strong_rex\"),\n",
    "                    #(8,\"X\",\"weak_rex\",1,\"X\",\"weak_rex\"))\n",
    "                    #(8,\"X\",\"center_SDC3_chip_albretton\",1,\"X\",\"center_SDC3_chip_albretton\"),\n",
    "                    #(8,\"Autosome\",\"center_SDC3_chip_albretton\",1,\"Autosome\",\"center_SDC3_chip_albretton\"))\n",
    "                    #(1,\"Autosome\",\"center_SDC3_chip_albretton\",1,\"X\",\"center_SDC3_chip_albretton\"),\n",
    "                    #(8,\"Autosome\",\"center_SDC3_chip_albretton\",8,\"X\",\"center_SDC3_chip_albretton\"))\n",
    "                    #(1, \"X\", \"TSS_q4\", 1, \"Autosome\", \"TSS_q4\"),\n",
    "                    #(8, \"X\", \"TSS_q4\", 8, \"Autosome\", \"TSS_q4\"))\n",
    "                    #(8, \"X\", \"TSS_q4\", 1, \"X\", \"TSS_q4\"),\n",
    "                    #(8, \"Autosome\", \"TSS_q4\", 1, \"Autosome\", \"TSS_q4\"))\n",
    "                    #(8, \"X\", \"TSS_q3\", 1, \"X\", \"TSS_q3\"),\n",
    "                    #(8, \"X\", \"TSS_q2\", 1, \"X\", \"TSS_q2\"),\n",
    "                    #(8, \"X\", \"TSS_q1\", 1, \"X\", \"TSS_q1\"))\n",
    "\n",
    "diff_fig[0].write_image(\"images_11_14_23/\"+region_fig[1]+\"sdc2degron_minus_N2_strong_rex_1000_centered.svg\")\n",
    "diff_fig[0].write_image(\"images_11_14_23/\"+region_fig[1]+\"sdc2degron_minus_N2_strong_rex_1000_centered.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c5ea4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "force_replace = False\n",
    "\n",
    "### Extracting per read modifications\n",
    "out_file_names = [output_stem + \"modkit-extract-\" + each_condition +\"_\"+ str(round(each_thresh,2))+\"_\"+str(each_index)+ \"_\"+str(each_bamfrac)+\"_\"+str(bed_window)+\n",
    "                  # convert the first 3 characters of each element in \"type_selected\" into a single string separated by \"-\"\n",
    "                  \"-\".join([str(x)[0:7] for x in type_selected])+\"_\"+\n",
    "                  # convert first character and the last 3 characters of each element in \"choromosome_selected\" into a single string separated by \"-\"\n",
    "                  \"-\".join([str(x)[0]+str(x)[-3:] for x in chromosome_selected])+\"_\"+\n",
    "                  \".bed\"\n",
    "                  for each_condition,each_thresh,each_index, each_bamfrac in zip(conditions,thresh_list,sample_indices,bam_fracs)]\n",
    "\n",
    "modkit_bed_df = pd.read_csv(modkit_bed_name,sep='\\t',header=None)\n",
    "### Define bed file for modkit\n",
    "\n",
    "# Function to run a single extract command\n",
    "def modkit_extract(args):\n",
    "    each_bam, each_thresh, each_condition, each_index, each_bamfrac,modkit_path, output_stem, modkit_bed_name, bed_window = args\n",
    "\n",
    "    each_output = output_stem + \"modkit-extract-\" + each_condition +\"_\"+ str(round(each_thresh,2))+\"_\"+str(each_index)+ \"_\"+str(each_bamfrac)+\"_\"+str(bed_window)+ \"-\".join([str(x)[0:7] for x in type_selected])+\"_\"+ \"-\".join([str(x)[0]+str(x)[-3:] for x in chromosome_selected])+\"_\"+ \".bed\"\n",
    "\n",
    "    ### NOTE: Name of pileup file is not based on configurations\n",
    "    ### TODO: Name of output file should be based on configs so that we aren't recomputing pileups withidentical conditions.\n",
    "\n",
    "    # If each_output exsits, skip\n",
    "    if not force_replace and os.path.exists(each_output):\n",
    "        print(f\"Skipping: {each_output}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Starting on: {each_bam}\")\n",
    "    command = [\n",
    "        modkit_path,\n",
    "        \"extract\",\n",
    "        \"--threads\",\n",
    "        \"128\",\n",
    "        \"--force\",\n",
    "        \"--mapped\",\n",
    "        #\"--ignore\",\n",
    "        #\"m\",\n",
    "        \"--include-bed\",\n",
    "        modkit_bed_name,\n",
    "        \"--log-filepath\",\n",
    "        each_output + each_condition + \"_modkit-extract.log\",\n",
    "        each_bam,\n",
    "        each_output\n",
    "    ]\n",
    "    subprocess.run(command, text=True)\n",
    "\n",
    "    # Create a list of arguments for each task\n",
    "task_args = list(zip(\n",
    "    new_bam_files,\n",
    "    thresh_list,\n",
    "    conditions,\n",
    "    sample_indices,\n",
    "    bam_fracs,\n",
    "    [modkit_path]*len(new_bam_files),\n",
    "    [output_stem]*len(new_bam_files),\n",
    "    [modkit_bed_name]*len(new_bam_files),\n",
    "    [bed_window]*len(new_bam_files)\n",
    "))\n",
    "\n",
    "# Execute commands in parallel\n",
    "with Pool() as pool:\n",
    "    pool.map(modkit_extract, task_args)\n",
    "\n",
    "print(\"finished with:\")\n",
    "print(out_file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5bb718",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add bed and condition details to modkit output for plotting\n",
    "# Using DataFrame merging to achieve the task without explicit loops\n",
    "## Looks up the bed_start and bed_end values for each row in bedmethyl_df\n",
    "def add_bed_columns_no_loops(bedmethyl_df, bed_df):\n",
    "\n",
    "    # Initialize an empty DataFrame to store the merged data\n",
    "    merged_data = pd.DataFrame()\n",
    "    filtered_df = pd.DataFrame()\n",
    "\n",
    "    # drop all rows from bedmethly_df where mod_strand != '+' or '-'\n",
    "    bedmethyl_df = bedmethyl_df[(bedmethyl_df['mod_strand'] == '+') | (bedmethyl_df['mod_strand'] == '-')]\n",
    "    # Get all unique chromosome-strand combinations without headers in bedmethyl_df\n",
    "    unique_chrom_strand_combinations = bedmethyl_df[['chrom']].drop_duplicates().values #, 'mod_strand'\n",
    "    for chrom in unique_chrom_strand_combinations: #, strand\n",
    "        #print(\"Starting on:\", chrom, strand)\n",
    "        bed_subset = bed_df[(bed_df['chrom'] == chrom[0])] #& (bed_df['bed_strand'] == strand)]\n",
    "\n",
    "        if bed_subset.empty:\n",
    "            #print(\"No bed entries for {}\".format(chrom[0]))\n",
    "            # then skip this chromosome-strand combination\n",
    "            continue\n",
    "        #display(bed_subset.head(10))\n",
    "        # Subset data for the current chromosome\n",
    "        bedmethyl_subset = bedmethyl_df[(bedmethyl_df['chrom'] == chrom[0])]\n",
    "        #print(\"bedmethyl_subset:\",bedmethyl_subset.head(10))\n",
    "        #display(bedmethyl_subset.head(10))\n",
    "        # Explicitly cast to numeric data type\n",
    "        bedmethyl_subset['ref_position'] = bedmethyl_subset['ref_position'].astype('int64')\n",
    "        bed_subset['bed_start'] = bed_subset['bed_start'].astype('int64')\n",
    "\n",
    "        # Perform the merge for the current subset\n",
    "        merged_subset = pd.merge_asof(\n",
    "            bedmethyl_subset,\n",
    "            bed_subset[['bed_start', 'bed_end', 'bed_strand','chr_type','type']],  # Exclude 'chrom' from right DF\n",
    "            left_on='ref_position',\n",
    "            right_on='bed_start',\n",
    "            direction='nearest'\n",
    "        )\n",
    "\n",
    "        # Append the merged data to the overall result\n",
    "        merged_data = merged_data.append(merged_subset)\n",
    "\n",
    "    #print(\"bedmethyl_df:\")\n",
    "    #display(merged_data.head(10))\n",
    "    # Filter out rows where ref_position is not within bed_start and bed_end\n",
    "    filtered_df = merged_data.loc[\n",
    "        (merged_data['ref_position'] >= merged_data['bed_start']) &\n",
    "        (merged_data['ref_position'] <= merged_data['bed_end'])\n",
    "    ]\n",
    "    # Filter out rows where bed_start is not a number\n",
    "    filtered_df = filtered_df[filtered_df['bed_start'].notna()]\n",
    "    filtered_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Print number rows in filtered_df\n",
    "    print(\"Found {} rows in filtered_df\".format(len(filtered_df)))\n",
    "\n",
    "    #display(filtered_df)\n",
    "    return filtered_df\n",
    "\n",
    "# Initialize comb_readmethyl_df as an empty dataframe\n",
    "comb_readmethyl_df = pd.DataFrame()\n",
    "\n",
    "# Build combined bed df\n",
    "combined_bed_df=pd.DataFrame()\n",
    "for each_bed in new_bed_files:\n",
    "    bed_path = each_bed[:-3] # remove .gz\n",
    "    print(\"Starting on:\",bed_path)\n",
    "    # read bedpath and append to bed_df\n",
    "    combined_bed_df = combined_bed_df.append(pd.read_csv(bed_path, sep=\"\\t\", header=None))#skiprows=1))\n",
    "combined_bed_df.columns = ['chrom','bed_start','bed_end','bed_strand','type','chr_type']\n",
    "\n",
    "combined_bed_df.sort_values(['chrom','bed_start'], inplace=True)\n",
    "combined_bed_df.reset_index(drop=True, inplace=True)\n",
    "display(combined_bed_df.head(10))\n",
    "\n",
    "# Initialize comb_bedmethyl_plot_df\n",
    "comb_bedmethyl_plot_df = pd.DataFrame()\n",
    "\n",
    "# Create combined plotting dataframe\n",
    "for each_output,each_condition in zip(out_file_names,conditions):\n",
    "    print(\"Starting on:\",each_output)\n",
    "    # Define bed methyl columns and import bedmethyl file\n",
    "    bedmethyl_df = pd.DataFrame()\n",
    "    bedmethyl_cols = ['read_id',\n",
    "    'forward_read_position',\n",
    "    'ref_position',\n",
    "    'chrom',\n",
    "    'mod_strand',\n",
    "    'ref_strand',\n",
    "    'ref_mod_strand',\n",
    "    'fw_soft_clipped_start',\n",
    "    'fw_soft_clipped_end',\n",
    "    'read_length',\n",
    "    'mod_qual',\n",
    "    'mod_code',\n",
    "    'base_qual',\n",
    "    'ref_kmer',\n",
    "    'query_kmer',\n",
    "    'canonical_base',\n",
    "    'modified_primary_base',\n",
    "    'inferred']\n",
    "    bedmethyl_df=pd.read_csv(each_output, sep=\"\\t\", header=None, names=bedmethyl_cols,skiprows=1)\n",
    "\n",
    "    # sort bedmethyl_df by chrom and start_position, required for merging in bed matching.\n",
    "    bedmethyl_df.sort_values(['chrom','ref_position'], inplace=True)\n",
    "    bedmethyl_df.reset_index(drop=True, inplace=True)\n",
    "    #display(bedmethyl_df.head(10))\n",
    "\n",
    "    # Adding new columns to bedmethyl_df using condition matching\n",
    "    bedmethyl_df = add_bed_columns_no_loops(bedmethyl_df.copy(), combined_bed_df)\n",
    "    # Add rel_start and rel_end columns equal to start-bed_start and end-bed_start\n",
    "    bedmethyl_df['rel_pos'] = bedmethyl_df['ref_position'] - bedmethyl_df['bed_start'] -bed_window +1\n",
    "\n",
    "    # add condition column\n",
    "    bedmethyl_df['condition'] = each_condition\n",
    "\n",
    "    # if comb_bedmethyl_plot_df is null, set it equal to bedmethyl_plot\n",
    "    if comb_bedmethyl_plot_df.empty:\n",
    "        print(\"comb_bedmethyl_plot_df is empty, setting it equal to bedmethyl_plot...\")\n",
    "        comb_bedmethyl_plot_df = bedmethyl_df.copy()\n",
    "    # else append bedmethyl_plot to comb_bedmethyl_plot_df\n",
    "    else:\n",
    "        print(\"comb_bedmethyl_plot_df is not empty, appending bedmethyl_plot...\")\n",
    "        comb_bedmethyl_plot_df = comb_bedmethyl_plot_df.append(bedmethyl_df)\n",
    "\n",
    "if any(x in type_selected[0] for x in (\"TSS\", \"TES\", \"MEX\")):\n",
    "    print(\"Strand orientation sensitive type selected, multiplying rel_start by -1 for '-' strand genes...\")\n",
    "    mask = comb_bedmethyl_plot_df['bed_strand'] == '-'\n",
    "    comb_bedmethyl_plot_df.loc[mask, 'rel_pos'] *= -1\n",
    "    #comb_bedmethyl_plot_df.sort_values(['chrom','rel_pos'], inplace=True)\n",
    "\n",
    "# Set entire mod_qual column equal to 1 base on m6A thresh\n",
    "threshold = m6A_thresh / 255\n",
    "# convert comb_bedmethly_plot_df_final['mod_qual'] to a float\n",
    "comb_bedmethyl_plot_df['mod_qual'] = comb_bedmethyl_plot_df['mod_qual'].astype(float)\n",
    "comb_bedmethyl_plot_df['mod_qual_bin'] = np.where(comb_bedmethyl_plot_df['mod_qual'] > threshold, 1, 0)\n",
    "\n",
    "#add read_start and read_end columns to comb_bedmethyl_plot_df_final based on min and max ref_position for each read_id\n",
    "print(\"Calculating read start and end...\")\n",
    "# Reduce the DataFrame size by selecting only the columns you need\n",
    "small_df = comb_bedmethyl_plot_df[['read_id', 'rel_pos']]\n",
    "# Perform the grouping and aggregation in one step\n",
    "grouped_df = small_df.groupby('read_id')['rel_pos'].agg(['min', 'max']).reset_index()\n",
    "# Rename the columns\n",
    "grouped_df.rename(columns={'min': 'rel_read_start', 'max': 'rel_read_end'}, inplace=True)\n",
    "# Merge the aggregated results back to the original DataFrame\n",
    "comb_bedmethyl_plot_df = pd.merge(comb_bedmethyl_plot_df, grouped_df, on='read_id', how='left')\n",
    "# delete small_df and grouped_df\n",
    "del small_df\n",
    "del grouped_df\n",
    "\n",
    "#comb_bedmethyl_plot_df.sort_values(['chrom','rel_pos'], inplace=True)\n",
    "comb_bedmethyl_plot_df = comb_bedmethyl_plot_df[comb_bedmethyl_plot_df['bed_start'].notna()]\n",
    "comb_bedmethyl_plot_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# print count of unique read_ids for each condition\n",
    "print(\"All unique read_ids for each condition:\")\n",
    "print(comb_bedmethyl_plot_df.groupby(['condition'])['read_id'].nunique())\n",
    "\n",
    "#display(comb_bedmethyl_plot_df.head(10))\n",
    "# drop all rows where read_length is less than 1000\n",
    "comb_bedmethyl_plot_df = comb_bedmethyl_plot_df[comb_bedmethyl_plot_df['read_length'] >= 500]\n",
    "\n",
    "\n",
    "# print count of unique read_ids for each condition\n",
    "print(\"All unique read_ids for each condition > read_len of 1000:\")\n",
    "print(comb_bedmethyl_plot_df.groupby(['condition'])['read_id'].nunique())\n",
    "\n",
    "# Define SQLite database file name\n",
    "#db_fn = \"temp_files/\" + \"plot_db_\" + \"-\".join([str(x)[:6] for x in type_selected]) + \"_\" + str(round(thresh_list[0],2)) + \"_\" + str(bam_fracs[0]) + \".db\"\n",
    "# Create a SQLite database connection\n",
    "#conn = sqlite3.connect(db_fn)\n",
    "# Save the DataFrame to SQLite database\n",
    "#comb_bedmethyl_plot_df.to_sql('bedmethyl_plot', conn, if_exists='replace', index=False)\n",
    "# Close the connection\n",
    "#conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ea8de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar to other dataframes, define fn based on configurations. If it doesn't exist, create it, otherwise import it.\n",
    "plot_df_fn = \"temp_files/\" + \"plot_df_\" + \"-\".join([str(x)[0:12] for x in type_selected])+\"_\"+ str(thresh_list[0])+\"_\"+str(bam_fracs[0])+str(bed_window)+\".csv\"\n",
    "#comb_bedmethyl_plot_df.to_csv(plot_df_fn, index=False)\n",
    "if os.path.exists(plot_df_fn):\n",
    "    print(\"plot_df_fn exists, importing...\")\n",
    "    plot_df = pd.read_csv(plot_df_fn)\n",
    "else:\n",
    "    print(\"plot_df_fn does not exist, creating...\")\n",
    "    comb_bedmethyl_plot_df.to_csv(plot_df_fn, index=False)\n",
    "    plot_df = comb_bedmethyl_plot_df.copy()\n",
    "\n",
    "metadata_cols = ['chrom', 'chr_type', 'condition', 'bed_start','type', 'read_id', 'rel_read_start','rel_read_end']\n",
    "\n",
    "nanotools.display_sample_rows(plot_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b58d3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_kmer_plots(plot_df):\n",
    "    kmer_plot_df = plot_df.copy()\n",
    "    kmer_plot_df['3mer'] = kmer_plot_df['query_kmer'].str[1:4]\n",
    "\n",
    "    query_kmer_counts = kmer_plot_df.groupby(['3mer', 'canonical_base', 'condition']).size().reset_index(name='3mer_occurence')\n",
    "    nanotools.display_sample_rows(query_kmer_counts, 2)\n",
    "\n",
    "    kmer_mod_plot_df = kmer_plot_df[kmer_plot_df['mod_qual'] > 0.9]\n",
    "    mod_kmer_counts = kmer_mod_plot_df.groupby(['3mer', 'canonical_base', 'condition']).size().reset_index(name='mod_occurence')\n",
    "    nanotools.display_sample_rows(mod_kmer_counts, 2)\n",
    "\n",
    "    merged_kmer_counts = query_kmer_counts.merge(mod_kmer_counts, on=['3mer', 'canonical_base', 'condition'], how='left')\n",
    "    merged_kmer_counts['percent'] = merged_kmer_counts['mod_occurence'] / merged_kmer_counts['3mer_occurence']\n",
    "    merged_kmer_counts = merged_kmer_counts.sort_values(by=['condition', 'percent'], ascending=[False, False])\n",
    "    merged_kmer_counts = merged_kmer_counts[merged_kmer_counts['3mer_occurence'] >= 100]\n",
    "    nanotools.display_sample_rows(merged_kmer_counts, 2)\n",
    "\n",
    "    # Create vertical subplots for 'A' canonical base\n",
    "    fig_50 = make_subplots(rows=2, cols=1, shared_xaxes=True, vertical_spacing=0.25, subplot_titles=[\"Percent Methylated\", \"Trinucleotide Occurrence\"])\n",
    "    filtered_df = merged_kmer_counts[merged_kmer_counts['canonical_base'] == 'A']\n",
    "    for condition in filtered_df['condition'].unique():\n",
    "        df_condition = filtered_df[filtered_df['condition'] == condition]\n",
    "        fig_50.add_trace(go.Bar(x=df_condition['3mer'], y=df_condition['percent'], name=f\"Percent - {condition}\",text=df_condition['percent'], textposition='outside',texttemplate='%{text:.0%}'), row=1, col=1)\n",
    "        if condition != 'N2_bg':\n",
    "            fig_50.add_trace(go.Bar(x=df_condition['3mer'], y=df_condition['3mer_occurence'], name=f\"Occurrence - {condition}\",), row=2, col=1)\n",
    "\n",
    "    fig_50.update_yaxes(tickformat='.0%', row=1, col=1)\n",
    "    fig_50.update_layout(template=\"plotly_white\", title_text=\"3mer Analysis for Canonical Base 'A'\")\n",
    "    # set width to 800\n",
    "    fig_50.update_layout(width=1200)\n",
    "\n",
    "    # Create vertical subplots for 'C' canonical base\n",
    "    fig_bg = make_subplots(rows=2, cols=1, shared_xaxes=True, vertical_spacing=0.25, subplot_titles=[\"Percent Methylated\", \"Trinucleotide Occurrence\"])\n",
    "    filtered_df = merged_kmer_counts[merged_kmer_counts['canonical_base'] == 'C']\n",
    "    for condition in filtered_df['condition'].unique():\n",
    "        df_condition = filtered_df[filtered_df['condition'] == condition]\n",
    "        fig_bg.add_trace(go.Bar(x=df_condition['3mer'], y=df_condition['percent'], name=f\"Percent - {condition}\",text=df_condition['percent'], textposition='outside',texttemplate='%{text:.0%}'), row=1, col=1)\n",
    "        if condition != 'N2_bg':\n",
    "            fig_bg.add_trace(go.Bar(x=df_condition['3mer'], y=df_condition['3mer_occurence'], name=f\"Occurrence - {condition}\"), row=2, col=1)\n",
    "\n",
    "    fig_bg.update_yaxes(tickformat='.0%', row=1, col=1)\n",
    "    fig_bg.update_layout(template=\"plotly_white\", title_text=\"3mer Analysis for Canonical Base 'C'\")\n",
    "    # set width to 800\n",
    "    fig_bg.update_layout(width=1200)\n",
    "\n",
    "    max_percent = merged_kmer_counts[merged_kmer_counts['canonical_base']=='A']['percent'].max()\n",
    "    padding = max_percent * 0.2  # Adjust the padding as needed\n",
    "\n",
    "    fig_50.update_yaxes(range=[0, max_percent + padding], row=1, col=1)\n",
    "    max_percent = merged_kmer_counts[merged_kmer_counts['canonical_base']=='C']['percent'].max()\n",
    "    padding = max_percent * 0.2  # Adjust the padding as needed\n",
    "    fig_bg.update_yaxes(range=[0, max_percent + padding], row=1, col=1)\n",
    "\n",
    "    fig_50.show()\n",
    "    fig_bg.show()\n",
    "\n",
    "    return fig_50, fig_bg\n",
    "\n",
    "# Example usage\n",
    "fig_50, fig_bg = output_kmer_plots(plot_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012c1f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Create read_id to m6A 5mC lookup table\n",
    "plot_df_cutoff = plot_df#[plot_df['condition'] == '50_dpy27-3xGNB_GFP-Hia5_mcvipi']\n",
    "canonical_count = plot_df_cutoff.groupby(['read_id','canonical_base','condition','type']).size().reset_index(name='canonical_count')\n",
    "plot_df_cutoff = plot_df[plot_df['mod_qual'] > 0.8]\n",
    "# drop all rows where condition == N2_bg\n",
    "\n",
    "\n",
    "plot_df_cutoff = plot_df_cutoff[(plot_df_cutoff['rel_pos'] <= 250) & (plot_df_cutoff['rel_pos'] >= -250)]\n",
    "mod_count = plot_df_cutoff.groupby(['read_id','canonical_base','condition','type']).size().reset_index(name='mod_count')\n",
    "mod_lookup = canonical_count.merge(mod_count, on=['read_id','canonical_base','condition','type'], how='left')\n",
    "mod_lookup['mod_frac'] = mod_lookup['mod_count']/mod_lookup['canonical_count']\n",
    "# fill nan values with 0\n",
    "mod_lookup['mod_frac'] = mod_lookup['mod_frac'].fillna(0)\n",
    "nanotools.display_sample_rows(mod_lookup,10)\n",
    "\n",
    "\n",
    "# Calculate quartiles separately for 'A' and 'C'\n",
    "quartiles_A = mod_lookup[mod_lookup['canonical_base'] == 'A'].groupby('condition')['mod_frac'].transform(lambda x: pd.qcut(x, 2, labels=False))\n",
    "quartiles_C = mod_lookup[mod_lookup['canonical_base'] == 'C'].groupby('condition')['mod_frac'].transform(lambda x: pd.qcut(x, 2, labels=False))\n",
    "\n",
    "# add 1 to the value of quartiles_A and quartiles_C\n",
    "quartiles_A += 1\n",
    "quartiles_C += 1\n",
    "\n",
    "# Assign these quartiles back to mod_lookup in new columns\n",
    "mod_lookup.loc[mod_lookup['canonical_base'] == 'A', 'm6A_quartile'] = quartiles_A\n",
    "mod_lookup.loc[mod_lookup['canonical_base'] == 'C', '5mC_quartile'] = quartiles_C\n",
    "# set m6A_quartile to \"q_\" concatenated with the first digit ONLY of m6A_quartile\n",
    "mod_lookup['m6A_quartile'] = \"m6A_q_\" + mod_lookup['m6A_quartile'].astype(str).str[0]\n",
    "# set 5mC_quartile to \"q_\" concatenated with the first digit ONLY of 5mC_quartile\n",
    "mod_lookup['5mC_quartile'] = \"5mC_q_\" + mod_lookup['5mC_quartile'].astype(str).str[0]\n",
    "\n",
    "nanotools.display_sample_rows(mod_lookup,10)\n",
    "\n",
    "# Create two separate DataFrames for 'A' and 'C'\n",
    "mod_lookup_A = mod_lookup[mod_lookup['canonical_base'] == 'A'][['read_id', 'm6A_quartile']]\n",
    "mod_lookup_C = mod_lookup[mod_lookup['canonical_base'] == 'C'][['read_id', '5mC_quartile']]\n",
    "\n",
    "# Merge the DataFrames on read_id\n",
    "mod_lookup_final = pd.merge(mod_lookup_A, mod_lookup_C, on='read_id')\n",
    "\n",
    "# Merge this final DataFrame with plot_df\n",
    "plot_df = pd.merge(plot_df, mod_lookup_final, on='read_id', how='left')\n",
    "\n",
    "nanotools.display_sample_rows(plot_df,10)\n",
    "# plot scatter plot of with mod qual of cannonical_base == A on x axis and mod_qual of cannonical_base == C on y axis\n",
    "\"\"\"\n",
    "# Separate the dataframe into two based on canonical_base\n",
    "mod_lookup_A = mod_lookup[mod_lookup['canonical_base'] == 'A']\n",
    "mod_lookup_C = mod_lookup[mod_lookup['canonical_base'] == 'C']\n",
    "mod_merged = pd.merge(mod_lookup_A[['read_id', 'mod_frac']], mod_lookup_C[['read_id', 'mod_frac']], on='read_id', suffixes=('_A', '_C'))\n",
    "\n",
    "# Create scatter plot\n",
    "fig = go.Figure(data=go.Scatter(\n",
    "    x=mod_merged['mod_frac_A'],\n",
    "    y=mod_merged['mod_frac_C'],\n",
    "    mode='markers',\n",
    "    #color by condition and type\n",
    "    marker=dict(color=mod_merged['condition'], size=5)\n",
    "))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Scatter Plot of Modification Fraction',\n",
    "    xaxis_title='Modification Fraction for A',\n",
    "    yaxis_title='Modification Fraction for C',\n",
    "    template='plotly_white'\n",
    ")\n",
    "\n",
    "# Show plot\n",
    "fig.show()\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e95f46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### PROCESSING AUTOCORRELATON\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from scipy.interpolate import interp1d\n",
    "import tqdm\n",
    "metadata_cols = ['chrom', 'chr_type', 'condition', 'bed_start','type', 'read_id', 'rel_read_start','rel_read_end']#,'m6A_quartile']\n",
    "#version that requires a nuc and linker region (does not allow nucleosomes to be stuck next to eachother)\n",
    "def process_raw_read(read_id, group, metadata_cols,gauss_std_dev, crr_length,corr_start,mod_code):\n",
    "    global m6A_thresh\n",
    "    # drop all rows where 'mod_code' != mod_code\n",
    "    group = group[group['mod_code'] == mod_code]\n",
    "    m6A_thresh_local = m6A_thresh\n",
    "    # Calculate the total number of modified bases, total bases, and minimum base position in the read\n",
    "    BASE_NUM = max(group['rel_pos']) - min(group['rel_pos']) + 1\n",
    "    BASE_MIN = min(group['rel_pos'])\n",
    "\n",
    "    if np.mean(group['mod_qual']) > m6A_thresh_local/255:\n",
    "        return read_id, group.iloc[0][metadata_cols], np.zeros(1), None\n",
    "\n",
    "    # Initialize the calling_vec with -1 and populate it with mod_qual values based on relative position\n",
    "    calling_vec_raw = np.full(BASE_NUM+1, np.nan)\n",
    "    for i in range(len(group['rel_pos'])):\n",
    "        calling_vec_raw[group.iloc[i]['rel_pos'] - BASE_MIN] = group.iloc[i]['mod_qual']\n",
    "\n",
    "    #print(\"read_mean: \", read_mean)\n",
    "    # Impute -1 values with mean\n",
    "    #calling_vec[calling_vec == -1] = 0#read_mean\n",
    "    #print(\"calling_vec RAW: \", calling_vec[0:100])\n",
    "    # Interpolate to fill NaN values\n",
    "    not_nan = ~np.isnan(calling_vec_raw)\n",
    "    indices = np.arange(len(calling_vec_raw))\n",
    "\n",
    "    # set calling vec as a copy of calling_vec_raw\n",
    "    calling_vec = calling_vec_raw.copy()\n",
    "\n",
    "    # set all values in calling_vec > m6A_thresh to 1\n",
    "    calling_vec[calling_vec > m6A_thresh_local/255] = 1\n",
    "    # set all values in calling_vec <= m6A_thresh to 0\n",
    "    calling_vec[calling_vec <= m6A_thresh_local/255] = 0\n",
    "\n",
    "    interp_func = interp1d(indices[not_nan], calling_vec[not_nan], bounds_error=False, copy=False, fill_value=\"extrapolate\", kind='nearest')\n",
    "    calling_vec_filled = interp_func(indices)\n",
    "    #print(\"calling_vec INTERP: \", calling_vec_filled[0:100])\n",
    "\n",
    "    # set all values in calling_vec > m6A_thresh to 1\n",
    "    #calling_vec_filled[calling_vec_filled > m6A_thresh/255] = 1\n",
    "    # set all values in calling_vec <= m6A_thresh to 0\n",
    "    #calling_vec_filled[calling_vec_filled <= m6A_thresh/255] = 0\n",
    "    # Ensure no NaNs remain after smoothing\n",
    "    if np.isnan(calling_vec_filled).any():\n",
    "        # Handle remaining NaNs after smoothing if they exist\n",
    "        calling_vec_filled = np.nan_to_num(calling_vec_filled, nan=0.0)\n",
    "    #print(\"calling_vec THRESH: \", calling_vec_filled[0:100])\n",
    "    # Apply gaussian smoothing\n",
    "    calling_vec_smoothed = gaussian_filter(calling_vec_filled, sigma=gauss_std_dev)\n",
    "    #print(\"calling_vec GAUSS: \", calling_vec_smoothed[0:100])\n",
    "    read_mean = np.mean(calling_vec_smoothed[calling_vec != -1])\n",
    "    read_std = np.std(calling_vec_smoothed[calling_vec != -1])\n",
    "    if read_std == 0 or read_mean == 0:\n",
    "        return read_id, group.iloc[0][metadata_cols], np.zeros(1), None\n",
    "\n",
    "    # Calculate 1D autocorrelation\n",
    "    autocorr = np.correlate(calling_vec_smoothed-read_mean, calling_vec_smoothed-read_mean, mode='same')/(read_std * read_std * len(calling_vec_smoothed))\n",
    "    autocorr_centered = autocorr[autocorr.size // 2:]  # Taking one side as it's symmetric\n",
    "    #autocorr_normalized = autocorr_centered / autocorr_centered[0]\n",
    "\n",
    "    # Limit the autocorrelation calculation to a lag of crr_length\n",
    "\n",
    "    if len(autocorr_centered) < (crr_length+1): # np.isnan(autocorr_limited).any() or np.isinf(autocorr_limited).any() or np.max(autocorr_limited) > np.finfo(np.float32).max:\n",
    "        return read_id, group.iloc[0][metadata_cols], np.zeros(1), None\n",
    "\n",
    "    else:\n",
    "        autocorr_limited = autocorr_centered[corr_start:crr_length + 1]  # Include lag 0 to 500\n",
    "        # if autocorr_limited contains NaN, infinity or a value too large for dtype('float32'). return 0\n",
    "        # output scatter plot of first 500 values of calling_vec,calling_vec_filled, and calling_vec_smoothed\n",
    "        # Instead of creating a single figure, create a subplot figure\n",
    "        read_fig = make_subplots(rows=2, cols=1, subplot_titles=('Methylation', 'Autocorrelation'), shared_xaxes=False)# Set subfigure distribution:\n",
    "\n",
    "        colors_scheme = plotly.colors.qualitative.Prism\n",
    "        # define dictionary with one color for each of \"Raw\", \"Extrapolated\", \"Smoothed\" and \"Autocorrelation\"\n",
    "        colors_dict = dict(zip([\"Raw\", \"Extrapolated\", \"Smoothed\", \"Autocorrelation\"], colors_scheme))\n",
    "        # Add the methylation plot to the first subplot\n",
    "        read_fig.add_trace(go.Scatter(\n",
    "            x=np.arange(1000),\n",
    "            y=calling_vec_raw[100:1100],\n",
    "            mode='lines',\n",
    "            name=\"Raw\",\n",
    "            marker=dict(color=colors_dict[\"Raw\"])\n",
    "        ), row=1, col=1)\n",
    "        read_fig.add_trace(go.Scatter(\n",
    "            x=np.arange(1000),\n",
    "            y=calling_vec_filled[100:1100],\n",
    "            mode='lines',\n",
    "            name=\"Extrapolated\",\n",
    "            marker=dict(color=colors_dict[\"Extrapolated\"])\n",
    "        ), row=1, col=1)\n",
    "        read_fig.add_trace(go.Scatter(\n",
    "            x=np.arange(1000),\n",
    "            y=calling_vec_smoothed[100:1100],\n",
    "            mode='lines',\n",
    "            name=\"Smoothed\",\n",
    "            marker=dict(color=colors_dict[\"Smoothed\"])\n",
    "        ), row=1, col=1)\n",
    "\n",
    "        # Add the autocorrelation scatter plot to the second subplot\n",
    "        read_fig.add_trace(go.Scatter(\n",
    "            x=np.arange(1000),\n",
    "            y=autocorr_limited[100:1100],\n",
    "            mode='markers',\n",
    "            name=\"Autocorrelation\",\n",
    "            # reduce marker size\n",
    "            marker=dict(size=2.5,color=colors_dict[\"Autocorrelation\"])\n",
    "        ), row=2, col=1)\n",
    "\n",
    "        # Update the layout of the subplot figure\n",
    "        read_fig.update_layout(\n",
    "            title='Read Analysis',\n",
    "            width=800,\n",
    "            height=400,\n",
    "            template='plotly_white'\n",
    "        )\n",
    "        # Update xaxis and yaxis properties if needed\n",
    "        read_fig.update_xaxes(title_text='Genomic Position', row=1, col=1)\n",
    "        read_fig.update_yaxes(title_text='Mod Probability', row=1, col=1)\n",
    "        read_fig.update_xaxes(title_text='Lag (bp)', row=1, col=2)\n",
    "        read_fig.update_yaxes(title_text='Autocorrelation Value', row=2, col=1)\n",
    "        return read_id, group.iloc[0][metadata_cols], autocorr_limited, read_fig\n",
    "\n",
    "### CONFIGS\n",
    "#grouped = grouped_subset.groupby('read_id')\n",
    "crr_length = 1000\n",
    "gauss_std = 10\n",
    "corr_start = 100\n",
    "# show this many single read tracks:\n",
    "figures_shown = 0\n",
    "# Process this many reads\n",
    "reads_to_process = 0\n",
    "corr_buff = 500\n",
    "\n",
    "print(\"Grouping df...\")\n",
    "\n",
    "\n",
    "# grouped_auto = plot_df where (rel_read_end - rel_read_start) >= crr_length + 100\n",
    "grouped_auto = plot_df.copy() #[plot_df['chr_type'] == 'X']\n",
    "grouped_auto = grouped_auto[grouped_auto['condition'] != 'N2_mixed_endogenous_R10']\n",
    "#grouped_auto = grouped_auto[plot_df['type'].str.contains('all_rex')]\n",
    "grouped_auto = grouped_auto[(grouped_auto['rel_read_end'] - grouped_auto['rel_read_start']) >= (2*crr_length + corr_buff)]\n",
    "grouped_auto = grouped_auto[grouped_auto['mod_code'] == 'm']\n",
    "#set grouped_auto type column equal to m6A_quartile\n",
    "#grouped_auto['type'] = grouped_auto['m6A_quartile']\n",
    "nanotools.display_sample_rows(grouped_auto,10)\n",
    "\n",
    "grouped_auto.sort_values(by=[\"read_id\",\"rel_pos\"], inplace=True)\n",
    "grouped_auto.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# drop rows where read_id not in first 5 read_ids\n",
    "if reads_to_process > 0:\n",
    "    first_rows = grouped_auto['read_id'].unique()[:reads_to_process]#[plot_df['type'] == 'intergenic_control']\n",
    "    grouped_auto = grouped_auto[grouped_auto['read_id'].isin(first_rows)]\n",
    "    # reset index\n",
    "    grouped_auto.reset_index(inplace=True, drop=True)\n",
    "\n",
    "grouped_auto = grouped_auto.groupby('read_id')\n",
    "#display(grouped_auto.head(10))\n",
    "#nanotools.display_sample_rows(grouped_auto)\n",
    "\n",
    "print(\"Processing autocorrelations...\")\n",
    "grouped_data_with_constants = [(read_id,group,metadata_cols,gauss_std,crr_length,corr_start,'a') for read_id,group in grouped_auto]\n",
    "\n",
    "#processes=multiprocessing.cpu_count()\n",
    "with multiprocessing.Pool() as pool:\n",
    "    # set results equal to pool.starmap() with the function and grouped_data_with_constants as arguments using tqdm to track progress\n",
    "    results = pool.starmap(process_raw_read, tqdm.tqdm(grouped_data_with_constants, total=len(grouped_data_with_constants)))\n",
    "\n",
    "# Clear grouped_auto dataframe\n",
    "grouped_auto = None\n",
    "\n",
    "# Extracting autocorrelations and their corresponding metadata\n",
    "grouped_autocorrelations = {}\n",
    "# Initialize lists to hold the filtered conditions, clusters, and chr_types\n",
    "conditions_list = []\n",
    "chr_types_list = []\n",
    "types_list = []\n",
    "\n",
    "for read_id, metadata, autocorr, read_fig in results:\n",
    "    #if autocorrs has nan values skip\n",
    "    if len(autocorr) >1:\n",
    "        # Create a unique key for each combination of type, chr_type, and condition\n",
    "        key = (metadata['type'], metadata['chr_type'], metadata['condition'])\n",
    "        if key not in grouped_autocorrelations:\n",
    "            grouped_autocorrelations[key] = []\n",
    "        grouped_autocorrelations[key].append(autocorr)\n",
    "\n",
    "        conditions_list.append(metadata['condition'])\n",
    "        chr_types_list.append(metadata['chr_type'])\n",
    "        types_list.append(metadata['type'])\n",
    "\n",
    "# Create a heatmap for each group\n",
    "fig = go.Figure()\n",
    "\n",
    "y_labels = []  # To store y-axis labels\n",
    "z_data = []  # To store autocorrelation data for heatmap\n",
    "\n",
    "for group_key, autocorrs in grouped_autocorrelations.items():\n",
    "    group_label = f\"{group_key[0]}, {group_key[1]}, {group_key[2]}\"\n",
    "    for i, autocorr in enumerate(autocorrs, start=1):\n",
    "        read_label = f\"{group_label} - Read {i}\"\n",
    "        y_labels.append(read_label)\n",
    "        z_data.append(autocorr)\n",
    "\n",
    "from scipy.signal import find_peaks, peak_prominences, peak_widths\n",
    "# Extracting autocorrelations and their corresponding metadata for clustering, only if results[2] does not contain any nan values\n",
    "autocorrelation_data = [result[2] for result in results if len(result[2])>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9980ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PLOTTING AUTOCORRELATIONS\n",
    "leiden_res = 0.4\n",
    "\n",
    "def extract_peak_features(autocorr, num_peaks=4):\n",
    "    # Find peaks\n",
    "    peaks, _ = find_peaks(autocorr)\n",
    "\n",
    "    # Initialize a fixed-length array filled with placeholders\n",
    "    features = np.full(num_peaks * 4, -1.0) # 4 features per peak\n",
    "\n",
    "    # If there are peaks, extract their features\n",
    "    if len(peaks) > 0:\n",
    "        # Sort peaks by height and select the top ones\n",
    "        sorted_peaks = sorted(peaks, key=lambda x: autocorr[x], reverse=True)[:num_peaks]\n",
    "\n",
    "        # if any two peaks are < 50 apart then remove the one with the lower height\n",
    "        if len(sorted_peaks) > 1:\n",
    "            for i in range(len(sorted_peaks)-1):\n",
    "                if sorted_peaks[i+1] - sorted_peaks[i] < 50:\n",
    "                    if autocorr[sorted_peaks[i+1]] > autocorr[sorted_peaks[i]]:\n",
    "                        sorted_peaks[i] = -1\n",
    "                    else:\n",
    "                        sorted_peaks[i+1] = -1\n",
    "            sorted_peaks = [x for x in sorted_peaks if x != -1]\n",
    "\n",
    "\n",
    "        # Extract peak heights\n",
    "        peak_heights = autocorr[sorted_peaks]\n",
    "\n",
    "        # Extract peak prominences\n",
    "        prominences = peak_prominences(autocorr, sorted_peaks)[0]\n",
    "\n",
    "        # Extract peak widths\n",
    "        widths = peak_widths(autocorr, sorted_peaks)[0]\n",
    "\n",
    "        # Fill the features array with actual values\n",
    "        for i, peak in enumerate(sorted_peaks):\n",
    "            features[i * 4] = peak                    # Peak position\n",
    "            features[i * 4 + 1] = peak_heights[i]     # Height\n",
    "            features[i * 4 + 2] = prominences[i]      # Prominence\n",
    "            features[i * 4 + 3] = widths[i]           # Width\n",
    "\n",
    "    return features\n",
    "\n",
    "# Apply the function to all autocorrelograms\n",
    "expanded_peak_features = np.array([extract_peak_features(autocorr) for autocorr in autocorrelation_data])\n",
    "# Check the shape of the expanded_peak_features array\n",
    "import scanpy as sc\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "# Convert autocorrelation data to a DataFrame for ease of handling\n",
    "autocorr_df = pd.DataFrame(autocorrelation_data)\n",
    "\n",
    "# Compute the distance matrix on expanded peak features\n",
    "#distance_matrix_expanded = squareform(pdist(expanded_peak_features, metric='euclidean'))\n",
    "\n",
    "# Compute the distance matrix (Euclidean distance is used here, modify if needed)\n",
    "distance_matrix_expanded = squareform(pdist(autocorr_df, metric='correlation'))\n",
    "\n",
    "# Convert the distance matrix to a similarity matrix for expanded peak features\n",
    "similarity_matrix_expanded = 1 / (1 + distance_matrix_expanded)\n",
    "\n",
    "# Create an AnnData object with the expanded peak features similarity matrix\n",
    "adata_expanded = sc.AnnData(similarity_matrix_expanded)\n",
    "adata_expanded.obs_names = [f'Read_{i}' for i in range(adata_expanded.shape[0])]\n",
    "adata_expanded.var_names = adata_expanded.obs_names\n",
    "\n",
    "# Computing the neighborhood graph on the expanded peak features\n",
    "sc.pp.neighbors(adata_expanded, use_rep='X', metric='correlation')\n",
    "\n",
    "### DEFINE CLUSTERING RESOLUTION HERE: ###\n",
    "# Applying Leiden clustering on the expanded peak features\n",
    "sc.tl.leiden(adata_expanded, resolution=leiden_res)\n",
    "\n",
    "# Determine the cluster for each read\n",
    "# Reset the index of cluster_labels to align with autocorr_df\n",
    "cluster_labels = adata_expanded.obs['leiden'].astype(int).reset_index(drop=True)\n",
    "\n",
    "# Ensure that the lengths match\n",
    "if len(cluster_labels) != len(autocorr_df):\n",
    "    raise ValueError(\"Mismatch in length between autocorrelation data and cluster labels\")\n",
    "\n",
    "# Add all cluster labels to the DataFrame\n",
    "autocorr_df['cluster'] = cluster_labels\n",
    "autocorr_df_plotting = autocorr_df.copy()\n",
    "autocorr_df_plotting['read_id'] = [result[0] for result in results if len(result[2])>1]\n",
    "\n",
    "# Count the number of reads in each cluster\n",
    "cluster_counts = cluster_labels.value_counts()\n",
    "print(\"cluster_counts = \", cluster_counts)\n",
    "\n",
    "# Total number of reads\n",
    "total_reads = len(cluster_labels)\n",
    "\n",
    "# Calculate the threshold for 5% of the total dataset\n",
    "threshold_count = 0.05 * total_reads\n",
    "\n",
    "# Filter clusters that are less than 5% of total\n",
    "significant_clusters = cluster_counts[cluster_counts >= threshold_count].index\n",
    "\n",
    "print(\"Significant clusters:\", significant_clusters)\n",
    "\n",
    "# Filter autocorrelations based on significant clusters\n",
    "filtered_df = autocorr_df[autocorr_df['cluster'].isin(significant_clusters)]\n",
    "filtered_df_plotting = autocorr_df_plotting[autocorr_df_plotting['cluster'].isin(significant_clusters)]\n",
    "\n",
    "# Group the filtered autocorrelation data by cluster\n",
    "grouped_by_cluster = filtered_df.groupby('cluster')\n",
    "\n",
    "# Calculate the representative autocorrelogram for each cluster\n",
    "representative_autocorrs = grouped_by_cluster.mean()\n",
    "\n",
    "### Plot unique read plots:\n",
    "# Extract the unique cluster names\n",
    "unique_clusters = filtered_df_plotting['cluster'].unique()\n",
    "\n",
    "# representative_reads_tuple = autocorr_df_plotting.groupby('cluster')['read_id'].nth(1)\n",
    "# Plot one figure from one read for each unique cluster_name\n",
    "#for cluster, rep_read_id in representative_reads_tuple.iteritems():\n",
    "# choose 10 reads from cluster == 5 as representative_reads\n",
    "representative_reads_list = autocorr_df_plotting[autocorr_df_plotting['cluster'] == 3]['read_id'].unique()[25:35]\n",
    "for rep_read_id in representative_reads_list:\n",
    "    # Loop through the results to find the figure for the representative read\n",
    "    for result in results:\n",
    "        if result[0] == rep_read_id:\n",
    "            print(\"read_id\", rep_read_id)\n",
    "            # Display the figure for the representative read of this cluster\n",
    "            disp_fig = result[3]\n",
    "            # set x range for disp fig to 0-600\n",
    "            disp_fig.update_xaxes(range=[0, 1000])\n",
    "            # set width to 500\n",
    "            disp_fig.update_layout(width=800, height = 500)\n",
    "            #disp_fig.show()\n",
    "            #if rep_read_id == \"0b9e1dda-a4ae-4841-869b-533dd829136a\":\n",
    "                # save png and svg to images_11_14_23/raw_read_extrapolation_smoothing_strong_rex\n",
    "                #disp_fig.write_image(\"images_11_14_23/raw_read_extrap_smooth_strong_rex\" + rep_read_id[0:6] + \".png\")\n",
    "                #disp_fig.write_image(\"images_11_14_23/raw_read_extrap_smooth_strong_rex\" + rep_read_id[0:6] + \".svg\")\n",
    "            break\n",
    "\n",
    "\"\"\"def determine_cluster_name2(autocorr, cluster_id, prominence_threshold=0.1):\n",
    "    # Find peaks and their prominences\n",
    "    peaks, _ = find_peaks(autocorr, prominence=prominence_threshold,width=25)\n",
    "    prominences = peak_prominences(autocorr, peaks)[0]\n",
    "    print(\"peaks = \", peaks)\n",
    "    print(\"prominences = \", prominences)\n",
    "\n",
    "    # Check if there are any prominent peaks\n",
    "    if len(peaks) > 0 and any(prominences >= prominence_threshold):\n",
    "        # Find the position of the first prominent peak\n",
    "        first_prominent_peak = peaks[np.argmax(prominences >= prominence_threshold)]\n",
    "        return f'NRL-{first_prominent_peak + 100}-C{cluster_id}'\n",
    "    else:\n",
    "        # For clusters without prominent peaks, use 'NP' followed by the cluster id\n",
    "        return f'NP{cluster_id}'\"\"\"\n",
    "\n",
    "def determine_cluster_name(autocorr, cluster_id, prominence_threshold=0.06, distance_threshold=50):\n",
    "    global corr_start\n",
    "    # Find peaks in the autocorrelation signal\n",
    "    peaks, properties = find_peaks(autocorr, prominence=prominence_threshold, width=25)\n",
    "    prominences = properties[\"prominences\"]\n",
    "\n",
    "    # Filter peaks based on prominence to ensure they are significant\n",
    "    significant_peaks = peaks[prominences >= prominence_threshold]\n",
    "    print(\"peaks = \", peaks)\n",
    "    print(\"prominences = \", prominences)\n",
    "    print(\"significant_peaks = \", significant_peaks)\n",
    "\n",
    "    # If there are enough significant peaks, calculate peak-to-peak distances including the first peak\n",
    "    if len(significant_peaks) > 1:\n",
    "        # if any two peaks are < 50 apart then remove the one with the lower height\n",
    "        for i in range(len(significant_peaks)-1):\n",
    "            if significant_peaks[i+1] - significant_peaks[i] < distance_threshold:\n",
    "                if autocorr[significant_peaks[i+1]] > autocorr[significant_peaks[i]]:\n",
    "                    significant_peaks[i] = -1\n",
    "                else:\n",
    "                    significant_peaks[i+1] = -1\n",
    "        significant_peaks = [x for x in significant_peaks if x != -1]\n",
    "\n",
    "        # Calculate distances between consecutive significant peaks\n",
    "        peak_distances = np.diff(significant_peaks)\n",
    "\n",
    "        # Include the distance from start (0 + 100) to the first significant peak\n",
    "        #peak_distances_with_first = np.insert(peak_distances, 0, significant_peaks[0] + corr_start)\n",
    "        print(\"peak_distances_with_first = \", peak_distances)\n",
    "        if len(peak_distances) > 0:\n",
    "            average_distance = round(np.mean(peak_distances))\n",
    "            return f'NRL-{average_distance:.1f}-C{cluster_id}'\n",
    "        # Calculate the average distance if we have enough valid peak distances\n",
    "        \"\"\"if len(significant_peaks) == 1:\n",
    "            return f'NRL-{significant_peaks[0]:.1f}-C{cluster_id}'\"\"\"\n",
    "    else:\n",
    "        # If not enough valid peak distances, consider it as no pattern found\n",
    "        return f'NP{cluster_id}'\n",
    "\n",
    "# Apply the function to each cluster's average autocorrelogram to determine names\n",
    "cluster_name_mapping = {cluster_id: determine_cluster_name(autocorr, cluster_id)\n",
    "                        for cluster_id, autocorr in representative_autocorrs.iterrows()}\n",
    "\n",
    "\n",
    "\n",
    "# Update the index of representative_autocorrs with new cluster names\n",
    "representative_autocorrs.index = representative_autocorrs.index.map(cluster_name_mapping)\n",
    "\n",
    "# Sort representative_autocorrs by index (cluster names)\n",
    "#representative_autocorrs_sorted = representative_autocorrs.sort_index(ascending=False)\n",
    "# Calculate the range for each row\n",
    "representative_autocorrs['range'] = representative_autocorrs.apply(lambda row: row.iloc[220:].max() - row.iloc[220:].min(), axis=1)\n",
    "\n",
    "# Sort the DataFrame based on the range\n",
    "representative_autocorrs_sorted = representative_autocorrs.sort_values(by='range', ascending=True)\n",
    "\n",
    "# Drop the 'range' column if you want to revert back to the original columns\n",
    "representative_autocorrs_sorted.drop(columns=['range'], inplace=True)\n",
    "\n",
    "# Prepare data for the heatmap\n",
    "heatmap_data = representative_autocorrs_sorted.values\n",
    "heatmap_labels = representative_autocorrs_sorted.index.to_list()\n",
    "\n",
    "# Create the heatmap\n",
    "fig_heatmap = go.Figure(go.Heatmap(\n",
    "    z=heatmap_data,\n",
    "    x=list(range(len(heatmap_data[0]))),\n",
    "    y=heatmap_labels,\n",
    "    colorscale='Inferno'\n",
    "))\n",
    "fig_heatmap.update_layout(\n",
    "    title='Heatmap of Representative Autocorrelograms for Each Cluster',\n",
    "    xaxis_title='Lag',\n",
    "    yaxis_title='Cluster',\n",
    "    yaxis={'type': 'category'},\n",
    "    width=800,\n",
    "    height=600,\n",
    "    template='plotly_white'\n",
    ")\n",
    "fig_heatmap.update_xaxes(tickmode='array', tickvals=list(range(0, crr_length-corr_start, 100)), ticktext=list(range(corr_start, crr_length+1, 100)))\n",
    "\n",
    "\n",
    "#display(autocorr_df.head(10))\n",
    "\n",
    "### PRINT PER-READ HEATMAP\n",
    "print(\"Number of rows after filtering:\", len(filtered_df))\n",
    "# Sort the DataFrame based on the cluster labels\n",
    "sorted_df = filtered_df.sort_values(by='cluster')\n",
    "# Step 1: Map the index of representative_autocorrs_sorted to full cluster names\n",
    "unique_clusters = representative_autocorrs_sorted.index.map(cluster_name_mapping).unique()\n",
    "\n",
    "\n",
    "# Step 2: Create a dictionary for sorting order based on full cluster names\n",
    "sort_order = {name: i for i, name in enumerate(unique_clusters)}\n",
    "\n",
    "# Step 3: Map the 'cluster' column to the full cluster names using cluster_name_mapping\n",
    "sorted_df['full_cluster_name'] = sorted_df['cluster'].map(cluster_name_mapping)\n",
    "\n",
    "# Step 4: Sort 'sorted_df' based on the order in 'representative_autocorrs_sorted'\n",
    "sorted_df['sort_order'] = sorted_df['full_cluster_name'].map(sort_order)\n",
    "sorted_df = sorted_df.sort_values(by='sort_order')\n",
    "\n",
    "# Optional: Remove the 'sort_order' column if it's no longer needed\n",
    "sorted_df.drop(columns=['sort_order'], inplace=True)\n",
    "\n",
    "# Check if we have more than 500 rows\n",
    "if sorted_df.shape[0] > 500:\n",
    "    # Randomly sample 500 rows from sorted_df\n",
    "    sampled_df = sorted_df.sample(n=500, random_state=np.random.RandomState())\n",
    "    sampled_df = sampled_df.sort_values(by='full_cluster_name')\n",
    "    # reset index\n",
    "    sampled_df.reset_index(inplace=True, drop=True)\n",
    "else:\n",
    "    # If we have 500 rows or less, just use the entire dataframe\n",
    "    sampled_df = sorted_df\n",
    "\n",
    "# Use the 'read_id' as the y-axis labels for the sampled dataframe\n",
    "y_labels_sampled = [f'{row[\"full_cluster_name\"]}_{row.name}' for index, row in sampled_df.iterrows()]\n",
    "\n",
    "# Extract the autocorrelation data for the sampled dataframe\n",
    "z_data_sampled = sampled_df.drop(['cluster', 'full_cluster_name'], axis=1).values\n",
    "\n",
    "# Create the heatmap with the sampled data\n",
    "read_fig = go.Figure(go.Heatmap(\n",
    "    z=z_data_sampled,\n",
    "    x=list(range(z_data_sampled.shape[1])),  # Use the number of columns for the x-axis\n",
    "    y=y_labels_sampled,\n",
    "    colorscale='Inferno'\n",
    "))\n",
    "\n",
    "read_fig.update_layout(\n",
    "    title='Random Per-Read Autocorrelation Heatmaps Grouped by Full Cluster Name',\n",
    "    xaxis_title='Lag',\n",
    "    yaxis_title='Read ID with Cluster Name',\n",
    "    yaxis={'type': 'category'},\n",
    "    width=800,\n",
    "    height=800,\n",
    "    template='plotly_white',\n",
    ")\n",
    "# Relabel x-axis labels to start from 100\n",
    "read_fig.update_xaxes(tickmode='array', tickvals=list(range(0, z_data_sampled.shape[1], 100)), ticktext=list(range(corr_start, z_data_sampled.shape[1]+corr_start, 100)))\n",
    "\n",
    "\n",
    "# Assign colors from 'prism' scheme to each cluster_name\n",
    "colors_scheme = plotly.colors.qualitative.Prism\n",
    "cluster_colors = {cluster_name: colors_scheme[i % len(colors_scheme)] for i, cluster_name in enumerate(representative_autocorrs_sorted.index)}\n",
    "\n",
    "# Create scatter plots for each cluster with updated cluster names\n",
    "fig_scatter = go.Figure()\n",
    "\n",
    "for cluster_name, autocorr in representative_autocorrs_sorted.iterrows():\n",
    "    fig_scatter.add_trace(go.Scatter(\n",
    "        x=list(range(len(autocorr))),\n",
    "        y=autocorr,\n",
    "        mode='markers+lines',\n",
    "        # reduce line width\n",
    "        line=dict(width=3, color=cluster_colors[cluster_name]),\n",
    "        # reduce marker size\n",
    "        marker=dict(size=3, color=cluster_colors[cluster_name]),\n",
    "        name=cluster_name,  # Updated cluster name\n",
    "    ))\n",
    "\n",
    "fig_scatter.update_layout(\n",
    "    title='Scatter Plots of Representative Autocorrelograms for Each Cluster',\n",
    "    xaxis_title='Lag',\n",
    "    yaxis_title='Autocorrelation',\n",
    "    width=800,\n",
    "    height=600,\n",
    "    template='plotly_white'\n",
    ")\n",
    "fig_scatter.update_xaxes(tickmode='array', tickvals=list(range(0, crr_length-corr_start, 100)), ticktext=list(range(corr_start, crr_length+1, 100)))\n",
    "\n",
    "# Replace this with your actual DataFrame containing 'condition' and 'cluster' information\n",
    "# For example, if you have a DataFrame 'metadata_df' with these columns, use that\n",
    "# Extract conditions and clusters\n",
    "\"\"\"conditions = [metadata['condition'] for _, metadata, _, _ in results\n",
    "chr_types = [metadata['chr_type'] for _, metadata, _ , _ in results]\"\"\"\n",
    "clusters = adata_expanded.obs['leiden'].astype(int).tolist()\n",
    "\n",
    "\"\"\"for read_id, metadata, autocorr, read_fig in results:\n",
    "    #if autocorrs has nan values skip\n",
    "    if len(autocorr) < 100:\n",
    "        continue\"\"\"\n",
    "\n",
    "# Create DataFrame for plotting\n",
    "metadata_df = pd.DataFrame({\n",
    "    'condition': conditions_list,\n",
    "    'chr_type': chr_types_list,\n",
    "    'type': types_list,\n",
    "    'cluster': clusters\n",
    "})\n",
    "\n",
    "# Update the cluster names in metadata_df with new cluster names\n",
    "metadata_df['cluster'] = metadata_df['cluster'].map(cluster_name_mapping)\n",
    "metadata_df_sorted = metadata_df.sort_values(by=['type', 'condition','cluster'])\n",
    "\n",
    "# Create a new column that combines 'condition' and 'chr_type'\n",
    "metadata_df_sorted['combined'] = metadata_df_sorted['type'] + ', '+ metadata_df_sorted['condition']  +  ', ' + metadata_df_sorted['chr_type']\n",
    "\n",
    "# Now group by this new combined column and cluster, then count the number of reads\n",
    "cluster_counts_by_combined = metadata_df_sorted.groupby(['combined', 'cluster']).size().unstack(fill_value=0)\n",
    "\n",
    "# Sort the index of the resulting DataFrame to ensure the rows are ordered by 'type' and then by 'condition'\n",
    "cluster_counts_by_combined = cluster_counts_by_combined.sort_index(key=lambda x: [tuple(i.split(', ')[1:]) for i in x])\n",
    "\n",
    "\n",
    "# Calculate the percentage of reads in each cluster for each unique combination\n",
    "cluster_percentages_by_combined = cluster_counts_by_combined.div(cluster_counts_by_combined.sum(axis=1), axis=0) * 100\n",
    "\n",
    "# Sort the combined column to ensure the order of N2 Fiber, X, etc.\n",
    "sorted_combinations = sorted(cluster_percentages_by_combined.index, key=lambda x: (x.split(', ')[0], x.split(', ')[1]))\n",
    "\n",
    "nanotools.display_sample_rows(cluster_percentages_by_combined)\n",
    "\n",
    "# Plotting with sorted combinations\n",
    "fig_stacked = go.Figure()\n",
    "\n",
    "for cluster_name in cluster_percentages_by_combined.columns:\n",
    "    fig_stacked.add_trace(go.Bar(\n",
    "        x=sorted_combinations,\n",
    "        y=cluster_percentages_by_combined.loc[sorted_combinations, cluster_name],\n",
    "        name=cluster_name,\n",
    "        # set colorscheme to prism\n",
    "        marker=dict(color=cluster_colors[cluster_name]),\n",
    "        # Add % data labels\n",
    "        text=cluster_percentages_by_combined.loc[sorted_combinations, cluster_name].round(1),\n",
    "    ))\n",
    "\n",
    "fig_stacked.update_layout(\n",
    "    barmode='stack',\n",
    "    title='Percentage of Reads in Each Cluster by Condition and Chr_Type',\n",
    "    xaxis_title='Condition, Chr_Type',\n",
    "    yaxis_title='Percentage of Reads',\n",
    "    yaxis=dict(type='linear', ticksuffix='%'),\n",
    "    legend_title='Clusters',\n",
    "    template='plotly_white',\n",
    "    width=800,\n",
    "    height=600\n",
    ")\n",
    "fig_heatmap.show()\n",
    "fig_scatter.show()\n",
    "fig_stacked.show()\n",
    "read_fig.show()\n",
    "\n",
    "# Save all figures as svgs and pngs to images_11_14_23/\n",
    "\"\"\"fig_heatmap.write_image(\"images_11_14_23/average_autocorrelograms_heatmap_2000bp_10gs_0p5res.png\")\n",
    "fig_scatter.write_image(\"images_11_14_23/average_autocorrelograms_scatter_2000bp_10gs_0p5res.png\")\n",
    "fig_stacked.write_image(\"images_11_14_23/average_autocorrelograms_stacked_box_2000bp_10gs_0p5res.png\")\n",
    "read_fig.write_image(\"images_11_14_23/per_read_500_autocorrelograms_heatmap_2000bp_10gs_0p5res.png\")\n",
    "fig_heatmap.write_image(\"images_11_14_23/average_autocorrelograms_heatmap_2000bp_10gs_0p5res.svg\")\n",
    "fig_scatter.write_image(\"images_11_14_23/average_autocorrelograms_scatter_2000bp_10gs_0p5res.svg\")\n",
    "fig_stacked.write_image(\"images_11_14_23/average_autocorrelograms_stacked_box_2000bp_10gs_0p5res.svg\")\n",
    "read_fig.write_image(\"images_11_14_23/per_read_500_autocorrelograms_heatmap_2000bp_10gs_0p5res.svg\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f29526",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Drop rows where 'combined' column contains 'weak_rex'\n",
    "cluster_percentages_by_combined = cluster_percentages_by_combined[~cluster_percentages_by_combined.index.to_series().str.contains('strong_rex')]\n",
    "\n",
    "# Create a bar plot\n",
    "fig = go.Figure()\n",
    "\n",
    "condition_colors = {\n",
    "    'N2_fiber': 'rgba(0, 77, 153, 0.8)',  # Darker Blue\n",
    "    'SDC2_degron_fiber': 'rgba(204, 0, 0, 0.8)'  # More Vivid Red\n",
    "}\n",
    "\n",
    "# Extract clusters\n",
    "clusters = cluster_percentages_by_combined.columns\n",
    "clusters_reversed = clusters[::-1]  # Reverse the cluster order\n",
    "\n",
    "# Add bars for each condition in each cluster\n",
    "for condition in ['N2_fiber', 'SDC2_degron_fiber']:\n",
    "    condition_df = cluster_percentages_by_combined[cluster_percentages_by_combined.index.to_series().str.contains(condition)]\n",
    "    fig.add_trace(go.Bar(\n",
    "        name=condition,\n",
    "        x=clusters_reversed,\n",
    "        y=[condition_df[cluster].mean() for cluster in clusters_reversed],  # Use mean percentage for each cluster\n",
    "        marker_color=condition_colors[condition]\n",
    "    ))\n",
    "\n",
    "# Update layout to group bars\n",
    "fig.update_layout(\n",
    "    title='Bar Plot of Cluster Percentages by Condition (STRONG REX)',\n",
    "    xaxis_title='Cluster',\n",
    "    yaxis_title='Percentage of Reads',\n",
    "    yaxis=dict(type='linear', ticksuffix='%'),\n",
    "    barmode='group',  # Group bars by cluster\n",
    "    template='plotly_white',\n",
    "    width=800,\n",
    "    height=600\n",
    ")\n",
    "fig.update_xaxes(showgrid=False)  # Remove gridlines from X-axis\n",
    "fig.update_yaxes(showgrid=False)  # Remove gridlines from Y-axis\n",
    "\n",
    "# Display the figure\n",
    "fig.show()\n",
    "\n",
    "fig.write_image(\"images_11_14_23/average_autocorrelograms_bar_plot_2000bp_10gs_0p5res_WEAKrex.png\")\n",
    "fig.write_image(\"images_11_14_23/average_autocorrelograms_bar_plot_2000bp_10gs_0p5res_WEAKrex.svg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5863a01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculating Nucleosome, Met Accessible Domain regions and midpoints\n",
    "# Simple Algo\n",
    "import multiprocessing\n",
    "metadata_cols = ['chrom', 'chr_type', 'condition', 'bed_start','type', 'read_id', 'rel_read_start','rel_read_end']\n",
    "MAD_dist_max = 65 # Distance below which m6A marks are combined into MAD\n",
    "NUC_max_width  = 170 # Distance below which m6A marks are combined into NUC\n",
    "NUC_min_width  = 110 # Distance above which m6A marks are combined into NUC\n",
    "\n",
    "def calculate_midpoints_for_group(read_id,group,MAD_dist_max,NUC_max_width ,NUC_min_width ,metadata_cols):\n",
    "    # if read_id %2000 == 0 then print progress message\n",
    "    #iter=0\n",
    "    #if iter % 5000 == 0:\n",
    "    #    print(\"Processing read:\", iter, sep='\\n')\n",
    "    #if group.empty:\n",
    "    #    print(f\"Warning: Empty group for read_id: {read_id}. Skipping.\")\n",
    "    #    return read_id, [], [], None  # Return None to indicate empty group for later checks\n",
    "\n",
    "    regions_MAD_list = []\n",
    "    regions_NUC_list = []\n",
    "    #print(\"Processing read:\", read_id, sep='\\n')\n",
    "    \"\"\"print(\"group:\",group.head(10))\"\"\"\n",
    "\n",
    "    min_position = min(group['rel_pos'])\n",
    "\n",
    "    MAD_start, MAD_end = min_position, min_position\n",
    "    NUC_start, NUC_end = min_position, min_position\n",
    "\n",
    "    for i in range(len(group['rel_pos'])):\n",
    "        x = group.iloc[i]['rel_pos']\n",
    "        '''if iter % 2000 == 0:\n",
    "            print(\"x: \", x)'''\n",
    "        \"\"\"if read_id == \"0037677b-c871-4a92-943a-2062ebecaf2f\" and x > 200:\n",
    "            print(\"MAD_start: \", MAD_start, \" | MAD_end: \", MAD_end, \" | dist from x:\", x - MAD_end)\n",
    "            print(\"NUC_start: \", NUC_start, \"NUC_end: \", NUC_end, \" | dist from x:\", x - NUC_end)\n",
    "            print(\"regions_NUC_list:\",regions_NUC_list)\n",
    "            print(\"regions_MAD_list:\",regions_MAD_list)\n",
    "            print(\"x: \", x)\"\"\"\n",
    "\n",
    "        # Initialize current state if no MAD_region\n",
    "        if MAD_start is None and NUC_start is None:\n",
    "            MAD_start, MAD_end = x, x\n",
    "            NUC_start, NUC_end = x, x\n",
    "\n",
    "        '''if iter % 2000 == 0:\n",
    "            print(\"MAD_start: \", MAD_start, \" | MAD_end: \", MAD_end, \" | MAD_length: \", MAD_end - MAD_start)\n",
    "            print(\"NUC_start: \", NUC_start, \"NUC_end: \", NUC_end, \" | NUC_length: \", NUC_end - NUC_start)'''\n",
    "\n",
    "        # if x is within MAD_dist_max of MAD_end, extend MAD by setting MAD_end equal to x\n",
    "        if (x - MAD_end) <= MAD_dist_max:\n",
    "            MAD_end = x\n",
    "            # if NUC meets NUC_min_width  and NUC_max_width , add midpoint to midpoints_NUC_list\n",
    "            if NUC_min_width  < (NUC_end - NUC_start) <= NUC_max_width :\n",
    "                regions_NUC_list.append((NUC_start,NUC_end))\n",
    "                '''if iter % 2000 == 0:\n",
    "                    print(\"Appending NUC:\", (NUC_end + NUC_start) / 2)'''\n",
    "            # Reset NUC_start and NUC_end to x regardless of whether it was appended or not.\n",
    "            NUC_start, NUC_end = x, x\n",
    "\n",
    "        # if x is greater than MAD_dist_max from MAD_end, add MAD region to regions_MAD_list and reset MAD_start and MAD_end to x\n",
    "        elif (x - MAD_end) > MAD_dist_max:\n",
    "            if (MAD_end - MAD_start) > 0:\n",
    "                regions_MAD_list.append((MAD_start, MAD_end))\n",
    "                '''if iter % 2000 == 0:\n",
    "                    print(\"Appending MAD:\", (MAD_start, MAD_end))'''\n",
    "                MAD_start, MAD_end = x, x\n",
    "            # if MAD is 0 update to new location\n",
    "            if (MAD_end - MAD_start) == 0:\n",
    "                MAD_start, MAD_end = x, x\n",
    "            # if extended nuc would not be greater than NUC_max_width , extend NUC by setting NUC_end equal to x\n",
    "            if (x - NUC_start) <= NUC_max_width :\n",
    "                NUC_end = x\n",
    "            # else if extended nuc would be greater than NUC_max_width , add midpoint to midpoints_NUC_list and reset NUC_start and NUC_end to x\n",
    "            if (x - NUC_start) > NUC_max_width :\n",
    "                if NUC_min_width  < (NUC_end - NUC_start) <= NUC_max_width :\n",
    "                    regions_NUC_list.append((NUC_start,NUC_end))\n",
    "                    '''if iter % 2000 == 0:\n",
    "                        print(\"Appending NUC:\", (NUC_end + NUC_start) / 2)'''\n",
    "                    # if new position is < MAD dist from end of nuc, update MAD\n",
    "                    if (x-NUC_end) <= MAD_dist_max:\n",
    "                        MAD_start, MAD_end = NUC_end, x\n",
    "                    NUC_start, NUC_end = x, x\n",
    "                MAD_start, MAD_end = x, x\n",
    "    # append last nucs / mads\n",
    "    if (MAD_end - MAD_start) > 0:\n",
    "        regions_MAD_list.append((MAD_start, MAD_end))\n",
    "    if NUC_min_width  < (NUC_end - NUC_start) <= NUC_max_width :\n",
    "        regions_NUC_list.append((NUC_start,NUC_end))\n",
    "    return read_id, regions_MAD_list, regions_NUC_list, group.iloc[0][metadata_cols]\n",
    "\n",
    "def create_dataframe_from_results(results, kind='NUC', metadata_cols=metadata_cols):\n",
    "    # if \"rel_start\" in res[3] == -21 then print res[0:3]\n",
    "    if kind == 'NUC':\n",
    "        df = pd.DataFrame.from_dict({res[0]: [(x[0] + x[1]) / 2 for x in res[2]] for res in results}, orient='index')\n",
    "    elif kind == 'MAD':\n",
    "        df = pd.DataFrame.from_dict({res[0]: [(x[0] + x[1]) / 2 for x in res[1]] for res in results}, orient='index')\n",
    "    elif kind == 'MAD_region':\n",
    "        df = pd.DataFrame.from_dict({res[0]: res[1] for res in results}, orient='index')\n",
    "    elif kind == 'NUC_region':\n",
    "        df = pd.DataFrame.from_dict({res[0]: res[2] for res in results}, orient='index')\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid kind: {kind}\")\n",
    "\n",
    "    # Reset the index to make it a new column\n",
    "    df.reset_index(inplace=True)\n",
    "\n",
    "    # Rename the new column to something meaningful\n",
    "    df.rename(columns={'index': 'read_id'}, inplace=True)\n",
    "    # Adding additional metadata columns to each df.\n",
    "    # Create a new DataFrame for the metadata_cols\n",
    "    metadata_df = pd.DataFrame({cols: [res[3][cols] if res[3] is not None else None for res in results] for cols in metadata_cols})\n",
    "    # Concatenate the new DataFrame and the original DataFrame along axis 1 (columns)\n",
    "    df = pd.merge(metadata_df, df, on='read_id', how='left')\n",
    "\n",
    "    return df\n",
    "\n",
    "# Using multiprocessing to parallelize the calculations\n",
    "print(\"Grouping df...\")\n",
    "# Sort plot_df by read_id then by rel_pos\n",
    "#print(\"Plot_df:\")\n",
    "#display(plot_df.head(10))\n",
    "plot_df.sort_values(by=[\"read_id\",\"rel_pos\"], inplace=True)\n",
    "plot_df.reset_index(inplace=True, drop=True)\n",
    "# grouped = plot_df dropping all rows where mod_qual != 1, then grouped by read_id\n",
    "plot_df_m6a_ony = plot_df[plot_df['mod_qual_bin'] == 1]\n",
    "\n",
    "grouped = plot_df_m6a_ony.groupby('read_id')\n",
    "print(\"Grouped_df...\")\n",
    "display(grouped.head(10))\n",
    "print(\"Calculating nucleosome and MAD positions...\")\n",
    "\n",
    "grouped_data_with_constants = [(read_id,group,MAD_dist_max, NUC_max_width , NUC_min_width ,metadata_cols) for read_id,group in grouped]\n",
    "\n",
    "#processes=multiprocessing.cpu_count()\n",
    "with multiprocessing.Pool(processes=multiprocessing.cpu_count()) as pool:\n",
    "    results = pool.starmap(calculate_midpoints_for_group, grouped_data_with_constants)\n",
    "\n",
    "print(\"creating result dfs...\")\n",
    "midpoint_NUC = create_dataframe_from_results(results, kind='NUC', metadata_cols=metadata_cols)\n",
    "midpoint_MAD = create_dataframe_from_results(results, kind='MAD', metadata_cols=metadata_cols)\n",
    "region_MAD = create_dataframe_from_results(results, kind='MAD_region', metadata_cols=metadata_cols)\n",
    "region_NUC = create_dataframe_from_results(results, kind='NUC_region', metadata_cols=metadata_cols)\n",
    "\n",
    "# drop duplicates based on read_id from all dfs\n",
    "midpoint_NUC.drop_duplicates(subset=['read_id'], keep='first', inplace=True)\n",
    "midpoint_MAD.drop_duplicates(subset=['read_id'], keep='first', inplace=True)\n",
    "region_MAD.drop_duplicates(subset=['read_id'], keep='first', inplace=True)\n",
    "region_NUC.drop_duplicates(subset=['read_id'], keep='first', inplace=True)\n",
    "\n",
    "#print column names\n",
    "print(\"midpoint_MAD columns:\")\n",
    "display(midpoint_MAD.head(10))\n",
    "print(\"midpoint_NUC columns:\")\n",
    "display(midpoint_NUC.head(3))\n",
    "print(\"region_MAD columns:\")\n",
    "display(region_MAD.head(10))\n",
    "print(\"region_NUC columns:\")\n",
    "display(region_NUC.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0f1cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate positive and negative controls\n",
    "# Define percentile\n",
    "percentile = 0.95\n",
    "\n",
    "# Filter condition\n",
    "filtered_df = plot_df.copy()#[plot_df['condition'] == \"N2_bg\"]\n",
    "filtered_df = filtered_df[filtered_df['condition'] == 'N2_fiber']\n",
    "# Add column for average_mod_qual for each read_id\n",
    "filtered_df['avg_mod_qual'] = filtered_df.groupby('read_id')['mod_qual'].transform('mean')\n",
    "percentile_99 = filtered_df['avg_mod_qual'].quantile(percentile)\n",
    "high_methylation_read_ids = filtered_df[filtered_df['avg_mod_qual'] <= percentile_99]['read_id']\n",
    "plot_df_filtered_99 = plot_df[plot_df['read_id'].isin(high_methylation_read_ids)]\n",
    "print(\"reads above threshold:\", len(high_methylation_read_ids),\" | bases above threshold:\", len(plot_df_filtered_99))\n",
    "\n",
    "x_values = np.linspace(0, 1, 1000)\n",
    "kde = gaussian_kde(plot_df_filtered_99[\"mod_qual\"])\n",
    "y_values = kde(x_values)/1000\n",
    "\n",
    "# Create the KDE plot using Plotly\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x_values, y=y_values, mode='lines', name='KDE'))\n",
    "fig.update_layout(title='KDE plot of mod_qual',\n",
    "                  xaxis_title='mod_qual',\n",
    "                  yaxis_title='Density')\n",
    "# set theme to plotly white\n",
    "fig.update_layout(template=\"plotly_white\")\n",
    "fig.show()\n",
    "\n",
    "output_fn = f\"temp_files/N2-intergenic_0p1-neg-ctrl-{str(percentile)}.txt\"\n",
    "# output y_values list to txt file\n",
    "with open(output_fn, 'w') as f:\n",
    "    for item in y_values:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87981f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### MATRIX BASED NUCLEOSOME POSITIONS\n",
    "import multiprocessing\n",
    "output_file = \"temp_files/nucleosomes.csv\"\n",
    "NUC_width  = 147 # Distance below which m6A marks are combined into NUC\n",
    "\n",
    "emission_NEG_fn = \"temp_files/N2-intergenic_0p1-neg-ctrl-0.01.txt\"\n",
    "emission_PGC_fn = \"temp_files/N2-sdc2-intergenic_0p1-pos-ctrl-0.95.txt\"\n",
    "\n",
    "# Assume emission_NEG_array and emission_PGC_array are defined\n",
    "if emission_NEG_fn is None:\n",
    "    print(\"Using default Negative Control array...\")\n",
    "    emission_NEG_array = np.array([0.00149399264776291,\n",
    "\t0.00152416739428112,\n",
    "\t0.00155428407897689,\n",
    "\t0.00158366982352413,\n",
    "\t0.00161263012922332,\n",
    "\t0.00164116736093195,\n",
    "\t0.00166881741930283,\n",
    "\t0.00169644480709635,\n",
    "\t0.00172243991332119,\n",
    "\t0.00174843501954603,\n",
    "\t0.00177314930158316,\n",
    "\t0.00179732634903045,\n",
    "\t0.0018208293765571,\n",
    "\t0.00184330135380909,\n",
    "\t0.00186562176698358,\n",
    "\t0.00188633353415362,\n",
    "\t0.00190704530132367,\n",
    "\t0.00192636133771002,\n",
    "\t0.00194527658490408,\n",
    "\t0.00196334324282506,\n",
    "\t0.0019804441422428,\n",
    "\t0.00199721573026683,\n",
    "\t0.00201223912200322,\n",
    "\t0.00202726251373961,\n",
    "\t0.00204062362531173,\n",
    "\t0.00205368981620955,\n",
    "\t0.00206583124459128,\n",
    "\t0.00207718528459883,\n",
    "\t0.00208815849360906,\n",
    "\t0.00209786334175532,\n",
    "\t0.00210756818990159,\n",
    "\t0.00211581913836465,\n",
    "\t0.00212394661339389,\n",
    "\t0.00213115648509963,\n",
    "\t0.00213778589208315,\n",
    "\t0.00214392113051369,\n",
    "\t0.00214892137536451,\n",
    "\t0.00215392162021533,\n",
    "\t0.00215755004456053,\n",
    "\t0.00216117036760352,\n",
    "\t0.00216398797335146,\n",
    "\t0.00216643632914367,\n",
    "\t0.00216847696309653,\n",
    "\t0.00216984018420171,\n",
    "\t0.00217113675922387,\n",
    "\t0.00217149832890489,\n",
    "\t0.0021718598985859,\n",
    "\t0.00217152301775043,\n",
    "\t0.00217096234398462,\n",
    "\t0.00216995779524117,\n",
    "\t0.00216840664063006,\n",
    "\t0.00216674524343967,\n",
    "\t0.00216440065280018,\n",
    "\t0.00216205606216068,\n",
    "\t0.00215914191711877,\n",
    "\t0.00215611090596126,\n",
    "\t0.00215274778308761,\n",
    "\t0.00214907876784382,\n",
    "\t0.00214528404933982,\n",
    "\t0.00214102027144815,\n",
    "\t0.00213675649355648,\n",
    "\t0.00213199079662317,\n",
    "\t0.00212717066161593,\n",
    "\t0.00212198067237151,\n",
    "\t0.00211653678978936,\n",
    "\t0.00211095201976393,\n",
    "\t0.00210501163039892,\n",
    "\t0.00209907124103391,\n",
    "\t0.00209269337479204,\n",
    "\t0.00208630408847105,\n",
    "\t0.00207963303317977,\n",
    "\t0.00207282026467108,\n",
    "\t0.00206586580125198,\n",
    "\t0.00205865519703331,\n",
    "\t0.00205142714861267,\n",
    "\t0.00204384512826049,\n",
    "\t0.00203626310790831,\n",
    "\t0.00202837761686597,\n",
    "\t0.00202038426090988,\n",
    "\t0.00201226433018618,\n",
    "\t0.00200397563310567,\n",
    "\t0.00199565703906218,\n",
    "\t0.00198711789365238,\n",
    "\t0.00197857874824259,\n",
    "\t0.00196987018727312,\n",
    "\t0.00196112193231988,\n",
    "\t0.0019522914291573,\n",
    "\t0.00194337899906746,\n",
    "\t0.00193444440342173,\n",
    "\t0.00192541639579044,\n",
    "\t0.00191638838815915,\n",
    "\t0.00190731276271764,\n",
    "\t0.00189823080231246,\n",
    "\t0.00188916972807923,\n",
    "\t0.0018801241937282,\n",
    "\t0.0018711050764496,\n",
    "\t0.00186215953560976,\n",
    "\t0.00185321399476991,\n",
    "\t0.00184442692402003,\n",
    "\t0.00183564732214734,\n",
    "\t0.00182701881316729,\n",
    "\t0.00181847317634955,\n",
    "\t0.00181003266704129,\n",
    "\t0.00180179935671974,\n",
    "\t0.00179357963248821,\n",
    "\t0.00178584221445295,\n",
    "\t0.00177810479641768,\n",
    "\t0.00177072403798445,\n",
    "\t0.00176348322228335,\n",
    "\t0.00175647482276216,\n",
    "\t0.00174980225906073,\n",
    "\t0.0017431934656268,\n",
    "\t0.00173715985499785,\n",
    "\t0.0017311262443689,\n",
    "\t0.00172565269607944,\n",
    "\t0.00172032748113237,\n",
    "\t0.00171539371441344,\n",
    "\t0.00171088162972888,\n",
    "\t0.00170655046389971,\n",
    "\t0.00170308894211409,\n",
    "\t0.00169962742032846,\n",
    "\t0.0016969800459273,\n",
    "\t0.0016944619012697,\n",
    "\t0.00169249675040448,\n",
    "\t0.00169097712103983,\n",
    "\t0.00168971428709704,\n",
    "\t0.00168924443741698,\n",
    "\t0.00168877458773692,\n",
    "\t0.00168933081369971,\n",
    "\t0.00168995787752069,\n",
    "\t0.0016913461472447,\n",
    "\t0.00169318876174207,\n",
    "\t0.00169548047964238,\n",
    "\t0.00169873971042928,\n",
    "\t0.0017020085047235,\n",
    "\t0.00170648947357022,\n",
    "\t0.00171097044241694,\n",
    "\t0.00171631727885466,\n",
    "\t0.00172203762680224,\n",
    "\t0.00172824495561917,\n",
    "\t0.00173521579213078,\n",
    "\t0.00174228730888025,\n",
    "\t0.00175051296494183,\n",
    "\t0.00175873862100341,\n",
    "\t0.00176801038609194,\n",
    "\t0.00177759279776968,\n",
    "\t0.00178783967310848,\n",
    "\t0.00179886081754385,\n",
    "\t0.00181006443736322,\n",
    "\t0.00182228102950662,\n",
    "\t0.00183449762165001,\n",
    "\t0.00184768975197826,\n",
    "\t0.00186106295667957,\n",
    "\t0.00187502883980271,\n",
    "\t0.00188951141693461,\n",
    "\t0.00190423112833026,\n",
    "\t0.00191976740640802,\n",
    "\t0.00193530368448579,\n",
    "\t0.00195183130849742,\n",
    "\t0.00196845001771239,\n",
    "\t0.00198567721797348,\n",
    "\t0.00200329895597738,\n",
    "\t0.00202114813398883,\n",
    "\t0.00203953448938654,\n",
    "\t0.00205792084478426,\n",
    "\t0.00207696436002779,\n",
    "\t0.00209601568311108,\n",
    "\t0.00211544651033297,\n",
    "\t0.00213505669974141,\n",
    "\t0.0021548321436499,\n",
    "\t0.00217488913226217,\n",
    "\t0.00219496718641124,\n",
    "\t0.00221537141661406,\n",
    "\t0.00223577564681688,\n",
    "\t0.002256299317718,\n",
    "\t0.00227686249114174,\n",
    "\t0.00229742175345622,\n",
    "\t0.00231797608361963,\n",
    "\t0.00233851210455439,\n",
    "\t0.00235892884624954,\n",
    "\t0.00237934558794468,\n",
    "\t0.00239954336575385,\n",
    "\t0.00241969433418121,\n",
    "\t0.00243964324843439,\n",
    "\t0.00245940163251569,\n",
    "\t0.00247903240997627,\n",
    "\t0.00249816977517,\n",
    "\t0.00251730714036373,\n",
    "\t0.00253575320236407,\n",
    "\t0.00255411926639662,\n",
    "\t0.00257200383160063,\n",
    "\t0.00258954974262857,\n",
    "\t0.0026068402979811,\n",
    "\t0.00262346728823345,\n",
    "\t0.00264009427848579,\n",
    "\t0.00265574328491529,\n",
    "\t0.00267136067974647,\n",
    "\t0.00268625812992732,\n",
    "\t0.00270078385794448,\n",
    "\t0.00271482710676313,\n",
    "\t0.00272797547796219,\n",
    "\t0.00274106811104646,\n",
    "\t0.00275286018811893,\n",
    "\t0.00276465226519139,\n",
    "\t0.00277549804233091,\n",
    "\t0.00278599718019019,\n",
    "\t0.00279593860426456,\n",
    "\t0.00280511833590122,\n",
    "\t0.00281414646296176,\n",
    "\t0.00282199050600528,\n",
    "\t0.00282983454904881,\n",
    "\t0.00283659926587619,\n",
    "\t0.00284310137291861,\n",
    "\t0.00284881712314759,\n",
    "\t0.00285373097591462,\n",
    "\t0.0028583866666396,\n",
    "\t0.00286191187936988,\n",
    "\t0.00286543709210015,\n",
    "\t0.00286784905596031,\n",
    "\t0.00287010446919811,\n",
    "\t0.00287166224046996,\n",
    "\t0.00287268836045052,\n",
    "\t0.00287340899169051,\n",
    "\t0.00287325249995569,\n",
    "\t0.00287309600822087,\n",
    "\t0.00287186670592455,\n",
    "\t0.00287057989213421,\n",
    "\t0.00286847444583565,\n",
    "\t0.00286590836228474,\n",
    "\t0.00286300801757572,\n",
    "\t0.00285943123904376,\n",
    "\t0.00285583510643775,\n",
    "\t0.00285135924319256,\n",
    "\t0.00284688337994738,\n",
    "\t0.00284181504428998,\n",
    "\t0.00283650744092764,\n",
    "\t0.00283089171678661,\n",
    "\t0.00282481979440526,\n",
    "\t0.00281868236584968,\n",
    "\t0.00281191307565515,\n",
    "\t0.00280514378546061,\n",
    "\t0.00279778159178072,\n",
    "\t0.00279025675624698,\n",
    "\t0.00278247440790956,\n",
    "\t0.00277440805368165,\n",
    "\t0.00276626310802112,\n",
    "\t0.00275772428059235,\n",
    "\t0.00274918545316358,\n",
    "\t0.0027402863069457,\n",
    "\t0.00273132710759631,\n",
    "\t0.00272216384304706,\n",
    "\t0.00271283222470792,\n",
    "\t0.00270342209370996,\n",
    "\t0.00269376162316871,\n",
    "\t0.00268410115262745,\n",
    "\t0.00267411397844846,\n",
    "\t0.00266410204357272,\n",
    "\t0.0026539405715072,\n",
    "\t0.00264368758456844,\n",
    "\t0.00263336631929775,\n",
    "\t0.00262289389696513,\n",
    "\t0.00261242107840481,\n",
    "\t0.00260174618371062,\n",
    "\t0.00259107128901644,\n",
    "\t0.00258026477300697,\n",
    "\t0.00256939988257235,\n",
    "\t0.00255846378400356,\n",
    "\t0.00254741324379054,\n",
    "\t0.00253634629093877,\n",
    "\t0.00252507504392566,\n",
    "\t0.00251380379691255,\n",
    "\t0.0025023930007641,\n",
    "\t0.00249093937623652,\n",
    "\t0.00247939968750435,\n",
    "\t0.00246775731272549,\n",
    "\t0.0024560860411578,\n",
    "\t0.00244424678292431,\n",
    "\t0.00243240752469082,\n",
    "\t0.00242039539324634,\n",
    "\t0.00240834973732403,\n",
    "\t0.00239618425532223,\n",
    "\t0.00238391181744946,\n",
    "\t0.00237157772447086,\n",
    "\t0.00235902398517773,\n",
    "\t0.00234647024588459,\n",
    "\t0.0023336939501436,\n",
    "\t0.00232089563806583,\n",
    "\t0.00230794559982686,\n",
    "\t0.00229489474029509,\n",
    "\t0.00228176844467743,\n",
    "\t0.00226845887481082,\n",
    "\t0.00225514930494422,\n",
    "\t0.0022415815317423,\n",
    "\t0.00222800912939691,\n",
    "\t0.00221424733004706,\n",
    "\t0.0022003935850584,\n",
    "\t0.00218642661016831,\n",
    "\t0.0021722617873322,\n",
    "\t0.00215808312729033,\n",
    "\t0.00214366577544926,\n",
    "\t0.00212924842360819,\n",
    "\t0.00211465082044989,\n",
    "\t0.00209999171421747,\n",
    "\t0.00208523301609069,\n",
    "\t0.00207034569704204,\n",
    "\t0.00205543141202063,\n",
    "\t0.00204033209893997,\n",
    "\t0.00202523278585931,\n",
    "\t0.00200996428509849,\n",
    "\t0.0019946581421026,\n",
    "\t0.00197925654419814,\n",
    "\t0.00196376279559351,\n",
    "\t0.00194824325308297,\n",
    "\t0.0019326202820674,\n",
    "\t0.00191699731105183,\n",
    "\t0.00190128344073455,\n",
    "\t0.00188555838281092,\n",
    "\t0.00186979031776118,\n",
    "\t0.00185399126441182,\n",
    "\t0.00183817992156937,\n",
    "\t0.00182233568886051,\n",
    "\t0.00180649145615164,\n",
    "\t0.00179063475186534,\n",
    "\t0.00177477756595309,\n",
    "\t0.00175894100514134,\n",
    "\t0.00174311537439783,\n",
    "\t0.0017273109446994,\n",
    "\t0.00171154686926291,\n",
    "\t0.0016957859228771,\n",
    "\t0.00168011067771277,\n",
    "\t0.00166443543254843,\n",
    "\t0.00164884368014338,\n",
    "\t0.00163328343443994,\n",
    "\t0.00161778151875516,\n",
    "\t0.00160236121032654,\n",
    "\t0.00158696118087024,\n",
    "\t0.0015717292825299,\n",
    "\t0.00155649738418957,\n",
    "\t0.00154144106989098,\n",
    "\t0.00152642908195586,\n",
    "\t0.00151152299297459,\n",
    "\t0.00149672746250112,\n",
    "\t0.0014819738214615,\n",
    "\t0.00146741095947384,\n",
    "\t0.00145284809748619,\n",
    "\t0.00143850054783584,\n",
    "\t0.00142418493215013,\n",
    "\t0.00141001546650868,\n",
    "\t0.00139596006919445,\n",
    "\t0.00138198396196036,\n",
    "\t0.00136824265170775,\n",
    "\t0.00135450134145514,\n",
    "\t0.00134105764081501,\n",
    "\t0.00132763184614742,\n",
    "\t0.00131439181724396,\n",
    "\t0.00130125900500067,\n",
    "\t0.00128822313563168,\n",
    "\t0.00127538874392315,\n",
    "\t0.001262559093001,\n",
    "\t0.00125002751902218,\n",
    "\t0.00123749594504335,\n",
    "\t0.00122518071851831,\n",
    "\t0.00121295538705716,\n",
    "\t0.00120087371641448,\n",
    "\t0.00118901001391015,\n",
    "\t0.00117717598865721,\n",
    "\t0.00116564928782962,\n",
    "\t0.00115412258700203,\n",
    "\t0.00114283794089724,\n",
    "\t0.00113162201887202,\n",
    "\t0.00112055184536474,\n",
    "\t0.00110964624622561,\n",
    "\t0.00109879029004084,\n",
    "\t0.00108819405126365,\n",
    "\t0.00107759781248645,\n",
    "\t0.00106726369661379,\n",
    "\t0.00105697537692674,\n",
    "\t0.00104688426775375,\n",
    "\t0.00103695975518832,\n",
    "\t0.0010271099969022,\n",
    "\t0.00101750648800675,\n",
    "\t0.00100790297911129,\n",
    "\t0.000998574902724826,\n",
    "\t0.000989269584532959,\n",
    "\t0.000980145224832513,\n",
    "\t0.000971134396414901,\n",
    "\t0.000962212227150721,\n",
    "\t0.000953491814293388,\n",
    "\t0.000944771401436055,\n",
    "\t0.000936335842153333,\n",
    "\t0.000927901402138094,\n",
    "\t0.000919699878608294,\n",
    "\t0.000911604527835387,\n",
    "\t0.000903616376577226,\n",
    "\t0.000895804854571337,\n",
    "\t0.000888011547514706,\n",
    "\t0.000880465963767979,\n",
    "\t0.000872920380021253,\n",
    "\t0.000865571953731993,\n",
    "\t0.000858286028389301,\n",
    "\t0.0008511140273685,\n",
    "\t0.000844081212149661,\n",
    "\t0.000837083102316501,\n",
    "\t0.000830296598932136,\n",
    "\t0.00082351009554777,\n",
    "\t0.000816966812248717,\n",
    "\t0.00081047274524928,\n",
    "\t0.000804099870408216,\n",
    "\t0.000797837747951584,\n",
    "\t0.000791622424112246,\n",
    "\t0.00078557969700825,\n",
    "\t0.000779536969904255,\n",
    "\t0.000773685769414869,\n",
    "\t0.000767854882354304,\n",
    "\t0.000762145512890653,\n",
    "\t0.000756518887317006,\n",
    "\t0.000750948457352519,\n",
    "\t0.000745518516410936,\n",
    "\t0.000740088575469351,\n",
    "\t0.000734887391392363,\n",
    "\t0.000729691708482459,\n",
    "\t0.000724613960722743,\n",
    "\t0.000719595007612682,\n",
    "\t0.000714636779849552,\n",
    "\t0.000709787393145405,\n",
    "\t0.000704945959474296,\n",
    "\t0.000700259132765475,\n",
    "\t0.000695572306056653,\n",
    "\t0.000691000669700496,\n",
    "\t0.00068646956328324,\n",
    "\t0.000682003915709478,\n",
    "\t0.000677624852333247,\n",
    "\t0.000673266799812099,\n",
    "\t0.000669060906547919,\n",
    "\t0.00066485501328374,\n",
    "\t0.000660756608283642,\n",
    "\t0.000656683068038609,\n",
    "\t0.000652673186708866,\n",
    "\t0.000648726221194235,\n",
    "\t0.000644802695524232,\n",
    "\t0.000640976717490456,\n",
    "\t0.00063715073945668,\n",
    "\t0.000633427003179422,\n",
    "\t0.000629716612618168,\n",
    "\t0.000626072256016885,\n",
    "\t0.000622476638767007,\n",
    "\t0.000618914414036679,\n",
    "\t0.000615444264268344,\n",
    "\t0.000611974114500009,\n",
    "\t0.000608597190210564,\n",
    "\t0.000605224460114442,\n",
    "\t0.000601911931742057,\n",
    "\t0.000598632141467172,\n",
    "\t0.000595382346985485,\n",
    "\t0.00059219115700149,\n",
    "\t0.000589002444417624,\n",
    "\t0.00058589565119799,\n",
    "\t0.000582788857978357,\n",
    "\t0.000579743349561872,\n",
    "\t0.000576721655620523,\n",
    "\t0.000573736587747257,\n",
    "\t0.000570804017238244,\n",
    "\t0.000567878586911705,\n",
    "\t0.000565016182426628,\n",
    "\t0.00056215377794155,\n",
    "\t0.000559344191342501,\n",
    "\t0.000556548428683992,\n",
    "\t0.00055378323086729,\n",
    "\t0.000551050701545474,\n",
    "\t0.0005483286120001,\n",
    "\t0.000545656023417182,\n",
    "\t0.000542983434834264,\n",
    "\t0.000540363805116811,\n",
    "\t0.000537752442733559,\n",
    "\t0.00053517517832031,\n",
    "\t0.000532625168270409,\n",
    "\t0.000530087240315178,\n",
    "\t0.000527586229871436,\n",
    "\t0.000525085219427695,\n",
    "\t0.000522627681576383,\n",
    "\t0.000520173047948365,\n",
    "\t0.000517745979727691,\n",
    "\t0.000515335227480211,\n",
    "\t0.000512937717693019,\n",
    "\t0.000510568480152332,\n",
    "\t0.000508199664723461,\n",
    "\t0.000505873567010415,\n",
    "\t0.000503547469297369,\n",
    "\t0.000501250589220506,\n",
    "\t0.000498966196010502,\n",
    "\t0.000496695162962353,\n",
    "\t0.000494444904965888,\n",
    "\t0.00049219731869335,\n",
    "\t0.000489979566671334,\n",
    "\t0.000487761814649318,\n",
    "\t0.000485568031671005,\n",
    "\t0.000483381287702995,\n",
    "\t0.0004812082906857,\n",
    "\t0.000479051186767545,\n",
    "\t0.000476899003524202,\n",
    "\t0.000474773728274767,\n",
    "\t0.000472648453025332,\n",
    "\t0.000470548777374678,\n",
    "\t0.000468453783132135,\n",
    "\t0.000466372655635534,\n",
    "\t0.000464303522368087,\n",
    "\t0.000462240098070154,\n",
    "\t0.000460196113797724,\n",
    "\t0.000458152129525293,\n",
    "\t0.000456130665525532,\n",
    "\t0.000454111218266608,\n",
    "\t0.000452106390033155,\n",
    "\t0.000450110963102256,\n",
    "\t0.000448123632972984,\n",
    "\t0.000446155248301112,\n",
    "\t0.000444186863629241,\n",
    "\t0.000442243842531968,\n",
    "\t0.000440301072062903,\n",
    "\t0.000438373921295375,\n",
    "\t0.000436454086652417,\n",
    "\t0.000434542734007763,\n",
    "\t0.000432645711476032,\n",
    "\t0.000430750112587146,\n",
    "\t0.000428875823852097,\n",
    "\t0.000427001535117047,\n",
    "\t0.000425144345785697,\n",
    "\t0.00042329275262283,\n",
    "\t0.000421452959951471,\n",
    "\t0.000419627930890632,\n",
    "\t0.000417806234963531,\n",
    "\t0.000416005890414868,\n",
    "\t0.000414205545866205,\n",
    "\t0.000412424037570675,\n",
    "\t0.000410646501849858,\n",
    "\t0.000408880780328978,\n",
    "\t0.000407126112245158,\n",
    "\t0.000405376199837444,\n",
    "\t0.000403644457701639,\n",
    "\t0.000401912715565835,\n",
    "\t0.000400201615548831,\n",
    "\t0.000398492854072518,\n",
    "\t0.000396800259167048,\n",
    "\t0.000395118943264493,\n",
    "\t0.000393444494351909,\n",
    "\t0.000391787717133798,\n",
    "\t0.000390130939915688,\n",
    "\t0.000388496551523359,\n",
    "\t0.000386862840212455,\n",
    "\t0.000385244372316799,\n",
    "\t0.000383633706524213,\n",
    "\t0.000382031144945843,\n",
    "\t0.000380443486116947,\n",
    "\t0.000378856815347661,\n",
    "\t0.000377292106448783,\n",
    "\t0.000375727397549905,\n",
    "\t0.000374183018515518,\n",
    "\t0.000372646012445319,\n",
    "\t0.000371119054035018,\n",
    "\t0.000369605708583288,\n",
    "\t0.000368094967065863,\n",
    "\t0.000366604174333278,\n",
    "\t0.000365113381600693,\n",
    "\t0.000363640651466014,\n",
    "\t0.000362172261615784,\n",
    "\t0.000360714926551738,\n",
    "\t0.000359268776842218,\n",
    "\t0.000357826772517435,\n",
    "\t0.000356402688345427,\n",
    "\t0.000354978604173419,\n",
    "\t0.000353578176775674,\n",
    "\t0.000352181016014909,\n",
    "\t0.000350796387518862,\n",
    "\t0.000349421233587207,\n",
    "\t0.000348051632342441,\n",
    "\t0.000346697812418305,\n",
    "\t0.000345343992494169,\n",
    "\t0.000344010261141992,\n",
    "\t0.00034267756315257,\n",
    "\t0.000341358284997707,\n",
    "\t0.000340046494128588,\n",
    "\t0.000338741586524437,\n",
    "\t0.000337450485940016,\n",
    "\t0.000336159986650359,\n",
    "\t0.000334894491201272,\n",
    "\t0.000333628995752185,\n",
    "\t0.000332377860385073,\n",
    "\t0.000331132469050751,\n",
    "\t0.000329895133393405,\n",
    "\t0.000328669628295482,\n",
    "\t0.000327446009242417,\n",
    "\t0.000326240172898006,\n",
    "\t0.000325034336553596,\n",
    "\t0.000323843802632659,\n",
    "\t0.000322657417876296,\n",
    "\t0.000321480409969967,\n",
    "\t0.000320313662797077,\n",
    "\t0.000319150848565898,\n",
    "\t0.000318007470383873,\n",
    "\t0.000316864092201848,\n",
    "\t0.000315736736668329,\n",
    "\t0.000314612008995157,\n",
    "\t0.000313497416614113,\n",
    "\t0.000312391120023778,\n",
    "\t0.000311289206730967,\n",
    "\t0.00031020112042547,\n",
    "\t0.000309113034119972,\n",
    "\t0.000308041703754552,\n",
    "\t0.000306971605443548,\n",
    "\t0.000305913023897305,\n",
    "\t0.000304861432085648,\n",
    "\t0.000303816465382589,\n",
    "\t0.000302786032511515,\n",
    "\t0.000301755666774007,\n",
    "\t0.000300742386528938,\n",
    "\t0.000299729106283869,\n",
    "\t0.000298727582240846,\n",
    "\t0.000297731224303511,\n",
    "\t0.000296741301278296,\n",
    "\t0.000295761634915037,\n",
    "\t0.000294783224857443,\n",
    "\t0.000293820019319691,\n",
    "\t0.000292856813781938,\n",
    "\t0.000291906837801576,\n",
    "\t0.000290960877936851,\n",
    "\t0.000290023596044622,\n",
    "\t0.00028909658679505,\n",
    "\t0.000288171901203492,\n",
    "\t0.000287260515496621,\n",
    "\t0.000286349129789751,\n",
    "\t0.000285450676845293,\n",
    "\t0.00028455469589738,\n",
    "\t0.00028366677093643,\n",
    "\t0.000282785980428877,\n",
    "\t0.000281908502735774,\n",
    "\t0.000281042693185597,\n",
    "\t0.000280176883635421,\n",
    "\t0.00027932567435724,\n",
    "\t0.00027847587497657,\n",
    "\t0.000277636298150485,\n",
    "\t0.000276803458917196,\n",
    "\t0.000275974807062331,\n",
    "\t0.000275156232831541,\n",
    "\t0.000274337658600752,\n",
    "\t0.000273532950404955,\n",
    "\t0.000272728462742518,\n",
    "\t0.000271933368354912,\n",
    "\t0.000271142793629978,\n",
    "\t0.000270357248373163,\n",
    "\t0.000269580417275012,\n",
    "\t0.000268804437281335,\n",
    "\t0.000268042603230979,\n",
    "\t0.000267280769180623,\n",
    "\t0.000266530390658594,\n",
    "\t0.000265783880626498,\n",
    "\t0.000265043132268183,\n",
    "\t0.000264309766054401,\n",
    "\t0.000263578076364993,\n",
    "\t0.000262857690514166,\n",
    "\t0.000262137304663338,\n",
    "\t0.000261427425761381,\n",
    "\t0.000260719853874449,\n",
    "\t0.000260018739978868,\n",
    "\t0.000259323811898758,\n",
    "\t0.000258631710753275,\n",
    "\t0.000257950807562433,\n",
    "\t0.000257269904371592,\n",
    "\t0.000256601265399393,\n",
    "\t0.000255934105664093,\n",
    "\t0.000255273919170365,\n",
    "\t0.000254618716906352,\n",
    "\t0.000253966734398879,\n",
    "\t0.000253323284246238,\n",
    "\t0.000252679834093597,\n",
    "\t0.000252047517582553,\n",
    "\t0.000251415607573636,\n",
    "\t0.000250791120036053,\n",
    "\t0.000250170532065499,\n",
    "\t0.000249554342610917,\n",
    "\t0.000248946453180823,\n",
    "\t0.000248339005770367,\n",
    "\t0.000247743004342121,\n",
    "\t0.000247147002913874,\n",
    "\t0.000246558633187281,\n",
    "\t0.000245973115091144,\n",
    "\t0.000245391893239115,\n",
    "\t0.00024481663393148,\n",
    "\t0.000244242474958107,\n",
    "\t0.000243677256200615,\n",
    "\t0.000243112037443122,\n",
    "\t0.000242554686957331,\n",
    "\t0.000241999298730009,\n",
    "\t0.000241449423858154,\n",
    "\t0.000240905260031805,\n",
    "\t0.00024036294523246,\n",
    "\t0.000239828941113632,\n",
    "\t0.000239294936994804,\n",
    "\t0.000238768967787017,\n",
    "\t0.000238244169586444,\n",
    "\t0.000237724495420592,\n",
    "\t0.000237208788823291,\n",
    "\t0.000236695372210134,\n",
    "\t0.000236188667012042,\n",
    "\t0.00023568196181395,\n",
    "\t0.000235183703228886,\n",
    "\t0.000234685934302548,\n",
    "\t0.000234194909884835,\n",
    "\t0.00023370774540129,\n",
    "\t0.000233223680435969,\n",
    "\t0.00023274600010699,\n",
    "\t0.000232268477733303,\n",
    "\t0.000231799765754802,\n",
    "\t0.000231331053776302,\n",
    "\t0.000230868747713796,\n",
    "\t0.00023040907834047,\n",
    "\t0.000229953056537195,\n",
    "\t0.000229502523969241,\n",
    "\t0.000229052823992294,\n",
    "\t0.00022861154042444,\n",
    "\t0.000228170256856586,\n",
    "\t0.000227737891724723,\n",
    "\t0.00022730803001349,\n",
    "\t0.000226882893428852,\n",
    "\t0.000226463050554506,\n",
    "\t0.000226044793367349,\n",
    "\t0.000225634712976775,\n",
    "\t0.000225224632586202,\n",
    "\t0.000224822999457194,\n",
    "\t0.000224422819412171,\n",
    "\t0.000224028095108463,\n",
    "\t0.000223637943422475,\n",
    "\t0.000223250173711301,\n",
    "\t0.000222870165268062,\n",
    "\t0.000222490156824824,\n",
    "\t0.000222121864192542,\n",
    "\t0.000221754512788202,\n",
    "\t0.000221393658863558,\n",
    "\t0.000221036847815168,\n",
    "\t0.000220683208351109,\n",
    "\t0.000220336720102671,\n",
    "\t0.000219990231854234,\n",
    "\t0.000219654013032453,\n",
    "\t0.000219317814346803,\n",
    "\t0.000218988656646756,\n",
    "\t0.00021866267939191,\n",
    "\t0.000218340543167296,\n",
    "\t0.000218024683265277,\n",
    "\t0.000217709699142918,\n",
    "\t0.00021740627044663,\n",
    "\t0.000217102841750341,\n",
    "\t0.000216806732872727,\n",
    "\t0.000216512919670892,\n",
    "\t0.00021622333370687,\n",
    "\t0.00021593887166747,\n",
    "\t0.000215655703589276,\n",
    "\t0.000215380299278321,\n",
    "\t0.000215104894967367,\n",
    "\t0.000214836774936073,\n",
    "\t0.000214570108340863,\n",
    "\t0.000214307931846458,\n",
    "\t0.000214049826600171,\n",
    "\t0.000213793815724045,\n",
    "\t0.000213545439779141,\n",
    "\t0.000213297063834237,\n",
    "\t0.000213055370049964,\n",
    "\t0.000212814369016686,\n",
    "\t0.00021257753598141,\n",
    "\t0.00021234351805298,\n",
    "\t0.000212111397471346,\n",
    "\t0.000211883975081779,\n",
    "\t0.000211656552692212,\n",
    "\t0.000211435212188537,\n",
    "\t0.000211214005486352,\n",
    "\t0.000210996865091232,\n",
    "\t0.000210781734070072,\n",
    "\t0.000210568949130645,\n",
    "\t0.00021036033358647,\n",
    "\t0.000210151971603062,\n",
    "\t0.000209948339503172,\n",
    "\t0.000209744707403282,\n",
    "\t0.00020954457722148,\n",
    "\t0.00020934566670508,\n",
    "\t0.000209148698349941,\n",
    "\t0.000208954278532204,\n",
    "\t0.000208760387808217,\n",
    "\t0.000208570259528677,\n",
    "\t0.000208380131249136,\n",
    "\t0.000208193566946129,\n",
    "\t0.000208007816531995,\n",
    "\t0.000207824449975262,\n",
    "\t0.000207643421077145,\n",
    "\t0.000207463150993792,\n",
    "\t0.000207285999639121,\n",
    "\t0.00020710884828445,\n",
    "\t0.000206935107171597,\n",
    "\t0.000206761802690145,\n",
    "\t0.000206590714329373,\n",
    "\t0.000206421248619673,\n",
    "\t0.000206252815287235,\n",
    "\t0.000206087200269366,\n",
    "\t0.000205921585251497,\n",
    "\t0.000205760013058788,\n",
    "\t0.000205598614130014,\n",
    "\t0.000205440217173112,\n",
    "\t0.000205283438749239,\n",
    "\t0.000205128027190769,\n",
    "\t0.000204975262952075,\n",
    "\t0.000204822626027799,\n",
    "\t0.000204673927893311,\n",
    "\t0.000204525229758824,\n",
    "\t0.000204379501751994,\n",
    "\t0.00020423491672112,\n",
    "\t0.000204092046723699,\n",
    "\t0.000203951615162467,\n",
    "\t0.000203811664116433,\n",
    "\t0.000203675865447009,\n",
    "\t0.000203540066777586,\n",
    "\t0.000203408108477148,\n",
    "\t0.000203277143375585,\n",
    "\t0.000203148220553568,\n",
    "\t0.000203021463536072,\n",
    "\t0.000202895446517695,\n",
    "\t0.000202772891050758,\n",
    "\t0.000202650335583821,\n",
    "\t0.000202531415746654,\n",
    "\t0.000202413053974553,\n",
    "\t0.000202297027843048,\n",
    "\t0.000202182853833558,\n",
    "\t0.000202069854713078,\n",
    "\t0.000201960408012907,\n",
    "\t0.000201850961312736,\n",
    "\t0.00020174599791707,\n",
    "\t0.000201641324068154,\n",
    "\t0.000201539371328492,\n",
    "\t0.000201439015761654,\n",
    "\t0.000201340085857401,\n",
    "\t0.000201244172446545,\n",
    "\t0.000201148313224282,\n",
    "\t0.000201057014875246,\n",
    "\t0.00020096571652621,\n",
    "\t0.000200877820309825,\n",
    "\t0.000200791364550719,\n",
    "\t0.000200707265618541,\n",
    "\t0.000200626801593565,\n",
    "\t0.000200546875878303,\n",
    "\t0.000200472808975743,\n",
    "\t0.000200398742073183,\n",
    "\t0.000200329639676169,\n",
    "\t0.000200261978991624,\n",
    "\t0.000200197605597393,\n",
    "\t0.000200137002918519,\n",
    "\t0.000200077621070956,\n",
    "\t0.000200024816452072,\n",
    "\t0.000199972011833189,\n",
    "\t0.000199926517229558,\n",
    "\t0.000199882339441608,\n",
    "\t0.000199844327192172,\n",
    "\t0.000199811606095788,\n",
    "\t0.00019978167545251,\n",
    "\t0.000199761141719694,\n",
    "\t0.000199740607986877,\n",
    "\t0.000199731621329111,\n",
    "\t0.000199723641969381,\n",
    "\t0.000199724098440952,\n",
    "\t0.000199729935458768,\n",
    "\t0.000199740333157121,\n",
    "\t0.000199761303342173,\n",
    "\t0.000199782273527224,\n",
    "\t0.000199819612513777,\n",
    "\t0.000199857080642749,\n",
    "\t0.000199909419283246,\n",
    "\t0.000199968660568062,\n",
    "\t0.000200035936954253,\n",
    "\t0.000200116675290393,\n",
    "\t0.000200198819625938,\n",
    "\t0.000200301329649831,\n",
    "\t0.000200403839673724,\n",
    "\t0.00020052387080789,\n",
    "\t0.000200649575876729,\n",
    "\t0.000200786254254552,\n",
    "\t0.00020093655301151,\n",
    "\t0.000201090408014908,\n",
    "\t0.000201266667370873,\n",
    "\t0.000201442926726838,\n",
    "\t0.00020164693178497,\n",
    "\t0.000201856708998635,\n",
    "\t0.000202082315666143,\n",
    "\t0.000202322616845521,\n",
    "\t0.000202569298123725,\n",
    "\t0.00020284006874703,\n",
    "\t0.000203110839370335,\n",
    "\t0.000203409986717346,\n",
    "\t0.000203712280179377,\n",
    "\t0.000204033785043592,\n",
    "\t0.000204368585215941,\n",
    "\t0.000204712803032829,\n",
    "\t0.000205081022763226,\n",
    "\t0.000205449242493624,\n",
    "\t0.000205858843705319,\n",
    "\t0.000206269610592543,\n",
    "\t0.000206704366648947,\n",
    "\t0.000207151294201218,\n",
    "\t0.000207610972814619,\n",
    "\t0.000208093899219027,\n",
    "\t0.000208578467946045,\n",
    "\t0.000209097882474504,\n",
    "\t0.000209617297002964,\n",
    "\t0.000210163855596826,\n",
    "\t0.000210720160065236,\n",
    "\t0.000211292334392631,\n",
    "\t0.000211885838392886,\n",
    "\t0.000212484853947511,\n",
    "\t0.000213125298019156,\n",
    "\t0.000213765742090802,\n",
    "\t0.000214436342965348,\n",
    "\t0.00021511409969143,\n",
    "\t0.000215810317775301,\n",
    "\t0.000216525069614443,\n",
    "\t0.000217246742629908,\n",
    "\t0.000217997955511329,\n",
    "\t0.00021874916839275,\n",
    "\t0.000219531803703943,\n",
    "\t0.000220318698500062,\n",
    "\t0.000221125776118586,\n",
    "\t0.000221947990853914,\n",
    "\t0.000222780702994833,\n",
    "\t0.000223642948881333,\n",
    "\t0.000224505194767833,\n",
    "\t0.000225395531828983,\n",
    "\t0.000226287253260136,\n",
    "\t0.000227196000586105,\n",
    "\t0.000228114166492185,\n",
    "\t0.000229039960001378,\n",
    "\t0.000229980919505062,\n",
    "\t0.000230922349020108,\n",
    "\t0.00023188178358581,\n",
    "\t0.000232841218151512,\n",
    "\t0.000233810417976696,\n",
    "\t0.00023478348655222,\n",
    "\t0.000235758799559412,\n",
    "\t0.000236737382099275,\n",
    "\t0.000237715437531063,\n",
    "\t0.000238688633026399,\n",
    "\t0.000239661828521735,\n",
    "\t0.00024062359010255,\n",
    "\t0.000241582287507762,\n",
    "\t0.000242529187859978,\n",
    "\t0.000243463279983227,\n",
    "\t0.000244391292284552,\n",
    "\t0.000245289674189373,\n",
    "\t0.000246188056094193,\n",
    "\t0.000247041563206147,\n",
    "\t0.000247887829158342,\n",
    "\t0.000248689811368131,\n",
    "\t0.000249455832666534,\n",
    "\t0.000250200947164118,\n",
    "\t0.00025088081221076,\n",
    "\t0.000251560677257402,\n",
    "\t0.000252145254830345,\n",
    "\t0.00025272304042403,\n",
    "\t0.000253226844667207,\n",
    "\t0.000253686120950204,\n",
    "\t0.00025410276062841,\n",
    "\t0.000254426712035328,\n",
    "\t0.000254749680232945,\n",
    "\t0.000254906158322218,\n",
    "\t0.000255062636411492,\n",
    "\t0.000255074786952388,\n",
    "\t0.000255024098251716,\n",
    "\t0.000254897104726594,\n",
    "\t0.000254649487918376,\n",
    "\t0.00025438527813475,\n",
    "\t0.000253925686065691,\n",
    "\t0.000253466093996631,\n",
    "\t0.000252832809555784,\n",
    "\t0.000252147373206868,\n",
    "\t0.000251352324908476,\n",
    "\t0.000250428540873678,\n",
    "\t0.000249463017922746,\n",
    "\t0.000248262239262703,\n",
    "\t0.00024706146060266,\n",
    "\t0.000245608930612002,\n",
    "\t0.000244108977696273,\n",
    "\t0.000242467896104711,\n",
    "\t0.000240702808066369,\n",
    "\t0.000238878433365831,\n",
    "\t0.000236847595464494,\n",
    "\t0.000234816757563156,\n",
    "\t0.00023254494552298,\n",
    "\t0.000230250429281286,\n",
    "\t0.000227799376482782,\n",
    "\t0.000225245992342934,\n",
    "\t0.000222609597455605,\n",
    "\t0.000219775296150444,\n",
    "\t0.000216940994845282,\n",
    "\t0.000213848306558107,\n",
    "\t0.000210752029562849,\n",
    "\t0.00020751151176366,\n",
    "\t0.000204202208609996,\n",
    "\t0.000200821681705679,\n",
    "\t0.000197318786336146,\n",
    "\t0.000193805789483209])\n",
    "else:\n",
    "    # read file into np.array. File has one element on each new line.\n",
    "    print(\"Loading Negative Control array from file:\", emission_NEG_fn)\n",
    "    emission_NEG_array = np.loadtxt(emission_NEG_fn, delimiter='\\n')\n",
    "print(\"emission_NEG_array: \",emission_NEG_array[0:3])\n",
    "if emission_PGC_fn is None:\n",
    "    print(\"Using default Positive Control array...\")\n",
    "    mission_PGC_array = np.array([0.000218394723023875,\n",
    "\t0.000222978465201427,\n",
    "\t0.000227555235612708,\n",
    "\t0.000232042256740602,\n",
    "\t0.000236477039895504,\n",
    "\t0.000240859072758921,\n",
    "\t0.000245130489859589,\n",
    "\t0.000249399045315033,\n",
    "\t0.00025346156231431,\n",
    "\t0.000257524079313588,\n",
    "\t0.000261423437111359,\n",
    "\t0.000265254358688498,\n",
    "\t0.000268998994625546,\n",
    "\t0.000272611639162654,\n",
    "\t0.000276204832147616,\n",
    "\t0.000279591573431396,\n",
    "\t0.000282978314715176,\n",
    "\t0.000286185689607641,\n",
    "\t0.000289341558785724,\n",
    "\t0.000292388346455785,\n",
    "\t0.000295310991154374,\n",
    "\t0.000298191360734618,\n",
    "\t0.000300847306847099,\n",
    "\t0.000303503252959581,\n",
    "\t0.000305946590457699,\n",
    "\t0.000308352207072622,\n",
    "\t0.000310640154361108,\n",
    "\t0.000312827912186866,\n",
    "\t0.000314967507226035,\n",
    "\t0.00031694669569631,\n",
    "\t0.000318925884166584,\n",
    "\t0.000320722603463631,\n",
    "\t0.000322503826440235,\n",
    "\t0.000324170972597796,\n",
    "\t0.000325765955144589,\n",
    "\t0.000327300402987551,\n",
    "\t0.000328695816284937,\n",
    "\t0.000330091229582323,\n",
    "\t0.000331322025577324,\n",
    "\t0.000332551849422903,\n",
    "\t0.000333687359540695,\n",
    "\t0.000334779485343705,\n",
    "\t0.000335824760758471,\n",
    "\t0.000336792196205269,\n",
    "\t0.000337752168342873,\n",
    "\t0.000338607434642682,\n",
    "\t0.000339462700942492,\n",
    "\t0.000340242041926293,\n",
    "\t0.00034099705541866,\n",
    "\t0.000341705810617947,\n",
    "\t0.000342357601456322,\n",
    "\t0.000342998463121428,\n",
    "\t0.000343571594698672,\n",
    "\t0.000344144726275916,\n",
    "\t0.000344663695707856,\n",
    "\t0.000345171551680735,\n",
    "\t0.000345649015912825,\n",
    "\t0.000346098487752084,\n",
    "\t0.000346536881064369,\n",
    "\t0.000346933935058412,\n",
    "\t0.000347330989052454,\n",
    "\t0.000347685332563613,\n",
    "\t0.000348035043701356,\n",
    "\t0.000348354352914425,\n",
    "\t0.0003486527921594,\n",
    "\t0.000348939900488314,\n",
    "\t0.000349198408022205,\n",
    "\t0.000349456915556097,\n",
    "\t0.000349680405036328,\n",
    "\t0.00034990298039066,\n",
    "\t0.000350102932050512,\n",
    "\t0.000350291505322845,\n",
    "\t0.000350468591594087,\n",
    "\t0.000350624912901818,\n",
    "\t0.000350779797406728,\n",
    "\t0.000350905526787732,\n",
    "\t0.000351031256168735,\n",
    "\t0.0003511312685659,\n",
    "\t0.000351222140178359,\n",
    "\t0.000351301922414389,\n",
    "\t0.000351366918815182,\n",
    "\t0.000351429231461231,\n",
    "\t0.00035147174591655,\n",
    "\t0.000351514260371869,\n",
    "\t0.000351541249832706,\n",
    "\t0.000351564601794836,\n",
    "\t0.000351580303151066,\n",
    "\t0.000351588383786576,\n",
    "\t0.000351594400336041,\n",
    "\t0.00035159171823718,\n",
    "\t0.000351589036138319,\n",
    "\t0.000351582688323641,\n",
    "\t0.000351575852830583,\n",
    "\t0.00035157224358863,\n",
    "\t0.000351571034765586,\n",
    "\t0.000351573367225251,\n",
    "\t0.00035158556281305,\n",
    "\t0.000351597758400849,\n",
    "\t0.000351630877968515,\n",
    "\t0.000351664983707363,\n",
    "\t0.000351719192014218,\n",
    "\t0.00035178442627504,\n",
    "\t0.000351863877322186,\n",
    "\t0.000351971348663306,\n",
    "\t0.000352080714295238,\n",
    "\t0.000352257327250995,\n",
    "\t0.000352433940206751,\n",
    "\t0.000352661934264006,\n",
    "\t0.000352910088753456,\n",
    "\t0.000353192630472559,\n",
    "\t0.000353524860915848,\n",
    "\t0.000353866773759944,\n",
    "\t0.000354296018062288,\n",
    "\t0.000354725262364633,\n",
    "\t0.000355241714232557,\n",
    "\t0.000355781263153741,\n",
    "\t0.000356383393728729,\n",
    "\t0.000357052939499887,\n",
    "\t0.00035775241829841,\n",
    "\t0.000358595779717108,\n",
    "\t0.000359439141135807,\n",
    "\t0.000360421662612702,\n",
    "\t0.000361426272987725,\n",
    "\t0.000362527833131959,\n",
    "\t0.000363707501217466,\n",
    "\t0.000364933313022451,\n",
    "\t0.000366301616633184,\n",
    "\t0.000367669920243917,\n",
    "\t0.000369227091459414,\n",
    "\t0.000370797301651808,\n",
    "\t0.00037251128922475,\n",
    "\t0.000374311093921705,\n",
    "\t0.00037619847690961,\n",
    "\t0.000378274531647771,\n",
    "\t0.000380352507520218,\n",
    "\t0.000382673987163385,\n",
    "\t0.000384995466806552,\n",
    "\t0.000387495041689219,\n",
    "\t0.000390071441969319,\n",
    "\t0.000392750388190677,\n",
    "\t0.000395590109958127,\n",
    "\t0.000398451538494382,\n",
    "\t0.000401561800721822,\n",
    "\t0.000404672062949262,\n",
    "\t0.000408013982270395,\n",
    "\t0.000411424693165034,\n",
    "\t0.00041498755249337,\n",
    "\t0.000418727703428767,\n",
    "\t0.000422511009486048,\n",
    "\t0.000426533881796863,\n",
    "\t0.000430556754107679,\n",
    "\t0.000434816908899611,\n",
    "\t0.00043912110684349,\n",
    "\t0.000443573871068032,\n",
    "\t0.000448156154614176,\n",
    "\t0.000452799868440732,\n",
    "\t0.000457655116102447,\n",
    "\t0.000462510363764161,\n",
    "\t0.000467633555107455,\n",
    "\t0.000472781365207816,\n",
    "\t0.000478103726356244,\n",
    "\t0.000483539264151966,\n",
    "\t0.000489044531073141,\n",
    "\t0.000494714487178773,\n",
    "\t0.000500384443284405,\n",
    "\t0.000506270514427137,\n",
    "\t0.00051215915327328,\n",
    "\t0.000518184431539964,\n",
    "\t0.000524274288668231,\n",
    "\t0.000530431498930391,\n",
    "\t0.000536703458976223,\n",
    "\t0.000542986191924638,\n",
    "\t0.000549435731106588,\n",
    "\t0.000555885270288539,\n",
    "\t0.00056244211671957,\n",
    "\t0.000569034452787666,\n",
    "\t0.000575670102389663,\n",
    "\t0.000582360373041047,\n",
    "\t0.00058906029933304,\n",
    "\t0.000595823129283702,\n",
    "\t0.000602585959234365,\n",
    "\t0.000609387289704112,\n",
    "\t0.000616196850688627,\n",
    "\t0.000623017018977834,\n",
    "\t0.000629847189592377,\n",
    "\t0.000636674815598702,\n",
    "\t0.000643492602453251,\n",
    "\t0.000650310389307799,\n",
    "\t0.000657083288111348,\n",
    "\t0.000663850992446114,\n",
    "\t0.000670575539320763,\n",
    "\t0.000677269732115129,\n",
    "\t0.00068393694657477,\n",
    "\t0.000690534055361778,\n",
    "\t0.000697131164148786,\n",
    "\t0.000703612482952332,\n",
    "\t0.000710090059049341,\n",
    "\t0.000716474820420568,\n",
    "\t0.000722811659648513,\n",
    "\t0.000729081101137085,\n",
    "\t0.000735225536762686,\n",
    "\t0.000741361632973395,\n",
    "\t0.000747303142836627,\n",
    "\t0.00075324465269986,\n",
    "\t0.000759037728611016,\n",
    "\t0.000764776431657374,\n",
    "\t0.0007704243176846,\n",
    "\t0.000775948171208844,\n",
    "\t0.000781446480169015,\n",
    "\t0.000786745277275307,\n",
    "\t0.0007920440743816,\n",
    "\t0.000797155130133698,\n",
    "\t0.000802220506724191,\n",
    "\t0.000807143865374689,\n",
    "\t0.000811922399406534,\n",
    "\t0.000816652484961936,\n",
    "\t0.000821170417188913,\n",
    "\t0.000825688349415891,\n",
    "\t0.000829990924892564,\n",
    "\t0.000834263215826226,\n",
    "\t0.000838396723650937,\n",
    "\t0.000842424469175378,\n",
    "\t0.000846389693983552,\n",
    "\t0.000850175408553351,\n",
    "\t0.000853961123123149,\n",
    "\t0.00085752077684141,\n",
    "\t0.000861068311833609,\n",
    "\t0.000864436510307483,\n",
    "\t0.000867703797712943,\n",
    "\t0.000870894638391669,\n",
    "\t0.000873930776226943,\n",
    "\t0.000876962325502774,\n",
    "\t0.000879785303894813,\n",
    "\t0.000882608282286852,\n",
    "\t0.000885285577662803,\n",
    "\t0.000887904039513025,\n",
    "\t0.000890443689823824,\n",
    "\t0.00089286665314664,\n",
    "\t0.000895272124559082,\n",
    "\t0.000897508871919366,\n",
    "\t0.00089974561927965,\n",
    "\t0.000901813936052142,\n",
    "\t0.000903836049920251,\n",
    "\t0.000905779619472603,\n",
    "\t0.000907636564018275,\n",
    "\t0.000909468061593912,\n",
    "\t0.000911172024943254,\n",
    "\t0.000912875988292595,\n",
    "\t0.00091445560642861,\n",
    "\t0.000916014500362404,\n",
    "\t0.000917497882505659,\n",
    "\t0.00091891896742172,\n",
    "\t0.000920308710335504,\n",
    "\t0.000921598518504327,\n",
    "\t0.00092288832667315,\n",
    "\t0.000924033142749313,\n",
    "\t0.000925166969951085,\n",
    "\t0.000926226866763281,\n",
    "\t0.000927241519109932,\n",
    "\t0.00092821979320619,\n",
    "\t0.000929117531804726,\n",
    "\t0.000930015044059764,\n",
    "\t0.000930797121131362,\n",
    "\t0.000931579198202959,\n",
    "\t0.000932281330170592,\n",
    "\t0.000932948006258782,\n",
    "\t0.000933569173117553,\n",
    "\t0.000934117200143328,\n",
    "\t0.000934654305182335,\n",
    "\t0.000935055460228136,\n",
    "\t0.000935456615273937,\n",
    "\t0.000935763390724809,\n",
    "\t0.000936041200571098,\n",
    "\t0.00093626084924554,\n",
    "\t0.000936411103903786,\n",
    "\t0.000936541953841058,\n",
    "\t0.000936559997667074,\n",
    "\t0.000936578041493089,\n",
    "\t0.00093648115659902,\n",
    "\t0.000936361984126056,\n",
    "\t0.00093616416073995,\n",
    "\t0.000935896134131372,\n",
    "\t0.000935588264732996,\n",
    "\t0.000935138455395966,\n",
    "\t0.000934688646058935,\n",
    "\t0.000934096579935242,\n",
    "\t0.000933490441097169,\n",
    "\t0.000932787712683358,\n",
    "\t0.000932020800968407,\n",
    "\t0.000931205878566927,\n",
    "\t0.000930274312752543,\n",
    "\t0.000929342746938158,\n",
    "\t0.000928246243065466,\n",
    "\t0.000927146782135951,\n",
    "\t0.000925925208964196,\n",
    "\t0.000924644354558562,\n",
    "\t0.000923288912725867,\n",
    "\t0.000921803143399651,\n",
    "\t0.000920307974493325,\n",
    "\t0.000918650662830106,\n",
    "\t0.000916993351166887,\n",
    "\t0.0009152094354125,\n",
    "\t0.000913382321411783,\n",
    "\t0.000911482304643578,\n",
    "\t0.000909488135422295,\n",
    "\t0.000907473192949486,\n",
    "\t0.000905315714012363,\n",
    "\t0.00090315823507524,\n",
    "\t0.000900860641103677,\n",
    "\t0.000898531873212969,\n",
    "\t0.000896114147362088,\n",
    "\t0.000893610542865042,\n",
    "\t0.000891078966862419,\n",
    "\t0.000888435230607041,\n",
    "\t0.000885791494351664,\n",
    "\t0.00088303055955132,\n",
    "\t0.000880255200314673,\n",
    "\t0.000877408637360903,\n",
    "\t0.0008745107693719,\n",
    "\t0.000871582203438364,\n",
    "\t0.00086857148185471,\n",
    "\t0.000865560760271057,\n",
    "\t0.000862442235678448,\n",
    "\t0.000859319547961511,\n",
    "\t0.000856131612852902,\n",
    "\t0.000852909100533736,\n",
    "\t0.000849661175678016,\n",
    "\t0.000846364880369195,\n",
    "\t0.000843066372575779,\n",
    "\t0.000839707267287603,\n",
    "\t0.000836348161999428,\n",
    "\t0.000832951387900238,\n",
    "\t0.000829540399155383,\n",
    "\t0.00082611229807238,\n",
    "\t0.000822660255783891,\n",
    "\t0.000819204740267805,\n",
    "\t0.000815720428537459,\n",
    "\t0.000812236116807113,\n",
    "\t0.000808738418615214,\n",
    "\t0.000805237340997972,\n",
    "\t0.00080173458505071,\n",
    "\t0.000798230076926907,\n",
    "\t0.00079472662769909,\n",
    "\t0.000791228001051902,\n",
    "\t0.000787729374404713,\n",
    "\t0.000784243646654687,\n",
    "\t0.00078075983199952,\n",
    "\t0.000777289130977434,\n",
    "\t0.000773828664986046,\n",
    "\t0.000770377756776348,\n",
    "\t0.000766955151455066,\n",
    "\t0.000763532546133784,\n",
    "\t0.000760154654254514,\n",
    "\t0.000756779452603083,\n",
    "\t0.000753436291569518,\n",
    "\t0.000750111623114781,\n",
    "\t0.000746805389943305,\n",
    "\t0.000743537471065353,\n",
    "\t0.000740270531434388,\n",
    "\t0.000737065161957817,\n",
    "\t0.000733859792481245,\n",
    "\t0.000730702415869147,\n",
    "\t0.000727564980890488,\n",
    "\t0.000724461886173006,\n",
    "\t0.00072141089392076,\n",
    "\t0.000718367500589473,\n",
    "\t0.000715402798306326,\n",
    "\t0.00071243809602318,\n",
    "\t0.000709538448109523,\n",
    "\t0.0007066572704063,\n",
    "\t0.000703816785435171,\n",
    "\t0.000701022249340699,\n",
    "\t0.00069824205199385,\n",
    "\t0.000695536870777856,\n",
    "\t0.000692831689561863,\n",
    "\t0.000690204540288889,\n",
    "\t0.000687591024182971,\n",
    "\t0.000685038135088265,\n",
    "\t0.000682536461591478,\n",
    "\t0.000680058425289435,\n",
    "\t0.000677658252687724,\n",
    "\t0.000675258080086013,\n",
    "\t0.000672946596691343,\n",
    "\t0.000670642441430305,\n",
    "\t0.000668397370152596,\n",
    "\t0.000666189367488631,\n",
    "\t0.000664010649210049,\n",
    "\t0.000661898571680254,\n",
    "\t0.000659786494150459,\n",
    "\t0.000657769388226167,\n",
    "\t0.000655752655471249,\n",
    "\t0.00065381422052602,\n",
    "\t0.000651911476890052,\n",
    "\t0.000650044947545228,\n",
    "\t0.000648238087343326,\n",
    "\t0.000646437388905121,\n",
    "\t0.000644720490453201,\n",
    "\t0.000643003592001282,\n",
    "\t0.000641353380258228,\n",
    "\t0.000639724308889376,\n",
    "\t0.00063813372187888,\n",
    "\t0.000636590152714898,\n",
    "\t0.000635058279575077,\n",
    "\t0.000633597719693693,\n",
    "\t0.000632137159812308,\n",
    "\t0.000630758190380054,\n",
    "\t0.000629395731015154,\n",
    "\t0.000628073694572396,\n",
    "\t0.0006267885989274,\n",
    "\t0.000625519040278574,\n",
    "\t0.000624306783211952,\n",
    "\t0.000623094526145331,\n",
    "\t0.00062194557330668,\n",
    "\t0.000620803334552815,\n",
    "\t0.000619701090730542,\n",
    "\t0.000618626080299188,\n",
    "\t0.000617569494336162,\n",
    "\t0.000616558969543957,\n",
    "\t0.000615548444751751,\n",
    "\t0.000614612644878304,\n",
    "\t0.00061367864199689,\n",
    "\t0.000612783096373646,\n",
    "\t0.000611906722990546,\n",
    "\t0.000611050160545483,\n",
    "\t0.000610229106229797,\n",
    "\t0.000609410650973877,\n",
    "\t0.000608642721439824,\n",
    "\t0.00060787479190577,\n",
    "\t0.000607144622295878,\n",
    "\t0.000606427738585229,\n",
    "\t0.000605732412931554,\n",
    "\t0.000605065602707787,\n",
    "\t0.000604405772218546,\n",
    "\t0.000603796488516437,\n",
    "\t0.000603187204814327,\n",
    "\t0.000602614012411541,\n",
    "\t0.000602049168839442,\n",
    "\t0.000601505899700324,\n",
    "\t0.000600983953152558,\n",
    "\t0.000600470029609514,\n",
    "\t0.000599989494732583,\n",
    "\t0.000599508959855652,\n",
    "\t0.000599063783974608,\n",
    "\t0.000598623223537716,\n",
    "\t0.000598205754717613,\n",
    "\t0.0005978053297099,\n",
    "\t0.000597416736030119,\n",
    "\t0.000597060765497211,\n",
    "\t0.000596704794964303,\n",
    "\t0.000596382212131776,\n",
    "\t0.000596061131404379,\n",
    "\t0.000595761745640372,\n",
    "\t0.000595474157741653,\n",
    "\t0.000595197426671371,\n",
    "\t0.000594941907208094,\n",
    "\t0.000594687286349716,\n",
    "\t0.000594462379360029,\n",
    "\t0.000594237472370342,\n",
    "\t0.000594034773035221,\n",
    "\t0.000593840703305,\n",
    "\t0.000593659802593801,\n",
    "\t0.000593497777476533,\n",
    "\t0.00059333828213923,\n",
    "\t0.000593201116975081,\n",
    "\t0.000593063951810931,\n",
    "\t0.000592945168400905,\n",
    "\t0.000592831196017883,\n",
    "\t0.000592727614919201,\n",
    "\t0.000592635140294471,\n",
    "\t0.000592546110701244,\n",
    "\t0.000592473415976495,\n",
    "\t0.000592400721251746,\n",
    "\t0.000592344776058365,\n",
    "\t0.000592291445610832,\n",
    "\t0.000592248202997888,\n",
    "\t0.000592213023548508,\n",
    "\t0.000592181148339416,\n",
    "\t0.000592159369420095,\n",
    "\t0.000592137590500773,\n",
    "\t0.000592126800707829,\n",
    "\t0.000592116745052764,\n",
    "\t0.000592113027440863,\n",
    "\t0.000592113061318685,\n",
    "\t0.000592115808770647,\n",
    "\t0.00059212434962016,\n",
    "\t0.000592132963919072,\n",
    "\t0.000592149011297151,\n",
    "\t0.000592165058675231,\n",
    "\t0.00059218506429714,\n",
    "\t0.000592206761570742,\n",
    "\t0.00059222979406909,\n",
    "\t0.000592254902841916,\n",
    "\t0.000592280199499509,\n",
    "\t0.000592307594203675,\n",
    "\t0.00059233498890784,\n",
    "\t0.000592363361839817,\n",
    "\t0.000592392022048822,\n",
    "\t0.00059242084602359,\n",
    "\t0.000592449859330927,\n",
    "\t0.000592478776580631,\n",
    "\t0.000592507168553147,\n",
    "\t0.000592535560525664,\n",
    "\t0.000592562660181378,\n",
    "\t0.000592589523510639,\n",
    "\t0.000592615391265963,\n",
    "\t0.000592640397886167,\n",
    "\t0.000592664917198194,\n",
    "\t0.000592687777141864,\n",
    "\t0.000592710637085533,\n",
    "\t0.000592731357244762,\n",
    "\t0.000592751885781504,\n",
    "\t0.00059277094518019,\n",
    "\t0.00059278905979556,\n",
    "\t0.000592806361907693,\n",
    "\t0.000592821762868463,\n",
    "\t0.000592837163829234,\n",
    "\t0.000592850237690147,\n",
    "\t0.000592863288556002,\n",
    "\t0.000592875123394168,\n",
    "\t0.000592886388656147,\n",
    "\t0.000592897131039091,\n",
    "\t0.000592906990031666,\n",
    "\t0.000592916788553247,\n",
    "\t0.000592925681899632,\n",
    "\t0.000592934575246018,\n",
    "\t0.000592943115622039,\n",
    "\t0.000592951540480486,\n",
    "\t0.000592960081058767,\n",
    "\t0.000592968766414283,\n",
    "\t0.000592977606857003,\n",
    "\t0.000592987440756889,\n",
    "\t0.000592997274656774,\n",
    "\t0.000593008599023902,\n",
    "\t0.000593020237731278,\n",
    "\t0.000593033169367441,\n",
    "\t0.000593047310675612,\n",
    "\t0.000593062119970463,\n",
    "\t0.000593079481478572,\n",
    "\t0.000593096842986681,\n",
    "\t0.00059311775683079,\n",
    "\t0.000593139073118193,\n",
    "\t0.000593163828504855,\n",
    "\t0.000593190983263093,\n",
    "\t0.000593219902557034,\n",
    "\t0.000593253362754043,\n",
    "\t0.000593286822951052,\n",
    "\t0.00059332685047335,\n",
    "\t0.000593367076604277,\n",
    "\t0.000593412291057477,\n",
    "\t0.000593460058705214,\n",
    "\t0.000593510756564866,\n",
    "\t0.000593566842758649,\n",
    "\t0.000593623320565975,\n",
    "\t0.000593688502874333,\n",
    "\t0.000593753685182692,\n",
    "\t0.000593827851214409,\n",
    "\t0.00059390527534313,\n",
    "\t0.000593987627763663,\n",
    "\t0.000594076657224713,\n",
    "\t0.000594167065531452,\n",
    "\t0.000594268037198389,\n",
    "\t0.000594369008865326,\n",
    "\t0.000594480222438743,\n",
    "\t0.000594593897052795,\n",
    "\t0.000594714260684274,\n",
    "\t0.000594841392337322,\n",
    "\t0.00059497119236053,\n",
    "\t0.000595112527525574,\n",
    "\t0.000595253862690618,\n",
    "\t0.000595411641843845,\n",
    "\t0.000595571691659137,\n",
    "\t0.000595741117782164,\n",
    "\t0.000595917632522721,\n",
    "\t0.000596098527511738,\n",
    "\t0.000596291871627953,\n",
    "\t0.000596485215744169,\n",
    "\t0.000596695200324178,\n",
    "\t0.000596906040895123,\n",
    "\t0.000597128526129575,\n",
    "\t0.000597357508234215,\n",
    "\t0.000597592732321052,\n",
    "\t0.000597840477089824,\n",
    "\t0.000598088797854742,\n",
    "\t0.000598361070459372,\n",
    "\t0.000598633343064003,\n",
    "\t0.000598920079429399,\n",
    "\t0.000599212601299102,\n",
    "\t0.000599513517632457,\n",
    "\t0.000599826762067023,\n",
    "\t0.000600142035138305,\n",
    "\t0.000600476435355776,\n",
    "\t0.000600810835573247,\n",
    "\t0.000601162187075376,\n",
    "\t0.000601518134821354,\n",
    "\t0.000601884764312424,\n",
    "\t0.000602263082434395,\n",
    "\t0.000602646027435377,\n",
    "\t0.000603051837826823,\n",
    "\t0.000603457648218269,\n",
    "\t0.000603882833388092,\n",
    "\t0.000604311196198104,\n",
    "\t0.000604752038390955,\n",
    "\t0.000605203095025277,\n",
    "\t0.000605659637664174,\n",
    "\t0.0006061334857484,\n",
    "\t0.000606607333832627,\n",
    "\t0.000607102463829317,\n",
    "\t0.000607599158672513,\n",
    "\t0.000608110684005642,\n",
    "\t0.000608631210233605,\n",
    "\t0.000609160394462458,\n",
    "\t0.00060970857218076,\n",
    "\t0.00061025683868796,\n",
    "\t0.000610827701969662,\n",
    "\t0.000611398565251363,\n",
    "\t0.000611985103359496,\n",
    "\t0.000612578529560371,\n",
    "\t0.000613180600633589,\n",
    "\t0.000613796450843435,\n",
    "\t0.000614414000924118,\n",
    "\t0.000615052123800565,\n",
    "\t0.000615690246677013,\n",
    "\t0.000616346404411806,\n",
    "\t0.000617008037014311,\n",
    "\t0.000617681610837895,\n",
    "\t0.000618369320209508,\n",
    "\t0.000619060257335116,\n",
    "\t0.000619769669105301,\n",
    "\t0.000620479080875487,\n",
    "\t0.000621206608104756,\n",
    "\t0.000621937597962568,\n",
    "\t0.000622679976443195,\n",
    "\t0.000623432440789044,\n",
    "\t0.000624189636083787,\n",
    "\t0.000624963494366673,\n",
    "\t0.000625737352649559,\n",
    "\t0.000626532326089027,\n",
    "\t0.000627329338545763,\n",
    "\t0.000628141395567509,\n",
    "\t0.000628963368325284,\n",
    "\t0.000629791616267602,\n",
    "\t0.000630634966487383,\n",
    "\t0.000631478316707164,\n",
    "\t0.00063234278601301,\n",
    "\t0.000633207591208892,\n",
    "\t0.000634086951766792,\n",
    "\t0.000634973315774242,\n",
    "\t0.000635867616157656,\n",
    "\t0.000636775667267338,\n",
    "\t0.000637685091154005,\n",
    "\t0.000638617331540917,\n",
    "\t0.000639549571927829,\n",
    "\t0.000640500795643636,\n",
    "\t0.000641458429960038,\n",
    "\t0.000642425853336763,\n",
    "\t0.000643405818947027,\n",
    "\t0.000644388693738128,\n",
    "\t0.000645391183460628,\n",
    "\t0.000646393673183129,\n",
    "\t0.00064741478645544,\n",
    "\t0.00064843998890814,\n",
    "\t0.000649476884714488,\n",
    "\t0.00065052498105115,\n",
    "\t0.000651578325213818,\n",
    "\t0.000652652456881438,\n",
    "\t0.000653726588549059,\n",
    "\t0.000654824140865433,\n",
    "\t0.000655924518040757,\n",
    "\t0.000657038540734793,\n",
    "\t0.000658162316769386,\n",
    "\t0.000659292524636586,\n",
    "\t0.000660439776860197,\n",
    "\t0.000661587029083808,\n",
    "\t0.000662756983452414,\n",
    "\t0.000663927766702582,\n",
    "\t0.000665113998121805,\n",
    "\t0.000666308345594024,\n",
    "\t0.00066751207476046,\n",
    "\t0.000668733507236887,\n",
    "\t0.00066995590878459,\n",
    "\t0.000671203404177973,\n",
    "\t0.000672450899571356,\n",
    "\t0.000673715525707908,\n",
    "\t0.000674986552847955,\n",
    "\t0.000676267427286205,\n",
    "\t0.000677561968301867,\n",
    "\t0.000678859085408484,\n",
    "\t0.000680177133254108,\n",
    "\t0.000681495181099732,\n",
    "\t0.000682832050822501,\n",
    "\t0.000684173614509742,\n",
    "\t0.000685528725696515,\n",
    "\t0.000686897870149738,\n",
    "\t0.000688271695422548,\n",
    "\t0.000689666559217806,\n",
    "\t0.000691061423013064,\n",
    "\t0.000692477140231675,\n",
    "\t0.000693895896626334,\n",
    "\t0.000695328252400697,\n",
    "\t0.000696771138250316,\n",
    "\t0.00069822023839242,\n",
    "\t0.00069968755119173,\n",
    "\t0.00070115486399104,\n",
    "\t0.000702645606567474,\n",
    "\t0.000704137707391858,\n",
    "\t0.000705649025642396,\n",
    "\t0.000707171342173749,\n",
    "\t0.000708702736467397,\n",
    "\t0.000710252829864218,\n",
    "\t0.00071180339575871,\n",
    "\t0.000713380316523326,\n",
    "\t0.000714957237287942,\n",
    "\t0.00071655368033604,\n",
    "\t0.000718158158799163,\n",
    "\t0.000719773953098194,\n",
    "\t0.000721406776620874,\n",
    "\t0.000723042227410737,\n",
    "\t0.00072470423644495,\n",
    "\t0.000726366245479162,\n",
    "\t0.000728056991530223,\n",
    "\t0.000729755804112328,\n",
    "\t0.000731470166148265,\n",
    "\t0.000733201948734139,\n",
    "\t0.000734939036080134,\n",
    "\t0.000736703478092782,\n",
    "\t0.00073846792010543,\n",
    "\t0.000740261067766464,\n",
    "\t0.000742059153325729,\n",
    "\t0.000743876075764799,\n",
    "\t0.000745708785948454,\n",
    "\t0.000747549855339688,\n",
    "\t0.000749418161815614,\n",
    "\t0.000751286468291541,\n",
    "\t0.000753196847242768,\n",
    "\t0.000755110606223725,\n",
    "\t0.000757048307346835,\n",
    "\t0.000759000905802841,\n",
    "\t0.000760965451607507,\n",
    "\t0.000762956936019976,\n",
    "\t0.000764948420432445,\n",
    "\t0.000766979498383766,\n",
    "\t0.000769010653969476,\n",
    "\t0.000771069646335534,\n",
    "\t0.000773141212701806,\n",
    "\t0.000775228384091265,\n",
    "\t0.000777341054410367,\n",
    "\t0.000779457434111872,\n",
    "\t0.000781622757053423,\n",
    "\t0.000783788079994974,\n",
    "\t0.000785985910860532,\n",
    "\t0.00078819393701331,\n",
    "\t0.000790421502281194,\n",
    "\t0.000792672751324963,\n",
    "\t0.000794930246744883,\n",
    "\t0.000797225220421711,\n",
    "\t0.000799520194098538,\n",
    "\t0.000801852028630546,\n",
    "\t0.000804191218028024,\n",
    "\t0.000806554359270955,\n",
    "\t0.000808939218045397,\n",
    "\t0.000811336138235996,\n",
    "\t0.000813777027770949,\n",
    "\t0.000816217917305902,\n",
    "\t0.000818700617387576,\n",
    "\t0.00082118765203997,\n",
    "\t0.0008237026146183,\n",
    "\t0.000826236439992509,\n",
    "\t0.000828783933222733,\n",
    "\t0.000831365270667848,\n",
    "\t0.000833946608112963,\n",
    "\t0.00083657522461193,\n",
    "\t0.000839204881250082,\n",
    "\t0.000841868926497476,\n",
    "\t0.000844549964946513,\n",
    "\t0.000847253165795237,\n",
    "\t0.000849995753082534,\n",
    "\t0.000852741007488514,\n",
    "\t0.000855536013916083,\n",
    "\t0.000858331020343653,\n",
    "\t0.000861166027508755,\n",
    "\t0.000864014966329145,\n",
    "\t0.000866887954210278,\n",
    "\t0.000869792499682431,\n",
    "\t0.000872704127538948,\n",
    "\t0.000875666119017602,\n",
    "\t0.000878628110496256,\n",
    "\t0.000881641763355293,\n",
    "\t0.000884667213885331,\n",
    "\t0.000887730403543006,\n",
    "\t0.000890830600949875,\n",
    "\t0.000893943704542551,\n",
    "\t0.000897109852558889,\n",
    "\t0.000900276000575227,\n",
    "\t0.000903503089578598,\n",
    "\t0.000906737981180751,\n",
    "\t0.000910014263986748,\n",
    "\t0.000913320853572508,\n",
    "\t0.000916647499598504,\n",
    "\t0.000920028898242369,\n",
    "\t0.000923410296886234,\n",
    "\t0.000926873346205817,\n",
    "\t0.000930339894840074,\n",
    "\t0.000933869674649324,\n",
    "\t0.000937433545965573,\n",
    "\t0.000941027278840493,\n",
    "\t0.000944678847033066,\n",
    "\t0.000948333281424669,\n",
    "\t0.000952076388848754,\n",
    "\t0.000955819496272838,\n",
    "\t0.000959631558690689,\n",
    "\t0.000963470156634597,\n",
    "\t0.000967349859834764,\n",
    "\t0.000971288006527242,\n",
    "\t0.000975238125670542,\n",
    "\t0.000979291704860575,\n",
    "\t0.000983345284050607,\n",
    "\t0.000987499440167837,\n",
    "\t0.000991679607559342,\n",
    "\t0.000995915889907207,\n",
    "\t0.00100021168126121,\n",
    "\t0.00100452866883133,\n",
    "\t0.00100894480759017,\n",
    "\t0.00101336094634901,\n",
    "\t0.00101788565644797,\n",
    "\t0.00102242703212508,\n",
    "\t0.00102704108531194,\n",
    "\t0.00103171277048897,\n",
    "\t0.00103642272951668,\n",
    "\t0.00104124841420309,\n",
    "\t0.00104607409888951,\n",
    "\t0.00105105260344727,\n",
    "\t0.00105604097762172,\n",
    "\t0.00106112485716333,\n",
    "\t0.00106626479420305,\n",
    "\t0.00107145552644442,\n",
    "\t0.00107675373389904,\n",
    "\t0.00108205388358284,\n",
    "\t0.00108751750422307,\n",
    "\t0.0010929811248633,\n",
    "\t0.0010985663035624,\n",
    "\t0.00110420294974046,\n",
    "\t0.00110992229276355,\n",
    "\t0.00111576917818447,\n",
    "\t0.00112163425235141,\n",
    "\t0.00112769728775417,\n",
    "\t0.00113376032315694,\n",
    "\t0.00113998384159905,\n",
    "\t0.0011462539649642,\n",
    "\t0.00115262574783751,\n",
    "\t0.00115911414014664,\n",
    "\t0.00116563858159329,\n",
    "\t0.0011723572377683,\n",
    "\t0.0011790758939433,\n",
    "\t0.00118600055168519,\n",
    "\t0.00119296231825438,\n",
    "\t0.00120008808260684,\n",
    "\t0.00120735458687513,\n",
    "\t0.00121469099641726,\n",
    "\t0.0012222628134627,\n",
    "\t0.00122983463050815,\n",
    "\t0.00123768292890811,\n",
    "\t0.00124555534589431,\n",
    "\t0.0012536226889269,\n",
    "\t0.00126181435979035,\n",
    "\t0.00127010824587836,\n",
    "\t0.00127863908544147,\n",
    "\t0.00128716992500457,\n",
    "\t0.00129605834477527,\n",
    "\t0.00130494958569159,\n",
    "\t0.00131415786849641,\n",
    "\t0.00132351331687698,\n",
    "\t0.0013330373841918,\n",
    "\t0.00134284395443312,\n",
    "\t0.00135267989943836,\n",
    "\t0.00136294133344842,\n",
    "\t0.00137320276745847,\n",
    "\t0.00138383095663463,\n",
    "\t0.00139457791367547,\n",
    "\t0.00140555615760454,\n",
    "\t0.00141682148131154,\n",
    "\t0.00142816263080447,\n",
    "\t0.00143998148274867,\n",
    "\t0.00145180033469287,\n",
    "\t0.00146422508859409,\n",
    "\t0.00147677589302065,\n",
    "\t0.00148968451343801,\n",
    "\t0.00150292529511478,\n",
    "\t0.00151631538733128,\n",
    "\t0.00153026923149225,\n",
    "\t0.00154422307565322,\n",
    "\t0.0015588661124482,\n",
    "\t0.00157358555973087,\n",
    "\t0.00158879086911441,\n",
    "\t0.00160433242081279,\n",
    "\t0.00162012273267228,\n",
    "\t0.00163654703744233,\n",
    "\t0.00165297134221239,\n",
    "\t0.00167056129536625,\n",
    "\t0.00168818408368585,\n",
    "\t0.00170653269115108,\n",
    "\t0.00172524956083767,\n",
    "\t0.00174437577825596,\n",
    "\t0.00176424832026792,\n",
    "\t0.00178417680133417,\n",
    "\t0.00180529216320438,\n",
    "\t0.00182640752507459,\n",
    "\t0.00184850498483911,\n",
    "\t0.00187095505953909,\n",
    "\t0.00189401571114457,\n",
    "\t0.00191789700051755,\n",
    "\t0.00194200954848064,\n",
    "\t0.00196786039017938,\n",
    "\t0.00199371123187812,\n",
    "\t0.00202094638583134,\n",
    "\t0.00204851002065852,\n",
    "\t0.00207698294190708,\n",
    "\t0.00210636871540614,\n",
    "\t0.00213612102092552,\n",
    "\t0.00216743770022235,\n",
    "\t0.00219875437951919,\n",
    "\t0.00223186562287322,\n",
    "\t0.002265220129355,\n",
    "\t0.00229982604316085,\n",
    "\t0.00233537051245975,\n",
    "\t0.00237164959623964,\n",
    "\t0.00240999546852203,\n",
    "\t0.00244834134080443,\n",
    "\t0.00248894817437497,\n",
    "\t0.00252966643108909,\n",
    "\t0.00257194806075135,\n",
    "\t0.00261509453502322,\n",
    "\t0.00265906570390398,\n",
    "\t0.00270467661645154,\n",
    "\t0.00275035058369873,\n",
    "\t0.00279844003097806,\n",
    "\t0.0028465294782574,\n",
    "\t0.00289645928223499,\n",
    "\t0.00294711818927611,\n",
    "\t0.00299895420706446,\n",
    "\t0.0030525049582971,\n",
    "\t0.00310627467351356,\n",
    "\t0.00316206323666071,\n",
    "\t0.00321785179980787,\n",
    "\t0.00327526927706371,\n",
    "\t0.0033331232871328,\n",
    "\t0.00339186369933182,\n",
    "\t0.00345156649098618,\n",
    "\t0.00351153936333938,\n",
    "\t0.00357282849105249,\n",
    "\t0.0036341176187656,\n",
    "\t0.00369654540506517,\n",
    "\t0.00375915692945482,\n",
    "\t0.00382224091084585,\n",
    "\t0.00388570855412807,\n",
    "\t0.00394920891838826,\n",
    "\t0.00401281140376521,\n",
    "\t0.00407641388914217,\n",
    "\t0.00413964933120217,\n",
    "\t0.00420285861084546,\n",
    "\t0.00426546685055729,\n",
    "\t0.00432771333582761,\n",
    "\t0.00438946493209569,\n",
    "\t0.00445014068270667,\n",
    "\t0.00451080183169063,\n",
    "\t0.0045689904384967,\n",
    "\t0.00462717904530277,\n",
    "\t0.0046827504948057,\n",
    "\t0.0047371824516626,\n",
    "\t0.00479003628663365,\n",
    "\t0.00484039541377503,\n",
    "\t0.00489038093094737,\n",
    "\t0.00493596719073419,\n",
    "\t0.00498155345052102,\n",
    "\t0.00502294128211577,\n",
    "\t0.00506306851695057,\n",
    "\t0.00510037945365653,\n",
    "\t0.00513438273809201,\n",
    "\t0.00516724582301342,\n",
    "\t0.00519368232885554,\n",
    "\t0.00522011883469766,\n",
    "\t0.00523922479227554,\n",
    "\t0.00525694987913387,\n",
    "\t0.0052703464034534,\n",
    "\t0.00527993952171863,\n",
    "\t0.00528763827575224,\n",
    "\t0.00528873998943537,\n",
    "\t0.0052898417031185,\n",
    "\t0.00528293164700858,\n",
    "\t0.00527526673464406,\n",
    "\t0.00526218784906826,\n",
    "\t0.00524556973181389,\n",
    "\t0.00522594188059144,\n",
    "\t0.00519913850467686,\n",
    "\t0.00517233512876227,\n",
    "\t0.00513561008799325,\n",
    "\t0.00509874724632348,\n",
    "\t0.00505603020799238,\n",
    "\t0.00501052142847885,\n",
    "\t0.00496196266345832,\n",
    "\t0.0049081637637635,\n",
    "\t0.00485390439145447])\n",
    "else:\n",
    "    # read file into np.array. File has one element on each new line.\n",
    "    print(\"Loading Positive Control array from file:\", emission_PGC_fn)\n",
    "    emission_PGC_array = np.loadtxt(emission_PGC_fn, delimiter='\\n')\n",
    "print(\"emission_PGC_array: \",emission_PGC_array[0:3])\n",
    "\n",
    "def single_fiber_nuc(read_id, group, LINK_width,NUC_width, metadata_cols, emission_NEG_array, emission_PGC_array):\n",
    "    # Initialize lists to store information about linker and nucleosome regions and their midpoints\n",
    "    regions_LINK_list = []\n",
    "    regions_NUC_list = []\n",
    "    mid_NUC_list = []\n",
    "    mid_LINK_list = []\n",
    "\n",
    "    # Calculate the total number of modified bases, total bases, and minimum base position in the read\n",
    "    MOD_BASE_NUM = len(group['rel_pos'])\n",
    "    BASE_NUM = max(group['rel_pos']) - min(group['rel_pos']) + 1\n",
    "    BASE_MIN = min(group['rel_pos'])\n",
    "\n",
    "    # Initialize the calling_vec with -1 and populate it with mod_qual values based on relative position\n",
    "    calling_vec = np.full(BASE_NUM+1, -1.0)\n",
    "    for i in range(len(group['rel_pos'])):\n",
    "        calling_vec[group.iloc[i]['rel_pos'] - BASE_MIN] = group.iloc[i]['mod_qual']\n",
    "\n",
    "    # Initialize probability matrix (prob_mat) and pointer matrix (ptr_mat)\n",
    "    # prob_mat stores log probabilities, and ptr_mat stores the previous state index for backtracking\n",
    "    prob_mat = np.zeros((BASE_NUM+1, 148))\n",
    "    ptr_mat = np.full((BASE_NUM+1, 148), -1, dtype=int)\n",
    "\n",
    "    # Initialization of first row in prob_mat and ptr_mat\n",
    "    initial_rate = 1 / 148.0\n",
    "    log_initial_rate = np.log(initial_rate)\n",
    "    prob_mat[1, :] = log_initial_rate\n",
    "    ptr_mat[1, :] = 0\n",
    "\n",
    "    # Dynamic Programming Step: Fill the prob_mat and ptr_mat\n",
    "    for i in range(2, BASE_NUM+1):\n",
    "        within_linker = 0.0\n",
    "        back_frm_ncls = 0.0\n",
    "\n",
    "        # Compute probabilities for staying within linker and going back to linker from nucleosome\n",
    "        if calling_vec[i] == -1:\n",
    "            within_linker = prob_mat[i-1, 0]\n",
    "            if prob_mat[i-1, 147] != 0:\n",
    "                back_frm_ncls = prob_mat[i-1, 147]\n",
    "        else:\n",
    "            k = int(calling_vec[i] * 1000)\n",
    "            within_linker = np.log(emission_PGC_array[k]) + prob_mat[i-1, 0]\n",
    "            if prob_mat[i-1, 147] != 0:\n",
    "                back_frm_ncls = np.log(emission_PGC_array[k]) + prob_mat[i-1, 147]\n",
    "\n",
    "        # Update first column of prob_mat and ptr_mat based on calculated probabilities\n",
    "        if back_frm_ncls != 0 and back_frm_ncls > within_linker:\n",
    "            prob_mat[i, 0] = back_frm_ncls\n",
    "            ptr_mat[i, 0] = 147\n",
    "        else:\n",
    "            prob_mat[i, 0] = within_linker\n",
    "            ptr_mat[i, 0] = 0\n",
    "\n",
    "        # Update second column of prob_mat and ptr_mat\n",
    "        if calling_vec[i] == -1:\n",
    "            prob_mat[i, 1] = prob_mat[i-1, 0]\n",
    "        else:\n",
    "            k = int(calling_vec[i] * 1000)\n",
    "            prob_mat[i, 1] = np.log(emission_NEG_array[k]) + prob_mat[i-1, 0]\n",
    "        ptr_mat[i, 1] = 0\n",
    "\n",
    "        # Update the remaining columns of prob_mat and ptr_mat\n",
    "        for j in range(2, 148):\n",
    "            if calling_vec[i] == -1:\n",
    "                if prob_mat[i-1, j-1] != 0:\n",
    "                    prob_mat[i, j] = prob_mat[i-1, j-1]\n",
    "            else:\n",
    "                k = int(calling_vec[i] * 1000)\n",
    "                if prob_mat[i-1, j-1] != 0:\n",
    "                    prob_mat[i, j] = np.log(emission_NEG_array[k]) + prob_mat[i-1, j-1]\n",
    "            if prob_mat[i, j] != 0:\n",
    "                ptr_mat[i, j] = j-1\n",
    "\n",
    "    # Backtrack to identify most probable states\n",
    "    max_index = np.argmax(prob_mat[BASE_NUM, :])\n",
    "    backtrack_vec = []\n",
    "    for i in range(BASE_NUM, 1, -1):\n",
    "        backtrack_vec.append(max_index)\n",
    "        max_index = ptr_mat[i, max_index]\n",
    "    backtrack_vec.reverse()\n",
    "\n",
    "    # Identify nucleosome and linker regions based on backtracking results\n",
    "    ncls_start = 0\n",
    "    ncls_end = 0\n",
    "    shift = min(group['rel_pos']) - 1\n",
    "    InNucleosome = False\n",
    "    for i, val in enumerate(backtrack_vec):\n",
    "        if val > 0:\n",
    "            if not InNucleosome:\n",
    "                ncls_start = i + 1 + shift\n",
    "                InNucleosome = True\n",
    "        else:\n",
    "            if InNucleosome:\n",
    "                ncls_end = i + 1 + shift\n",
    "                ncls_mid = round((ncls_end-ncls_start)/2 + ncls_start, 0)\n",
    "                if ncls_end - ncls_start < NUC_width:\n",
    "                    ncls_start = ncls_end - NUC_width\n",
    "                    ncls_mid = round((ncls_end-ncls_start)/2 + ncls_start, 0)\n",
    "                regions_NUC_list.append((ncls_start, ncls_end))\n",
    "                mid_NUC_list.append(ncls_mid)\n",
    "                InNucleosome = False\n",
    "\n",
    "    # Check if the nucleosome extends to the end of the read\n",
    "    if InNucleosome:\n",
    "        ncls_end = ncls_start + NUC_width\n",
    "        ncls_mid = round((ncls_end-ncls_start)/2 + ncls_start, 0)\n",
    "        regions_NUC_list.append((ncls_start, ncls_end))\n",
    "        mid_NUC_list.append(ncls_mid)\n",
    "\n",
    "    # Infer linker regions and their midpoints\n",
    "    for i in range(len(regions_NUC_list)-1):\n",
    "        regions_LINK_list.append((regions_NUC_list[i][1], regions_NUC_list[i+1][0]))\n",
    "        mid_LINK_list.append(round((regions_NUC_list[i][1] + regions_NUC_list[i+1][0])/2, 0))\n",
    "\n",
    "    return read_id, mid_NUC_list, mid_LINK_list, regions_LINK_list, regions_NUC_list, group.iloc[0][metadata_cols]\n",
    "\n",
    "#version that requires a nuc and linker region (does not allow nucleosomes to be stuck next to eachother)\n",
    "def single_fiber_nuc_linker(read_id, group, LINK_width,NUC_width, metadata_cols, emission_NEG_array, emission_PGC_array):\n",
    "    # Initialize lists to store information about linker and nucleosome regions and their midpoints\n",
    "    regions_LINK_list = []\n",
    "    regions_NUC_list = []\n",
    "    mid_NUC_list = []\n",
    "    mid_LINK_list = []\n",
    "    mod_qual_LINK_list = []  # List to store mod_qual values for linker regions\n",
    "    mod_qual_NUC_list = []  # List to store mod_qual values for nucleosome region\n",
    "    width = LINK_width + NUC_width + 1\n",
    "\n",
    "    # Calculate the total number of modified bases, total bases, and minimum base position in the read\n",
    "    BASE_NUM = max(group['rel_pos']) - min(group['rel_pos']) + 1\n",
    "    BASE_MIN = min(group['rel_pos'])\n",
    "\n",
    "    # Initialize the calling_vec with -1 and populate it with mod_qual values based on relative position\n",
    "    calling_vec = np.full(BASE_NUM+1, -1.0)\n",
    "    for i in range(len(group['rel_pos'])):\n",
    "        calling_vec[group.iloc[i]['rel_pos'] - BASE_MIN] = group.iloc[i]['mod_qual']\n",
    "\n",
    "\n",
    "    # Initialize probability matrix (prob_mat) and pointer matrix (ptr_mat)\n",
    "    # prob_mat stores log probabilities, and ptr_mat stores the previous state index for backtracking\n",
    "    prob_mat = np.zeros((BASE_NUM+1, width))\n",
    "    ptr_mat = np.full((BASE_NUM+1, width), -1, dtype=int)\n",
    "\n",
    "    # Initialization of first row in prob_mat and ptr_mat\n",
    "    initial_rate = 1 / width\n",
    "    log_initial_rate = np.log(initial_rate)\n",
    "    prob_mat[1, :] = log_initial_rate\n",
    "    ptr_mat[1, :] = 0\n",
    "\n",
    "    # Dynamic Programming Step: Fill the prob_mat and ptr_mat\n",
    "    #high_mod_qual_indices = [i for i, val in enumerate(calling_vec) if val > 0.8]  # Store indices of high mod_qual\n",
    "\n",
    "    #distance_threshold = 20\n",
    "    for i in range(2, BASE_NUM+1):\n",
    "        within_linker = 0.0\n",
    "        back_frm_ncls = 0.0\n",
    "\n",
    "        # Compute probabilities for staying within linker and going back to linker from nucleosome\n",
    "        if calling_vec[i] == -1:\n",
    "            within_linker = prob_mat[i-1, 0]\n",
    "            if prob_mat[i-1, width-1] != 0:\n",
    "                back_frm_ncls = prob_mat[i-1, width-1]\n",
    "        else:\n",
    "            k = int(calling_vec[i] * 1000)\n",
    "            within_linker = np.log(emission_PGC_array[k]) + prob_mat[i-1, 0]\n",
    "            if prob_mat[i-1, width-1] != 0:\n",
    "                back_frm_ncls = np.log(emission_PGC_array[k]) + prob_mat[i-1, width-1]\n",
    "        \"\"\"# Special threshold condition for linker region\n",
    "                if i in high_mod_qual_indices:\n",
    "                    for idx in high_mod_qual_indices:\n",
    "                        if abs(i - idx) < distance_threshold:  # Check distance criterion\n",
    "                            within_linker += 100  # Boost the probability value\n",
    "                            break\"\"\"\n",
    "\n",
    "        # Update first column of prob_mat and ptr_mat based on calculated probabilities\n",
    "        if back_frm_ncls != 0 and back_frm_ncls > within_linker:\n",
    "            prob_mat[i, 0] = back_frm_ncls\n",
    "            ptr_mat[i, 0] = width-1\n",
    "        else:\n",
    "            prob_mat[i, 0] = within_linker\n",
    "            ptr_mat[i, 0] = 0\n",
    "\n",
    "        # Update columns for high methylation (1-30) in the nucleosome\n",
    "        for j in range(1, LINK_width+1):\n",
    "            if calling_vec[i] == -1:\n",
    "                prob_mat[i, j] = prob_mat[i-1, j-1]\n",
    "            else:\n",
    "                k = int(calling_vec[i] * 1000)\n",
    "                prob_mat[i, j] = np.log(emission_PGC_array[k]) + prob_mat[i-1, j-1]\n",
    "                \"\"\"# Special threshold condition for linker region\n",
    "                if i in high_mod_qual_indices:\n",
    "                    for idx in high_mod_qual_indices:\n",
    "                        if abs(i - idx) < distance_threshold:  # Check distance criterion\n",
    "                            prob_mat[i, j] += 100  # Boost the probability value\n",
    "                            break\"\"\"\n",
    "            ptr_mat[i, j] = j - 1\n",
    "\n",
    "        # Update columns for low methylation (31-177) in the nucleosome\n",
    "        for j in range(LINK_width+1, width):\n",
    "            if calling_vec[i] == -1:\n",
    "                if prob_mat[i-1, j-1] != 0:\n",
    "                    prob_mat[i, j] = prob_mat[i-1, j-1]\n",
    "            else:\n",
    "                k = int(calling_vec[i] * 1000)\n",
    "                if prob_mat[i-1, j-1] != 0:\n",
    "                    prob_mat[i, j] = np.log(emission_NEG_array[k]) + prob_mat[i-1, j-1]\n",
    "            if prob_mat[i, j] != 0:\n",
    "                ptr_mat[i, j] = j-1\n",
    "\n",
    "    # Backtrack to identify most probable states\n",
    "    current_qual_LINK = []\n",
    "    current_qual_NUC = []\n",
    "    max_index = np.argmax(prob_mat[BASE_NUM, :])\n",
    "    backtrack_vec = []\n",
    "    for i in range(BASE_NUM, 1, -1):\n",
    "        backtrack_vec.append(max_index)\n",
    "        ### STORE MOD-qual values for nuc and link regions\n",
    "        if max_index > 0:  # In nucleosome\n",
    "            if calling_vec[i-1] != -1:\n",
    "                current_qual_NUC.append(calling_vec[i-1])\n",
    "        else:  # In linker\n",
    "            if calling_vec[i-1] != -1:\n",
    "                current_qual_LINK.append(calling_vec[i-1])\n",
    "\n",
    "        if len(current_qual_NUC) > 0 and (max_index == 0 or i == 2):\n",
    "            mod_qual_NUC_list.append(np.mean(current_qual_NUC))\n",
    "            current_qual_NUC = []\n",
    "\n",
    "        if len(current_qual_LINK) > 0 and (max_index > 0 or i == 2):\n",
    "            mod_qual_LINK_list.append(np.mean(current_qual_LINK))\n",
    "            current_qual_LINK = []\n",
    "        max_index = ptr_mat[i, max_index]\n",
    "    backtrack_vec.reverse()\n",
    "\n",
    "    # Identify nucleosome and linker regions based on backtracking results\n",
    "    ncls_start = 0\n",
    "    ncls_end = 0\n",
    "    shift = min(group['rel_pos']) - 1\n",
    "    InNucleosome = False\n",
    "    for i, val in enumerate(backtrack_vec):\n",
    "        if val > 0:\n",
    "            if not InNucleosome:\n",
    "                ncls_start = i + 1 + shift\n",
    "                InNucleosome = True\n",
    "        else:\n",
    "            if InNucleosome:\n",
    "                ncls_end = i + 1 + shift\n",
    "                ncls_mid = round((ncls_end-ncls_start)/2 + ncls_start, 0)\n",
    "                if ncls_end - ncls_start < NUC_width:\n",
    "                    ncls_start = ncls_end - NUC_width\n",
    "                    ncls_mid = round((ncls_end-ncls_start)/2 + ncls_start, 0)\n",
    "                regions_NUC_list.append((ncls_start, ncls_end))\n",
    "                mid_NUC_list.append(ncls_mid)\n",
    "                InNucleosome = False\n",
    "\n",
    "    # Check if the nucleosome extends to the end of the read\n",
    "    if InNucleosome:\n",
    "        ncls_end = ncls_start + NUC_width\n",
    "        ncls_mid = round((ncls_end-ncls_start)/2 + ncls_start, 0)\n",
    "        regions_NUC_list.append((ncls_start, ncls_end))\n",
    "        mid_NUC_list.append(ncls_mid)\n",
    "\n",
    "    # Infer linker regions and their midpoints\n",
    "    for i in range(len(regions_NUC_list)-1):\n",
    "        regions_LINK_list.append((regions_NUC_list[i][1], regions_NUC_list[i+1][0]))\n",
    "        mid_LINK_list.append(round((regions_NUC_list[i][1] + regions_NUC_list[i+1][0])/2, 0))\n",
    "\n",
    "    return read_id, mid_NUC_list, mid_LINK_list, regions_LINK_list, regions_NUC_list, group.iloc[0][metadata_cols], mod_qual_LINK_list, mod_qual_NUC_list\n",
    "\n",
    "def create_dataframe_from_results(results, kind='NUC', metadata_cols=metadata_cols):\n",
    "    # if \"rel_start\" in res[3] == -21 then print res[0:3]\n",
    "    if kind == 'NUC_mid':\n",
    "        df = pd.DataFrame.from_dict({res[0]: res[1] for res in results}, orient='index')\n",
    "    elif kind == 'LINK_mid':\n",
    "        df = pd.DataFrame.from_dict({res[0]: res[2] for res in results}, orient='index')\n",
    "    elif kind == 'LINK_region':\n",
    "        df = pd.DataFrame.from_dict({res[0]: res[3] for res in results}, orient='index')\n",
    "    elif kind == 'NUC_region':\n",
    "        df = pd.DataFrame.from_dict({res[0]: res[4] for res in results}, orient='index')\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid kind: {kind}\")\n",
    "\n",
    "    # Reset the index to make it a new column\n",
    "    df.reset_index(inplace=True)\n",
    "\n",
    "    # Rename the new column to something meaningful\n",
    "    df.rename(columns={'index': 'read_id'}, inplace=True)\n",
    "    # Adding additional metadata columns to each df.\n",
    "    # Create a new DataFrame for the metadata_cols\n",
    "    metadata_df = pd.DataFrame({cols: [res[5][cols] if res[5] is not None else None for res in results] for cols in metadata_cols})\n",
    "    # Concatenate the new DataFrame and the original DataFrame along axis 1 (columns)\n",
    "    df = pd.merge(metadata_df, df, on='read_id', how='left')\n",
    "\n",
    "    return df\n",
    "\n",
    "def create_dataframe_from_mod_qual(results, kind='mod_qual_NUC', metadata_cols=metadata_cols):\n",
    "    df = pd.DataFrame.from_dict({res[0]: res[7 if kind == 'mod_qual_NUC' else 6] for res in results}, orient='index')\n",
    "    df.reset_index(inplace=True)\n",
    "    df.rename(columns={'index': 'read_id'}, inplace=True)\n",
    "    metadata_df = pd.DataFrame({cols: [res[5][cols] if res[5] is not None else None for res in results] for cols in metadata_cols})\n",
    "    df = pd.merge(metadata_df, df, on='read_id', how='left')\n",
    "    return df\n",
    "\n",
    "# Using multiprocessing to parallelize the calculations\n",
    "print(\"Grouping df...\")\n",
    "# Sort plot_df by read_id then by rel_pos\n",
    "#print(\"Plot_df:\")\n",
    "#display(plot_df.head(10))\n",
    "plot_df.sort_values(by=[\"read_id\",\"rel_pos\"], inplace=True)\n",
    "plot_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# drop rows where read_id not in first 5 read_ids\n",
    "first_rows = plot_df['read_id'].unique()#[plot_df['type'] == 'intergenic_control'] [:2000]\n",
    "#print(\"first_five: \", first_five)\n",
    "grouped = plot_df[plot_df['read_id'].isin(first_rows)]\n",
    "grouped = grouped.groupby('read_id')\n",
    "#grouped = plot_df.groupby('read_id')\n",
    "\n",
    "#grouped = grouped_subset.groupby('read_id')\n",
    "\n",
    "print(\"Calculating nucleosome positions...\")\n",
    "LINK_width = 1\n",
    "NUC_width = 146\n",
    "grouped_data_with_constants = [(read_id,group,LINK_width,NUC_width ,metadata_cols,emission_NEG_array,emission_PGC_array) for read_id,group in grouped]\n",
    "\n",
    "#processes=multiprocessing.cpu_count()\n",
    "with multiprocessing.Pool(processes=multiprocessing.cpu_count()) as pool:\n",
    "    # set results equal to pool.starmap() with the function and grouped_data_with_constants as arguments using tqdm to track progress\n",
    "    results = pool.starmap(single_fiber_nuc_linker, tqdm.tqdm(grouped_data_with_constants, total=len(grouped_data_with_constants)))\n",
    "\n",
    "#print(\"creating result dfs...\")\n",
    "midpoint_NUC = create_dataframe_from_results(results, kind='NUC_mid', metadata_cols=metadata_cols)\n",
    "midpoint_MAD = create_dataframe_from_results(results, kind='LINK_mid', metadata_cols=metadata_cols)\n",
    "region_MAD = create_dataframe_from_results(results, kind='LINK_region', metadata_cols=metadata_cols)\n",
    "region_NUC = create_dataframe_from_results(results, kind='NUC_region', metadata_cols=metadata_cols)\n",
    "mod_qual_LINK = create_dataframe_from_mod_qual(results, kind='mod_qual_LINK', metadata_cols=metadata_cols)\n",
    "mod_qual_NUC = create_dataframe_from_mod_qual(results, kind='mod_qual_NUC', metadata_cols=metadata_cols)\n",
    "\n",
    "\n",
    "# drop duplicates based on read_id from all dfs\n",
    "midpoint_NUC.drop_duplicates(subset=['read_id'], keep='first', inplace=True)\n",
    "midpoint_MAD.drop_duplicates(subset=['read_id'], keep='first', inplace=True)\n",
    "region_MAD.drop_duplicates(subset=['read_id'], keep='first', inplace=True)\n",
    "region_NUC.drop_duplicates(subset=['read_id'], keep='first', inplace=True)\n",
    "mod_qual_LINK.drop_duplicates(subset=['read_id'], keep='first', inplace=True)\n",
    "mod_qual_NUC.drop_duplicates(subset=['read_id'], keep='first', inplace=True)\n",
    "\n",
    "nanotools.display_sample_rows(midpoint_NUC)\n",
    "nanotools.display_sample_rows(region_NUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4309065",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Process to add per-read statistics\n",
    "#define minimum and maximum positions for nucs / MADs\n",
    "min_pos = -bed_window\n",
    "max_pos = bed_window\n",
    "\n",
    "def find_closest_tuple(row,min_pos,max_pos,metadata_cols=metadata_cols, min_size=35,max_bound=1000):\n",
    "    #print(\"row:\",row)\n",
    "    closest_distance = max_bound\n",
    "    closest_tuple = None\n",
    "    for col_name, tup in zip(row.index, row):\n",
    "        # Skip metadata columns\n",
    "        if col_name in metadata_cols:\n",
    "            continue\n",
    "        if tup is not None and tup is not np.nan:\n",
    "            # Check if the tuple is entirely contained within desired region\n",
    "            if tup[0] >= min_pos and tup[1] <= max_pos and abs(tup[0] - tup[1]) >= min_size:\n",
    "                distance_1 = abs(tup[0] - 0)\n",
    "                distance_2 = abs(tup[1] - 0)\n",
    "\n",
    "                min_distance = min(distance_1, distance_2)\n",
    "\n",
    "                if min_distance < closest_distance:\n",
    "                    closest_distance = min_distance\n",
    "                    closest_tuple = tup\n",
    "\n",
    "        if closest_tuple is None:\n",
    "            if tup is not None and tup is not np.nan:\n",
    "                # Check if the tuple is entirely contained within desired region\n",
    "                if tup[0] >= min_pos and tup[1] <= max_pos:\n",
    "                    distance_1 = abs(tup[0] - 0)\n",
    "                    distance_2 = abs(tup[1] - 0)\n",
    "\n",
    "                    min_distance = min(distance_1, distance_2)\n",
    "\n",
    "                    if min_distance < closest_distance:\n",
    "                        closest_distance = min_distance\n",
    "                        closest_tuple = tup\n",
    "    return closest_tuple\n",
    "\n",
    "### Function to calculate percent_MAD\n",
    "def calc_percent_region(group, min_pos, max_pos, metadata_cols):\n",
    "    total_MAD = 0\n",
    "    read_length = 0  # To store the total length for all reads\n",
    "    for index, row in group.iterrows():\n",
    "        read_start = row['rel_read_start']\n",
    "        read_start = max(min_pos, read_start)\n",
    "        read_end = row['rel_read_end']\n",
    "        read_end = min(max_pos, read_end)\n",
    "        read_length = (read_end - read_start)\n",
    "\n",
    "\n",
    "        for col in group.columns:\n",
    "            if col in metadata_cols:  # Skip metadata columns\n",
    "                continue\n",
    "            mad_tup = row[col]\n",
    "            if pd.notna(mad_tup):\n",
    "                # if at least one of mad_tup[0] and mad_tup[1] is within [min_pos, max_pos]\n",
    "\n",
    "                # Only include the part of the tuple that is within [min_pos, max_pos]\n",
    "                min_val = max(read_start, mad_tup[0])\n",
    "                max_val = min(read_end, mad_tup[1])\n",
    "\n",
    "                if (min_pos <= mad_tup[0] <= max_pos) or (min_pos <= mad_tup[1] <= max_pos):\n",
    "                    try:\n",
    "                        total_MAD += (max_val - min_val)\n",
    "                    except:\n",
    "                        print(\"col:\", col)\n",
    "                        print(\"Warning: mad_tup is None or NaN:\", mad_tup)\n",
    "\n",
    "    percent_MAD = total_MAD / read_length if read_length > 0 else None\n",
    "    if percent_MAD is not None:\n",
    "        if (percent_MAD < 0 or percent_MAD >1):\n",
    "            print(\"row:\")\n",
    "            print(row)\n",
    "            print(\"total_MAD:\", total_MAD)\n",
    "            print(\"read_length:\", read_length)\n",
    "            print(\"percent_MAD:\", percent_MAD)\n",
    "    return percent_MAD\n",
    "\n",
    "def calculate_fiber_NRL_list(row, metadata_cols,sign_filter=None):\n",
    "    row_values = row.drop(metadata_cols).dropna().values  # Drop metadata columns and NaNs\n",
    "    # If row_values is empty, return an empty list\n",
    "    if len(row_values) == 0:\n",
    "        return []\n",
    "\n",
    "    # Flatten the array, taking into account that some elements may be lists\n",
    "    n_values = np.concatenate([np.array(x) if isinstance(x, list) else np.array([x]) for x in row_values])\n",
    "\n",
    "    if n_values.size == 0:\n",
    "        return []\n",
    "\n",
    "    #print(\"n_values:\", n_values)\n",
    "\n",
    "    if sign_filter == '+':\n",
    "        filter_func = lambda x: x > 0\n",
    "    elif sign_filter == '-':\n",
    "        filter_func = lambda x: x < 0\n",
    "    else:\n",
    "        filter_func = lambda x: True\n",
    "\n",
    "    # Filter the n_values\n",
    "    n_values = np.array([x for x in n_values if filter_func(x)])\n",
    "\n",
    "    # Check if n_values is empty after filtering or originally\n",
    "    if n_values.size == 0:\n",
    "        return []\n",
    "\n",
    "    # Calculate the list of differences (each item from the next\n",
    "    #fiber_NRL_list = [n_values[j] - n_values[i]\n",
    "    #                  for i in range(len(n_values))\n",
    "    #                  for j in range(i + 1, len(n_values))]\n",
    "\n",
    "    # Calculate the list of differences for n_values greater than each item\n",
    "    fiber_NRL_list = [n_values[j] - n_values[i]\n",
    "                      for i in range(len(n_values))\n",
    "                      for j in range(i + 1, len(n_values))\n",
    "                      if n_values[j] > n_values[i]]\n",
    "\n",
    "    return fiber_NRL_list\n",
    "\n",
    "def closest_nuc(row):\n",
    "    smallest = row['smallest_positive_nuc_midpoint']\n",
    "    greatest = row['greatest_negative_nuc_midpoint']\n",
    "\n",
    "    if np.isnan(smallest) and np.isnan(greatest):\n",
    "        return np.nan\n",
    "\n",
    "    if np.isnan(smallest):\n",
    "        return greatest\n",
    "\n",
    "    if np.isnan(greatest):\n",
    "        return smallest\n",
    "\n",
    "    return smallest if abs(smallest) < abs(greatest) else greatest\n",
    "\n",
    "merged_df = pd.DataFrame()\n",
    "\n",
    "### Calculate smallest positive and greatest negative midpoints\n",
    "midpoint_NUC['smallest_positive_nuc_midpoint'] = midpoint_NUC.apply(lambda row: min([x for x in row[len(metadata_cols):] if (max_pos > x >= 0)], default=None), axis=1)\n",
    "midpoint_NUC['greatest_negative_nuc_midpoint'] = midpoint_NUC.apply(lambda row: max([x for x in row[len(metadata_cols):] if (min_pos < x < 0)], default=None), axis=1)\n",
    "midpoint_NUC['closest_nuc'] = midpoint_NUC.apply(closest_nuc, axis=1)\n",
    "midpoint_NUC['inter_nuc_dist'] = midpoint_NUC['smallest_positive_nuc_midpoint'] - midpoint_NUC['greatest_negative_nuc_midpoint']\n",
    "# set closest_region in region_MAD to be qual to the tupple who's element is closest to 0 in ros 0-19\n",
    "region_MAD['closest_MAD_region'] = region_MAD.apply(lambda row: find_closest_tuple(row, min_pos=min_pos, max_pos=max_pos, metadata_cols=metadata_cols, min_size=35,max_bound = max_pos), axis=1)\n",
    "region_MAD['MAD_size'] = region_MAD['closest_MAD_region'].apply(lambda x: x[1] - x[0] if x is not None else None)\n",
    "region_MAD['closest_MAD_midpoint'] = region_MAD.apply(lambda row: (row['closest_MAD_region'][0] + row['closest_MAD_region'][1]) / 2 if row['closest_MAD_region'] is not None else None, axis=1)\n",
    "merged_df = pd.merge(plot_df, midpoint_NUC[['read_id', 'smallest_positive_nuc_midpoint', 'greatest_negative_nuc_midpoint','inter_nuc_dist','closest_nuc']], on='read_id', how='left')\n",
    "merged_df = pd.merge(merged_df, region_MAD[['read_id', 'closest_MAD_region','MAD_size','closest_MAD_midpoint']], on='read_id', how='left')\n",
    "# drop merged columns from orginal dfs\n",
    "midpoint_NUC.drop(columns=['smallest_positive_nuc_midpoint', 'greatest_negative_nuc_midpoint','inter_nuc_dist','closest_nuc'], inplace=True)\n",
    "region_MAD.drop(columns=['closest_MAD_region','MAD_size','closest_MAD_midpoint'], inplace=True)\n",
    "\n",
    "# 1. Merge the DataFrames\n",
    "### Calculate % MAD and % NUC\n",
    "min_calc_region = -500\n",
    "max_calc_region = 100\n",
    "print(\"Status: Calculating percent_MAD...\")\n",
    "grouped_mad = region_MAD.groupby('read_id').apply(calc_percent_region, min_pos=min_calc_region, max_pos=max_calc_region, metadata_cols=metadata_cols).reset_index(name='percent_MAD')\n",
    "print(\"Status: Calculating percent_NUC...\")\n",
    "grouped_nuc = region_NUC.groupby('read_id').apply(calc_percent_region, min_pos=min_calc_region, max_pos=max_calc_region, metadata_cols=metadata_cols).reset_index(name='percent_NUC')\n",
    "merged_df = pd.merge(merged_df, grouped_mad, on='read_id', how='left')\n",
    "merged_df = pd.merge(merged_df, grouped_nuc, on='read_id', how='left')\n",
    "merged_df['percent_OTHER'] = 1 - merged_df['percent_MAD'] - merged_df['percent_NUC']\n",
    "\n",
    "### Create 'fiber_NRL_list' column in the midpoint_NUC DataFrame\n",
    "midpoint_NUC['fiber_NRL_list'] = midpoint_NUC.apply(lambda row: calculate_fiber_NRL_list(row, metadata_cols,sign_filter=None), axis=1)\n",
    "midpoint_NUC['fiber_NRL_list_pos'] = midpoint_NUC.apply(lambda row: calculate_fiber_NRL_list(row, metadata_cols,sign_filter=\"+\"), axis=1)\n",
    "midpoint_NUC['fiber_NRL_list_neg'] = midpoint_NUC.apply(lambda row: calculate_fiber_NRL_list(row, metadata_cols,sign_filter=\"-\"), axis=1)\n",
    "# Merge the new 'fiber_NRL_list' column into the 'merged_df' DataFrame\n",
    "merged_df = pd.merge(merged_df, midpoint_NUC[['read_id', 'fiber_NRL_list','fiber_NRL_list_pos','fiber_NRL_list_neg']], on='read_id', how='left')\n",
    "# Restore midpoint_NUC to it's original state\n",
    "midpoint_NUC.drop(columns=['fiber_NRL_list','fiber_NRL_list_pos','fiber_NRL_list_neg'], inplace=True)\n",
    "\n",
    "# 2. Sort the merged DataFrame\n",
    "merged_df = merged_df.sort_values(by=['percent_MAD'], ascending=True)\n",
    "# reset index\n",
    "merged_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Group by read_id and display 10 unique read_id rows\n",
    "grouped = merged_df.copy()\n",
    "# drop dupicate rows by read_id\n",
    "grouped.drop_duplicates(subset=['read_id'], inplace=True)\n",
    "# sort by read_id\n",
    "grouped.sort_values(by=['read_id'], inplace=True)\n",
    "# Set grouped column \"rel_read_length\" equal to rel_read_end - rel_read_start\n",
    "grouped.reset_index(inplace=True, drop=True)\n",
    "\n",
    "#set combined_nucs_df equal to columns 'read_id' and all columns with integer column names.\n",
    "combined_nucs_df = midpoint_NUC[['read_id'] + [col for col in midpoint_NUC.columns if isinstance(col, int)]]\n",
    "# add a new column that creates a list of all the nucleosome positions for each read that are not nan\n",
    "combined_nucs_df['nucs_list'] = combined_nucs_df.apply(lambda row: [x for x in row[1:] if pd.notna(x)], axis=1)\n",
    "grouped = pd.merge(grouped,combined_nucs_df[['read_id','nucs_list']], on='read_id', how='left')\n",
    "\n",
    "print(\"Creating aligned nuc and mad lists...\")\n",
    "def align_nuc_list(row, column_name):\n",
    "    subtract_val = row[column_name]\n",
    "    if isinstance(subtract_val, tuple):\n",
    "        midpoint = (subtract_val[0] + subtract_val[1]) / 2.0\n",
    "    else:\n",
    "        midpoint = subtract_val\n",
    "\n",
    "    if isinstance(midpoint, (int, float)) and not np.isnan(midpoint):\n",
    "        return [x - midpoint for x in row['nucs_list']]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def calculate_internuc(nuc_list):\n",
    "    return [j - i for i, j in zip(nuc_list[:-1], nuc_list[1:])]\n",
    "\n",
    "def align_nuc_list_internuc(row):\n",
    "    smallest_positive = row['smallest_positive_nuc_midpoint']\n",
    "    greatest_negative = row['greatest_negative_nuc_midpoint']\n",
    "\n",
    "    # Check if both are NaN\n",
    "    if np.isnan(smallest_positive) or np.isnan(greatest_negative):\n",
    "        return []\n",
    "    else:\n",
    "        midpoint = (smallest_positive + greatest_negative) / 2.0\n",
    "\n",
    "    return [x - midpoint for x in row['nucs_list']]\n",
    "\n",
    "def largest_gap_size(nucs_list, nuc_width=147):\n",
    "    if len(nucs_list) < 2:\n",
    "        return np.nan\n",
    "    sorted_list = sorted(nucs_list)\n",
    "    max_gap = max(np.diff(sorted_list)) - nuc_width\n",
    "    return max_gap\n",
    "\n",
    "def largest_gap_pos(nucs_list, nuc_width=147):\n",
    "    if len(nucs_list) < 2:\n",
    "        return np.nan\n",
    "    sorted_list = sorted(nucs_list)\n",
    "    max_gap = max(np.diff(sorted_list))\n",
    "    # Find the index of the largest gap\n",
    "    max_gap_index = np.argmax(np.diff(sorted_list))\n",
    "    # Calculate the center position of the largest gap\n",
    "    center_position = (sorted_list[max_gap_index] + sorted_list[max_gap_index + 1]) / 2\n",
    "    return center_position\n",
    "\n",
    "grouped = grouped.dropna(subset=['nucs_list'])\n",
    "\n",
    "grouped['inter_nuc_sub'] = grouped['nucs_list'].apply(calculate_internuc)\n",
    "grouped['largest_nfr_size'] = grouped['nucs_list'].apply(largest_gap_size, NUC_width)\n",
    "grouped['largest_nfr_pos'] = grouped['nucs_list'].apply(largest_gap_pos, NUC_width)\n",
    "grouped['nuc_list_internuc_aligned'] = grouped.apply(align_nuc_list_internuc, axis=1)\n",
    "grouped['nucs_list_closest_aligned'] = grouped.apply(align_nuc_list, args=('closest_nuc',), axis=1)\n",
    "grouped['nucs_list_largest_nfr_aligned'] = grouped.apply(align_nuc_list, args=('largest_nfr_pos',), axis=1)\n",
    "\n",
    "\n",
    "if 'exp_id' not in grouped.columns:\n",
    "    import pysam\n",
    "    # Check that all lists have the same length\n",
    "    if not (len(new_bam_files) == len(exp_ids)):\n",
    "        raise ValueError(\"The lists new_bam_files, exp_ids, and conditions must have the same length.\")\n",
    "\n",
    "    # Initialize an empty list to store the data\n",
    "    data = []\n",
    "\n",
    "    print(\"Status: Looping through bam files to append exp_ids\")\n",
    "    # Loop over bam files, their corresponding experiment ids, and conditions\n",
    "    for bam_file, exp_id in zip(new_bam_files, exp_ids):\n",
    "        # Open the bam file\n",
    "        with pysam.AlignmentFile(bam_file, \"rb\") as bam:\n",
    "            # Fetch the first 3 read ids as an example\n",
    "            # In practice, you would iterate over all reads as needed\n",
    "            for read in bam.fetch(): # Adjust the number as needed for actual use\n",
    "                data.append({\n",
    "                    \"bam_file_name\": bam_file,\n",
    "                    \"read_id\": read.query_name,\n",
    "                    \"exp_id\": exp_id,\n",
    "                })\n",
    "\n",
    "    # Convert the list of dictionaries to a DataFrame\n",
    "    read_exp_ids_df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "\n",
    "    # Add exp_id column to grouped dataframe by merging on read_id\n",
    "    # if exp_id column does not exist in grouped then:\n",
    "\n",
    "    grouped = pd.merge(grouped, read_exp_ids_df[['read_id', 'exp_id']], on='read_id', how='left')\n",
    "\n",
    "nanotools.display_sample_rows(grouped, 5)\n",
    "nanotools.display_sample_rows(merged_df, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc56f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Calculate positioning statistics\n",
    "# Function to compute the midpoint\n",
    "\"\"\"def compute_midpoint(row):\n",
    "    pos_values = [v for v in row[integer_columns] if v > 0]\n",
    "    neg_values = [v for v in row[integer_columns] if v < 0]\n",
    "\n",
    "    # If either list is empty, return NaN as the midpoint\n",
    "    if not pos_values or not neg_values:\n",
    "        return float('nan')\n",
    "\n",
    "    least_pos = min(pos_values)\n",
    "    greatest_neg = max(neg_values)\n",
    "\n",
    "    return (least_pos + greatest_neg) / 2\"\"\"\n",
    "\n",
    "def calculate_feature(args):\n",
    "    occ_cutoff, midpoint_NUC, NUC_max_width, metadata_cols, align_to, condition, chr_type, dtype  = args\n",
    "\n",
    "    # Filter the dataframe based on condition, chr_type, and dtype\n",
    "    print(\"Processing with following filters:\", condition, chr_type, dtype, sep=\"\\n\")\n",
    "    midpoint_NUC_filtered = midpoint_NUC.copy(deep=True)\n",
    "    midpoint_NUC_filtered = midpoint_NUC_filtered[\n",
    "        (midpoint_NUC_filtered['condition'] == condition) &\n",
    "        (midpoint_NUC_filtered['chr_type'] == chr_type) &\n",
    "        (midpoint_NUC_filtered['type'] == dtype) &\n",
    "        (midpoint_NUC_filtered[align_to] is not np.nan)\n",
    "    ]\n",
    "\n",
    "    # Identify columns with integer names\n",
    "    integer_columns = [col for col in midpoint_NUC_filtered.columns if isinstance(col, int)]\n",
    "\n",
    "    # Shift columns by the midpoint\n",
    "    #for col in integer_columns:\n",
    "    #    midpoint_NUC_filtered[col] = midpoint_NUC_filtered[col] - midpoint_NUC_filtered[align_to]\n",
    "\n",
    "    midpoint_NUC_filtered.set_index('read_id', inplace=True)\n",
    "    #print(\"midpoint_NUC_filtered:\")\n",
    "    #display(midpoint_NUC_filtered.head(20))\n",
    "    results = {\n",
    "        'mean_nuc_pos': [],\n",
    "        'total_reads': [],\n",
    "        'total_nucs': [],\n",
    "        'percent_occ': [],\n",
    "        'nucs_list': [],\n",
    "        'subs_list': []\n",
    "    }\n",
    "\n",
    "    used_nucleosomes = set()\n",
    "\n",
    "    print(f\"Looping through  {len(midpoint_NUC_filtered)} reads...\")\n",
    "    iter=0\n",
    "    for idx, row in midpoint_NUC_filtered.iterrows():\n",
    "        # print progress whenever idx % 2000 == 0\n",
    "        iter += 1\n",
    "        if iter % 200 == 0:\n",
    "            print(\"Processing read:\", iter, sep='\\n')\n",
    "        nuc_positions = row[row.index.difference(metadata_cols)].dropna()\n",
    "        for nuc in nuc_positions:\n",
    "            if (idx,nuc) in used_nucleosomes:\n",
    "                #print(f\"Warning: Duplicate nucleosome found for read_id: {idx}, nuc: {nuc}\")\n",
    "                continue\n",
    "\n",
    "            temp_used_nucleosomes = set()\n",
    "            temp_subs_list = []\n",
    "            temp_mean_subs_list = []\n",
    "            #print(\"idx:\", idx, \" | nuc:\", nuc)\n",
    "\n",
    "            temp_subtractions = abs(midpoint_NUC_filtered[row.index.difference(metadata_cols)] - nuc)\n",
    "            #Drop the current row from the subtractions table\n",
    "            temp_subtractions.drop(index=idx, inplace=True)\n",
    "            #temp_subtractions = temp_subtractions.apply(lambda row: row.where(row == row.min(), np.nan), axis=1)\n",
    "            temp_subtractions = temp_subtractions.apply(lambda row: row.nsmallest(2), axis=1)\n",
    "            temp_subtractions = temp_subtractions.where(temp_subtractions <= ((NUC_max_width / 2 + 12)), np.nan)\n",
    "            #print(\"temp_subtractions:\")\n",
    "            #display(temp_subtractions.head(20))\n",
    "\n",
    "            non_nan_rows = temp_subtractions.dropna(how='all')\n",
    "            # add current nucleosome to temp_used_nucleosomes\n",
    "            temp_used_nucleosomes.add((idx, nuc))\n",
    "\n",
    "            for used_row_idx in non_nan_rows.index:\n",
    "                for col in non_nan_rows.columns:\n",
    "                    if not pd.isna(non_nan_rows.at[used_row_idx, col]) and (used_row_idx, midpoint_NUC_filtered.at[used_row_idx, col]) not in used_nucleosomes:\n",
    "                        original_nuc_value = midpoint_NUC_filtered.at[used_row_idx, col]\n",
    "                        temp_used_nucleosomes.add((used_row_idx, original_nuc_value))\n",
    "                        temp_subs_list.append(abs(original_nuc_value - nuc))\n",
    "\n",
    "\n",
    "            used_nucleosomes.update(temp_used_nucleosomes)\n",
    "            #print(f\"Used nucleosomes: {used_nucleosomes}\")\n",
    "\n",
    "            # Calculate the results\n",
    "            mean_nuc = np.mean([x[1] for x in temp_used_nucleosomes])\n",
    "            total_reads = len(midpoint_NUC_filtered)\n",
    "            total_nucs = len(temp_used_nucleosomes)\n",
    "            percent_occ = total_nucs / total_reads if total_reads else 0\n",
    "            nucs_list = [x[1] for x in temp_used_nucleosomes]\n",
    "\n",
    "            # Append results\n",
    "            results['mean_nuc_pos'].append(mean_nuc)\n",
    "            results['total_reads'].append(total_reads)\n",
    "            results['total_nucs'].append(total_nucs)\n",
    "            results['percent_occ'].append(percent_occ)\n",
    "            results['nucs_list'].append(nucs_list)\n",
    "            results['subs_list'].append(temp_subs_list)\n",
    "            #print(f\"Results: {results}\")\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df['condition'] = condition\n",
    "    results_df['chr_type'] = chr_type\n",
    "    results_df['type'] = dtype\n",
    "    # Sort the results_df by mean_nuc_pos\n",
    "    # drop all rows where percent_occ is < occ_cutoff\n",
    "    results_df = results_df[results_df['percent_occ'] >= occ_cutoff]\n",
    "    results_df.sort_values('mean_nuc_pos', inplace=True)\n",
    "    results_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Add the \"nuc_id\" column\n",
    "    # Count negative and positive values to get the maximum n- and n+ steps\n",
    "    neg_count = sum(results_df['mean_nuc_pos'] < 0)\n",
    "    pos_count = 1\n",
    "\n",
    "    # Initialize nuc_id list\n",
    "    nuc_id_list = []\n",
    "\n",
    "    # Loop through DataFrame and assign nuc_id\n",
    "    for idx, row in results_df.iterrows():\n",
    "        if row['mean_nuc_pos'] < 0:\n",
    "            nuc_id = f'n-{neg_count}'\n",
    "            neg_count -= 1\n",
    "        elif row['mean_nuc_pos'] > 0:\n",
    "            nuc_id = f'n+{pos_count}'\n",
    "            pos_count += 1\n",
    "        else:\n",
    "            nuc_id = 'n'\n",
    "        nuc_id_list.append(nuc_id)\n",
    "\n",
    "    results_df['nuc_id'] = nuc_id_list\n",
    "    return results_df\n",
    "\n",
    "def calculate_feature2(args):\n",
    "    bin_width, occ_cutoff, midpoint_NUC, NUC_max_width, metadata_cols, align_to, condition, chr_type, dtype  = args\n",
    "\n",
    "    # Filter the dataframe based on condition, chr_type, and dtype\n",
    "    print(\"Processing with following filters:\", condition, chr_type, dtype, sep=\"\\n\")\n",
    "    midpoint_NUC_filtered = midpoint_NUC.copy(deep=True)\n",
    "    midpoint_NUC_filtered = midpoint_NUC_filtered[\n",
    "        (midpoint_NUC_filtered['condition'] == condition) &\n",
    "        (midpoint_NUC_filtered['chr_type'] == chr_type) &\n",
    "        (midpoint_NUC_filtered['type'] == dtype)\n",
    "    ]\n",
    "\n",
    "    # Identify columns with integer names\n",
    "    integer_columns = [col for col in midpoint_NUC_filtered.columns if isinstance(col, int)]\n",
    "\n",
    "\n",
    "    # Shift columns by the midpoint\n",
    "    if align_to is not None:\n",
    "        # drop rows where from midpoint_NUC_filtered where align_to column is not np.nan\n",
    "        print(\"Dropping this many rows where align_to column is np.nan:\", len(midpoint_NUC_filtered[midpoint_NUC_filtered[align_to].isna()]))\n",
    "        midpoint_NUC_filtered = midpoint_NUC_filtered[midpoint_NUC_filtered[align_to].notna()]\n",
    "        for col in integer_columns:\n",
    "            midpoint_NUC_filtered[col] = midpoint_NUC_filtered[col] - midpoint_NUC_filtered[align_to]\n",
    "        # drop align_to column\n",
    "        midpoint_NUC_filtered.drop(columns=[align_to], inplace=True)\n",
    "\n",
    "    # drop rows with duplicate read_ids\n",
    "    # print rows with duplicate read_ids in midpoint_NUC_filtered\n",
    "    midpoint_NUC_filtered.drop_duplicates(subset=['read_id'], keep='first', inplace=True)\n",
    "    #reset index\n",
    "    midpoint_NUC_filtered.reset_index(drop=True, inplace=True)\n",
    "    midpoint_NUC_filtered.set_index('read_id', inplace=True)\n",
    "    #print(\"midpoint_NUC_filtered:\")\n",
    "    #display(midpoint_NUC_filtered.head(20))\n",
    "    results = {\n",
    "        'mean_nuc_pos': [],\n",
    "        'std_nuc_pos': [],\n",
    "        'total_reads': [],\n",
    "        'total_nucs': [],\n",
    "        'percent_occ': [],\n",
    "        'nucs_list': [],\n",
    "        'subs_list': []\n",
    "    }\n",
    "\n",
    "    used_nucleosomes = set()\n",
    "\n",
    "    # Initialize list to hold mean nucleosome positions\n",
    "    mean_nuc_positions = []\n",
    "\n",
    "    print(f\"Looping through  {len(midpoint_NUC_filtered)} reads...\")\n",
    "    iter=0\n",
    "    for idx, row in midpoint_NUC_filtered.iterrows():\n",
    "        # print progress whenever idx % 2000 == 0\n",
    "        iter += 1\n",
    "        if iter % 200 == 0:\n",
    "            print(\"Processing read:\", iter, sep='\\n')\n",
    "\n",
    "        nuc_positions = row[row.index.difference(metadata_cols)].dropna()\n",
    "        # drop nuc_positions that are within bin_width of any mean_nuc_positions\n",
    "        #nuc_positions = [x for x in nuc_positions if not any(abs(mean_nuc - x) <= bin_width for mean_nuc in mean_nuc_positions)]\n",
    "\n",
    "        # drop nuc_positions that are within |5| of any value in results['std_nuc_pos']\n",
    "        nuc_positions = [x for x in nuc_positions if not any(abs(x - std_nuc) <= 5 for std_nuc in results['std_nuc_pos'])]\n",
    "\n",
    "        #nuc_positions = [x for x in nuc_positions if x not in results['std_nuc_pos']]\n",
    "\n",
    "        #print(nuc_positions)\n",
    "        for nuc in nuc_positions:\n",
    "            #if (idx,nuc) in used_nucleosomes:\n",
    "                #print(f\"Warning: Duplicate nucleosome found for read_id: {idx}, nuc: {nuc}\")\n",
    "            #    continue\n",
    "\n",
    "            temp_used_nucleosomes = set()\n",
    "            temp_subs_list = []\n",
    "            temp_mean_subs_list = []\n",
    "            #print(\"idx:\", idx, \" | nuc:\", nuc)\n",
    "\n",
    "            temp_subtractions = abs(midpoint_NUC_filtered[row.index.difference(metadata_cols)] - nuc)\n",
    "            #Drop the current row from the subtractions table\n",
    "            temp_subtractions.drop(index=idx, inplace=True)\n",
    "            #temp_subtractions = temp_subtractions.apply(lambda row: row.where(row == row.min(), np.nan), axis=1)\n",
    "            temp_subtractions = temp_subtractions.apply(lambda row: row.nsmallest(2), axis=1)\n",
    "            temp_subtractions = temp_subtractions.where(temp_subtractions <= ((NUC_max_width / 2 + 12)), np.nan)\n",
    "            #print(\"temp_subtractions:\")\n",
    "            #display(temp_subtractions.head(20))\n",
    "\n",
    "            non_nan_rows = temp_subtractions.dropna(how='all')\n",
    "            # add current nucleosome to temp_used_nucleosomes\n",
    "            temp_used_nucleosomes.add((idx, nuc))\n",
    "\n",
    "            for used_row_idx in non_nan_rows.index:\n",
    "                for col in non_nan_rows.columns:\n",
    "                    # check if non_nan_rows.at[used_row_idx, col] is a series\n",
    "                    if isinstance(non_nan_rows.at[used_row_idx, col], pd.Series):\n",
    "                        print(\"Warning: non_nan_rows.at[used_row_idx, col] is a series\")\n",
    "                        print(\"non_nan_rows.at[used_row_idx, col]:\", non_nan_rows.at[used_row_idx, col])\n",
    "                        print(\"non_nan_rows.at[used_row_idx, col].values:\", non_nan_rows.at[used_row_idx, col].values)\n",
    "                    if not pd.isna(non_nan_rows.at[used_row_idx, col]) and (used_row_idx, midpoint_NUC_filtered.at[used_row_idx, col]) not in used_nucleosomes:\n",
    "                        original_nuc_value = midpoint_NUC_filtered.at[used_row_idx, col]\n",
    "                        temp_used_nucleosomes.add((used_row_idx, original_nuc_value))\n",
    "                        temp_subs_list.append(abs(original_nuc_value - nuc))\n",
    "\n",
    "            mean_nuc = np.mean([x[1] for x in temp_used_nucleosomes])\n",
    "            mean_nuc_positions.append(mean_nuc)\n",
    "\n",
    "            used_nucleosomes.update(temp_used_nucleosomes)\n",
    "            #print(f\"Used nucleosomes: {used_nucleosomes}\")\n",
    "\n",
    "            # Calculate the results\n",
    "            total_reads = len(midpoint_NUC_filtered)\n",
    "            total_nucs = len(temp_used_nucleosomes)\n",
    "            percent_occ = total_nucs / total_reads if total_reads else 0\n",
    "            nucs_list = [x[1] for x in temp_used_nucleosomes]\n",
    "\n",
    "            # Append results\n",
    "            results['mean_nuc_pos'].append(mean_nuc)\n",
    "            results['std_nuc_pos'].append(nuc)\n",
    "            results['total_reads'].append(total_reads)\n",
    "            results['total_nucs'].append(total_nucs)\n",
    "            results['percent_occ'].append(percent_occ)\n",
    "            results['nucs_list'].append(nucs_list)\n",
    "            results['subs_list'].append(temp_subs_list)\n",
    "            #print(f\"Results: {results}\")\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df['condition'] = condition\n",
    "    results_df['chr_type'] = chr_type\n",
    "    results_df['type'] = dtype\n",
    "    # Sort the results_df by mean_nuc_pos\n",
    "    # drop all rows where percent_occ is < occ_cutoff\n",
    "    results_df = results_df[results_df['percent_occ'] >= occ_cutoff]\n",
    "    results_df.sort_values('mean_nuc_pos', inplace=True)\n",
    "    results_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Initialize an empty dataframe to hold the final result\n",
    "    final_df = pd.DataFrame()\n",
    "    # Find the min and max std_nuc_pos\n",
    "    min_std_nuc_pos = -bed_window #results_df['std_nuc_pos'].min()\n",
    "    max_std_nuc_pos = bed_window #results_df['std_nuc_pos'].max()\n",
    "\n",
    "    results_df = results_df.drop_duplicates(subset=['std_nuc_pos'])\n",
    "    # Create bins\n",
    "    bins = np.arange(min_std_nuc_pos, max_std_nuc_pos, bin_width)\n",
    "\n",
    "    # Initialize a list to hold the rows for each bin\n",
    "    bin_rows = []\n",
    "\n",
    "    for lower_bound in bins:\n",
    "        upper_bound = lower_bound + bin_width\n",
    "\n",
    "        lower_bound = lower_bound + bin_width/4\n",
    "        upper_bound = upper_bound - bin_width/4\n",
    "\n",
    "        # Filter rows where mean_nuc_pos falls within the bin\n",
    "        bin_data = results_df[(results_df['std_nuc_pos'] >= lower_bound) & (results_df['std_nuc_pos'] < upper_bound)]\n",
    "\n",
    "        if not bin_data.empty:\n",
    "            # Find the row with the lowest average of all elements in subs_list\n",
    "            #bin_data['avg_subs'] = bin_data['subs_list'].apply(np.mean)\n",
    "            #min_avg_subs_row = bin_data[bin_data['avg_subs'] == bin_data['avg_subs'].min()].copy()\n",
    "\n",
    "            # Find the row with the max occupancy\n",
    "            min_avg_subs_row = bin_data[bin_data['percent_occ'] == bin_data['percent_occ'].max()].copy()\n",
    "\n",
    "            # Add bin_start and bin_end columns\n",
    "            min_avg_subs_row['bin_start'] = lower_bound\n",
    "            min_avg_subs_row['bin_end'] = upper_bound\n",
    "            min_avg_subs_row['bin_pos'] = lower_bound + (upper_bound - lower_bound)/2\n",
    "\n",
    "\n",
    "            bin_rows.append(min_avg_subs_row)\n",
    "\n",
    "    # Concatenate all rows for each bin into a single dataframe\n",
    "    final_df = pd.concat(bin_rows)\n",
    "    # sort by std_nuc_pos then reset index\n",
    "    final_df.sort_values(by=['std_nuc_pos'], inplace=True)\n",
    "    final_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    # Drop the temporary column used for calculation\n",
    "    #final_df.drop(columns=['avg_subs'], inplace=True)\n",
    "\n",
    "    # Add the \"nuc_id\" column\n",
    "    # Count negative and positive values to get the maximum n- and n+ steps\n",
    "    neg_count = sum(results_df['std_nuc_pos'] < 0)\n",
    "    pos_count = 1\n",
    "\n",
    "    # Initialize nuc_id list\n",
    "    nuc_id_list = []\n",
    "\n",
    "    # Loop through DataFrame and assign nuc_id\n",
    "    for idx, row in final_df.iterrows():\n",
    "        if row['mean_nuc_pos'] < 0:\n",
    "            nuc_id = f'n-{neg_count}'\n",
    "            neg_count -= 1\n",
    "        elif row['mean_nuc_pos'] > 0:\n",
    "            nuc_id = f'n+{pos_count}'\n",
    "            pos_count += 1\n",
    "        else:\n",
    "            nuc_id = 'n'\n",
    "        nuc_id_list.append(nuc_id)\n",
    "\n",
    "    final_df['nuc_id'] = nuc_id_list\n",
    "    return final_df\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    specific_comps = [\n",
    "    ('N2_fiber', 'X', 'xol-1_TSS'),\n",
    "    #('N2_fiber', 'X', 'intergenic_control'),\n",
    "    ('SDC2_degron_fiber', 'X', 'xol-1_TSS')]\n",
    "    #('SDC2_degron_fiber', 'X', 'intergenic_control')]\n",
    "    # set occupancy cutoff, (ignore nucleosomes with occupancy below cutoff)\n",
    "    occ_cutoff = 0\n",
    "    bin_width = 40\n",
    "\n",
    "    # Align to inter-nuc distance?\n",
    "    #align_to = None # \"smallest_positive_nuc_midpoint\", \"greatest_negative_nuc_midpoint\", \"closest_nuc\", \"inter_nuc_dist\" or None \"closest_nuc\"\n",
    "    #align_to = \"closest_nuc\"\n",
    "    align_to = \"closest_MAD_midpoint\"\n",
    "\n",
    "    # create copy of midpoint_NUC and merge with grouped on read_id, keeping align_to column\n",
    "    midpoint_NUC_merged = midpoint_NUC.copy(deep=True)\n",
    "    if align_to is not None:\n",
    "        print(\"Merging...\")\n",
    "        midpoint_NUC_merged = pd.merge(midpoint_NUC, grouped[['read_id', align_to]], on='read_id', how='left')\n",
    "\n",
    "    # keep first 100 rows of midpoint_NUC_merged\n",
    "    #midpoint_NUC_merged = midpoint_NUC_merged.sample(n=100)\n",
    "    args_list = [(bin_width, occ_cutoff,midpoint_NUC_merged, NUC_width, metadata_cols, align_to, c, ch, d) for c, ch, d in specific_comps]\n",
    "\n",
    "    #processes=multiprocessing.cpu_count()\n",
    "    with Pool(processes=multiprocessing.cpu_count()) as pool:\n",
    "        results = pool.map(calculate_feature2, args_list)\n",
    "\n",
    "    combined_res_df = pd.concat(results)\n",
    "\n",
    "    # Display first 100 rows\n",
    "    nanotools.display_sample_rows(combined_res_df, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a9bae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PLOT NRL STATISTICS\n",
    "import seaborn as sns\n",
    "importlib.reload(nanotools)\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "# Your DataFrame is assumed to be in a variable named `df`\n",
    "\n",
    "# Create an empty list to store the data for the distplot\n",
    "distplot_df = pd.DataFrame()\n",
    "#grouped_plot = grouped[grouped['fiber_NRL_list'].apply(lambda x: len(x) > 0)]\n",
    "grouped_plot = grouped.copy()\n",
    "grouped_plot[\"rel_read_len\"] = grouped_plot[\"rel_read_end\"] - grouped_plot[\"rel_read_start\"]\n",
    "# drop all rows where rel_read_end < bed_window * 2 -100\n",
    "#display(grouped_plot.sample(n=10))\n",
    "#grouped_plot = grouped_plot[grouped_plot[\"rel_read_len\"] >= (bed_window * 2 - 100)]\n",
    "#display(grouped_plot.sample(n=10))\n",
    "# sample_num = minimumum counts of each unique combination of 'type' and 'condition'\n",
    "#sample_num = grouped_plot[['type', 'condition','read_id']].drop_duplicates().groupby(['type', 'condition']).size().min()\n",
    "#print(\"sample_num:\", sample_num)\n",
    "#grouped_plot = grouped_plot.groupby(['condition','type']).apply(lambda x: x.sample(sample_num)).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Iterate over each unique combination of 'type' and 'condition'\n",
    "for unique_comb in grouped_plot[['type', 'condition','chr_type']].drop_duplicates().values:\n",
    "    type_val, condition_val, chr_type_val = unique_comb\n",
    "    print(\"condition_val:\", condition_val, sep=\"\\n\")\n",
    "    # Filter data based on the unique combination of 'type' and 'condition'\n",
    "    sub_df = grouped_plot[(grouped['type'] == type_val) & (grouped_plot['condition'] == condition_val) & (grouped_plot['chr_type'] == chr_type_val)]\n",
    "    # explod sub_df['fiber_NRL_list'] and drop NaN values and convert to column in fiber_data called \"dist\"\n",
    "    fiber_data = sub_df['fiber_NRL_list'].explode().dropna().to_frame()\n",
    "    fiber_data.rename(columns={'fiber_NRL_list': 'dist'}, inplace=True)\n",
    "    # Add a column called \"type\" with the value of type_val and condition_val\n",
    "    fiber_data['condition'] = f\"{condition_val}\" #{type_val}_\n",
    "    fiber_data['type'] = f\"{type_val}\"\n",
    "    fiber_data['chr_type'] = f\"{chr_type_val}\"\n",
    "    # reset index\n",
    "    fiber_data.reset_index(inplace=True, drop=True)\n",
    "    #print length of fiber_data\n",
    "    print(\"len(fiber_data):\", len(fiber_data['condition']))\n",
    "    print(\"len(sub_df):\", len(sub_df['type']))\n",
    "    #display(fiber_data.head(10))\n",
    "    # if distplot_df is empty, set distplot_df equal to fiber_data\n",
    "    if distplot_df.empty:\n",
    "        distplot_df = fiber_data\n",
    "    # else append fiber_data to distplot_df\n",
    "    else:\n",
    "        distplot_df = distplot_df.append(fiber_data)\n",
    "\n",
    "display(distplot_df.sample(n=20))\n",
    "peaks1, fig1 = nanotools.plot_NRL_dist(distplot_df[(distplot_df['condition'] == \"N2_fiber\") & (distplot_df['type'] == type_selected[4])] ,\n",
    "                                       \"X\",\"#16415e\",\"N2\",smoothing_val=0.2)\n",
    "display(peaks1.head(10))\n",
    "peaks2, fig2 = nanotools.plot_NRL_dist(distplot_df[(distplot_df['condition'] == \"SDC2_degron_fiber\") & (distplot_df['type'] == type_selected[4])],\n",
    "                                       \"X\",\"#16415e\",\"SDC2_degron\",smoothing_val=0.2)\n",
    "display(peaks2.head(10))\n",
    "\n",
    "fig = nanotools.plot_NRL_dist_compare(distplot_df[(distplot_df['type'] == type_selected[4])] ,\n",
    "                                      \"X\",\"N2 and SDC2_degron\",smoothing_val=0.2,norm_bool=False,hue='condition')\n",
    "\n",
    "\n",
    "fig.savefig('images/dpy27_sdc2_sdc3_fiber_comparison_NRL.png', dpi=300, bbox_inches='tight')\n",
    "fig.savefig('images/dpy27_sdc2_sdc3_fiber_comparison_NRL.svg', bbox_inches='tight')\n",
    "\n",
    "fig1.savefig('images/dpy27_sdc2_sdc3_fiber_NRL_N2.png', dpi=300, bbox_inches='tight')\n",
    "fig1.savefig('images/dpy27_sdc2_sdc3_fiber_NRL_N2.svg', bbox_inches='tight')\n",
    "\n",
    "fig2.savefig('images/dpy27_sdc2_sdc3_fiber_NRL_SDC2_degron.png', dpi=300, bbox_inches='tight')\n",
    "fig2.savefig('images/dpy27_sdc2_sdc3_fiber_NRL_SDC2_degron.svg', bbox_inches='tight')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98952f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "importlib.reload(nanotools)\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "# Your DataFrame is assumed to be in a variable named `df`\n",
    "\n",
    "# Create an empty list to store the data for the distplot\n",
    "distplot_df = pd.DataFrame()\n",
    "\n",
    "align_on = 'nuc_list_largest_nfr_aligned'\n",
    "    #\"nucs_list\"\n",
    "    # 'nuc_list_internuc_aligned'\n",
    "    # \"nucs_list\"\n",
    "    # \"nucs_list_MAD_aligned\"\n",
    "    # 'nucs_list_n1_aligned'\n",
    "\n",
    "# drop rows from grouped where align_on column is a float\n",
    "grouped_plot = grouped[grouped[align_on].apply(lambda x: isinstance(x, list))]\n",
    "# drop rows where align_on column == []\n",
    "grouped_plot = grouped_plot[grouped_plot[align_on].apply(lambda x: len(x) > 0)]\n",
    "display(grouped_plot.head(10))\n",
    "grouped_plot[\"rel_read_len\"] = grouped_plot[\"rel_read_end\"] - grouped_plot[\"rel_read_start\"]\n",
    "# drop all rows where rel_read_end < bed_window * 2 -100\n",
    "#grouped_plot = grouped_plot[grouped_plot[\"rel_read_len\"] >= (bed_window * 2 - 400)]\n",
    "#sample_num = min(grouped_plot['condition'].value_counts())\n",
    "#grouped_plot = grouped.groupby('condition').apply(lambda x: x.sample(sample_num)).reset_index(drop=True)\n",
    "#display(grouped_plot.sample(n=10))\n",
    "# print number of N2_fiber reads and number of SDC2_degron_fiber reads\n",
    "print(\"N2_fiber reads:\", len(grouped_plot[grouped_plot['condition'] == \"N2_fiber\"]))\n",
    "print(\"SDC2_degron_fiber reads:\", len(grouped_plot[grouped_plot['condition'] == \"SDC2_degron_fiber\"]))\n",
    "\n",
    "# select type for plotting\n",
    "grouped_plot = grouped_plot[grouped_plot['type'] == type_selected[0]]\n",
    "\"\"\"# find the number of reads in each condition, and set num_reads to the max\n",
    "num_reads = min(grouped_plot['condition'].value_counts())\n",
    "# drop reads from condition with most reads to match reads from condition with least reads\n",
    "grouped_plot = grouped_plot.groupby('condition').apply(lambda x: x.sample(num_reads)).reset_index(drop=True)\"\"\"\n",
    "\n",
    "\n",
    "# Iterate over each unique combination of 'type' and 'condition'\n",
    "for unique_comb in grouped_plot[['type', 'condition']].drop_duplicates().values:\n",
    "    type_val, condition_val = unique_comb\n",
    "    print(\"condition_val:\", condition_val, sep=\"\\n\")\n",
    "    # Filter data based on the unique combination of 'type' and 'condition'\n",
    "    sub_df = grouped_plot[(grouped_plot['type'] == type_val) & (grouped_plot['condition'] == condition_val)]\n",
    "    # explod sub_df['fiber_NRL_list'] and drop NaN values and convert to column in fiber_data called \"dist\"\n",
    "    fiber_data = sub_df[align_on].explode().dropna().to_frame()\n",
    "    fiber_data.rename(columns={align_on: 'dist'}, inplace=True)\n",
    "    # Add a column called \"type\" with the value of type_val and condition_val\n",
    "    fiber_data['condition'] = f\"{condition_val}\" #{type_val}_\n",
    "    # reset index\n",
    "    fiber_data.reset_index(inplace=True, drop=True)\n",
    "    # print length of fiber_data\n",
    "    print(\"len(fiber_data):\", len(fiber_data['condition']))\n",
    "    #display(fiber_data.head(10))\n",
    "    # if distplot_df is empty, set distplot_df equal to fiber_data\n",
    "    if distplot_df.empty:\n",
    "        distplot_df = fiber_data\n",
    "    # else append fiber_data to distplot_df\n",
    "    else:\n",
    "        distplot_df = distplot_df.append(fiber_data)\n",
    "\n",
    "nanotools.display_sample_rows(distplot_df, 5)\n",
    "#plot_title = concatenate \"N2\" and align_on\n",
    "plot_title = \"N2_\" + align_on\n",
    "\n",
    "#display(distplot_df.head(10))\n",
    "peaks1,fig1 = nanotools.plot_NRL_dist(distplot_df[(distplot_df['condition'] == \"N2_fiber\") ],\"X\",\"#16415e\",str(\"N2_\"+align_on),smoothing_val=0.2)\n",
    "display(peaks1.head(10))\n",
    "peaks2,fig2 = nanotools.plot_NRL_dist(distplot_df[(distplot_df['condition'] == \"SDC2_degron_fiber\") ],\"X\",\"#16415e\",str(\"SDC2_degron_\"+align_on),smoothing_val=0.2)\n",
    "display(peaks2.head(10))\n",
    "\n",
    "fig = nanotools.plot_NRL_dist_compare(distplot_df,\"X\",\"N2 and SDC2_degron\",smoothing_val=0.2,norm_bool=False,window=bed_window)\n",
    "fig.savefig('images/dpy_27_aligned_on_internuc_KDE_COMB.png', dpi=300, bbox_inches='tight')\n",
    "fig.savefig('images/dpy_27_aligned_on_internuc_KDE_COMB.svg', bbox_inches='tight')\n",
    "#fig.show()\n",
    "\n",
    "fig1.savefig('images/dpy_27_aligned_on_internuc_KDE_N2.png', dpi=300, bbox_inches='tight')\n",
    "fig1.savefig('images/dpy_27_aligned_on_internuc_KDE_N2.svg', bbox_inches='tight')\n",
    "\n",
    "fig2.savefig('images/dpy_27_aligned_on_internuc_KDE_SDC2_degron.png', dpi=300, bbox_inches='tight')\n",
    "fig2.savefig('images/dpy_27_aligned_on_internuc_KDE_SDC2_degron.svg', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007e7c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PLOT PERCENT MAD, NUC, OTHER\n",
    "# Initialize the Plotly figure\n",
    "# Create subplot\n",
    "\n",
    "# Create a temporary helper column that combines 'condition' and 'type'\n",
    "grouped['condition_type'] = grouped['condition'].astype(str) + \"_\" + grouped['type'].astype(str) + \"_\" + grouped['chr_type'].astype(str)\n",
    "\n",
    "# Create subplot with 1 row and 3 columns\n",
    "fig = make_subplots(rows=1, cols=3, subplot_titles=('Percent MAD', 'Percent NUC', 'Percent OTHER'))\n",
    "\n",
    "# Unique metrics\n",
    "metrics = ['percent_MAD', 'percent_NUC', 'percent_OTHER']\n",
    "\n",
    "# Loop over metrics\n",
    "for col, metric in enumerate(metrics, start=1):\n",
    "    fig.add_trace(\n",
    "        go.Box(\n",
    "            y=grouped[metric],\n",
    "            x=grouped['condition_type'],\n",
    "            name=metric,\n",
    "            legendgroup=metric,\n",
    "        ),\n",
    "        row=1, col=col\n",
    "    )\n",
    "\n",
    "# Drop the temporary helper column\n",
    "grouped.drop('condition_type', axis=1, inplace=True)\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Box Plots for Metrics by Condition and Type',\n",
    "    template='plotly_white'\n",
    ")\n",
    "\n",
    "#fig.show()\n",
    "\n",
    "# Extract integer column names\n",
    "int_columns = [col for col in mod_qual_LINK.columns if str(col).isdigit()]\n",
    "\n",
    "# Concatenate all integer columns for LINK and drop NaN values\n",
    "all_values_LINK = pd.concat([mod_qual_LINK[col] for col in int_columns]).dropna()\n",
    "\n",
    "# Concatenate all integer columns for NUC and drop NaN values\n",
    "all_values_NUC = pd.concat([mod_qual_NUC[col] for col in int_columns]).dropna()\n",
    "\n",
    "# Create a subplot with 1 row and 2 columns\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Boxplots\", \"Counts\"))\n",
    "\n",
    "# Add boxplot for LINK to the first subplot\n",
    "fig.add_trace(\n",
    "    go.Box(\n",
    "        y=all_values_LINK,\n",
    "        name='LINK',\n",
    "        marker_color='blue'\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Add boxplot for NUC to the first subplot\n",
    "fig.add_trace(\n",
    "    go.Box(\n",
    "        y=all_values_NUC,\n",
    "        name='NUC',\n",
    "        marker_color='red'\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Add bar chart for counts to the second subplot\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=['LINK', 'NUC'],\n",
    "        y=[len(all_values_LINK), len(all_values_NUC)],\n",
    "        name='Counts',\n",
    "        marker_color=['blue', 'red']\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Box Plots for NUCs and LINKs',\n",
    "    template='plotly_white'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48afdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot n - n+1 nucleosome positioning variance\n",
    "import plotly.graph_objects as go\n",
    "# Assuming combined_res_df is the DataFrame you got after running calculate_feature for all combinations\n",
    "# Process combined_df similar to how you processed results_df\n",
    "df = combined_res_df.copy()\n",
    "\n",
    "# Create an empty list to store the data for the plot\n",
    "data = []\n",
    "\n",
    "# Loop through each unique combination of ['condition','chr_type','type']\n",
    "for comb in df[['condition', 'chr_type', 'type']].drop_duplicates().values:\n",
    "    condition, chr_type, dtype = comb\n",
    "    subset_df = df[(df['condition'] == condition) & (df['chr_type'] == chr_type) & (df['type'] == dtype)]\n",
    "\n",
    "    subs_list = []\n",
    "    nuc_ids = []\n",
    "    # Loop through each unique nuc_id\n",
    "    for nuc_id in subset_df['nuc_id'].unique():\n",
    "        individual_subs_list = subset_df[subset_df['nuc_id'] == nuc_id]['subs_list'].explode().dropna()\n",
    "        subs_list.extend(individual_subs_list)\n",
    "        nuc_ids.extend([nuc_id] * len(individual_subs_list))\n",
    "\n",
    "    trace_name = f\"{condition}_{chr_type}_{dtype}\"  # Name based on the unique combination\n",
    "    trace = go.Box(y=subs_list, name=trace_name, x=nuc_ids,\n",
    "                   #offset boxes from eachother\n",
    "                   offsetgroup=trace_name\n",
    "                    )  # x-axis is nuc_id\n",
    "    data.append(trace)\n",
    "\n",
    "# Create layout\n",
    "layout = go.Layout(\n",
    "    title=\"Boxplot of subs_list for each nuc_id\",\n",
    "    xaxis_title=\"Nucleotide ID\",\n",
    "    yaxis_title=\"Subs List Value\",\n",
    "    template='plotly_white',\n",
    "    boxmode='group'\n",
    ")\n",
    "\n",
    "# Create figure and add data and layout\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "\n",
    "\n",
    "# Show the figure\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f18d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot singe fiber nucleosome positioing and distribution\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Assuming combined_res_df is the DataFrame you got after running calculate_feature for all combinations\n",
    "df = combined_res_df.copy()\n",
    "\n",
    "# Drop all rows where abs(mean_nuc_pos) is > cutoff\n",
    "df = df[df['mean_nuc_pos'].abs() <= 750]\n",
    "\n",
    "# Create subplots with 2 rows and 1 column, sharing the same x-axis\n",
    "fig = make_subplots(rows=2, cols=1, shared_xaxes=True)\n",
    "\n",
    "# Color map to store unique colors for each trace_name\n",
    "color_map = {}\n",
    "\n",
    "# Loop through each unique combination of ['condition','chr_type','type']\n",
    "for idx, comb in enumerate(df[['condition', 'chr_type', 'type']].drop_duplicates().values):\n",
    "    condition, chr_type, dtype = comb\n",
    "    subset_df = df[(df['condition'] == condition) & (df['chr_type'] == chr_type) & (df['type'] == dtype)]\n",
    "\n",
    "    subs_list = []\n",
    "    mean_nuc_pos_list = []\n",
    "    percent_occ_list = []\n",
    "\n",
    "    # Loop through each unique nuc_id\n",
    "    for nuc_id in subset_df['nuc_id'].unique():\n",
    "        nuc_subset = subset_df[subset_df['nuc_id'] == nuc_id]\n",
    "        individual_subs_list = nuc_subset['subs_list'].explode().dropna()\n",
    "        subs_list.extend(individual_subs_list)\n",
    "\n",
    "        mean_nuc_pos_values = nuc_subset['mean_nuc_pos'].unique()\n",
    "        #mean_nuc_pos_values = nuc_subset['std_nuc_pos'].unique()\n",
    "        mean_nuc_pos_list.extend([mean_nuc_pos_values[0]] * len(individual_subs_list))\n",
    "\n",
    "        percent_occ_values = nuc_subset['percent_occ'].unique()\n",
    "        percent_occ_list.append((mean_nuc_pos_values[0], percent_occ_values[0]))\n",
    "\n",
    "    # Generate a unique color for each trace_name\n",
    "    unique_color = f'rgba({50+idx*500},{100+idx*5},{150+idx*5},0.8)'\n",
    "    color_map[tuple(comb)] = unique_color  # Convert numpy array to tuple\n",
    "\n",
    "\n",
    "    # Boxplot Trace\n",
    "    trace_name = f\"{condition}_{chr_type}_{dtype}\"\n",
    "    box_trace = go.Box(y=subs_list, name=trace_name, x=mean_nuc_pos_list,\n",
    "                       width=10, line=dict(color=unique_color))\n",
    "    fig.add_trace(box_trace, row=1, col=1)\n",
    "\n",
    "    # Bar Trace for percent_occ\n",
    "    bar_x, bar_y = zip(*percent_occ_list)\n",
    "    bar_trace = go.Bar(x=bar_x, y=bar_y, name=f\"{trace_name}_percent_occ\",\n",
    "                       marker=dict(color=unique_color), width=10)\n",
    "    fig.add_trace(bar_trace, row=2, col=1)\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title=\"Boxplot of subs_list and Barplot of percent_occ\",\n",
    "    xaxis_title=\"Mean Nucleotide Position\",\n",
    "    yaxis_title=\"Nuceosome offset\",\n",
    "    yaxis2_title=\"Percent Occupancy\",\n",
    "    template='plotly_white'\n",
    ")\n",
    "\n",
    "# Add a dashed vertical line at x=0\n",
    "fig.add_shape(type=\"line\", x0=0, y0=0, x1=0, y1=0.5, line=dict(color=\"Black\", width=1, dash=\"dash\"), row=2, col=1)\n",
    "fig.add_shape(type=\"line\", x0=0, y0=0, x1=0, y1=120, line=dict(color=\"Black\", width=1, dash=\"dash\"), row=1, col=1)\n",
    "\n",
    "# Change x axis tick marks to every 250\n",
    "fig.update_xaxes(dtick=250, row=1, col=1)\n",
    "fig.update_xaxes(dtick=250, row=2, col=1)\n",
    "\n",
    "# Show figure\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9205016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reimport nanotools\n",
    "importlib.reload(nanotools)\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'grouped' is your DataFrame\n",
    "grouped_clean = grouped.copy(deep=True)\n",
    "grouped_clean=grouped_clean.dropna(subset=['inter_nuc_dist'])\n",
    "# keep only rows where type == \"TSS_q4\"\n",
    "grouped_clean = grouped_clean[grouped_clean['type'] == \"strong_rex\"]\n",
    "# sort by type in alphabetical order\n",
    "grouped_clean = grouped_clean.sort_values(by=['condition','chr_type'])\n",
    "\n",
    "# Define subplot structure: 1 row, 2 columns\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=grouped_clean['chr_type'].unique())\n",
    "\n",
    "colors = {\"SDC2_degron_fiber\": 'red', \"N2_fiber\": 'blue', \"X\": 'red', \"Autosome\": 'blue'}\n",
    "\n",
    "# Add traces, specifying the correct subplot for each 'type'\n",
    "for type_value in grouped_clean['chr_type'].unique():\n",
    "    for i, condition_value in enumerate(grouped_clean['condition'].unique()):\n",
    "        df_filtered = grouped_clean[(grouped_clean['condition'] == condition_value) & (grouped_clean['chr_type'] == type_value)]\n",
    "        fig.add_trace(go.Box(\n",
    "            y=df_filtered['inter_nuc_dist'], #- NUC_width,\n",
    "            name=condition_value,\n",
    "            marker_color=colors.get(condition_value, 'black'),  # Default color if condition not in colors dict\n",
    "            fillcolor='rgba(0,0,0,0)',\n",
    "            boxmean=True\n",
    "        ), row=1, col=grouped_clean['chr_type'].unique().tolist().index(type_value) + 1)\n",
    "\n",
    "        # label the mean, centered\n",
    "        fig.add_annotation(\n",
    "            x=grouped_clean['condition'].unique().tolist().index(condition_value),\n",
    "            y=df_filtered['inter_nuc_dist'].mean(), #- NUC_width-50,\n",
    "            text=f\"{df_filtered['inter_nuc_dist'].mean():.2f}\",\n",
    "            showarrow=False,\n",
    "            yshift=8,\n",
    "            font=dict(\n",
    "                size=11,\n",
    "                color=\"black\"\n",
    "            ),\n",
    "            row=1, col=grouped_clean['chr_type'].unique().tolist().index(type_value) + 1\n",
    "        )\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title=\"Nucleosome N+1 - N Distance by Condition and Type\",\n",
    "    yaxis_title=\"Inter-nucleosome Distance (bp)\",\n",
    "    template=\"plotly_white\",\n",
    "    width = 600\n",
    ")\n",
    "\n",
    "# Set y-axis range\n",
    "fig.update_yaxes(range=[(grouped_clean['inter_nuc_dist']-NUC_width).quantile(0.05) * 0.8, (grouped_clean['inter_nuc_dist']-NUC_width).quantile(0.95) * 1.2], row=1, col=1)\n",
    "fig.update_yaxes(range=[(grouped_clean['inter_nuc_dist']-NUC_width).quantile(0.05) * 0.8, (grouped_clean['inter_nuc_dist']-NUC_width).quantile(0.95) * 1.2], row=1, col=2)\n",
    "\n",
    "try:\n",
    "    fig = nanotools.add_p_value_annotation(fig,[[0,1]],1)\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    fig = nanotools.add_p_value_annotation(fig,[[0,1]],2)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "# Define subplot structure: 1 row, 2 columns\n",
    "fig0 = make_subplots(rows=1, cols=2, subplot_titles=grouped_clean['chr_type'].unique())\n",
    "# Assuming 'grouped' is your DataFrame\n",
    "grouped_clean = grouped_clean.dropna(subset=['largest_nfr_size'])\n",
    "\n",
    "# Add traces, specifying the correct subplot for each 'type'\n",
    "for type_value in grouped_clean['chr_type'].unique():\n",
    "    for i, condition_value in enumerate(grouped_clean['condition'].unique()):\n",
    "        df_filtered = grouped_clean[(grouped_clean['condition'] == condition_value) & (grouped_clean['chr_type'] == type_value)]\n",
    "        fig0.add_trace(go.Box(\n",
    "            y=df_filtered['largest_nfr_size'],\n",
    "            name=condition_value,\n",
    "            marker_color=colors.get(condition_value, 'black'),  # Default color if condition not in colors dict\n",
    "            fillcolor='rgba(0,0,0,0)'\n",
    "        ), row=1, col=grouped_clean['chr_type'].unique().tolist().index(type_value) + 1)\n",
    "\n",
    "# Update layout\n",
    "fig0.update_layout(\n",
    "    title=\"Largest NFR Distance by Condition and Type\",\n",
    "    yaxis_title=\"Inter-nucleosome distance (bp)\",\n",
    "    template=\"plotly_white\",\n",
    "    width = 600\n",
    ")\n",
    "\n",
    "# Set y-axis range\n",
    "fig0.update_yaxes(range=[0, grouped_clean['largest_nfr_size'].quantile(0.95) * 1.1], row=1, col=1)\n",
    "fig0.update_yaxes(range=[0, grouped_clean['largest_nfr_size'].quantile(0.95) * 1.1], row=1, col=2)\n",
    "\n",
    "try:\n",
    "    fig0 = nanotools.add_p_value_annotation(fig0,[[0,1]],1)\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    fig0 = nanotools.add_p_value_annotation(fig0,[[0,1]],2)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Step 1: Expand the lists in 'inter_nuc_sub' into separate rows\n",
    "grouped_exploded = grouped_clean.explode('inter_nuc_sub')\n",
    "\n",
    "# Step 2: Drop rows with NaNs (which were originally empty lists)\n",
    "grouped_exploded = grouped_exploded.dropna(subset=['inter_nuc_sub'])\n",
    "\n",
    "# Define subplot structure: 1 row, 2 columns\n",
    "fig2 = make_subplots(rows=1, cols=2, subplot_titles=grouped_exploded['condition'].unique())\n",
    "\n",
    "# Generate a unique color for each 'condition'\n",
    "conditions_selec = grouped_exploded['condition'].unique()\n",
    "# reverse conditions\n",
    "conditions_selec = conditions_selec[::-1]\n",
    "\n",
    "# Add traces, specifying the correct subplot for each 'type'\n",
    "for type_value in grouped_exploded['chr_type'].unique():\n",
    "    for condition in conditions_selec:\n",
    "        df_filtered = grouped_exploded[(grouped_exploded['condition'] == condition) & (grouped_exploded['chr_type'] == type_value)]\n",
    "        fig2.add_trace(go.Box(\n",
    "            y=df_filtered['inter_nuc_sub'],\n",
    "            name=condition,\n",
    "            marker_color=colors.get(condition, 'black'),  # Default color if condition not in colors dict\n",
    "            fillcolor='rgba(0,0,0,0)'\n",
    "        ), row=1, col=grouped_exploded['chr_type'].unique().tolist().index(type_value) + 1)\n",
    "\n",
    "# Update layout\n",
    "fig2.update_layout(\n",
    "    title=\"Per-fiber nucleosome subtraction values\",\n",
    "    yaxis_title=\"Ni+1 - Ni (bp)\",\n",
    "    template=\"plotly_white\",\n",
    "    width = 600\n",
    ")\n",
    "\n",
    "# Set y-axis range\n",
    "fig2.update_yaxes(range=[120, 250], row=1, col=1)\n",
    "fig2.update_yaxes(range=[120, 250], row=1, col=2)\n",
    "\n",
    "# Optional: Add p-value annotation\n",
    "# This depends on the function `nanotools.add_p_value_annotation2`\n",
    "# fig2 = nanotools.add_p_value_annotation2(fig2, [[0,1]])\n",
    "\n",
    "try:\n",
    "    fig2 = nanotools.add_p_value_annotation(fig2,[[0,1]],1)\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    fig2 = nanotools.add_p_value_annotation(fig2,[[0,1]],2)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Reshape the DataFrame for percent_NUC and percent_MAD plotting\n",
    "nanotools.display_sample_rows(grouped_clean)\n",
    "melted_df = grouped_clean.melt(id_vars=['condition','chr_type'], value_vars=['percent_NUC', 'percent_MAD'], var_name='stat', value_name='percent')\n",
    "melted_df = melted_df.dropna(subset=['percent'])\n",
    "# reset index\n",
    "melted_df.reset_index(inplace=True, drop=True)\n",
    "conditions_list = melted_df['condition'].unique()\n",
    "fig3 = make_subplots(rows=2, cols=2, subplot_titles=melted_df['condition'].unique(),shared_yaxes=True)\n",
    "\n",
    "# Adding plots for percent_NUC and percent_MAD by condition\n",
    "for each_cond in conditions_list:\n",
    "    for each_type in melted_df['chr_type'].unique():\n",
    "        df_filtered = melted_df[(melted_df['condition'] == each_cond) & (melted_df['chr_type'] == each_type)]\n",
    "        #display(df_filtered.head(10))\n",
    "        fig3.add_trace(go.Box(y=df_filtered[df_filtered['stat'] == 'percent_NUC']['percent'],\n",
    "                              name=each_type,\n",
    "                              marker_color=colors.get(each_cond, 'black'),  # Default color if condition not in colors dict\n",
    "                              fillcolor='rgba(0,0,0,0)'  # Transparent fill\n",
    "                              ), row=1, col=melted_df['condition'].unique().tolist().index(each_cond) + 1)\n",
    "        fig3.add_trace(go.Box(y=df_filtered[df_filtered['stat'] == 'percent_MAD']['percent'],\n",
    "                              name=each_type,\n",
    "                              marker_color=colors.get(each_cond, 'black'),  # Default color if condition not in colors dict\n",
    "                              fillcolor='rgba(0,0,0,0)'  # Transparent fill\n",
    "                              ), row=2, col=melted_df['condition'].unique().tolist().index(each_cond) + 1)\n",
    "\n",
    "\n",
    "# Update layout for the fifth row's y-axis to show percentage\n",
    "fig3.update_yaxes(title_text=\"Percentage\", tickformat='.0%', row=1, col=1)\n",
    "fig3.update_yaxes(title_text=\"Percentage\", tickformat='.0%', row=2, col=1)\n",
    "# update y range of row=2 col=1\n",
    "\n",
    "# Update layout\n",
    "fig3.update_layout(\n",
    "    title=\"Boxplot of % Nucleosome and % NFR\",\n",
    "    yaxis_title=\"% Occupied\",\n",
    "    template=\"plotly_white\",\n",
    "    width = 600,\n",
    "    height = 800\n",
    "    # group box plots of the same subplot together\n",
    ")\n",
    "\n",
    "# set y axis range between 20% and 110%\n",
    "fig3.update_yaxes(range=[0.2, 1.1], row=1, col=1)\n",
    "fig3.update_yaxes(range=[0.2, 1.1], row=1, col=2)\n",
    "# set y axis range between 20% and 110%\n",
    "fig3.update_yaxes(range=[0, 0.95], row=2, col=1)\n",
    "fig3.update_yaxes(range=[0, 0.95], row=2, col=2)\n",
    "\n",
    "try:\n",
    "    fig3 = nanotools.add_p_value_annotation(fig3,[[0,1]],1)\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    fig3 = nanotools.add_p_value_annotation(fig3,[[0,1]],2)\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    fig3 = nanotools.add_p_value_annotation(fig3,[[0,1]],3)\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    fig3 = nanotools.add_p_value_annotation(fig3,[[0,1]],4)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Show plot\n",
    "fig.show()\n",
    "fig0.show()\n",
    "fig2.show()\n",
    "fig3.show()\n",
    "# save fig to images_11_14_23/inter_nuc_dist_boxplot_strong_rex.png and .svg\n",
    "fig.write_image(\"images_11_14_23/inter_nuc_dist_boxplot_strong_rex.png\")\n",
    "fig.write_image(\"images_11_14_23/inter_nuc_dist_boxplot_strong_rex.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d99ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'grouped' is your DataFrame\n",
    "grouped_clean = grouped.copy(deep=True)\n",
    "grouped_clean=grouped_clean.dropna(subset=['inter_nuc_dist'])\n",
    "# keep only rows where type == \"TSS_q4\" or == \"TSS_q3\"\n",
    "grouped_clean = grouped_clean[grouped_clean['type'].isin([\"strong_rex\"])]\n",
    "#grouped_clean = grouped_clean[grouped_clean['chr_type'] == \"Autosome\"]\n",
    "nanotools.display_sample_rows(grouped_clean)\n",
    "\n",
    "colors = {\"SDC2_degron_fiber\": 'red', \"N2_fiber\": 'blue', \"X\": 'red', \"Autosome\": 'blue'}\n",
    "\n",
    "### Plot dist of internuc dist\n",
    "# Parameters\n",
    "bin_width = 1\n",
    "bin_range = range(147-NUC_width, 700, bin_width)  # Bin range from 170 to 600\n",
    "\n",
    "# Grouping the data by 'condition', 'type', and 'chr_type'\n",
    "grouped_clean_grouped = grouped_clean.groupby(['condition', 'type', 'chr_type'])\n",
    "\n",
    "# Updated plot data preparation with cumulative percentages\n",
    "cumulative_percentage_plot_data = []\n",
    "\n",
    "for group_name, group_df in grouped_clean_grouped:\n",
    "    group_df['inter_nuc_dist_adjusted'] = group_df['inter_nuc_dist'].clip(lower=147, upper=600+NUC_width) -NUC_width\n",
    "    # Total number of reads in the group\n",
    "    total_reads = group_df.shape[0]\n",
    "    # Create bins and calculate cumulative count for each bin\n",
    "    bin_counts = [group_df[group_df['inter_nuc_dist_adjusted'] >= bin_edge].shape[0] for bin_edge in bin_range]\n",
    "\n",
    "    # Convert counts to a cumulative count\n",
    "    #cumulative_counts = np.cumsum(bin_counts)\n",
    "\n",
    "    # Convert cumulative counts to percentages (excluding the first bin)\n",
    "    bin_percentages = (np.array(bin_counts[1:]) / total_reads)\n",
    "\n",
    "    # Bin centers for plotting (excluding the first bin)\n",
    "    bin_centers = np.array(bin_range[1:]) + (bin_width / 2)\n",
    "\n",
    "    # Adding to cumulative percentage plot data\n",
    "    cumulative_percentage_plot_data.append(go.Scatter(x=bin_centers,\n",
    "                                                      y=bin_percentages,\n",
    "                                                      mode='lines',\n",
    "                                                      name=str(group_name[0]+' '+group_name[2]+' '+group_name[1])))\n",
    "                                                      #marker_color=colors.get(group_name[0], 'black')))\n",
    "\n",
    "# Create the updated plot layout for cumulative percentages\n",
    "cumulative_percentage_layout = go.Layout(\n",
    "    title='Cumulative Percentage Distribution of inter_nuc_dist',\n",
    "    xaxis=dict(title='Inter-nucleosome distance (bp)'),\n",
    "    yaxis=dict(title='Cumulative Percentage of Reads (%)'),\n",
    "    template='plotly_white',\n",
    "    width = 600\n",
    ")\n",
    "\n",
    "# Set y axis to %\n",
    "cumulative_percentage_layout['yaxis']['tickformat'] = '.0%'\n",
    "\n",
    "# Create the updated figure for cumulative percentages\n",
    "cumulative_percentage_fig = go.Figure(data=cumulative_percentage_plot_data, layout=cumulative_percentage_layout)\n",
    "\n",
    "# Grouping the data by 'bed_start', 'chrom', 'condition', 'type', and 'chr_type'\n",
    "grouped_genes = grouped_clean.groupby(['bed_start', 'chrom', 'condition', 'type', 'chr_type'])\n",
    "\n",
    "### Plot difference plot\n",
    "# Assuming the first two groups are the ones you want to compare\n",
    "first_group_data = None\n",
    "second_group_data = None\n",
    "\n",
    "for i, (group_name, group_df) in enumerate(grouped_clean_grouped):\n",
    "    group_df['inter_nuc_dist_adjusted'] = group_df['inter_nuc_dist'].clip(lower=147, upper=600+NUC_width) - NUC_width\n",
    "    total_reads = group_df.shape[0]\n",
    "    bin_counts = [group_df[group_df['inter_nuc_dist_adjusted'] >= bin_edge].shape[0] for bin_edge in bin_range]\n",
    "    bin_percentages = (np.array(bin_counts[1:]) / total_reads)\n",
    "    bin_centers = np.array(bin_range[1:]) + (bin_width / 2)\n",
    "\n",
    "    if i == 0:\n",
    "        first_group_data = bin_percentages\n",
    "    elif i == 1:\n",
    "        second_group_data = bin_percentages\n",
    "        break\n",
    "\n",
    "# Calculate the difference between the first and second group\n",
    "difference = first_group_data - second_group_data\n",
    "\n",
    "# Plot for the difference\n",
    "difference_plot_data = go.Scatter(x=bin_centers, y=difference, mode='lines', name='Difference between first and second group')\n",
    "difference_layout = go.Layout(\n",
    "    title='Difference in Cumulative Percentage Distribution between First and Second Group',\n",
    "    xaxis=dict(title='Inter-nucleosome distance (bp)'),\n",
    "    yaxis=dict(title='Difference in Cumulative Percentage (%)'),\n",
    "    template='plotly_white',\n",
    "    width = 600,\n",
    "    #yaxis tickformat\n",
    "    yaxis_tickformat = '.0%'\n",
    ")\n",
    "\n",
    "difference_fig = go.Figure(data=[difference_plot_data], layout=difference_layout)\n",
    "\n",
    "\n",
    "# Preparing data for the box plot\n",
    "box_plot_data = []\n",
    "\n",
    "for group_name, group_df in grouped_genes:\n",
    "    # Calculate the percentage of reads in open configuration (inter_nuc_dist >= 180)\n",
    "    percent_open_reads = (group_df['inter_nuc_dist'] >= 190).sum() / group_df.shape[0]\n",
    "    #print(\"Starting on:\", group_name,\"with this many reads:\", len(group_df),\"with % open:\", percent_open_reads,sep=\"\\n\")\n",
    "\n",
    "    # Append the result with the condition and chr_type as categories\n",
    "    condition, chr_type = group_name[2], group_name[4]\n",
    "    box_plot_data.append({'condition': condition, 'chr_type': chr_type, 'percent_open_reads': percent_open_reads})\n",
    "\n",
    "\n",
    "# Convert to DataFrame for easier plotting\n",
    "box_plot_df = pd.DataFrame(box_plot_data)\n",
    "nanotools.display_sample_rows(box_plot_df)\n",
    "\n",
    "# Create the box plot\n",
    "box_fig = go.Figure()\n",
    "\n",
    "# Adding box plots for each chr_type\n",
    "chr_types = box_plot_df['chr_type'].unique()\n",
    "for chr_type in chr_types:\n",
    "    filtered_df = box_plot_df[box_plot_df['chr_type'] == chr_type]\n",
    "    box_fig.add_trace(go.Box(y=filtered_df['percent_open_reads'],\n",
    "                             x=filtered_df['condition'],\n",
    "                             name=chr_type,\n",
    "                             boxpoints='all',\n",
    "                             jitter=0.3))\n",
    "\n",
    "\n",
    "# Update layout\n",
    "box_fig.update_layout(\n",
    "    title='Percentage of Reads in Open Configuration by Condition and Chr_Type',\n",
    "    xaxis_title='Condition',\n",
    "    yaxis_title='Percentage of Reads in Open Configuration (%)',\n",
    "    template='plotly_white',\n",
    "    # group box plots\n",
    "    boxmode='group',\n",
    "    width = 300\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Create a box plot\n",
    "n1_fig = go.Figure()\n",
    "\n",
    "# Adding box plots for each chr_type\n",
    "chr_types = grouped_clean['condition'].unique()\n",
    "# flip the order of chr_types\n",
    "#chr_types = chr_types[::-1]\n",
    "for chr_type in chr_types:\n",
    "    filtered_df = grouped_clean[grouped_clean['condition'] == chr_type]\n",
    "    n1_fig.add_trace(go.Box(y=filtered_df['smallest_positive_nuc_midpoint'],\n",
    "                            #x=filtered_df['condition'],\n",
    "                            name=chr_type,\n",
    "                            marker_color=colors.get(chr_type, 'black'),\n",
    "                            fillcolor='rgba(0,0,0,0)'))  # Transparent fill\n",
    "\n",
    "n1_fig.update_layout(\n",
    "    title=\"Distribution of N+1 nucleosome position\",\n",
    "    xaxis_title=\"Type\",\n",
    "    yaxis_title=\"Smallest Positive Nucleotide Midpoint\",\n",
    "    template=\"plotly_white\",\n",
    "    width = 300,\n",
    "    #do not show legend\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "# Calculating and printing the variance\n",
    "print(\"\\nVariance of 'smallest_positive_nuc_midpoint' by 'condition':\")\n",
    "conditions = grouped_clean['condition'].unique()\n",
    "for condition in conditions:\n",
    "    subset_df = grouped_clean[grouped_clean['condition'] == condition]\n",
    "    variance = subset_df['smallest_positive_nuc_midpoint'].var()\n",
    "    print(f\"Condition: {condition}, Variance: {variance:.2f}\")\n",
    "\n",
    "# Show the updated plot\n",
    "cumulative_percentage_fig.show()\n",
    "\n",
    "difference_fig.show()\n",
    "# Show plot\n",
    "box_fig.show()\n",
    "# Distribution of N+1 nucleosome\n",
    "n1_fig.show()\n",
    "\n",
    "# save cumulative_percentage_fig and difference_fig to png and svg files in images_11_14_23/\n",
    "cumulative_percentage_fig.write_image(\"images_11_14_23/cumulative_percentage_dist_strong_rex.png\")\n",
    "cumulative_percentage_fig.write_image(\"images_11_14_23/cumulative_percentage_dist_strong_rex.svg\")\n",
    "difference_fig.write_image(\"images_11_14_23/difference_cumulative_percentage_dist_strong_rex.png\")\n",
    "difference_fig.write_image(\"images_11_14_23/difference_cumulative_percentage_dist_strong_rex.svg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce0ebd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot accessibility pileups based on single fiber alignments\n",
    "print(\"plot_df\")\n",
    "display(plot_df.head(3))\n",
    "print(\"grouped\")\n",
    "display(grouped.head(3))\n",
    "print(\"coverage_df\")\n",
    "display(coverage_df.head(3))\n",
    "# merge comb_bedmethyl_plot_df with grouped on read_id addig grouped's 'closest_nuc' column\n",
    "print(\"Merging plot_df and grouped...\")\n",
    "merged_df_access = pd.merge(plot_df, grouped[['read_id', 'closest_nuc','closest_MAD_midpoint','smallest_positive_nuc_midpoint','greatest_negative_nuc_midpoint']], on='read_id', how='left')\n",
    "# drop all rows where closest_nuc is NaN\n",
    "#merged_df_access.dropna(subset=['closest_nuc'], inplace=True)\n",
    "\n",
    "#subtract closest_nuc from rel_pos\n",
    "#merged_df_access['rel_pos'] -= merged_df_access['closest_MAD_midpoint']\n",
    "#merged_df_access['rel_pos'] -= ((merged_df_access['smallest_positive_nuc_midpoint']-merged_df_access['greatest_negative_nuc_midpoint'])/2+merged_df_access['greatest_negative_nuc_midpoint'])\n",
    "\n",
    "# drop rows such that each unique combination of condition, type and chr_type has the same number of records. Match the combination that has the fewest.\n",
    "\n",
    "# Group by 'condition', 'type', 'chr-type' and 'rel_pos', adding sum and count of 'mod_qual' column\n",
    "group_merge = merged_df_access.groupby(['condition', 'type', 'chr_type', 'rel_pos'])['mod_qual_bin'].agg(['sum', 'count']).reset_index()\n",
    "group_merge['raw_meth_frac'] = group_merge['sum'] / group_merge['count']\n",
    "\n",
    "# group coverage_df by 'condition' summing total_m6A and total_A\n",
    "coverage_group = coverage_df.groupby(['condition'])['total_m6a', 'total_A_m6a'].sum().reset_index()\n",
    "# set condition_m6A_frac to total_m6A / total_A\n",
    "coverage_group['condition_m6A_frac'] = coverage_group['total_m6a'] / coverage_group['total_A_m6a']\n",
    "\n",
    "#merge group_merge with coverage_group on 'condition'\n",
    "group_merge = pd.merge(group_merge, coverage_group[['condition', 'condition_m6A_frac']], on='condition', how='left')\n",
    "#Add weighted_m6A_frac column\n",
    "group_merge['weighted_norm_mod_frac'] = group_merge['raw_meth_frac'] / group_merge['condition_m6A_frac']\n",
    "\n",
    "#Rename rel_pos column to rel_start\n",
    "group_merge.rename(columns={'rel_pos': 'rel_start'}, inplace=True)\n",
    "display(group_merge.sample(n=10))\n",
    "\n",
    "\n",
    "region_fig = plot_bedmethyl(group_merge, conditions, chr_types=[\"X\"], types=[\"strong_rex\"], strands=[\"all\"], window_size=50, selection_indices=[0,1,2], bed_window=1500)\n",
    "\n",
    "# save region_fig to temp folder\n",
    "region_fig[0].show()\n",
    "#region_fig[0].write_image(\"images/dpy27_sdc2_sdc3_aligned_nearest_nuc_n2vsSDC2.svg\")\n",
    "#region_fig[0].write_image(\"images/dpy27_sdc2_sdc3_aligned_nearest_nuc_n2vsSDC2.png\",width=1600,height=1300)\n",
    "# merge grouped with coverage_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888ee2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Downsamplin for read plotting\n",
    "n_read_ids = 100000  # max reads / condition for plotting\n",
    "# Function to downsample each group\n",
    "def downsample_group(group):\n",
    "    global bed_window\n",
    "    print(\"\\nProcessing group:\", group.name)  # Display group name (combination of 'condition', 'chr_type', 'type')\n",
    "    unique_read_ids = group['read_id'].unique()\n",
    "\n",
    "    # Check if downsampling is needed\n",
    "    if len(unique_read_ids) > n_read_ids:\n",
    "        # Filter read_ids based on length requirement\n",
    "        sampled_read_ids_long = group[\n",
    "            (group['rel_read_end'] - group['rel_read_start']) > (3/4 * bed_window)\n",
    "        ]['read_id'].unique()\n",
    "\n",
    "        # Sample read_ids based on the number required\n",
    "        if len(sampled_read_ids_long) > n_read_ids:\n",
    "            sampled_read_ids = pd.Series(sampled_read_ids_long).sample(n=n_read_ids).tolist()\n",
    "        else:\n",
    "            # Include additional read_ids if long ones are not enough\n",
    "            sampled_read_ids = sampled_read_ids_long.tolist()\n",
    "            remaining_ids = group[~group['read_id'].isin(sampled_read_ids)]['read_id'].unique()\n",
    "            additional_sampled_ids = pd.Series(remaining_ids).sample(n=(n_read_ids - len(sampled_read_ids))).tolist()\n",
    "            sampled_read_ids.extend(additional_sampled_ids)\n",
    "\n",
    "        downsampled_group = group[group['read_id'].isin(sampled_read_ids)]\n",
    "        return downsampled_group\n",
    "    else:\n",
    "        return group\n",
    "\n",
    "def process_bam_files(bam_exp_pairs):\n",
    "    data = []\n",
    "    for bam_file, exp_id in bam_exp_pairs:\n",
    "        with pysam.AlignmentFile(bam_file, \"rb\") as bam:\n",
    "            for read in bam.fetch():  # Adjust the number as needed for actual use\n",
    "                data.append({\n",
    "                    \"bam_file_name\": bam_file,\n",
    "                    \"read_id\": read.query_name,\n",
    "                    \"exp_id\": exp_id,\n",
    "                })\n",
    "    return data\n",
    "\n",
    "def parallel_process_bam_files(bam_files, exp_ids, num_processes=None):\n",
    "    # Pair bam files with their corresponding experiment IDs\n",
    "    paired_bam_exp = list(zip(bam_files, exp_ids))\n",
    "\n",
    "    # Determine the number of processes to use\n",
    "    if num_processes is None:\n",
    "        num_processes = multiprocessing.cpu_count()\n",
    "\n",
    "    # Adjust num_processes if it's greater than the length of paired_bam_exp\n",
    "    num_processes = min(num_processes, len(paired_bam_exp))\n",
    "\n",
    "    # Ensure chunksize is at least 1\n",
    "    chunksize = max(1, len(paired_bam_exp) // num_processes)\n",
    "\n",
    "    with multiprocessing.Pool(num_processes) as pool:\n",
    "        results = pool.map(process_bam_files, [paired_bam_exp[i:i + chunksize] for i in range(0, len(paired_bam_exp), chunksize)])\n",
    "\n",
    "    # Combine the results from all processes\n",
    "    combined_data = [item for sublist in results for item in sublist]\n",
    "    return combined_data\n",
    "\n",
    "merged_df = plot_df.copy(deep=True)\n",
    "\n",
    "# Apply downsampling for each unique combination of 'condition', 'chr_type', 'type'\n",
    "down_sampled_plot_df = merged_df.groupby(['condition', 'chr_type', 'type']).apply(downsample_group).reset_index(drop=True)\n",
    "# print number of groups\n",
    "print(\"Number of groups:\", len(down_sampled_plot_df.groupby(['condition', 'chr_type', 'type'])))\n",
    "\n",
    "# Further processing on the downsampled DataFrame\n",
    "try:\n",
    "    down_sampled_plot_df = down_sampled_plot_df.sort_values(by=['smallest_positive_nuc_midpoint', 'greatest_negative_nuc_midpoint'], ascending=[True, True])\n",
    "    down_sampled_plot_df.reset_index(inplace=True, drop=True)\n",
    "    down_sampled_group_df = grouped[grouped['read_id'].isin(down_sampled_plot_df['read_id'])]\n",
    "\n",
    "# else\n",
    "except:\n",
    "    print(\"Missing nucs, proceeding without...\")\n",
    "\n",
    "    # Assuming 'grouped' is another DataFrame you want to filter based on the downsampled read_ids\n",
    "    if 'exp_id' not in merged_df.columns:\n",
    "        print(\"Missing exp_id column, adding it...\")\n",
    "        data = parallel_process_bam_files(new_bam_files, exp_ids)\n",
    "\n",
    "        # Convert the list of dictionaries to a DataFrame\n",
    "        read_exp_ids_df = pd.DataFrame(data)\n",
    "\n",
    "    # Add exp_id column to grouped dataframe by merging on read_id\n",
    "    # if exp_id column does not exist in grouped then:\n",
    "\n",
    "        down_sampled_plot_df = pd.merge(down_sampled_plot_df, read_exp_ids_df[['read_id', 'exp_id']], on='read_id', how='left')\n",
    "\n",
    "    else:\n",
    "        down_sampled_plot_df = pd.merge(down_sampled_plot_df, read_exp_ids_df[['read_id', 'exp_id']], on='read_id', how='left')\n",
    "\n",
    "print(\"Number of unique reads in down_sampled_plot_df:\", len(down_sampled_plot_df['read_id'].unique()))\n",
    "nanotools.display_sample_rows(down_sampled_plot_df)\n",
    "#nanotools.display_sample_rows(down_sampled_group_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d368c96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataframe(down_sampled_plot_df, min_mod_qual, selec_conds, selec_type, selected_chr_type, num_read_ids, rel_pos_window, smoothing_window):\n",
    "    mC_aligned_dfs = []\n",
    "    output_df = pd.DataFrame()\n",
    "    read_id_suffix = 1\n",
    "\n",
    "    for selec_cond in selec_conds:\n",
    "        down_sampled_plot_df_filtered = down_sampled_plot_df[(down_sampled_plot_df['condition'] == selec_cond) & (down_sampled_plot_df['type'] == selec_type) & (down_sampled_plot_df['chr_type'] == selected_chr_type)]\n",
    "\n",
    "        # Limit the read_IDs if necessary\n",
    "        if num_read_ids is not None:\n",
    "            read_ids_to_keep = down_sampled_plot_df_filtered['read_id'].unique()[:num_read_ids]  # Get first num_read_ids\n",
    "            mC_aligned_df = down_sampled_plot_df_filtered[down_sampled_plot_df_filtered['read_id'].isin(read_ids_to_keep)]\n",
    "\n",
    "        # 1. Create and align the 'mC_aligned_df'\n",
    "        print(f\"*** Filtering for 5mC candidates in condition {selec_cond} ***\")\n",
    "        mC_aligned_df = mC_aligned_df[\n",
    "            (mC_aligned_df['mod_code'] == 'm') &\n",
    "            (mC_aligned_df['query_kmer'].str[1].isin(['C', 'G'])) &\n",
    "            (mC_aligned_df['mod_qual'] > min_mod_qual)\n",
    "        ].copy()\n",
    "        nanotools.display_sample_rows(mC_aligned_df)\n",
    "\n",
    "        print(f\"*** Filtering for rows with three nearby rows meeting the same condition in condition {selec_cond} ***\")\n",
    "        mC_aligned_df['nearby_rows_count'] = mC_aligned_df.apply(lambda row: mC_aligned_df[\n",
    "            (mC_aligned_df['read_id'] == row['read_id']) &\n",
    "            (mC_aligned_df.index != row.name) &\n",
    "            (abs(mC_aligned_df['rel_pos'] - row['rel_pos']) <= 10) &\n",
    "            (mC_aligned_df['mod_code'] == 'm') &\n",
    "            (mC_aligned_df['query_kmer'].str[1].isin(['C', 'G'])) &\n",
    "            (mC_aligned_df['mod_qual'] > min_mod_qual) &\n",
    "            (~mC_aligned_df['read_id'].isin(mC_aligned_df[\n",
    "                (mC_aligned_df['read_id'] == row['read_id']) &\n",
    "                (mC_aligned_df['mod_code'] == 'a') &\n",
    "                (mC_aligned_df['mod_qual'] > min_mod_qual) &\n",
    "                (abs(mC_aligned_df['rel_pos'] - row['rel_pos']) <= 10)\n",
    "            ]['read_id'].unique()))\n",
    "        ].shape[0], axis=1)\n",
    "\n",
    "        mC_aligned_df = mC_aligned_df[mC_aligned_df['nearby_rows_count'] >= 1].copy()\n",
    "        mC_aligned_df.drop(columns=['nearby_rows_count'], inplace=True)\n",
    "        nanotools.display_sample_rows(mC_aligned_df)\n",
    "\n",
    "        print(f\"*** Shifting to midpoint and skipping clustered rows for condition {selec_cond} ***\")\n",
    "        mC_aligned_df['cluster_id'] = mC_aligned_df.groupby('read_id').ngroup()\n",
    "        mC_aligned_df['min_rel_pos'] = mC_aligned_df.groupby('cluster_id')['rel_pos'].transform('min')\n",
    "        mC_aligned_df['max_rel_pos'] = mC_aligned_df.groupby('cluster_id')['rel_pos'].transform('max')\n",
    "        mC_aligned_df['midpoint_rel_pos'] = (mC_aligned_df['min_rel_pos'] + mC_aligned_df['max_rel_pos']) / 2\n",
    "        # round to nearest integer\n",
    "        mC_aligned_df['midpoint_rel_pos'] = mC_aligned_df['midpoint_rel_pos'].round()\n",
    "        mC_aligned_df = mC_aligned_df.drop_duplicates(subset=['cluster_id'], keep='first')\n",
    "        mC_aligned_df['rel_pos'] = mC_aligned_df['midpoint_rel_pos']\n",
    "        mC_aligned_df = mC_aligned_df.drop(columns=['cluster_id', 'min_rel_pos', 'max_rel_pos', 'midpoint_rel_pos'])\n",
    "        nanotools.display_sample_rows(mC_aligned_df)\n",
    "\n",
    "        print(f\"*** Aligning reads efficiently for condition {selec_cond} ***\")\n",
    "        mC_aligned_df['rel_pos_lower'] = mC_aligned_df['rel_pos'] - rel_pos_window\n",
    "        mC_aligned_df['rel_pos_upper'] = mC_aligned_df['rel_pos'] + rel_pos_window\n",
    "        mC_aligned_df = pd.merge(mC_aligned_df[['read_id', 'rel_pos', 'rel_pos_lower', 'rel_pos_upper']], down_sampled_plot_df_filtered, on='read_id')\n",
    "        mC_aligned_df = mC_aligned_df[\n",
    "            (mC_aligned_df['rel_pos_y'] >= mC_aligned_df['rel_pos_lower']) &\n",
    "            (mC_aligned_df['rel_pos_y'] <= mC_aligned_df['rel_pos_upper'])\n",
    "        ]\n",
    "        mC_aligned_df['rel_pos'] = mC_aligned_df['rel_pos_y'] - mC_aligned_df['rel_pos_x']\n",
    "        mC_aligned_df = mC_aligned_df.drop(columns=['rel_pos_lower', 'rel_pos_upper', 'rel_pos_x', 'rel_pos_y'])\n",
    "        nanotools.display_sample_rows(mC_aligned_df)\n",
    "\n",
    "        # Increment the suffix for read_id\n",
    "        mC_aligned_df['read_id'] = mC_aligned_df['read_id'].astype(str) + '_' + str(read_id_suffix)\n",
    "        read_id_suffix += 1\n",
    "\n",
    "        # Append mC_aligned_df to output_df\n",
    "        if output_df.empty:\n",
    "            output_df = mC_aligned_df\n",
    "        else:\n",
    "            output_df = pd.concat([output_df, mC_aligned_df])\n",
    "\n",
    "        # 2. Group, Calculate Fractions, and Filter\n",
    "        print(f\"*** Grouping, calculating, and filtering for condition {selec_cond} ***\")\n",
    "        grouped_df = mC_aligned_df.groupby('rel_pos')[\n",
    "            ['canonical_base', 'query_kmer', 'mod_qual']\n",
    "        ].apply(lambda g: pd.Series([\n",
    "            g[g['canonical_base'] == 'A'].shape[0],\n",
    "            g[(g['canonical_base'] == 'C') & (g['query_kmer'].str[1].isin(['C', 'G']))].shape[0],\n",
    "            g[(g['canonical_base'] == 'A') & (g['mod_qual'] > min_mod_qual)].shape[0],\n",
    "            g[(g['canonical_base'] == 'C') & (g['query_kmer'].str[1].isin(['C', 'G'])) & (g['mod_qual'] > min_mod_qual)].shape[0]\n",
    "        ], index=['A', 'GC_CC', 'm6A', '5mC']))\n",
    "        grouped_df.reset_index(inplace=True)\n",
    "        nanotools.display_sample_rows(grouped_df)\n",
    "\n",
    "        grouped_df['A_frac'] = grouped_df['A'] / (grouped_df['A'] + grouped_df['GC_CC'])\n",
    "        grouped_df['GC_CC_frac'] = grouped_df['GC_CC'] / (grouped_df['A'] + grouped_df['GC_CC'])\n",
    "        grouped_df['m6A_frac'] = grouped_df['m6A'] / grouped_df['A']\n",
    "        grouped_df['5mC_frac'] = grouped_df['5mC'] / grouped_df['GC_CC']\n",
    "\n",
    "        # Apply rolling average smoothing\n",
    "        m6A_frac_0 = grouped_df.loc[grouped_df['rel_pos'] == 0, 'm6A_frac'].iloc[0]\n",
    "        grouped_df.loc[grouped_df['rel_pos'] == 0, 'm6A_frac'] = np.nan\n",
    "        grouped_df['m6A_frac_smoothed'] = grouped_df['m6A_frac'].rolling(window=smoothing_window, center=True).mean()\n",
    "        grouped_df.loc[grouped_df['rel_pos'] == 0, 'm6A_frac_smoothed'] = m6A_frac_0\n",
    "\n",
    "        fivemC_frac_0 = grouped_df.loc[grouped_df['rel_pos'] == 0, '5mC_frac'].iloc[0]\n",
    "        grouped_df.loc[grouped_df['rel_pos'] == 0, '5mC_frac'] = np.nan\n",
    "        grouped_df['5mC_frac_smoothed'] = grouped_df['5mC_frac'].rolling(window=smoothing_window, center=True).mean()\n",
    "        grouped_df.loc[grouped_df['rel_pos'] == 0, '5mC_frac_smoothed'] = fivemC_frac_0\n",
    "\n",
    "        mC_aligned_dfs.append(grouped_df)\n",
    "\n",
    "    # Plot m6A_frac vs. 5mC_frac for all conditions (skipping bp 0)\n",
    "    fig1 = go.Figure()\n",
    "    for i, mC_aligned_df in enumerate(mC_aligned_dfs):\n",
    "        fig1.add_trace(go.Scatter(x=mC_aligned_df[mC_aligned_df['rel_pos'] != 0]['rel_pos'], y=mC_aligned_df[mC_aligned_df['rel_pos'] != 0]['m6A_frac_smoothed'], mode='lines', name=f'{selec_conds[i]} - m6A_frac'))\n",
    "        fig1.add_trace(go.Scatter(x=mC_aligned_df[mC_aligned_df['rel_pos'] != 0]['rel_pos'], y=mC_aligned_df[mC_aligned_df['rel_pos'] != 0]['5mC_frac_smoothed'], mode='lines', name=f'{selec_conds[i]} - 5mC_frac'))\n",
    "    fig1.update_layout(title='Methylation Fractions (Smoothed)', xaxis_title='Absolute Relative Position', yaxis_title='Fraction', template='plotly_white', showlegend=True)\n",
    "\n",
    "    # Plot count of A and count of GC_CC for all conditions\n",
    "    fig2 = go.Figure()\n",
    "    for i, mC_aligned_df in enumerate(mC_aligned_dfs):\n",
    "        fig2.add_trace(go.Scatter(x=mC_aligned_df['rel_pos'], y=mC_aligned_df['A'], mode='markers+lines', name=f'{selec_conds[i]} - Count of A'))\n",
    "        fig2.add_trace(go.Scatter(x=mC_aligned_df['rel_pos'], y=mC_aligned_df['GC_CC'], mode='markers+lines', name=f'{selec_conds[i]} - Count of GC_CC'))\n",
    "    fig2.update_layout(title='Count of A and GC_CC', xaxis_title='Absolute Relative Position', yaxis_title='Count', template='plotly_white', showlegend=True)\n",
    "\n",
    "    fig1.show(renderer='plotly_mimetype+notebook')\n",
    "    fig2.show(renderer='plotly_mimetype+notebook')\n",
    "\n",
    "    nanotools.display_sample_rows(output_df)\n",
    "    return output_df\n",
    "\n",
    "# Execute with your data and configurations\n",
    "mC_aligned_df = process_dataframe(\n",
    "    down_sampled_plot_df.copy(),\n",
    "    min_mod_qual=0.8,\n",
    "    selec_conds=[\"60_old_ama1_3xGNB_GFPHia5_mChMCVIPI\", \"54_mixed_sdc2_3xmCNB_mChMCVIPI_GFPHia5\"],\n",
    "    selec_type=\"all_rex\",\n",
    "    selected_chr_type=\"X\",\n",
    "    num_read_ids=500,\n",
    "    rel_pos_window=500,\n",
    "    smoothing_window=50\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e1b1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read plot without nucleosomes\n",
    "# Reimport nanotools\n",
    "importlib.reload(nanotools)\n",
    "### READ PLOT + NUCLEOSOME PLOT\n",
    "def create_plot(plot_df, condition, chr_type, data_type, plot_window, min_prob, bw_selection, bigwig_df_cropped,max_read_ids=100):\n",
    "    print(\"Creating dataframes...\")\n",
    "    plot_df_copy = plot_df[(plot_df['condition'] == condition) &\n",
    "                                (plot_df['chr_type'] == chr_type) &\n",
    "                                (plot_df['type'] == data_type) &\n",
    "                                (plot_df['rel_pos'] > -plot_window) &\n",
    "                                (plot_df['rel_pos'] < plot_window) &\n",
    "                                (plot_df['mod_qual'] > min_prob)]\n",
    "    # reset index\n",
    "    plot_df_copy.reset_index(inplace=True, drop=True)\n",
    "    nanotools.display_sample_rows(plot_df_copy)\n",
    "\n",
    "    # drop rows where canonical_base == C, query_kmer does not have a G or a C at character 2 and mod_qual < min_prob\n",
    "    plot_df_copy = plot_df_copy[~((plot_df_copy['canonical_base'] == 'C') &\n",
    "                                  (plot_df_copy['query_kmer'].str[1] != 'G') &\n",
    "                                  #(plot_df_copy['query_kmer'].str[1] != 'C') &\n",
    "                                  (plot_df_copy['mod_qual'] < min_prob))]\n",
    "\n",
    "\n",
    "\n",
    "    # Sort the DataFrame by 'bed_start' and 'bed_end'\n",
    "    plot_df_copy = plot_df_copy.sort_values(by=['bed_start', 'bed_end','ref_strand'], ascending=[True, True,True])\n",
    "    plot_df_copy.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    # Keep the first max_read_ids read_ids starting from the top\n",
    "    plot_df_copy = plot_df_copy[plot_df_copy['read_id'].isin(plot_df_copy['read_id'].unique()[:max_read_ids])]\n",
    "\n",
    "    plot_df_copy_nodups = plot_df_copy.drop_duplicates(subset=['read_id'])[['read_id']]\n",
    "    plot_df_copy_nodups.reset_index(inplace=True, drop=True)\n",
    "    plot_df_copy_nodups['read_count'] = range(1, len(plot_df_copy_nodups) + 1)\n",
    "    nanotools.display_sample_rows(plot_df_copy_nodups, 5)\n",
    "    nanotools.display_sample_rows(plot_df_copy, 10)\n",
    "\n",
    "    #merge the read_count column back into plot_df_copy\n",
    "    plot_df_copy = pd.merge(plot_df_copy, plot_df_copy_nodups[['read_id', 'read_count']], on='read_id', how='left')\n",
    "\n",
    "\n",
    "\n",
    "    #multiply read_count by 1.2\n",
    "    #plot_df_copy['read_count'] = plot_df_copy['read_count']*1.2\n",
    "\n",
    "    # Create a subplot with 3 rows and 1 column\n",
    "    fig = make_subplots(rows=4,\n",
    "                        cols=1,\n",
    "                        shared_xaxes=True,\n",
    "                        vertical_spacing=0.02,\n",
    "                        specs=[[{}], [{}], [{}], [{}]],\n",
    "                        row_heights=[0.65, 0.05, 0.15,0.15])\n",
    "\n",
    "    # Update xaxes for all subplots\n",
    "    fig.update_xaxes(range=[-plot_window, plot_window])\n",
    "\n",
    "    # Calculate sum and count of mod_qual at each rel_pos\n",
    "    agg_df_m6a = plot_df_copy[plot_df_copy[\"mod_code\"]==\"a\"].groupby('rel_pos')['mod_qual_bin'].agg(['sum', 'count']).reset_index()\n",
    "    agg_df_5mC = plot_df_copy[plot_df_copy[\"mod_code\"]==\"m\"].groupby('rel_pos')['mod_qual_bin'].agg(['sum', 'count']).reset_index()\n",
    "\n",
    "    agg_df_m6a['ratio'] = agg_df_m6a['sum'] / agg_df_m6a['count']\n",
    "    agg_df_5mC['ratio'] = agg_df_5mC['sum'] / agg_df_5mC['count']\n",
    "\n",
    "    # Calculate the moving average of the ratio with a centered window of 20\n",
    "    rolling_window_size=25\n",
    "    agg_df_m6a['moving_avg'] = agg_df_m6a['ratio'].rolling(window=rolling_window_size, center=True).mean()\n",
    "    agg_df_5mC['moving_avg'] = agg_df_5mC['ratio'].rolling(window=rolling_window_size, center=True).mean()\n",
    "\n",
    "    #drop nan values\n",
    "    agg_df_m6a.dropna(inplace=True)\n",
    "    agg_df_5mC.dropna(inplace=True)\n",
    "\n",
    "    ### Upper scatter plot\n",
    "    # Configuration: Minimum number of consecutive modifications\n",
    "    min_consecutive_mods = 1\n",
    "    min_prob_plus = min_prob-0.00001\n",
    "\n",
    "    print(\"Number of rows where mod_qual < min_prob:\", plot_df_copy[plot_df_copy['mod_qual'] < min_prob].shape[0])\n",
    "    if min_consecutive_mods == 1:\n",
    "        #  plot every m7a modification\n",
    "        scatter_trace_m6A = go.Scatter(\n",
    "            x=plot_df_copy[plot_df_copy['mod_code']=='a']['rel_pos'],\n",
    "            y=plot_df_copy[plot_df_copy['mod_code']=='a']['read_count'],\n",
    "            mode='markers',\n",
    "            marker=dict(size=6,\n",
    "                        color=plot_df_copy[plot_df_copy['mod_code']=='a']['mod_qual'],\n",
    "                        colorscale=[\n",
    "                            [0,'rgba(255, 255, 255, 0)'],\n",
    "                            [min_prob_plus,'rgba(255, 255, 255, 0)'],\n",
    "                            #[0,'#ffffff'],\n",
    "                            #[min_prob_plus,'#ffffff'],\n",
    "                            [min_prob,'#FF5733'],\n",
    "                            [1,'#FF5733']],\n",
    "                        #line=dict(\n",
    "                        #    color='darkgrey',\n",
    "                        #    width=0.25\n",
    "                        #)\n",
    "                        )\n",
    "        )\n",
    "        # Add scatter trace to the figure\n",
    "        fig.add_trace(scatter_trace_m6A, row=1, col=1)\n",
    "\n",
    "        # add histogram to 3rd row of the rel_pos of all mod_code == a\n",
    "        num_bins = 100  # Set your desired number of bins here\n",
    "        hist_trace = go.Histogram(x=plot_df_copy[plot_df_copy['mod_code']=='a']['rel_pos'], nbinsx=num_bins, marker_color='#FF5733')\n",
    "        fig.add_trace(hist_trace, row=3, col=1)\n",
    "\n",
    "    else:\n",
    "        # Initialize lists for scatter plot and lines\n",
    "        scatter_x = []\n",
    "        scatter_y = []\n",
    "        line_x = []\n",
    "        line_y = []\n",
    "        m6a_cluster_midpoints = []\n",
    "\n",
    "        for read_id, group in plot_df_copy[plot_df_copy['mod_code'] == 'a'].groupby('read_id'):\n",
    "            # Sort by relative position\n",
    "            sorted_group = group.sort_values(by='rel_pos')\n",
    "\n",
    "            # Variables to track consecutive modifications and midpoints\n",
    "            consecutive_count = 1\n",
    "            temp_scatter_x = []\n",
    "            temp_scatter_y = []\n",
    "            temp_line_x = []\n",
    "            temp_line_y = []\n",
    "            cluster_start_pos = None\n",
    "            prev_row = None\n",
    "\n",
    "            # Iterate through sorted 'a' modifications in the current read\n",
    "            for _, row in sorted_group.iterrows():\n",
    "                if prev_row is not None and (row['rel_pos'] - prev_row['rel_pos']) <= 10:\n",
    "                    # Increase count and add to temporary lists\n",
    "                    consecutive_count += 1\n",
    "                    if consecutive_count == min_consecutive_mods:\n",
    "                        cluster_start_pos = prev_row['rel_pos']\n",
    "                    temp_line_x.extend([prev_row['rel_pos'], row['rel_pos']])\n",
    "                    temp_line_y.extend([prev_row['read_count'], row['read_count']])\n",
    "                else:\n",
    "                    # Check if previous group met the threshold\n",
    "                    if consecutive_count >= min_consecutive_mods:\n",
    "                        cluster_midpoint = (cluster_start_pos + prev_row['rel_pos']) / 2\n",
    "                        m6a_cluster_midpoints.append(cluster_midpoint)\n",
    "                        scatter_x.extend(temp_scatter_x)\n",
    "                        scatter_y.extend(temp_scatter_y)\n",
    "                        line_x.extend(temp_line_x + [None])  # None to break the line segment\n",
    "                        line_y.extend(temp_line_y + [None])\n",
    "\n",
    "                    # Reset for the next group of modifications\n",
    "                    consecutive_count = 1\n",
    "                    temp_scatter_x = [row['rel_pos']]\n",
    "                    temp_scatter_y = [row['read_count']]\n",
    "                    temp_line_x = []\n",
    "                    temp_line_y = []\n",
    "                    cluster_start_pos = None\n",
    "\n",
    "                prev_row = row\n",
    "\n",
    "            # Check for the last group in the read\n",
    "            if consecutive_count >= min_consecutive_mods:\n",
    "                cluster_midpoint = (cluster_start_pos + prev_row['rel_pos']) / 2\n",
    "                m6a_cluster_midpoints.append(cluster_midpoint)\n",
    "                scatter_x.extend(temp_scatter_x)\n",
    "                scatter_y.extend(temp_scatter_y)\n",
    "                line_x.extend(temp_line_x + [None])\n",
    "                line_y.extend(temp_line_y + [None])\n",
    "\n",
    "        # Create scatter trace for 'a' modifications\n",
    "        scatter_trace_m6A = go.Scatter(x=scatter_x, y=scatter_y, mode='markers',\n",
    "                                       marker=dict(size=4, color='#FF5733'))\n",
    "\n",
    "        # Create and add line trace for connections\n",
    "        line_trace_m6A = go.Scatter(x=line_x, y=line_y, mode='lines',\n",
    "                                    line=dict(color='#FF5733', width=4))\n",
    "\n",
    "        fig.add_trace(line_trace_m6A, row=1, col=1)\n",
    "\n",
    "        # Add scatter trace to the figure\n",
    "        fig.add_trace(scatter_trace_m6A, row=1, col=1)\n",
    "\n",
    "        # Histogram subplot\n",
    "        num_bins = 100  # Set your desired number of bins here\n",
    "        hist_trace = go.Histogram(x=m6a_cluster_midpoints, nbinsx=num_bins, marker_color='#FF5733')\n",
    "        fig.add_trace(hist_trace, row=3, col=1)\n",
    "\n",
    "    scatter_trace_5mC = go.Scatter(\n",
    "        x=plot_df_copy[plot_df_copy['mod_code']=='m']['rel_pos'],\n",
    "        y=plot_df_copy[plot_df_copy['mod_code']=='m']['read_count'],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            symbol='triangle-up',\n",
    "            size=6,\n",
    "            color=plot_df_copy[plot_df_copy['mod_code']=='m']['mod_qual'],\n",
    "            colorscale=[\n",
    "                [0, 'rgba(255, 255, 255, 0)'],  # yellow\n",
    "                [min_prob_plus, 'rgba(255, 255, 255, 0)'],  # yellow\n",
    "                [min_prob, '#0047ab'],  # sky blue\n",
    "                [1, '#0047ab']  # sky blue\n",
    "            ]\n",
    "            #line=dict(\n",
    "            #    color='darkgrey',\n",
    "            #    width=0.25\n",
    "            #)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.add_trace(scatter_trace_5mC, row=1, col=1)\n",
    "\n",
    "    # add histogram to 4th row of the rel_pos of all mod_code == a\n",
    "    num_bins = 100  # Set your desired number of bins here\n",
    "    hist_trace_5mC = go.Histogram(x=plot_df_copy[plot_df_copy['mod_code']=='m']['rel_pos'], nbinsx=num_bins, marker_color='#87CEEB')\n",
    "    fig.add_trace(hist_trace_5mC, row=4, col=1)\n",
    "\n",
    "    print(\"Adding m6a line traces...\")\n",
    "    # Add line traces for each unique read_id\n",
    "    for read_id in plot_df_copy['read_count'].unique():\n",
    "        read_data = plot_df_copy[(plot_df_copy['read_count'] == read_id)]\n",
    "        read_data = read_data[read_data['mod_code'] == 'a']\n",
    "        #if read_data has >0 rows\n",
    "        if len(read_data) > 0:\n",
    "            #nanotools.display_sample_rows(read_data, 5)\n",
    "            # Add a line trace for the read\n",
    "\n",
    "            min_rel_pos = read_data['rel_pos'].min()\n",
    "            max_rel_pos = read_data['rel_pos'].max()\n",
    "\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=[min_rel_pos, max_rel_pos], y=[read_data['read_count'].iloc[0],read_data['read_count'].iloc[0]],\n",
    "                           mode='lines', line=dict(color='#000000', width=0.2),showlegend=False),row=1, col=1\n",
    "            )\n",
    "            # set y range\n",
    "\n",
    "\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(template=\"simple_white\",\n",
    "                      height=1200,\n",
    "                      width=1600,\n",
    "                      )\n",
    "    fig.update_yaxes(title_text=\"Read_ID\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Localization Counts\", row=3, col=1)\n",
    "    #fig.update_yaxes(title_text=\"Nucleosome Probability\", row=3, col=1)\n",
    "    fig.update_xaxes(title_text=\"Genomic location (bp)\", row=3, col=1)\n",
    "    fig.update_xaxes(range=[-plot_window, plot_window], row=3, col=1)\n",
    "\n",
    "    # Add Rex Line\n",
    "    fig.add_shape(\n",
    "        go.layout.Shape(\n",
    "            type=\"line\",\n",
    "            x0=0,\n",
    "            x1=0,\n",
    "            y0=0,\n",
    "            y1=1,\n",
    "            yref=\"paper\",\n",
    "            line=dict(\n",
    "                color=\"grey\",\n",
    "                width=1,\n",
    "                dash=\"dash\",\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    ### Chip plot\n",
    "    if bw_selection is not None:\n",
    "        bigwig_df_cropped = bigwig_df_cropped[(bigwig_df_cropped['rel_start'] <= bed_window) & (bigwig_df_cropped['rel_start'] >= -bed_window)]\n",
    "        # Apply filters appropriately\n",
    "        filters = []\n",
    "        filters.append(bigwig_df_cropped['condition'] == bw_selection)\n",
    "        filters.append(bigwig_df_cropped['chr_type'] == chr_type)\n",
    "        filters.append(bigwig_df_cropped['type'] == data_type)\n",
    "\n",
    "        base_filter = np.logical_and.reduce(filters)\n",
    "\n",
    "        value_data = bigwig_df_cropped.loc[base_filter]['value']\n",
    "        value_data_xaxis = bigwig_df_cropped.loc[base_filter]['rel_start']\n",
    "\n",
    "        smoothed_data = value_data.rolling(window=rolling_window_size, center=True).mean()\n",
    "        y_min = float('inf')\n",
    "        y_max = float('-inf')\n",
    "        y_min = min(y_min, smoothed_data.min())\n",
    "        y_max = max(y_max, smoothed_data.max())\n",
    "\n",
    "        label = f\"{bw_selection}_{selected_chr_type}_{data_type}\"  # Add selected_strand here\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=value_data_xaxis.values,\n",
    "                y=value_data.values,\n",
    "                mode='lines',\n",
    "                name=label,\n",
    "                opacity=0.9,\n",
    "                line=dict(color='green', width=1.5)),\n",
    "            row=2, col=1\n",
    "        )\n",
    "\n",
    "    # Generate a list of shades from light grey to dark grey\n",
    "    num_groups = len(plot_df_copy.groupby(['bed_start', 'ref_strand']))\n",
    "    color_list = ['rgb({}, {}, {})'.format(i, i, i) for i in range((num_groups*10), 0, -(num_groups*10) // num_groups)]\n",
    "\n",
    "    # Create a dictionary to store the tick positions and labels\n",
    "    tick_positions = []\n",
    "    tick_labels = []\n",
    "\n",
    "    # Add vertical bars for each group of read IDs\n",
    "    for i, ((bed_start, ref_strand), group) in enumerate(plot_df_copy.groupby(['bed_start', 'ref_strand'])):\n",
    "        min_read_count = group['read_count'].min()\n",
    "        max_read_count = group['read_count'].max()\n",
    "        mid_read_count = (min_read_count + max_read_count) / 2\n",
    "\n",
    "        # Get the rightmost element of the \"chrom\" column (split on \"_\")\n",
    "        chrom_rightmost = group['chrom'].iloc[0].split('_')[-1]\n",
    "\n",
    "        # Add the tick position and label to the dictionary\n",
    "        tick_positions.append(mid_read_count)\n",
    "        tick_labels.append(f\"{chrom_rightmost}:{bed_start} ({ref_strand})\")\n",
    "\n",
    "        # Add a vertical bar to designate the read IDs corresponding to the label\n",
    "        fig.add_shape(\n",
    "            type='line',\n",
    "            x0=-plot_window,  # Adjust this value to position the vertical bar within the plot area\n",
    "            y0=min_read_count,\n",
    "            x1=-plot_window,  # Adjust this value to position the vertical bar within the plot area\n",
    "            y1=max_read_count,\n",
    "            line=dict(color=color_list[i], width=10),\n",
    "            row=1,\n",
    "            col=1\n",
    "        )\n",
    "\n",
    "    # Update the y-axis tick labels\n",
    "    fig.update_yaxes(\n",
    "        tickmode='array',\n",
    "        tickvals=tick_positions,\n",
    "        ticktext=tick_labels,\n",
    "        row=1,\n",
    "        col=1\n",
    "    )\n",
    "\n",
    "    \"\"\" #if more than one bed_start in plot_df_copy\n",
    "    if len(plot_df_copy['bed_start'].unique()) > 1:\n",
    "        # Add light grey line boxes around reads with the same bed_start value\n",
    "        for bed_start, group in plot_df_copy.groupby('bed_start'):\n",
    "            min_rel_pos = group['rel_pos'].min()\n",
    "            max_rel_pos = group['rel_pos'].max()\n",
    "            min_read_count = group['read_count'].min()\n",
    "            max_read_count = group['read_count'].max()\n",
    "\n",
    "            # Define the rectangle shape\n",
    "            fig.add_shape(\n",
    "                type=\"rect\",\n",
    "                x0=min_rel_pos, y0=min_read_count,\n",
    "                x1=max_rel_pos, y1=max_read_count,\n",
    "                line=dict(\n",
    "                    color=\"lightgrey\",\n",
    "                    width=2,\n",
    "                ),\n",
    "                row=1, col=1\n",
    "            )\"\"\"\n",
    "\n",
    "    return fig\n",
    "\n",
    "# Sample usage of the function\n",
    "#analysis_cond = [\"N2_mixed_endogenous_R10\",\"50_mixed_dpy27-3xGNB_GFP-Hia5_mcvipi_R10\",\"54_old_MCVIPIsdc2_LMN1pAhia5_R10\",\"87_old_GFPhia5dpy27_mCmcvipi_ama1_R10\",\"54_mixed_sdc2_3xmCNB_mChMCVIPI_GFPHia5\",\"N2_mixed_DPY27_dimelo_pAHia5_R10\"]\n",
    "selec_cond = \"54_mixed_sdc2_3xmCNB_mChMCVIPI_GFPHia5\" #50_dpy27dimelo_mcvipi \"50_dpy27-3xGNB_GFP-Hia5_mcvipi\"\n",
    "selec_type = \"all_rex\" #center_SDC2_chip_albretton\n",
    "selected_chr_type = \"X\"\n",
    "# set title to path: \"images/\"+selec_cond+\"_t\"+selec_type+\"_r\"+str(n_read_ids)+\"_b\"+str(bed_window)+\"png\"\n",
    "fig_title = \"/Data1/git/meyer-nanopore/scripts/Analysis/combined_bam_analysis/images/50\"+selec_cond+\"_t_\"+selec_type+\"_r\"+str(n_read_ids)+\"_b\"+str(bed_window)+\"_chr\"+selected_chr_type+\"_1-12-2024.png\"\n",
    "\n",
    "N2_fig = create_plot(down_sampled_plot_df,selec_cond, selected_chr_type, selec_type, 500, 0.8,None,None,300)\n",
    "#down_sampled_plot_df\n",
    "#mC_aligned_df\n",
    "#plot_comb_bigwig_df\n",
    "print(\"saving image...\")\n",
    "print(fig_title)\n",
    "N2_fig.show(renderer='plotly_mimetype+notebook')\n",
    "# save N2_fig to png\n",
    "#N2_fig.write_image(fig_title)\n",
    "print(\"Done!\")\n",
    "\n",
    "#SDC2_fig = create_plot(down_sampled_plot_df, \"SDC2_degron_fiber\", \"X\", \"TSS_q4\", plot_window)\n",
    "#SDC2_fig.show(renderer='plotly_mimetype+notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4b9080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reimport nanotools\n",
    "importlib.reload(nanotools)\n",
    "from scipy.signal import find_peaks\n",
    "### READ PLOT + NUCLEOSOME PLOT\n",
    "def create_plot(plot_df, group_df, condition, chr_type, data_type, plot_window,plot_nucs=False, min_prob=0):\n",
    "    print(\"Creating dataframes...\")\n",
    "    plot_df_copy = plot_df.copy(deep=True)\n",
    "    plot_df_copy = plot_df_copy[(plot_df_copy['condition'] == condition) &\n",
    "                                (plot_df_copy['chr_type'] == chr_type) &\n",
    "                                (plot_df_copy['type'] == data_type) &\n",
    "                                (plot_df_copy['rel_pos'] > -plot_window) &\n",
    "                                (plot_df_copy['rel_pos'] < plot_window) &\n",
    "                                (plot_df_copy['mod_qual'] > min_prob)]\n",
    "\n",
    "    # drop rows where both smallest_positive_nuc_midpoint and greatest_negative_nuc_midpoint are NaN\n",
    "    plot_df_copy = plot_df_copy[~(plot_df_copy['smallest_positive_nuc_midpoint'].isna() & plot_df_copy['greatest_negative_nuc_midpoint'].isna())]\n",
    "    plot_df_copy = plot_df_copy.sort_values(by=['smallest_positive_nuc_midpoint', 'greatest_negative_nuc_midpoint'],ascending=[True, False])\n",
    "    plot_df_copy_nodups = plot_df_copy.drop_duplicates(subset=['read_id'])[['read_id','smallest_positive_nuc_midpoint', 'greatest_negative_nuc_midpoint']]\n",
    "    plot_df_copy.reset_index(inplace=True, drop=True)\n",
    "    plot_df_copy_nodups.reset_index(inplace=True, drop=True)\n",
    "    # use ngroup to create a incrementing column in ascending order\n",
    "    plot_df_copy_nodups['read_count'] = range(1, len(plot_df_copy_nodups) + 1)\n",
    "\n",
    "    #merge the read_count column back into plot_df_copy\n",
    "    plot_df_copy = pd.merge(plot_df_copy, plot_df_copy_nodups[['read_id', 'read_count']], on='read_id', how='left')\n",
    "\n",
    "    # drop rows from down_sampled_group_df_copy where read_id not in plot_df_copy read_ids\n",
    "    down_sampled_group_df_copy = group_df.copy(deep=True)\n",
    "    down_sampled_group_df_copy = down_sampled_group_df_copy[down_sampled_group_df_copy['read_id'].isin(plot_df_copy_nodups['read_id'])]\n",
    "    # merge read_count column from plot_df_copy_no_dups with down_sampled_group_df_copy on read_id\n",
    "    down_sampled_group_df_copy = pd.merge(down_sampled_group_df_copy, plot_df_copy_nodups[['read_id', 'read_count']], on='read_id', how='left')\n",
    "    #drop rows where nucs_list is nan\n",
    "    down_sampled_group_df_copy.dropna(subset=['nucs_list'], inplace=True)\n",
    "    nanotools.display_sample_rows(down_sampled_group_df_copy,10)\n",
    "\n",
    "    #display(plot_df_copy.head(100))\n",
    "\n",
    "    # Create a subplot with 3 rows and 1 column\n",
    "    fig = make_subplots(rows=3,\n",
    "                        cols=1,\n",
    "                        shared_xaxes=True,\n",
    "                        vertical_spacing=0.02,\n",
    "                        specs=[[{}], [{}], [{\"secondary_y\": True}]],# [{}]],\n",
    "                        row_heights=[0.7, 0.15, 0.15])\n",
    "\n",
    "    # Update xaxes for all subplots\n",
    "    fig.update_xaxes(range=[-plot_window, plot_window])\n",
    "\n",
    "\n",
    "    #print(\"plot_df_copy\")\n",
    "    #display(plot_df_copy.head(10))\n",
    "    # Calculate sum and count of mod_qual at each rel_pos\n",
    "    agg_df = plot_df_copy.groupby('rel_pos')['mod_qual_bin'].agg(['sum', 'count']).reset_index()\n",
    "    agg_df['ratio'] = agg_df['sum'] / agg_df['count']\n",
    "    # Calculate the moving average of the ratio with a centered window of 20\n",
    "    rolling_window_size=25\n",
    "    agg_df['moving_avg'] = agg_df['ratio'].rolling(window=rolling_window_size, center=True).mean()\n",
    "    #drop nan values\n",
    "    agg_df.dropna(inplace=True)\n",
    "    #print(\"agg_df:\",agg_df)\n",
    "    #display(agg_df.head(100))\n",
    "\n",
    "    # create occupancy_df where columns are read_id and\n",
    "    # Assuming genome_size is known\n",
    "    genome_size = 2 * plot_window  # Replace with your actual genome size\n",
    "\n",
    "    # Initialize a numpy array with zeros for each base pair in the genome region\n",
    "    read_counts = np.zeros(genome_size)\n",
    "\n",
    "    # Upper scatter plot\n",
    "    scatter_trace = go.Scatter(x=plot_df_copy['rel_pos'], y=plot_df_copy['read_count'], mode='markers',\n",
    "                               marker=dict(size=2, color=plot_df_copy['mod_qual'], colorscale=[[0, '#FF5733'], [1, '#FF5733']])) #33B8FF\n",
    "    fig.add_trace(scatter_trace, row=1, col=1)\n",
    "\n",
    "    print(\"Adding m6a line traces...\")\n",
    "    # Add line traces for each unique read_id\n",
    "    for read_id in plot_df_copy['read_count'].unique():\n",
    "        read_data = plot_df_copy[plot_df_copy['read_count'] == read_id]\n",
    "        min_rel_pos = read_data['rel_pos'].min()\n",
    "        max_rel_pos = read_data['rel_pos'].max()\n",
    "\n",
    "        ## FOR CALCULATING OCCUPANCY\n",
    "        # Loop through the range of positions between min and max positions\n",
    "        for pos in range(int(min_rel_pos + plot_window), int(max_rel_pos + plot_window + 1)):\n",
    "            if 0 <= pos < genome_size:  # Check if pos is within the range\n",
    "                read_counts[pos] += 1\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=[min_rel_pos, max_rel_pos], y=[read_data['read_count'].iloc[0],read_data['read_count'].iloc[0]],\n",
    "                       mode='lines', line=dict(color='#000000', width=0.2),showlegend=False),row=1, col=1\n",
    "        )\n",
    "        # set y range\n",
    "\n",
    "    # drop rows where mod_qual == 0\n",
    "    #plot_df_dropped = plot_df_copy[plot_df_copy['mod_qual'] != 0]\n",
    "    if plot_nucs == False:\n",
    "        print(\"Skipping nucleosome plotting...\")\n",
    "    elif plot_nucs == True:\n",
    "        print(\"Plotting nucleosomes...\")\n",
    "        ### PLOT NUCLEOSOMES\n",
    "        midpoints_list = []\n",
    "        x_coords = []\n",
    "        y_coords = []\n",
    "        # add a blue line for each read_count in down_sampled_group_df using read_count as y value and for each value in nuc_list (value-nuc_width/2),(value-nuc_width/2) as x values\n",
    "        for read_id in down_sampled_group_df_copy['read_count']:\n",
    "\n",
    "            read_data = down_sampled_group_df_copy[down_sampled_group_df_copy['read_count'] == read_id]\n",
    "            #print(read_data['nucs_list'])\n",
    "            # for each value in nuc_list column\n",
    "            # Initialize an empty list to store the x and y coordinates for the scatter plot\n",
    "\n",
    "            read_height = read_data['read_count'].iloc[0]\n",
    "            # drop nucs from nucs_list that are outside of plot_window\n",
    "            read_data['nucs_list'] = read_data['nucs_list'].apply(lambda x: [nuc for nuc in x if nuc >= -plot_window and nuc <= plot_window])\n",
    "\n",
    "            # Loop through the nucleotides and populate x_coords and y_coords\n",
    "            for nuc in read_data['nucs_list'].iloc[0]:\n",
    "                midpoints_list.append(nuc)  # Assuming midpoints_list is already defined\n",
    "                min_rel_pos = nuc - NUC_width / 2\n",
    "                max_rel_pos = nuc + NUC_width / 2\n",
    "\n",
    "                x_coords.extend([min_rel_pos, max_rel_pos, None])  # Use None to separate individual line segments\n",
    "                y_coords.extend([read_height, read_height, None])\n",
    "\n",
    "        # Add a single trace for all line segments\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=x_coords,\n",
    "                y=y_coords,\n",
    "                mode='lines',\n",
    "                line=dict(color='#33B8FF', width=2),\n",
    "                opacity=0.75,\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=1,\n",
    "            col=1\n",
    "            )\n",
    "\n",
    "        # Lower line plot for moving average of the ratio\n",
    "        line_trace = go.Scatter(x=agg_df['rel_pos'], y=agg_df['moving_avg'], mode='lines',\n",
    "                                #set color to match blue\n",
    "                                line=dict(color='#FF5733', width=2),\n",
    "                                # smooth line\n",
    "                                line_shape='spline')\n",
    "        fig.add_trace(line_trace, row=2, col=1)\n",
    "\n",
    "        print(\"Plotting histogram...\")\n",
    "        #midpoints_df = pd.DataFrame.from_dict(midpoints_dict, orient='index')\n",
    "        #display(midpoints_list)\n",
    "        # Add midpoint plot\n",
    "        rolling_window_size_hist = 20\n",
    "        hist_bins = int(round(2*plot_window/10)+1)\n",
    "        midpoint_histogram = go.Histogram(x=midpoints_list,\n",
    "                                          #histnorm='density',\n",
    "                                          nbinsx=hist_bins,\n",
    "                                          marker=dict(color='#33B8FF',opacity=0.8)\n",
    "                                          ) #\n",
    "        fig.add_trace(midpoint_histogram, row=3, col=1,secondary_y=False)\n",
    "\n",
    "        ### OVERLAY GAUSSIAN SMOOTHED PLOT ON TOP OF HISTOGRAM\n",
    "\n",
    "        print(\"Plotting gaussian smoothed plot...\")\n",
    "\n",
    "        # Assume midpoints_list contains midpoints of nucleosomes for the current plot\n",
    "        genome_size = 2 * plot_window  # Define the genome size based on the plot_window\n",
    "\n",
    "        # Initialize a numpy array with zeros for each base pair in the genome region\n",
    "        nucleosome_array = np.zeros(genome_size)\n",
    "\n",
    "        # Populate the nucleosome_array based on the midpoints\n",
    "        for midpoint in midpoints_list:\n",
    "            # Convert midpoint to an integer index\n",
    "            position_index = int(midpoint + plot_window)  # Shift by plot_window to handle negative positions\n",
    "            if 0 <= position_index < genome_size:  # Check if position_index is within the range\n",
    "                nucleosome_array[position_index] += 1\n",
    "\n",
    "        \"\"\"# Calculate the mean nucleosome density, avoiding division by zero\n",
    "        mean_density = np.mean(nucleosome_array[nucleosome_array > 0])\n",
    "        if mean_density == 0:\n",
    "            raise ValueError(\"Mean nucleosome density is zero. Check your midpoint values.\")\n",
    "\n",
    "        # Scale by 1/(mean nucleosome density)\n",
    "        scaled_nucleosome_array = nucleosome_array / mean_density\"\"\"\n",
    "\n",
    "        nucleosome_normalized = np.divide(nucleosome_array, read_counts, where=read_counts != 0)\n",
    "\n",
    "        # Apply Gaussian smoothing with a standard deviation of 20 base pairs\n",
    "        smoothed_nucleosome_array = gaussian_filter1d(nucleosome_normalized, 10)\n",
    "\n",
    "        # Generate x values for the smoothed density plot, shifting back by plot_window to align with the original coordinates\n",
    "        x_values = np.arange(-plot_window, plot_window, 1)\n",
    "\n",
    "        # Add the smoothed nucleosome density as a line trace to the third subplot\n",
    "        smoothed_trace = go.Scatter(\n",
    "            x=x_values,\n",
    "            y=smoothed_nucleosome_array,\n",
    "            mode='lines',\n",
    "            name='Smoothed Nucleosome Density',\n",
    "            line=dict(color='#007dfa', width=2),  # Adjust color and width as desired\n",
    "        )\n",
    "\n",
    "        print(\"Plotting peaks...\")\n",
    "        # Find indices of peaks in the smoothed nucleosome array\n",
    "        peaks, _ = find_peaks(smoothed_nucleosome_array)\n",
    "\n",
    "        # The y-range for the vertical lines\n",
    "        y_range = [smoothed_nucleosome_array.min(), smoothed_nucleosome_array.max()]\n",
    "\n",
    "        # Add vertical lines for each peak\n",
    "        for peak_idx in peaks:\n",
    "            # Convert index to x-coordinate\n",
    "            peak_pos = x_values[peak_idx]\n",
    "\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=[peak_pos, peak_pos],\n",
    "                    y=y_range,\n",
    "                    mode='lines',\n",
    "                    line=dict(color='grey', width=0.5, dash='dash'),\n",
    "                    showlegend=False\n",
    "                ),\n",
    "                row=3, col=1, secondary_y=True\n",
    "            )\n",
    "\n",
    "        # Add the new trace to the subplot\n",
    "        fig.add_trace(smoothed_trace, row=3, col=1,secondary_y=True)\n",
    "        ###\n",
    "\n",
    "    print(\"Plotting bigwig...\")\n",
    "    ## MNASE\n",
    "    \"\"\"bigwig_trace = nanotools.create_bigwig_trace(\"/Data1/reference/lieb_mnase_2017/GSM2098437_RT_rep1_MNaseTC_30m_smoothDyads_ce11.bw\", plot_df_copy)\"\"\"\n",
    "    ## GRO MINUS\n",
    "    #bigwig_trace = nanotools.create_bigwig_trace(\"/Data1/reference/lieb_gro_2013/GSM1056279_GRO-seq_N2_Emb_replicateAVG_WS230_RPKM_minus_ce11.bw\", plot_df_copy)\n",
    "    ## GRO PLUS\n",
    "    #bigwig_trace = nanotools.create_bigwig_trace(\"/Data1/reference/lieb_gro_2013/GSM1056279_GRO-seq_N2_Emb_replicateAVG_WS230_RPKM_plus_ce11.bw\", plot_df_copy)\n",
    "    # Now iterate through the list of traces and add them to the figure\n",
    "    \"\"\"for trace in bigwig_trace:\n",
    "        fig.add_trace(trace, row=4, col=1)\"\"\"\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(template=\"simple_white\",\n",
    "                      height=800,\n",
    "                      width=1100,\n",
    "                      )\n",
    "    fig.update_yaxes(title_text=\"Read_ID\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"% m6A\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Nucleosome Probability\", row=3, col=1)\n",
    "    fig.update_xaxes(title_text=\"Genomic location (bp)\", row=3, col=1)\n",
    "    # set y max to 60\n",
    "\n",
    "    # Add Rex Line\n",
    "    fig.add_shape(\n",
    "        go.layout.Shape(\n",
    "            type=\"line\",\n",
    "            x0=0,\n",
    "            x1=0,\n",
    "            y0=0,\n",
    "            y1=1,\n",
    "            yref=\"paper\",\n",
    "            line=dict(\n",
    "                color=\"grey\",\n",
    "                width=1,\n",
    "                dash=\"dash\",\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    #fig.add_annotation(\n",
    "    #    x=0,\n",
    "    #    y=1,\n",
    "    #    yref=\"paper\",\n",
    "    #    text=\"rex\",\n",
    "    #    showarrow=False,\n",
    "    #    font=dict(\n",
    "    #        size=15,\n",
    "    #        color=\"grey\"\n",
    "    #    )\n",
    "    #)\n",
    "\n",
    "    return fig\n",
    "\n",
    "# Sample usage of the function\n",
    "selec_cond = \"N2-DPY27_dimelo_pAHia5\" #50_dpy27dimelo_mcvipi\n",
    "selec_type = \"weak_rex\"\n",
    "selected_chr_type = \"X\"\n",
    "# set title to path: \"images/\"+selec_cond+\"_t\"+selec_type+\"_r\"+str(n_read_ids)+\"_b\"+str(bed_window)+\"png\"\n",
    "fig_title = \"/Data1/git/meyer-nanopore/scripts/Analysis/combined_bam_analysis/images/50\"+selec_cond+\"_t_\"+selec_type+\"_r\"+str(n_read_ids)+\"_b\"+str(bed_window)+\"_chr\"+selected_chr_type+\"_1-12-2024.png\"\n",
    "\n",
    "N2_fig = create_plot(down_sampled_plot_df, down_sampled_group_df,selec_cond, selected_chr_type, selec_type, int(round(bed_window,0)),plot_nucs=False, min_prob=0.9)\n",
    "\n",
    "print(\"saving image...\")\n",
    "print(fig_title)\n",
    "N2_fig.show(renderer='plotly_mimetype+notebook')\n",
    "# save N2_fig to png\n",
    "N2_fig.write_image(fig_title)\n",
    "print(\"Done!\")\n",
    "\n",
    "#SDC2_fig = create_plot(down_sampled_plot_df, \"SDC2_degron_fiber\", \"X\", \"TSS_q4\", plot_window)\n",
    "#SDC2_fig.show(renderer='plotly_mimetype+notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45737977",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyBigWig\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "def calculate_correlations(grouped_df, bigwig_paths, bin_size):\n",
    "    exp_data = {}\n",
    "    correlations = []  # To store correlation results\n",
    "    bw_objects = [pyBigWig.open(path) for path in bigwig_paths]  # Open all bigwig files\n",
    "    grouped_df_copy = grouped_df.copy(deep=True)\n",
    "    ### Temporarily replace \"CHROMOSOME_\" with \"chr\" in chrom column in grouped_df\n",
    "    grouped_df_copy['chrom'] = grouped_df_copy['chrom'].apply(lambda x: x.replace(\"CHROMOSOME_\", \"chr\"))\n",
    "        # Adjust positions in the 'nucs_list' column\n",
    "\n",
    "    def min_max_normalize(array):\n",
    "        return (array - array.min()) / (array.max() - array.min())\n",
    "\n",
    "    def adjust_positions(row):\n",
    "        mid_point = row['bed_start'] + (row['bed_end'] - row['bed_start']) // 2\n",
    "        # Adjust each position in the list\n",
    "        return [int(pos + mid_point - 1) for pos in row['nucs_list']]\n",
    "\n",
    "    # Apply the adjustment to each row\n",
    "    grouped_df_copy['adjusted_nucs_list'] = grouped_df_copy.apply(adjust_positions, axis=1)\n",
    "\n",
    "    # Drop rows where 'adjusted_nucs_list' is empty or not a list\n",
    "    grouped_df_copy = grouped_df_copy[grouped_df_copy['adjusted_nucs_list'].map(lambda d: isinstance(d, list) and len(d) > 0)]\n",
    "\n",
    "    # Reset index after dropping rows\n",
    "    grouped_df_copy.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "    # Iterate over each experiment and chromosome\n",
    "    for (exp_id, chrom, bed_start, bed_end, cond), group in grouped_df_copy.groupby(['exp_id','chrom', 'bed_start', 'bed_end', 'condition']):\n",
    "        print(f\"Processing {exp_id}, {chrom},bed:,{bed_start}-{bed_end}\")\n",
    "        # Get nucleosome positions and bin them for each chromosome\n",
    "        all_positions = np.concatenate(group['nucs_list'].values)\n",
    "        # Subtract bed_end-bed_start/2 from each position in all_positions\n",
    "        all_positions = all_positions + (bed_end - bed_start) // 2\n",
    "        # drop all positions that are outside of the bed window\n",
    "        all_positions = all_positions[(all_positions >= 0) & (all_positions <= (bed_end - bed_start))]\n",
    "        binned_positions = all_positions // bin_size\n",
    "        # initialize binned_nucleosome_counts as a series of 0s between 0 and bed_end-bed_start\n",
    "        binned_nucleosome_counts = pd.Series(0, index=np.arange(0, (bed_end - bed_start) // bin_size))\n",
    "        # Count the number of nucleosomes in each bin\n",
    "        for position in binned_positions:\n",
    "            binned_nucleosome_counts[position] += 1\n",
    "\n",
    "        # if nucleosome_array is not the same length as binned_nucleosome_counts, initialize nucleosome_array with 0s\n",
    "        #if len(nucleosome_array) != len(binned_nucleosome_counts):\n",
    "        nucleosome_array = np.zeros(len(binned_nucleosome_counts))\n",
    "\n",
    "        # Populate the nucleosome_array based on the binned counts\n",
    "        for bin_start, count in binned_nucleosome_counts.items():\n",
    "            position_index = int(bin_start * bin_size)\n",
    "            nucleosome_array[position_index:int(position_index + bin_size)] += count\n",
    "\n",
    "        # Calculate the smoothed nucleosome array\n",
    "        smoothed_nucleosome_array = gaussian_filter1d(nucleosome_array, 20)\n",
    "        smoothed_nucleosome_array = min_max_normalize(smoothed_nucleosome_array)\n",
    "\n",
    "        # Store the smoothed array with its exp_id and condition\n",
    "        if cond not in exp_data:\n",
    "            exp_data[cond] = {}\n",
    "        if exp_id not in exp_data[cond]:\n",
    "            exp_data[cond][exp_id] = []\n",
    "\n",
    "        exp_data[cond][exp_id].append(smoothed_nucleosome_array)\n",
    "\n",
    "    # make list of unique chrom ,bed_start and bed_end in grouped_df\n",
    "    chrom_bed_start_bed_end_list = grouped_df_copy[['chrom', 'bed_start', 'bed_end']].drop_duplicates().values.tolist()\n",
    "    print(\"chrom_bed_start_bed_end_list:\",chrom_bed_start_bed_end_list)\n",
    "    # For each bigwig replicate\n",
    "    #for chrom, bed_start, bed_end in chrom_bed_start_bed_end_list:\n",
    "    for chrom, bed_start, bed_end in chrom_bed_start_bed_end_list:\n",
    "        print(f\"Processing {chrom},bed:,{bed_start}-{bed_end}\")\n",
    "        for i, bw in enumerate(bw_objects):\n",
    "            cond = \"N2-MNase\"\n",
    "            exp_id = \"MNase-seq-rep\" + str(i + 1)\n",
    "            # Get bigwig values for the entire chromosome and then trim to match the length of smoothed nucleosome array\n",
    "            bigwig_values = bw.values(chrom, bed_start, bed_end)\n",
    "\n",
    "            # Convert to numpy array and handle None values\n",
    "            bigwig_values = np.nan_to_num(bigwig_values)\n",
    "            bigwig_values = min_max_normalize(bigwig_values)\n",
    "            # Store the smoothed array with its exp_id and condition\n",
    "            if cond not in exp_data:\n",
    "                exp_data[cond] = {}\n",
    "            if exp_id not in exp_data[cond]:\n",
    "                exp_data[cond][exp_id] = []\n",
    "            exp_data[cond][exp_id].append(bigwig_values)\n",
    "\n",
    "    # Now calculate pairwise correlations for each condition\n",
    "    pairwise_correlations = []\n",
    "    all_exp_ids = [exp_id for exps in exp_data.values() for exp_id in exps]\n",
    "\n",
    "    # Ensure you have a list of unique experiment IDs if they can repeat across conditions\n",
    "    all_exp_ids = list(set(all_exp_ids))\n",
    "\n",
    "    for i in range(len(all_exp_ids)):\n",
    "        for j in range(len(all_exp_ids)):\n",
    "            exp_id1 = all_exp_ids[i]\n",
    "            exp_id2 = all_exp_ids[j]\n",
    "\n",
    "            # Find the condition for each experiment ID\n",
    "            condition1 = [cond for cond, exps in exp_data.items() if exp_id1 in exps][0]\n",
    "            condition2 = [cond for cond, exps in exp_data.items() if exp_id2 in exps][0]\n",
    "\n",
    "            # Now perform the correlation check for each k\n",
    "            for k in range(len(exp_data[condition1][exp_id1])):\n",
    "                # When exp_id1 is the same as exp_id2, we are comparing the same arrays\n",
    "                if exp_id1 == exp_id2:\n",
    "                    correlation = 1.0\n",
    "                else:\n",
    "                    array1 = exp_data[condition1][exp_id1][k]\n",
    "                    array2 = exp_data[condition2][exp_id2][k]\n",
    "                    if array1.any() and array2.any():\n",
    "                        correlation = spearmanr(array1, array2)[0]\n",
    "                    else:\n",
    "                        correlation = np.nan  # Assign NaN if either array is empty\n",
    "\n",
    "                pairwise_correlations.append((exp_id1, exp_id2, condition1, condition2, correlation))\n",
    "\n",
    "\n",
    "    # Close bigwig files\n",
    "    for bw in bw_objects:\n",
    "        bw.close()\n",
    "\n",
    "    # Convert results to dataframe and pivot to wide format for heatmap\n",
    "    # Construct a DataFrame from the pairwise correlations\n",
    "    pairwise_correlation_df = pd.DataFrame(pairwise_correlations, columns=['exp_id1', 'exp_id2', 'condition1', 'condition2', 'correlation'])\n",
    "    # Step 1 & 2: Combine the 'exp_id' and 'condition' columns\n",
    "\n",
    "    pairwise_correlation_df['exp_condition1'] = pairwise_correlation_df['condition1'] + '-' + pairwise_correlation_df['exp_id1']\n",
    "    pairwise_correlation_df['exp_condition2'] = pairwise_correlation_df['condition2'] + '-' + pairwise_correlation_df['exp_id2']\n",
    "\n",
    "    # Step 3: Pivot the DataFrame\n",
    "    pivot_df = pairwise_correlation_df.pivot(index='exp_condition1', columns='exp_condition2', values='correlation')\n",
    "\n",
    "    # Display the first 3 rows of the pivoted DataFrame\n",
    "    display(pivot_df.head(3))\n",
    "\n",
    "    return pivot_df\n",
    "\n",
    "# Define bigwig paths and bin size\n",
    "# Replace with your actual bigwig paths\n",
    "bigwig_paths = [\"/Data1/reference/lieb_mnase_2017/GSM2098437_RT_rep1_MNaseTC_30m_smoothDyads_ce11.bw\", \"/Data1/reference/lieb_mnase_2017/GSM2098437_RT_rep2_MNaseTC_30m_smoothDyads_ce11.bw\"]  # Replace with your actual bigwig file paths\n",
    "\n",
    "bin_size = 1  # Define your bin size accordingly\n",
    "\n",
    "nanotools.display_sample_rows(grouped,10)\n",
    "\n",
    "# Calculate correlations\n",
    "correlation_df = calculate_correlations(grouped, bigwig_paths, bin_size)\n",
    "\n",
    "# Display the correlation table (first 3 rows)\n",
    "nanotools.display_sample_rows(correlation_df)\n",
    "\n",
    "def plot_heatmap(correlation_matrix):\n",
    "    heatmap = go.Figure(data=go.Heatmap(\n",
    "        z=correlation_matrix.values,  # Correlation values\n",
    "        x=correlation_matrix.columns,  # exp_id as x-axis\n",
    "        y=correlation_matrix.index,  # exp_id as y-axis\n",
    "        colorscale='Viridis'\n",
    "    ))\n",
    "\n",
    "    heatmap.update_layout(\n",
    "        title='Heatmap of Pearson Correlation Coefficients',\n",
    "        xaxis_title=\"exp_id (X-axis)\",\n",
    "        yaxis_title=\"exp_id (Y-axis)\",\n",
    "        template=\"simple_white\"\n",
    "    )\n",
    "\n",
    "    heatmap.show()\n",
    "\n",
    "# Assuming correlation_df is a correlation matrix with exp_ids as index and columns\n",
    "plot_heatmap(correlation_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ebcc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "### STANDARD READ PLOT\n",
    "def create_plot(plot_df, condition, chr_type, data_type, bed_window):\n",
    "    plot_df_copy = plot_df.copy()\n",
    "    plot_df_copy.reset_index(inplace=True, drop=True)\n",
    "    # Filter the DataFrame based on the specified condition, chr_type, and data_type\n",
    "    plot_df_copy = plot_df_copy[(plot_df_copy['condition'] == condition) & (plot_df_copy['chr_type'] == chr_type) & (plot_df_copy['type'] == data_type)]\n",
    "    plot_df_copy = plot_df_copy.sort_values(by=['smallest_positive_nuc_midpoint', 'greatest_negative_nuc_midpoint','rel_pos'])\n",
    "    plot_df_copy.reset_index(inplace=True, drop=True)\n",
    "    # Create a lookup table of unique read_ids and read_count\n",
    "    read_id_lookup = plot_df_copy[['read_id', 'read_count','smallest_positive_nuc_midpoint', 'greatest_negative_nuc_midpoint']].drop_duplicates().reset_index(drop=True)\n",
    "    # reset read_count column to increment by 1 for each row\n",
    "    read_id_lookup['read_count'] = read_id_lookup.index + 1\n",
    "    #print(\"read_id_lookup:\")\n",
    "    #display(read_id_lookup.head(100))\n",
    "    # Create a new column 'read_count' in plot_df_copy by mapping the read_id_lookup\n",
    "    plot_df_copy['read_count'] = plot_df_copy['read_id'].map(read_id_lookup.set_index('read_id')['read_count'])\n",
    "\n",
    "    #display(plot_df_copy.head(100))\n",
    "\n",
    "    # Create a subplot with 3 rows and 1 column\n",
    "    fig = make_subplots(rows=3, cols=1, shared_xaxes=True, vertical_spacing=0.02, row_heights=[0.5, 0.2, 0.2])\n",
    "\n",
    "    # Update xaxes for all subplots\n",
    "    fig.update_xaxes(range=[-bed_window, bed_window])\n",
    "\n",
    "\n",
    "    #print(\"plot_df_copy\")\n",
    "    #display(plot_df_copy.head(10))\n",
    "    # Calculate sum and count of mod_qual at each rel_pos\n",
    "    agg_df = plot_df_copy.groupby('rel_pos')['mod_qual'].agg(['sum', 'count']).reset_index()\n",
    "    agg_df['ratio'] = agg_df['sum'] / agg_df['count']\n",
    "    # Calculate the moving average of the ratio with a centered window of 20\n",
    "    agg_df['moving_avg'] = agg_df['ratio'].rolling(window=50, center=True).mean()\n",
    "    #drop nan values\n",
    "    agg_df.dropna(inplace=True)\n",
    "    #print(\"agg_df:\",agg_df)\n",
    "    #display(agg_df.head(100))\n",
    "\n",
    "    # Add line traces for each unique read_id\n",
    "    for read_id in plot_df_copy['read_count'].unique():\n",
    "        read_data = plot_df_copy[plot_df_copy['read_count'] == read_id]\n",
    "        min_rel_pos = read_data['rel_pos'].min()\n",
    "        max_rel_pos = read_data['rel_pos'].max()\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=[min_rel_pos, max_rel_pos], y=[read_data['read_count'].iloc[0]] * 2,\n",
    "                       mode='lines', line=dict(color='#000000', width=0.2),showlegend=False),row=1, col=1\n",
    "        )\n",
    "    # drop rows where mod_qual == 0\n",
    "    plot_df_dropped = plot_df_copy[plot_df_copy['mod_qual'] != 0]\n",
    "\n",
    "    # Upper scatter plot\n",
    "    scatter_trace = go.Scatter(x=plot_df_dropped['rel_pos'], y=plot_df_dropped['read_count'], mode='markers',\n",
    "                               marker=dict(size=2, color=plot_df_dropped['mod_qual'], colorscale=[[0, '#FFFFFF'], [1, '#0000FF']]))\n",
    "    fig.add_trace(scatter_trace, row=1, col=1)\n",
    "\n",
    "\n",
    "\n",
    "    # Lower line plot for moving average of the ratio\n",
    "    line_trace = go.Scatter(x=agg_df['rel_pos'], y=agg_df['moving_avg'], mode='lines',\n",
    "                            #set color to match blue\n",
    "                            line=dict(color='#0000FF', width=1),\n",
    "                            # smooth line\n",
    "                            line_shape='spline')\n",
    "    fig.add_trace(line_trace, row=2, col=1)\n",
    "\n",
    "    ### PLOT NUCLEOSOMES\n",
    "    #NUC_max_width  = 160 # Distance below which m6A marks are combined into NUC\n",
    "    #NUC_min_width  = 100 # Distance above which m6A marks are combined into NUC\n",
    "    # Group by 'read_count'\n",
    "    grouped = plot_df_dropped.groupby('read_count')\n",
    "    midpoints_dict = {}\n",
    "    for read_count, group in grouped:\n",
    "        # Sort by 'rel_pos'\n",
    "        group = group.sort_values(by='rel_pos')\n",
    "        # Initialize an empty list to hold midpoints for this read_count\n",
    "        midpoints_list = []\n",
    "        for i in range(len(group) - 1):\n",
    "            x1 = group.iloc[i]['rel_pos']\n",
    "            x2 = group.iloc[i + 1]['rel_pos']\n",
    "            y = read_count\n",
    "            # Check if the x-values are < MAD_dist_max apart\n",
    "            if x2 - x1 > NUC_min_width  and x2 - x1 < NUC_max_width :\n",
    "                # Check if there are no other points between x1 and x2\n",
    "                in_between = group[(group['rel_pos'] > x1) & (group['rel_pos'] < x2)]\n",
    "                if in_between.empty:\n",
    "                    fig.add_trace(\n",
    "                        go.Scatter(x=[x1, x2], y=[y, y], mode='lines',\n",
    "                                   line=dict(color='#FF9999', width=1), showlegend=False),\n",
    "                        row=1, col=1\n",
    "                    )\n",
    "                    # Calculate the midpoint and add to list\n",
    "                    midpoint = (x1 + x2) / 2\n",
    "                    midpoints_list.append(midpoint)\n",
    "\n",
    "            # Add methylase accessible DNA sequences\n",
    "            if x2 - x1 <MAD_dist_max:\n",
    "                # Check if there are no other points between x1 and x2\n",
    "                in_between = group[(group['rel_pos'] > x1) & (group['rel_pos'] < x2)]\n",
    "                if in_between.empty:\n",
    "                    fig.add_trace(\n",
    "                        go.Scatter(x=[x1, x2], y=[y, y], mode='lines',\n",
    "                                   line=dict(color='#0000FF', width=1), showlegend=False,opacity=0.5),\n",
    "                        row=1, col=1\n",
    "                    )\n",
    "\n",
    "        # Add the list of midpoints to the dictionary\n",
    "        midpoints_dict[read_count] = midpoints_list\n",
    "\n",
    "    midpoints_df = pd.DataFrame.from_dict(midpoints_dict, orient='index')\n",
    "    midpoint_series = [midpoint for sublist in midpoints_dict.values() for midpoint in sublist]\n",
    "\n",
    "    # Add midpoint plot\n",
    "    midpoint_trace = go.Histogram(x=midpoint_series, histnorm='probability', nbinsx=100, marker=dict(color='#FF9999'))\n",
    "    fig.add_trace(midpoint_trace, row=3, col=1)\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(template=\"simple_white\")\n",
    "    fig.update_yaxes(title_text=\"Read_ID\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"% m6A\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Nucleosome Probability\", row=3, col=1)\n",
    "    fig.update_xaxes(title_text=\"Genomic location (bp)\", row=3, col=1)\n",
    "    fig.update_layout(height=800)\n",
    "    fig.update_layout(width=1100)\n",
    "    # set y max to 60\n",
    "\n",
    "# Add Rex Line\n",
    "    fig.add_shape(\n",
    "        go.layout.Shape(\n",
    "            type=\"line\",\n",
    "            x0=0,\n",
    "            x1=0,\n",
    "            y0=0,\n",
    "            y1=1,\n",
    "            yref=\"paper\",\n",
    "            line=dict(\n",
    "                color=\"grey\",\n",
    "                width=1,\n",
    "                dash=\"dash\",\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    #fig.add_annotation(\n",
    "    #    x=0,\n",
    "    #    y=1,\n",
    "    #    yref=\"paper\",\n",
    "    #    text=\"rex\",\n",
    "    #    showarrow=False,\n",
    "    #    font=dict(\n",
    "    #        size=15,\n",
    "    #        color=\"grey\"\n",
    "    #    )\n",
    "    #)\n",
    "\n",
    "    return fig\n",
    "\n",
    "# Sample usage of the function\n",
    "N2_fig = create_plot(down_sampled_plot_df, \"N2_fiber\", \"X\", \"center_DPY27_chip_albretton;SDC2_ol1000;SDC3_ol1000\", bed_window)\n",
    "N2_fig.write_image(\"images/N2_fiber_sdc3_sdc2_dpy27-fibers.png\",width=1600,height=1300)\n",
    "#N2_fig.show(renderer='plotly_mimetype+notebook')\n",
    "#SDC2_fig = create_plot(down_sampled_plot_df, \"SDC2_degron_fiber\", \"X\", \"TSS_q4\", bed_window)\n",
    "#SDC2_fig.show(renderer='plotly_mimetype+notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbac2881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save each fig to /images folder as svg, with the filename incorporating the condition and chr_type and type\n",
    "N2_fig.write_image(\"images/SDC2-degron_fiber_sdc3_sdc2_dpy27-fibers.svg\")\n",
    "#SDC2_fig.write_image(\"images/SDC2_degron_fiber_X_TSS_q4.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921d3e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plotting nucleosome offset\n",
    "# Convert the dictionary to a DataFrame\n",
    "midpoints_df = pd.DataFrame.from_dict(midpoints_dict, orient='index')\n",
    "#display(midpoints_dict)\n",
    "\n",
    "print(\"midpoints_df:\")\n",
    "display(midpoints_df.head(100))\n",
    "\n",
    "# Find the least positive value for each row\n",
    "least_positive_per_row = midpoints_df[midpoints_df > 0].min(axis=1)\n",
    "#least_positive_per_row = least_positive_per_row[least_positive_per_row<480]\n",
    "\n",
    "# Find the least negative value for each row\n",
    "least_negative_per_row = midpoints_df[midpoints_df < 0].max(axis=1)\n",
    "#least_negative_per_row = least_negative_per_row[least_negative_per_row > -480]\n",
    "\n",
    "# Calculate the difference between the least positive and least negative nucleosome position for each read\n",
    "differences = least_positive_per_row -least_negative_per_row\n",
    "\n",
    "# Plot the distribution of the differences using a histogram\n",
    "fig_diff = go.Figure()\n",
    "fig_diff.add_trace(go.Histogram(x=differences, marker=dict(color='#FF9999'),nbinsx=100))\n",
    "fig_diff.update_layout(title=\"Distribution of Nucleosome Position Differences\",\n",
    "                       xaxis_title=\"Difference between Least Positive and Least Negative Nucleosome Position\",\n",
    "                       yaxis_title=\"Frequency\",\n",
    "                       template=\"simple_white\")\n",
    "fig_diff.show()\n",
    "\n",
    "# Identify the least negative and the second least negative nucleosome positions for each read\n",
    "least_negative_per_row_sorted = midpoints_df[midpoints_df < 0].apply(lambda x: sorted(x.dropna()), axis=1)\n",
    "second_least_negative_per_row = least_negative_per_row_sorted.apply(lambda x: x[-2] if len(x) > 1 else np.nan)\n",
    "\n",
    "# Calculate the difference between the least negative and the second least negative nucleosome position for each read\n",
    "diff_second_least = least_negative_per_row - second_least_negative_per_row\n",
    "\n",
    "# Remove NaN values (reads that might not have a second least negative position)\n",
    "diff_second_least = diff_second_least.dropna()\n",
    "\n",
    "# Plot the distribution of the differences using a histogram\n",
    "fig_diff_second = go.Figure()\n",
    "fig_diff_second.add_trace(go.Histogram(x=diff_second_least, marker=dict(color='#FF9999'),nbinsx=100))\n",
    "fig_diff_second.update_layout(title=\"Distribution of Differences between Least Negative and Second Least Negative Nucleosome Positions\",\n",
    "                              xaxis_title=\"Difference between Least Negative and Second Least Negative Nucleosome Position\",\n",
    "                              yaxis_title=\"Frequency\",\n",
    "                              template=\"simple_white\")\n",
    "fig_diff_second.show()\n",
    "\n",
    "# Compute the averages\n",
    "avg_least_positive = least_positive_per_row.mean()\n",
    "avg_least_negative = least_negative_per_row.mean()\n",
    "print(\"avg_least_positive:\",avg_least_positive)\n",
    "print(\"avg_least_negative:\",avg_least_negative)\n",
    "\n",
    "# Calculate the mean of each column, ignoring NaN values\n",
    "range_n = 9\n",
    "assigned_pos = [avg_least_positive + (160 * i) for i in range(range_n)]\n",
    "assigned_neg = [avg_least_negative - ((range_n-1)*160) + (160*i) for i in range(range_n)]\n",
    "\n",
    "assigned_col = assigned_neg + assigned_pos\n",
    "print(\"assigned_col:\")\n",
    "print(assigned_col)\n",
    "\n",
    "# Create an empty DataFrame to store the rearranged values\n",
    "# Make it large enough to accommodate shifts; you can adjust the size as needed\n",
    "max_cols = midpoints_df.shape[1] * 2  # Example size, adjust as needed\n",
    "rearranged_df = pd.DataFrame(index=midpoints_df.index, columns=range(max_cols))\n",
    "# Create an empty DataFrame to store the rearranged values\n",
    "rearranged_df = pd.DataFrame(index=midpoints_df.index, columns=range(len(assigned_col)))\n",
    "\n",
    "\n",
    "# Iterate through each row to find the closest column mean and rearrange\n",
    "for idx, row in midpoints_df.iterrows():\n",
    "    row_values = row.dropna().values  # Drop NaN values\n",
    "    if len(row_values) == 0:  # Skip empty rows\n",
    "        continue\n",
    "\n",
    "    # For each value in row, find the closest column mean\n",
    "    for value in row_values:\n",
    "        closest_column = np.argmin(np.abs(assigned_col - value))\n",
    "        rearranged_df.at[idx, closest_column] = value  # Place the value in the closest column\n",
    "\n",
    "# Drop columns that are entirely NaN, if desired\n",
    "rearranged_df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "display(rearranged_df.head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c6decc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# find average value of each column and save as a list\n",
    "mean_list = []\n",
    "for col in rearranged_df.columns:\n",
    "    mean_list.append(rearranged_df[col].mean())\n",
    "print(\"mean_list:\")\n",
    "print(mean_list)\n",
    "\n",
    "for index, row in rearranged_df.iterrows():\n",
    "    non_nan_indices = row.dropna().index.tolist()\n",
    "\n",
    "    if non_nan_indices:  # Check if there are any non-NaN values in the row\n",
    "        start, end = non_nan_indices[0], non_nan_indices[-1]\n",
    "        # Use the column average for filling NaNs\n",
    "        rearranged_df.loc[index, start+1:end] = rearranged_df.loc[index, start+1:end].fillna(100000)\n",
    "\n",
    "# For each column, calculate the % of non-NaN values == 100000\n",
    "percent_100000 = (rearranged_df == 100000).sum() / len(rearranged_df)\n",
    "print(\"percent_100000:\")\n",
    "print(percent_100000)\n",
    "# Plot a go bar plot of percent_100000 with\n",
    "fig = go.Figure(data=go.Bar(x=[\"n-9\",\"n-8\",\"n-7\",\"n-6\",\"n-5\",\"n-4\",\"n-3\",\"n-2\",\"n-1\",\"n+1\",\"n+2\",\"n+3\",\"n+4\",\"n+5\",\"n+6\",\"n+7\",\"n+8\",\"n+9\"], y=percent_100000.values))\n",
    "\n",
    "# Create an empty DataFrame with the same shape as rearranged_df to store the mean differences\n",
    "mean_diff_df = pd.DataFrame(index=rearranged_df.index, columns=rearranged_df.columns)\n",
    "print(\"Rearranged df:\")\n",
    "display(rearranged_df.head(100))\n",
    "# Iterate through each row of rearranged_df\n",
    "for idx, row in rearranged_df.iterrows():\n",
    "    for col in rearranged_df.columns:\n",
    "        current_value = row[col]\n",
    "\n",
    "        # Check if the value is NaN; if so, continue to the next iteration\n",
    "        if pd.isna(current_value):\n",
    "            continue\n",
    "\n",
    "        if current_value == 100000.0:\n",
    "            #mean_diff_df.at[idx, col] = 80\n",
    "            continue\n",
    "\n",
    "        mean_diff_df.at[idx, col] = abs(current_value - mean_list[col])\n",
    "         # Calculate the differences between the current value and all other values in the row\n",
    "        differences = rearranged_df.subtract(current_value, axis=1)\n",
    "        #differences.at[idx, col] = np.nan\n",
    "        if idx == 1:\n",
    "            print(\"differences:\")\n",
    "            print(differences)\n",
    "\n",
    "        # For the current row, filter differences with absolute values less than 80\n",
    "        #valid_diffs = differences.loc[idx][differences.loc[idx].abs() < 80].abs()\n",
    "\n",
    "        # Take all differences with absolute values less than 80 and convert to a list\n",
    "        valid_diffs = differences[abs(differences) < 160].values.flatten().tolist()\n",
    "\n",
    "        # drop all nan values\n",
    "        valid_diffs = [x for x in valid_diffs if str(x) != 'nan']\n",
    "\n",
    "        # Take absolute value\n",
    "        valid_diffs = [abs(x) for x in valid_diffs]\n",
    "\n",
    "        # Count number of 100000 values in current column in rearranged_df and add this many \"80\"s to valid diff list\n",
    "        #valid_diffs.extend([80] * (rearranged_df[col] == 100000).sum())\n",
    "\n",
    "\n",
    "        # if first iteration, print valid diffs:\n",
    "        if idx == 1:\n",
    "            print(\"valid_diffs:\")\n",
    "            print(valid_diffs)\n",
    "\n",
    "        # Get the smallest difference value for the row\n",
    "        #smallest_diff = valid_diffs.min() if not valid_diffs.empty else np.nan  # Set to NaN if there are no valid differences\n",
    "\n",
    "        #get the mean of valid_diffs list\n",
    "        mean_diff_df.at[idx, col] = np.mean(valid_diffs) if valid_diffs else np.nan\n",
    "        # Store the smallest difference in the mean_diff_df\n",
    "        #mean_diff_df.at[idx, col] = smallest_diff\n",
    "\n",
    "print(\"mean_diff_df:\")\n",
    "display(mean_diff_df.head(10))\n",
    "\n",
    "rearranged_df_abs_diff = mean_diff_df.abs()\n",
    "#set col names to \"n-7\",\"n-6\",...,\"n+7\"\n",
    "rearranged_df_abs_diff.columns = [\"n-9\",\"n-8\",\"n-7\",\"n-6\",\"n-5\",\"n-4\",\"n-3\",\"n-2\",\"n-1\",\"n+1\",\"n+2\",\"n+3\",\"n+4\",\"n+5\",\"n+6\",\"n+7\",\"n+8\",\"n+9\"]\n",
    "#display(rearranged_df_abs_diff)\n",
    "\n",
    "# If you want to combine n- and n+ nucleosomes:\n",
    "combine_nucleosomes=0\n",
    "if combine_nucleosomes==1:\n",
    "    # Separate the columns into two sets: 'n-x' and 'n+x'\n",
    "    cols_n_minus = [\"n-9\",\"n-8\",\"n-7\", \"n-6\", \"n-5\", \"n-4\", \"n-3\", \"n-2\", \"n-1\"]\n",
    "    cols_n_plus = [\"n+1\", \"n+2\", \"n+3\", \"n+4\", \"n+5\", \"n+6\", \"n+7\",\"n+8\",\"n+9\"]\n",
    "\n",
    "    # Extract columns for 'n-x' and 'n+x'\n",
    "    n_minus_df = rearranged_df_abs_diff[cols_n_minus]\n",
    "    n_plus_df = rearranged_df_abs_diff[cols_n_plus]\n",
    "\n",
    "    # Flip the 'n-x' columns in reverse order\n",
    "    flipped_n_minus_df = n_minus_df[cols_n_minus[::-1]]\n",
    "\n",
    "    # Convert both DataFrames to NumPy arrays\n",
    "    n_plus_np = n_plus_df.to_numpy()\n",
    "    flipped_n_minus_np = flipped_n_minus_df.to_numpy()\n",
    "\n",
    "    # Vertically concatenate the NumPy arrays\n",
    "    result_np = np.vstack([n_plus_np, flipped_n_minus_np])\n",
    "\n",
    "    # Convert the result back to a DataFrame with the original 'n+x' column names\n",
    "    result_df = pd.DataFrame(result_np, columns=cols_n_plus)\n",
    "\n",
    "else:\n",
    "    result_df = rearranged_df_abs_diff\n",
    "\n",
    "#print 100 rows of rearranged_df\n",
    "print(\"result_df:\")\n",
    "display(result_df.head(10))\n",
    "\n",
    "#print(result_df)\n",
    "# Prepare data for box plot\n",
    "data = []\n",
    "for col in result_df.columns:\n",
    "    col_data = result_df[col].dropna()  # Remove NaN values\n",
    "    trace = go.Box(\n",
    "        y=col_data,\n",
    "        name=str(col),\n",
    "        boxpoints='all',  # Show all points\n",
    "        jitter=0.3,  # Add some jitter for visibility\n",
    "        pointpos=-1.8  # Position of the points\n",
    "    )\n",
    "    data.append(trace)\n",
    "\n",
    "# Create layout\n",
    "layout = go.Layout(\n",
    "    title=\"Box Plot of Rearranged DataFrame\",\n",
    "    xaxis=dict(title=\"Index\"),\n",
    "    yaxis=dict(title=\"Values\")\n",
    ")\n",
    "\n",
    "# Create the figure\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "#set theme to plotly_white\n",
    "fig.update_layout(template=\"simple_white\")\n",
    "\n",
    "# Add label for average values to each box plot\n",
    "for i, col in enumerate(result_df.columns):\n",
    "    col_data = result_df[col].dropna()  # Remove NaN values\n",
    "    print(col_data.mean())\n",
    "    fig.add_annotation(\n",
    "        x=i ,\n",
    "        y=col_data.mean(),\n",
    "        text=f\"{col_data.mean():.2f}\",\n",
    "        showarrow=False,\n",
    "        font=dict(\n",
    "            size=10,\n",
    "            color=\"black\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Show the figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3aea50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "\n",
    "# Function to plot data for each Cluster_ID\n",
    "def plot_cluster(df_cluster):\n",
    "    fig = make_subplots(rows=2, cols=1, shared_xaxes=True, vertical_spacing=0.02, row_heights=[0.7, 0.3])\n",
    "\n",
    "    # Aggregate mod_qual by rel_pos\n",
    "    agg_df = df_cluster.groupby('rel_pos')['mod_qual'].agg(['sum', 'count']).reset_index()\n",
    "    agg_df['ratio'] = agg_df['sum'] / agg_df['count']\n",
    "    agg_df['moving_avg'] = agg_df['ratio'].rolling(window=50, center=True).mean()\n",
    "    agg_df.dropna(inplace=True)\n",
    "\n",
    "    # Add line traces for each unique read_id\n",
    "    for read_id in df_cluster['read_id'].unique():\n",
    "        read_data = df_cluster[df_cluster['read_id'] == read_id]\n",
    "        min_rel_pos = read_data['rel_pos'].min()\n",
    "        max_rel_pos = read_data['rel_pos'].max()\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=[min_rel_pos, max_rel_pos], y=[read_data['read_count'].iloc[0]] * 2,\n",
    "                       mode='lines', line=dict(color='gray', width=0.1), showlegend=False),\n",
    "            row=1, col=1\n",
    "        )\n",
    "\n",
    "    # Drop rows where mod_qual == 0\n",
    "    df_cluster_dropped = df_cluster[df_cluster['mod_qual'] != 0]\n",
    "\n",
    "    # Upper scatter plot\n",
    "    scatter_trace = go.Scatter(x=df_cluster_dropped['rel_pos'], y=df_cluster_dropped['read_count'], mode='markers',\n",
    "                               marker=dict(size=3, color=df_cluster_dropped['mod_qual'], colorscale=[[0, '#FFFFFF'], [1, '#0000FF']]))\n",
    "    fig.add_trace(scatter_trace, row=1, col=1)\n",
    "\n",
    "    # Lower line plot for moving average of the ratio\n",
    "    line_trace = go.Scatter(x=agg_df['rel_pos'], y=agg_df['moving_avg'], mode='lines',\n",
    "                            line=dict(color='#0000FF', width=1),\n",
    "                            line_shape='spline')\n",
    "    fig.add_trace(line_trace, row=2, col=1)\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(template=\"simple_white\", height=800)\n",
    "\n",
    "    # Show figure\n",
    "    fig.show()\n",
    "\n",
    "# Assuming plot_df is already loaded\n",
    "# Group by Cluster_ID and plot each group\n",
    "for cluster_id, group_df in plot_df.groupby('Cluster_ID'):\n",
    "    print(f\"Plotting for Cluster_ID: {cluster_id}\")\n",
    "    plot_cluster(group_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663d925f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Achieve the same as above using modbampy\n",
    "# NOTE This is unable to define methylation threshold\n",
    "# print(\"Kicking off loop for:\",new_bed_files,\"and\",new_bam_files)\n",
    "'''import modbampy\n",
    "importlib.reload(modbampy)\n",
    "from modbampy import ModBam\n",
    "\n",
    "# Assuming bedwindow is already defined in your code\n",
    "columns = [int(i) for i in range(1, 2*bed_window + 1)]\n",
    "index_desc = ['a', 'c', 'g', 't', 'A', 'C', 'G', 'T', 'd', 'D', 'm', 'M', 'f', 'F', 'n', 'N','Unk1','Unk2']\n",
    "'''A, C, G, T are the usual DNA bases,\n",
    "D indicates deletion counts,\n",
    "M modified base counts,\n",
    "F filtered counts - bases in reads with a modified-base record but which were filtered according to the thresholds provided.\n",
    "N no call base counts.'''\n",
    "counts_df = pd.DataFrame(columns=columns,index=index_desc)\n",
    "#Set all values to 0\n",
    "counts_df = counts_df.fillna(0)\n",
    "\n",
    "# Load the BED file\n",
    "print(\"Kicking off loop for:\",new_bed_files,\"and\",new_bam_files)\n",
    "for bed_file in new_bed_files:\n",
    "    print(\"Starting on bed_file:\",bed_file)\n",
    "    regions = pysam.TabixFile(bed_file)\n",
    "    # Iterate over the regions in the BED file\n",
    "    for region in regions.fetch(multiple_iterators=True):\n",
    "        #print(\"Region:\",region)\n",
    "        # Split the region string into the chromosome, start, and end positions\n",
    "        chromosome, start, end, strand, region_type, chr_type = region.split()\n",
    "        start = int(start)\n",
    "        end = int(end)\n",
    "        for bam_file in new_bam_files[0]:\n",
    "            with ModBam(bam_file) as bam:\n",
    "                positions, counts = bam.pileup(chromosome, start, end,threshold=0.1,mod_base=\"a\")\n",
    "            positions_reset = positions - start\n",
    "            #convert positions from np array to list\n",
    "            positions_list = positions_reset.tolist()\n",
    "\n",
    "            # Create temp_counts_df from numpy array counts\n",
    "            #print(\"counts shape\",counts.shape)\n",
    "            #print(\"counts\",counts)\n",
    "\n",
    "            #print(\"counts shape\",counts.T.shape)\n",
    "            #print(\"counts.T\",counts.T)\n",
    "            temp_counts_df = pd.DataFrame(counts.T,index=index_desc,columns=positions_list)\n",
    "            # set row index to be the same as counts_df\n",
    "            #print(\"print(temp_counts_df.shape)\",temp_counts_df.shape)\n",
    "            #print(\"print(counts_df.shape)\",counts_df.shape)\n",
    "            #print(\"Temp_count_df:\",temp_counts_df)\n",
    "            counts_df = counts_df.astype(float)\n",
    "            temp_counts_df = temp_counts_df.astype(float)\n",
    "\n",
    "            # Sum temp_counts_df to counts_df\n",
    "            counts_df = counts_df.add(temp_counts_df, fill_value=0)\n",
    "\n",
    "\n",
    "print(\"COUNTS_DF:\",counts_df)\n",
    "print(\"shape of counts_df:\",counts_df.shape)\n",
    "'''\n",
    "\n",
    "'''counts_df_plot = counts_df.copy()\n",
    "# Merge rows A and a, C and c, G and g, T and t, M and m, D and d, F and f, N and n, Unk1 and Unk2\n",
    "counts_df_plot.loc['A'] = counts_df_plot.loc['A'] + counts_df_plot.loc['a']\n",
    "counts_df_plot.loc['C'] = counts_df_plot.loc['C'] + counts_df_plot.loc['c']\n",
    "counts_df_plot.loc['G'] = counts_df_plot.loc['G'] + counts_df_plot.loc['g']\n",
    "counts_df_plot.loc['T'] = counts_df_plot.loc['T'] + counts_df_plot.loc['t']\n",
    "counts_df_plot.loc['M'] = counts_df_plot.loc['M'] + counts_df_plot.loc['m']\n",
    "counts_df_plot.loc['D'] = counts_df_plot.loc['D'] + counts_df_plot.loc['d']\n",
    "counts_df_plot.loc['F'] = counts_df_plot.loc['F'] + counts_df_plot.loc['f']\n",
    "counts_df_plot.loc['N'] = counts_df_plot.loc['N'] + counts_df_plot.loc['n']\n",
    "counts_df_plot.loc['Unk1'] = counts_df_plot.loc['Unk1'] + counts_df_plot.loc['Unk2']\n",
    "#drop merged rows\n",
    "counts_df_plot = counts_df_plot.drop(['a','c','g','t','m','d','f','n','Unk2'])\n",
    "# Add m6A_frac row\n",
    "counts_df_plot.loc['m6A_frac'] = counts_df_plot.loc['M'] / (counts_df_plot.loc['A'])\n",
    "print(counts_df_plot.index)\n",
    "#Sort dataframe by columns in ascending order\n",
    "counts_df_plot = counts_df_plot.sort_index(axis=1)'''\n",
    "\n",
    "'''# Plot m6A_frac in a line plot using plotly\n",
    "# Plotting with Plotly\n",
    "# Set plotly renderer to notebook\n",
    "\n",
    "# Compute the moving average for smoothing\n",
    "window_size = 25  # This defines the number of data points to use for each average value\n",
    "m6A_data = counts_df_plot.loc['m6A_frac']\n",
    "smoothed_data = m6A_data.rolling(window=window_size).mean()\n",
    "\n",
    "# Plotting with Plotly\n",
    "fig = go.Figure()\n",
    "\n",
    "# Original data\n",
    "fig.add_trace(go.Scatter(x=m6A_data.index, y=m6A_data.values,\n",
    "                         mode='lines',\n",
    "                         name='Original'))\n",
    "\n",
    "# Smoothed data\n",
    "fig.add_trace(go.Scatter(x=smoothed_data.index, y=smoothed_data.values,\n",
    "                         mode='lines',\n",
    "                         name=f'Smoothed (window size: {window_size})'))\n",
    "\n",
    "fig.update_layout(title='m6A Fraction vs Genomic Position',\n",
    "                  xaxis_title='Genomic Position',\n",
    "                  yaxis_title='m6A Fraction')\n",
    "\n",
    "# Set to plotly white theme\n",
    "fig.update_layout(template=\"plotly_white\")\n",
    "\n",
    "fig.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70070eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extract m6A frac by region\n",
    "importlib.reload(nanotools)\n",
    "result_list=[]\n",
    "result_df=pd.DataFrame()\n",
    "# Parallelize for each bam file:\n",
    "args_list = [(bam_file, condition, bam_frac,file_prefix, selection, m6A_thresh, output_stem,new_bed_files) for bam_file, condition, bam_frac in zip(new_bam_files,conditions,bam_fracs)]\n",
    "print(\"Args list:\",args_list)\n",
    "if __name__ == \"__main__\":\n",
    "    with Pool(processes=10) as pool: #processes=1\n",
    "        # append results to pandas df 'result'\n",
    "        result_list = pool.starmap(nanotools.extract_m6A_per_region_parellized, args_list)\n",
    "        for result in result_list:\n",
    "            result_df=pd.concat([result_df,result])\n",
    "    print(\"Program finished!\")\n",
    "\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e2dfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Build dataframe for plotting\n",
    "def reindex_df(df, weight_col):\n",
    "    \"\"\"expand the dataframe to prepare for resampling\n",
    "    result is 1 row per count per sample\"\"\"\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df = df.reindex(df.index.repeat(np.ceil(df[weight_col])/100000))\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return(df)\n",
    "\n",
    "'''# If combined regions file already exists, read dataframe from csv\n",
    "if os.path.exists(output_stem  + file_prefix + \"weighted_combined_regions_\"  + str(m6A_thresh) +\".csv\"):\n",
    "    weighted_combined_regions = pd.read_csv(output_stem + file_prefix + \"weighted_combined_regions_\"  + str(m6A_thresh) +\".csv\")\n",
    "    print(\"File: \",\n",
    "          output_stem +  file_prefix+\"combined_regions_\"  + str(m6A_thresh) +\".csv\",\n",
    "          \"already exists! Imported directly:\")\n",
    "    print(weighted_combined_regions)\n",
    "\n",
    "else:'''\n",
    "print(\"Building combined regions file...\")\n",
    "# Initialize variables\n",
    "filenames = []\n",
    "df_list = []\n",
    "combined_regions = []\n",
    "\n",
    "# Create \"filenames\" list that includes the name of each file to be read\n",
    "for each_type in selection:\n",
    "    for each_cond, each_frac in zip(conditions,bam_fracs):\n",
    "        filenames.append(output_stem + file_prefix+\"m6A_frac_\" + each_cond + \"_\"  + str(m6A_thresh)+\"_\"+each_type+\".csv\")\n",
    "\n",
    "# Loop through the list of file names\n",
    "for filename in filenames:\n",
    "    # Read each file into a dataframe\n",
    "    df = pd.read_csv(filename)\n",
    "    # Add the dataframe to the list of dataframes\n",
    "    df_list.append(df)\n",
    "\n",
    "# Concatenate the list of dataframes into a single dataframe\n",
    "combined_regions = pd.concat(df_list)\n",
    "\n",
    "# Reindex the dataframe to have the number repeated rows based on total bases in the region\n",
    "# This helps ensure plots are weighted correctly.\n",
    "weighted_combined_regions = reindex_df(combined_regions,'total_bases')\n",
    "\n",
    "# Add column equal to average of autosome m6A_frac column for each condition\n",
    "weighted_combined_regions['mean_autosome_m6A_frac'] = weighted_combined_regions.groupby('condition')['m6A_frac'].transform('mean')\n",
    "\n",
    "# Add column equal to m6A normalized by the condition's mean_autosome_m6A_frac\n",
    "weighted_combined_regions['norm_m6A_frac'] = weighted_combined_regions['m6A_frac']/weighted_combined_regions['mean_autosome_m6A_frac']\n",
    "\n",
    "# Save final dataframe to .csv file\n",
    "print(\"Weighted combined:\",weighted_combined_regions)\n",
    "print(\"Outputting file:\",output_stem  + file_prefix+\"combined_regions_\"  + str(m6A_thresh) +\".csv\")\n",
    "weighted_combined_regions.to_csv(output_stem  + file_prefix+\"weighted_combined_regions_\"  + str(m6A_thresh) +\".csv\", index=False, mode='w')\n",
    "\n",
    "# Extract average m6A/A across each chromosome for each condition from weighted_combined_regions\n",
    "# This is used for plotting the average m6A/A across the chromosome\n",
    "chromosome_m6A_frac = weighted_combined_regions.groupby(['condition','condition_min','chr_type'])['m6A_frac'].median().reset_index()\n",
    "# split condition column with character \"-\" and keep only first column\n",
    "chromosome_m6A_frac['genotype'] = chromosome_m6A_frac['condition'].str.split('-').str[0]\n",
    "\n",
    "# sort by genotype, chr_type and condition_min\n",
    "chromosome_m6A_frac.sort_values(by=['genotype','chr_type','condition_min'], inplace=True)\n",
    "\n",
    "#Add column for increase in methylation from previous timepoint for each condition and each chr_type, where the first timepoint is 0\n",
    "chromosome_m6A_frac['m6A_frac_diff'] = chromosome_m6A_frac.groupby(['genotype','chr_type'])['m6A_frac'].diff()\n",
    "# Set all Nan values in m6A_frac_diff to 0\n",
    "chromosome_m6A_frac['m6A_frac_diff'].fillna(0, inplace=True)\n",
    "\n",
    "#reset index\n",
    "chromosome_m6A_frac.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# normalize m6A_frac_diff by the first m6A_frac value for each genotype and chr_type\n",
    "print(\"chromosome_m6A_frac.groupby(['genotype','chr_type'])['m6A_frac'].transform(lambda x: x/x.iloc[0]):\",chromosome_m6A_frac.groupby(['genotype','chr_type'])['m6A_frac'].transform(lambda x: x.iloc[0]))\n",
    "\n",
    "chromosome_m6A_frac['norm_m6A_frac_diff'] = chromosome_m6A_frac['m6A_frac_diff']/chromosome_m6A_frac.groupby(['genotype','chr_type'])['m6A_frac'].transform(lambda x: x.iloc[0])\n",
    "\n",
    "chromosome_m6A_frac['m6A_frac_diff_from_first'] = chromosome_m6A_frac['m6A_frac']-chromosome_m6A_frac.groupby(['genotype','chr_type'])['m6A_frac'].transform(lambda x: x.iloc[0])\n",
    "\n",
    "print(chromosome_m6A_frac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87806902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot average m6A/A across the chromosome for each condition in a time course\n",
    "# Set px background to white\n",
    "px.defaults.template = \"plotly_white\"\n",
    "\n",
    "# list of samples to consider\n",
    "considered_samples = [0]\n",
    "\n",
    "# Plot title\n",
    "#plot_title = \"AID::SDC-2 + Auxin; 2uM Hia5 Timecourse; m6A thresh = 75%\"\n",
    "plot_title = \"Mean m6A/A across entire chromosomes; m6A Threshold = \" + str(round(m6A_thresh/254*100-1)) + \"%\"\n",
    "\n",
    "# plot boxplot of norm_m6A_frac by chromosome\n",
    "fig = px.box(result_df, x=\"condition\", y=\"m6A_frac\", color=\"chromosome\", title=plot_title, points=\"all\")\n",
    "#Update background to white\n",
    "fig.update_layout(plot_bgcolor='white')\n",
    "fig.show()\n",
    "\n",
    "# plot boxplot of norm_m6A_frac by chromosome\n",
    "fig = px.box(result_df, x=\"condition\", y=\"m6A_frac\", color=\"chr_type\", title=plot_title, points=\"all\")\n",
    "fig.update_layout(plot_bgcolor='white')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485e9723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot average m6A/A across the chromosome for each condition in a time course\n",
    "\n",
    "\n",
    "# list of samples to consider\n",
    "considered_samples = [0,1,2]\n",
    "\n",
    "# Plot title\n",
    "#plot_title = \"AID::SDC-2 + Auxin; 2uM Hia5 Timecourse; m6A thresh = 75%\"\n",
    "plot_title = \"Mean m6A/A on X Chromosome 200kb Regions;<br>3min 2uM Hia5 treatment; m6A thresh = \" + str(m6A_thresh/254) + \"%\"\n",
    "\n",
    "# Plot the boxplot\n",
    "marker_colors =[\"#c45746\",\"#16415e\"]\n",
    "\n",
    "plotly_conditions = conditions\n",
    "#plotly_conditions = [\"N2<br>No-Met\",\"N2<br>3-min\",\"N2<br>10-min\",\"N2<br>30-min\", \"N2<br>120-min\",\n",
    "#\"#021+Aux<br>No-Met\",\"#021+Aux<br>3-min\",\"#021+Aux<br>10-min\",\"#021+Aux<br>30-min\", \"#021+Aux<br>120-min\"]\n",
    "\n",
    "fig = make_subplots(rows=1, cols=len(considered_samples),\n",
    "                y_title = \"m6A/A\",\n",
    "                shared_yaxes=True,\n",
    "                subplot_titles=(list( plotly_conditions[i] for i in considered_samples )))\n",
    "\n",
    "plot_iter=0\n",
    "print(\"weighted_combined_regions \",weighted_combined_regions)\n",
    "for i in considered_samples:\n",
    "    tube_df = weighted_combined_regions.loc[weighted_combined_regions['condition']==conditions[i]]\n",
    "    chr_type = \"Autosome\"\n",
    "    df_plot=tube_df.loc[tube_df['chr_type']==chr_type]\n",
    "    #df_plot=tube_df.sample(frac=17/100,replace=False,random_state=1)\n",
    "    trace0 = go.Box(x=df_plot['condition']+\" \", y=df_plot['m6A_frac'], #+ \" \" makes box plots not overlap\n",
    "                         name=chr_type, marker_color =marker_colors[1],)\n",
    "    chr_type = \"X\"\n",
    "    df_plot=tube_df.loc[tube_df['chr_type']==chr_type]\n",
    "    trace1 = go.Box(x=df_plot['condition'], y=df_plot['m6A_frac'],\n",
    "                         name=chr_type, marker_color=marker_colors[0])#, #add scatter points\n",
    "                            #boxpoints='all', jitter=0.4, pointpos=0) #jitter for SDC-2 degron and N2 only for 3min\n",
    "    plot_iter += 1\n",
    "    fig.append_trace(trace0, row = 1, col = plot_iter)\n",
    "    fig.append_trace(trace1, row = 1, col = plot_iter)\n",
    "\n",
    "# remove boxplot fill color\n",
    "fig.update_traces(fillcolor='rgba(0,0,0,0)')\n",
    "fig['layout'].update(height = 600,width = 1000)\n",
    "fig.update_layout(template=\"plotly_white\",title=plot_title)\n",
    "fig.update_xaxes(showticklabels=False)\n",
    "fig.update_annotations(font_size=12)\n",
    "fig.update_traces(marker=dict(size=3))\n",
    "'''fig = add_p_value_annotation(fig, [[0,1]], 1, _format=dict(interline=0.07, text_height=1.07, color='black'))\n",
    "fig = add_p_value_annotation(fig, [[0,1]], 2, _format=dict(interline=0.07, text_height=1.07, color='black'))\n",
    "fig = add_p_value_annotation(fig, [[0,1]], 3, _format=dict(interline=0.07, text_height=1.07, color='black'))\n",
    "fig = add_p_value_annotation(fig, [[0,1]], 4, _format=dict(interline=0.07, text_height=1.07, color='black'))\n",
    "fig = add_p_value_annotation(fig, [[0,1]], 5, _format=dict(interline=0.07, text_height=1.07, color='black'))'''\n",
    "#fig.update_layout(boxmode='group', xaxis_tickangle=0)\n",
    "\n",
    "for i in range(0,len([0,10])):\n",
    "    fig.layout.annotations[i].update(y=-0.1)\n",
    "fig.update_yaxes(tickformat=\"1%\")\n",
    "fig.show()\n",
    "#Export plotly figure to .svg\n",
    "fig.write_image(output_stem + \"combined_regions_\"  + str(m6A_thresh) +\".svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e958b3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the boxplot\n",
    "marker_colors =[\"#fde725\",\"#a0da39\",\"#4ac16d\"]#,\"#1fa187\",\"#277f8e\",\"#365c8d\",\"#46327e\",\"#440154\",\"#c45746\",\"#16415e\"]\n",
    "\n",
    "plotly_conditions = conditions\n",
    "\n",
    "fig = make_subplots(rows=1, cols=len(conditions),\n",
    "                y_title = \"Coverage\",\n",
    "                shared_yaxes=True,\n",
    "                subplot_titles=(plotly_conditions))\n",
    "\n",
    "print(\"Total MB aligned for ALL conditons: \",int(combined_regions['total_bases'].sum()/1000000),\n",
    "     \" | across \", int(combined_regions['overlapping_reads'].sum()),\" reads with avg. length of: \",\n",
    "     int(combined_regions['total_bases'].sum()/combined_regions['overlapping_reads'].sum()))\n",
    "for i in range(0,len(conditions)):\n",
    "    tube_df = combined_regions.loc[combined_regions['condition']==conditions[i]]\n",
    "    m6A_frac_tube = [tube_df['total_bases'].sum()/100000000*3.125] #3.125 is the scaling factor for adenosines in c elegans genome.\n",
    "    print(\"Total MB aligned for \",conditions[i],\n",
    "          \": \",int(tube_df['total_bases'].sum()/1000000), \n",
    "          \" | across \", int(tube_df['overlapping_reads'].sum()),\n",
    "          \" reads with avg. length of: \",\n",
    "          int(tube_df['total_bases'].sum()/tube_df['overlapping_reads'].sum()))\n",
    "    trace0 = go.Bar(x=tube_df['condition']+\" \", y=m6A_frac_tube,\n",
    "                         name=plotly_conditions[i], marker_color =marker_colors[i])\n",
    "\n",
    "    fig.append_trace(trace0, row = 1, col = i+1)\n",
    "    \n",
    "fig['layout'].update(height = 800)\n",
    "fig.update_layout(template=\"plotly_white\")\n",
    "fig.update_xaxes(showticklabels=False)\n",
    "#fig.update_yaxes(range=[0.7, 1.3])\n",
    "    \n",
    "#fig.update_layout(boxmode='group', xaxis_tickangle=0)\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
