{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7f7519699219a1c",
   "metadata": {},
   "source": [
    "# Notebook for compiling and analyzing fiber-seq peaks and chip-seq peaks together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0d861f4a602b9f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T17:51:11.916570Z",
     "start_time": "2025-08-29T17:51:11.625869Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'nanotools' from '/Data1/git/meyer-nanopore/scripts/analysis/nanotools.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool, cpu_count # used for parallel processing\n",
    "import subprocess\n",
    "import os\n",
    "import multiprocessing\n",
    "import tempfile\n",
    "from plotly.subplots import make_subplots\n",
    "from qnorm import quantile_normalize\n",
    "import numpy as np\n",
    "# import go library\n",
    "import plotly.graph_objects as go\n",
    "from pathlib import Path\n",
    "import nanotools\n",
    "importlib.reload(nanotools) # reload nanotools module\n",
    "# import numpy as np\n",
    "# import plotly.io as pio\n",
    "# import plotly\n",
    "# import plotly.express as px # Used for plotting\n",
    "# import plotly.graph_objects as go # Used for plotting\n",
    "# from plotly.subplots import make_subplots\n",
    "# import pywt # for wavelet transform\n",
    "# import matplotlib.pyplot as plt # Use for plotting m6A frac and coverage plot\n",
    "# from matplotlib import cm # Use for plotting m6A frac and coverage plot\n",
    "# from qnorm import quantile_normalize\n",
    "# #import tqdm\n",
    "# #import pysam\n",
    "# #import pyBigWig\n",
    "\n",
    "# import other standard libraries\n",
    "\n",
    "# print date and time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b00cb69369fddf0c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T17:51:12.439486Z",
     "start_time": "2025-08-29T17:51:12.120912Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program finished!\n",
      "new_bam_files:  ['/Data1/seq_data/AG2_51dpy21null_fiberseq_1_16_24/no_sample/20240116_2226_X1_FAX30165_d8fc11b9/basecalls/mod_mappings.sorted.bam', '/Data1/seq_data/BN_96DPY27Deg_Fiber_Hia5_MCviPI_05_24_24/no_sample/basecalls/mod_mappings.sorted.bam', '/Data1/seq_data/BM_N2_old_Fiber_Hia5_MCVIPI_05_30_24/no_sample/20240530_1930_X2_FAX32001_56dbbc37/basecalls/mod_mappings.sorted.bam', '/Data1/seq_data/BS_N2_old_Hia5_MCVIPI_10_17_2024/no_sample/20241018_1815_X2_FAX31996_a75b7b27/basecalls/mod_mappings.sorted.bam', '/Data1/seq_data/BT_96DPY27Deg_old_Hia5_MCviPI_10_17_2024/no_sample/20241018_1752_X1_FAX20286_da16b0d3/basecalls/mod_mappings.sorted.bam', '/Data1/seq_data/CA_107_SDC2_degron_OLD_20250213/no_sample_id/basecalls/mod_mappings.sorted.bam', '/Data1/seq_data/CE1_1-14_19-22_fiber_2025_04_09/basecalls/SQK-NBD114-24_barcode02.sorted.bam', '/Data1/seq_data/CE1_1-14_19-22_fiber_2025_04_09/basecalls/SQK-NBD114-24_barcode03.sorted.bam', '/Data1/seq_data/CE1_1-14_19-22_fiber_2025_04_09/basecalls/SQK-NBD114-24_barcode04.sorted.bam', '/Data1/seq_data/CE1_1-14_19-22_fiber_2025_04_09/basecalls/SQK-NBD114-24_barcode05.sorted.bam', '/Data1/seq_data/CE1_1-14_19-22_fiber_2025_04_09/basecalls/SQK-NBD114-24_barcode06.sorted.bam', '/Data1/seq_data/CE1_1-14_19-22_fiber_2025_04_09/basecalls/SQK-NBD114-24_barcode10.sorted.bam', '/Data1/seq_data/CE1_1-14_19-22_fiber_2025_04_09/basecalls/SQK-NBD114-24_barcode11.sorted.bam', '/Data1/seq_data/CH_1_2_8_9_13_14_fiber_rep_4_26_25/basecalls/SQK-NBD114-24_barcode02.sorted.bam', '/Data1/seq_data/CH_1_2_8_9_13_14_fiber_rep_4_26_25/basecalls/SQK-NBD114-24_barcode05.sorted.bam', '/Data1/seq_data/CH_1_2_8_9_13_14_fiber_rep_4_26_25/basecalls/SQK-NBD114-24_barcode06.sorted.bam', '/Data1/seq_data/CH_1_2_8_9_13_14_fiber_rep_4_26_25/basecalls/SQK-NBD114-24_barcode10.sorted.bam', '/Data1/seq_data/CH_1_2_8_9_13_14_fiber_rep_4_26_25/basecalls/SQK-NBD114-24_barcode11.sorted.bam', '/Data1/seq_data/20_5_SDC3deg_dpy21null_SRE_fiberseq_4_23/basecalls/SQK-RBK114-96_barcode08.sorted.bam', '/Data1/seq_data/CE1_6_N2young_N2mid_SDC3degron_8_12_25/no_sample_id/combined_basecalls/SQK-RBK114-96_barcode10.sorted.bam', '/Data1/seq_data/CE1_6_N2young_N2mid_SDC3degron_8_12_25/no_sample_id/combined_basecalls/SQK-RBK114-96_barcode11.sorted.bam', '/Data1/seq_data/CE1_6_N2young_N2mid_SDC3degron_8_12_25/no_sample_id/combined_basecalls/SQK-RBK114-96_barcode12.sorted.bam', '/Data1/seq_data/CE1_6_N2young_N2mid_SDC3degron_8_12_25/no_sample_id/combined_basecalls/SQK-RBK114-96_barcode13.sorted.bam', '/Data1/seq_data/CE1_6_N2young_N2mid_SDC3degron_8_12_25/no_sample_id/combined_basecalls/SQK-RBK114-96_barcode15.sorted.bam', '/Data1/seq_data/CE1_6_N2young_N2mid_SDC3degron_8_12_25/no_sample_id/combined_basecalls/SQK-RBK114-96_barcode16.sorted.bam', '/Data1/seq_data/CE1_6_N2young_N2mid_SDC3degron_8_12_25/no_sample_id/N2Y_SDC3_basecalls/SQK-RBK114-96_barcode18.sorted.bam', '/Data1/seq_data/CE1_6_N2young_N2mid_SDC3degron_8_12_25/no_sample_id/N2Y_SDC3_basecalls/SQK-RBK114-96_barcode20.sorted.bam', '/Data1/seq_data/CE1_6_N2young_N2mid_SDC3degron_8_12_25/no_sample_id/N2Y_SDC3_basecalls/SQK-RBK114-96_barcode26.sorted.bam', '/Data1/seq_data/CE1_6_N2young_N2mid_SDC3degron_8_12_25/no_sample_id/N2Y_SDC3_basecalls/SQK-RBK114-96_barcode27.sorted.bam']\n",
      "exp_ids:  ['AG-23_11_30_23', 'BN_05_24_24_SMACseq_R10_rep1', 'BM_05_30_24_SMACseq_R10_rep1', 'BS_10_17_24_SMACseq_R10_rep2', 'BT_10_17_24_SMACseq_R10_rep2', 'CA_02_15_25', 'N2_biorep2_fiber_old_R10_04_2025_B', 'DPY21null_biorep1_fiber_old_R10_04_2025', 'DPY21null_biorep2_fiber_old_R10_04_2025', 'DPY27deg_biorep1_fiber_old_R10_04_2025_A', 'DPY27deg_biorep2_fiber_old_R10_04_2025_A', 'SDC2deg_biorep1_fiber_old_R10_04_2025_B', 'SDC2deg_biorep2_fiber_old_R10_04_2025_B', 'N2_biorep2_fiber_old_R10_04_2025_D', 'DPY27deg_biorep1_fiber_old_R10_04_2025_B', 'DPY27deg_biorep2_fiber_old_R10_04_2025_B', 'SDC2deg_biorep1_fiber_old_R10_04_2025_C', 'SDC2deg_biorep2_fiber_old_R10_04_2025_C', 'DPY21null_biorep1_fiber_old_R10_04_2025', 'N2_biorep1_fiber_young_R10_08_2025', 'N2_biorep1_fiber_young_R10_08_2025', 'N2_biorep1_fiber_mid_R10_08_2025', 'N2_biorep1_fiber_mid_R10_08_2025', 'SDC3deg_biorep1_fiber_old_R10_08_2025', 'SDC3deg_biorep1_fiber_old_R10_08_2025', 'N2_biorep1_fiber_young_R10_08_2025', 'N2_biorep1_fiber_young_R10_08_2025', 'SDC3deg_biorep1_fiber_old_R10_08_2025', 'SDC3deg_biorep1_fiber_old_R10_08_2025']\n",
      "conditions:  ['DPY21null_comb_R10', 'DPY27deg_comb_R10', 'N2_old_R10', 'N2_old_R10', 'DPY27deg_comb_R10', 'SDC2deg_comb_R10', 'N2_old_R10', 'DPY21null_comb_R10', 'DPY21null_comb_R10', 'DPY27deg_comb_R10', 'DPY27deg_comb_R10', 'SDC2deg_comb_R10', 'SDC2deg_comb_R10', 'N2_old_R10', 'DPY27deg_comb_R10', 'DPY27deg_comb_R10', 'SDC2deg_comb_R10', 'SDC2deg_comb_R10', 'DPY21null_comb_R10', 'N2_young_R10', 'N2_young_R10', 'N2_mid_R10', 'N2_mid_R10', 'SDC3deg_old_R10', 'SDC3deg_old_R10', 'N2_young_R10', 'N2_young_R10', 'SDC3deg_old_R10', 'SDC3deg_old_R10']\n"
     ]
    }
   ],
   "source": [
    " ### BAM Configurations\n",
    "R9_m6A_thresh_percent = 0.7\n",
    "R10_m6A_thresh_percent = 0.8\n",
    "R10_5mC_thresh_percent = 0.8  # Note: 0.7 in R9 ~ 0.9 in R10\n",
    "R9_m6A_thresh = int(\n",
    "    round(R9_m6A_thresh_percent * 258, 0))  #default is 129 = 50%; 181=70%; 194=75%; 207 = 80%; 232 = 90%\n",
    "m6A_thresh = int(round(R10_m6A_thresh_percent * 258, 0))\n",
    "mC_thresh = int(round(R10_5mC_thresh_percent * 258, 0))\n",
    "\n",
    "# modkit is used for aggregating methylation data from .bam files\n",
    "# https://nanoporetech.github.io/modkit/quick_start.html\n",
    "modkit_path = \"/Data1/software/modkit_v0.3/modkit\"\n",
    "bedgraphtobigwig_path = \"/Data1/software/ucsc_genome_browser/bedGraphToBigWig\"\n",
    "danpos_path = \"/Data1/software/DANPOS3/danpos.py\"\n",
    "chrom_sizes = \"/Data1/reference/chrom.sizes.ce11.txt\"\n",
    "\n",
    "# analysis_cond = [\n",
    "#     \"N2_mixed_endogenous_R10\",\n",
    "#     \"N2_old_fiber_R10\",\n",
    "#     \"SDC2degron_old_R10\",\n",
    "#     \"DPY27_degron_old_R10\",\n",
    "#     \"DPY21null_old_fiber_R10\",\n",
    "# ]\n",
    "\n",
    "compare_type = \"COND\" # \"COND\" or \"BATCH\"\n",
    "\n",
    "analysis_cond = [\n",
    "    \"N2_old_R10\",\n",
    "    \"N2_young_R10\",\n",
    "    \"N2_mid_R10\",\n",
    "    \"SDC2deg_comb_R10\",\n",
    "    \"SDC3deg_old_R10\",\n",
    "    \"DPY27deg_comb_R10\",\n",
    "    \"DPY21null_comb_R10\",\n",
    "\n",
    "    #\"N2_rep1\",\n",
    "    #\"N2_rep2\",\n",
    "    #\"N2_rep3\",\n",
    "    #\"SDC2deg_rep1\",\n",
    "    #\"SDC2deg_rep2\",\n",
    "    #\"SDC2deg_rep3\",\n",
    "\n",
    "    # \"N2_young_SMACseq_R10\",\n",
    "    #\"rex1_MEXIICtoG_R10\",\n",
    "    #\"rex1_MEXIIscramble_R10\"\n",
    "    # \"rex1_MEXIIscramble_R10\",\n",
    "    # \"rex1_MEXIICtoG_R10\"\n",
    "    #\"DPY27deg_rep1\",\n",
    "    #\"DPY27deg_rep2\",\n",
    "    #\"DPY27deg_rep3\",\n",
    "    #\"rex1_MEXIIscramble_biorep0_fiber_old_R10_04_2025\",\n",
    "    # \"rex1_MEXIIscramble_biorep1_fiber_old_R10_04_2025\",\n",
    "    #\"rex1_MEXIIscramble_biorep2_fiber_old_R10_04_2025\",\n",
    "    #\"rex1_MEXIIscramble_biorep3_fiber_old_R10_04_2025\",\n",
    "    #\"rex1_4thCtoG_biorep0_fiber_old_R10_04_2025\",\n",
    "    # \"rex1_4thCtoG_biorep1_fiber_old_R10_04_2025\"]\n",
    "#     \"N2_rep1\",\n",
    "#     \"SDC2deg_rep1\",\n",
    "#     \"SDC2_degron_mid_T0_rep2_R10\",\n",
    "#     \"SDC2_degron_mid_T0_rep3_R10\",\n",
    "#     \"SDC2_degron_mid_T0p5_rep3_R10\",\n",
    "#     \"SDC2_degron_mid_T1_rep3_R10\",\n",
    "#     \"SDC2_degron_mid_T1p5_rep3_R10\",\n",
    "#     \"SDC2_degron_mid_T2_rep2_R10\",\n",
    "#     \"SDC2_degron_mid_T3_rep2_R10\",\n",
    "#     \"SDC2_degron_mid_T4_rep3_R10\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "# analysis_cond = [\n",
    "#     \"N2_mixed_endogenous_R10\",\n",
    "#     \"N2_old_R10\",\n",
    "#     \"SDC2deg_old_R10\",\n",
    "#     \"DPY27deg_old_R10\",\n",
    "#     \"DPY21null_old_R10\",\n",
    "# ]\n",
    "\n",
    "# analysis_cond = [\n",
    "#     #\"N2_mixed_endogenous_R10\",\n",
    "#     \"N2_fiber_old_R10_04_2025\",\n",
    "#     \"SDC2deg_fiber_old_R10_04_2025\",\n",
    "#     \"SDC3deg_fiber_old_R10_04_2025\",\n",
    "#     \"SDC2_3deg_fiber_old_R10_04_2025\",\n",
    "#     \"DPY27deg_fiber_old_R10_04_2025\",\n",
    "#     \"DPY21null_fiber_old_R10_04_2025\",\n",
    "#     \"96_DPY27degron_rep1_fiber_old_R10_052424\",\n",
    "# ]\n",
    "\n",
    "### IMPORT BAM FILES AND METADATA FROM CSV FILE\n",
    "if compare_type == \"COND\":\n",
    "    input_metadata = pd.read_csv(\"/Data1/git/meyer-nanopore/scripts/bam_input_metadata_8_18_2025_COND_estim.txt\", sep=\"\\t\", header=0)\n",
    "else:\n",
    "    input_metadata = pd.read_csv(\"/Data1/git/meyer-nanopore/scripts/bam_input_metadata_4_28_2025_BATCH.txt\", sep=\"\\t\", header=0)\n",
    "# Set bam_files equal to list of items in column bam_files where conditions == N2_fiber\n",
    "bam_files = input_metadata[input_metadata[\"conditions\"].isin(analysis_cond)][\"bam_files\"].tolist()\n",
    "ft_files = [x.replace(\".bam\", \"_ftools0p8.bed\") for x in bam_files]\n",
    "\n",
    "conditions = input_metadata[input_metadata[\"conditions\"].isin(analysis_cond)][\"conditions\"].tolist()\n",
    "exp_ids = input_metadata[input_metadata[\"conditions\"].isin(analysis_cond)][\"exp_id_date\"].tolist()\n",
    "flowcells = input_metadata[input_metadata[\"conditions\"].isin(analysis_cond)][\"flowcell\"].tolist()\n",
    "bam_fracs = len(bam_files) * [1]  # For full .bam set to = 1\n",
    "sample_indices = list(range(len(bam_files)))\n",
    "\n",
    "\n",
    "thresh_list = len(bam_files) * [m6A_thresh / 258]  # For R10 flow cells use 0.5; for R9 flow cells use 0.9\n",
    "# for position in flowcells == R9 set item with same index in thresh_list to R9_m6A_thresh/258\n",
    "for i in range(len(flowcells)):\n",
    "    if \"R9\" in flowcells[i]:\n",
    "        thresh_list[i] = R9_m6A_thresh / 258\n",
    "\n",
    "file_prefix = \"08192025\"\n",
    "output_stem = \"/Data1/git/meyer-nanopore/scripts/analysis/temp_files/08192025/\"\n",
    "\n",
    "# for backwards compatibility\n",
    "exp_ids = input_metadata[input_metadata[\"conditions\"].isin(analysis_cond)][\"exp_id_date\"].tolist()\n",
    "ext_exp_ids = [\"None\"] * len(exp_ids)\n",
    "\n",
    "# Subsample bam based on bam_frac, used to accelerate testing\n",
    "# if bam_frac = 1 will use original bam files, otherwise will save new subsampled bam files to output_stem.\n",
    "\n",
    "new_bam_files = []\n",
    "new_bam_files = nanotools.parallel_subsample_bam(bam_files, conditions, bam_fracs, sample_indices, output_stem)\n",
    "\n",
    "\n",
    "# args_list = [(bam_file, condition, bam_frac, sample_index, output_stem, exp_id) for\n",
    "#              bam_file, condition, bam_frac, sample_index, exp_id in\n",
    "#              zip(bam_files, conditions, bam_fracs, sample_indices, ext_exp_ids)]\n",
    "\n",
    "args_list = [(bam_file, condition, bam_frac, sample_index, output_stem,exp_id) for bam_file, condition, bam_frac, sample_index, exp_id in zip(ft_files,conditions,bam_fracs,sample_indices,exp_ids)]\n",
    "\n",
    "print(\"Program finished!\")\n",
    "print(\"new_bam_files: \", new_bam_files)\n",
    "print(\"exp_ids: \", exp_ids)\n",
    "print(\"conditions: \", conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a829149f5fa6782",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T17:51:13.016886Z",
     "start_time": "2025-08-29T17:51:12.556374Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering bed file...\n",
      "Configs: chr_type ['X', 'Autosome'] ['CHROMOSOME_X', 'CHROMOSOME_V'] ['X', 'Autosome'] ['univ_nuc'] ['+', '-'] 100 2000 2000\n",
      "Chromosome ends:             chromosome  start         end strand       type  chr-type\n",
      "2         CHROMOSOME_I    0.0  15072434.0      +  whole_chr  Autosome\n",
      "41396    CHROMOSOME_II    0.0  15279421.0      +  whole_chr  Autosome\n",
      "86685   CHROMOSOME_III    0.0  13783801.0      +  whole_chr  Autosome\n",
      "125206   CHROMOSOME_IV    0.0  17493829.0      +  whole_chr  Autosome\n",
      "169772    CHROMOSOME_V    0.0  20924180.0      +  whole_chr  Autosome\n",
      "227276    CHROMOSOME_X    0.0  17718942.0      +  whole_chr         X\n",
      "Saved the following bedfiles: ['/Data1/reference/X.bed.gz', '/Data1/reference/Autosome.bed.gz']\n",
      "Combined BED written to modkit_temp_ext.bed (400 rows)\n",
      "| modkit_bed_df_ext | first 5, random 5 and last 5 out of total 400 rows.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHROMOSOME_X</td>\n",
       "      <td>171175.0</td>\n",
       "      <td>175175.0</td>\n",
       "      <td>+</td>\n",
       "      <td>univ_nuc</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHROMOSOME_X</td>\n",
       "      <td>201725.0</td>\n",
       "      <td>205725.0</td>\n",
       "      <td>+</td>\n",
       "      <td>univ_nuc</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHROMOSOME_X</td>\n",
       "      <td>223135.0</td>\n",
       "      <td>227135.0</td>\n",
       "      <td>+</td>\n",
       "      <td>univ_nuc</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHROMOSOME_X</td>\n",
       "      <td>319075.0</td>\n",
       "      <td>323075.0</td>\n",
       "      <td>+</td>\n",
       "      <td>univ_nuc</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHROMOSOME_X</td>\n",
       "      <td>507375.0</td>\n",
       "      <td>511375.0</td>\n",
       "      <td>+</td>\n",
       "      <td>univ_nuc</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CHROMOSOME_V</td>\n",
       "      <td>20487375.0</td>\n",
       "      <td>20491375.0</td>\n",
       "      <td>+</td>\n",
       "      <td>univ_nuc</td>\n",
       "      <td>Autosome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CHROMOSOME_X</td>\n",
       "      <td>4525935.0</td>\n",
       "      <td>4529935.0</td>\n",
       "      <td>+</td>\n",
       "      <td>univ_nuc</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CHROMOSOME_V</td>\n",
       "      <td>5620515.0</td>\n",
       "      <td>5624515.0</td>\n",
       "      <td>+</td>\n",
       "      <td>univ_nuc</td>\n",
       "      <td>Autosome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CHROMOSOME_V</td>\n",
       "      <td>7054675.0</td>\n",
       "      <td>7058675.0</td>\n",
       "      <td>+</td>\n",
       "      <td>univ_nuc</td>\n",
       "      <td>Autosome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CHROMOSOME_X</td>\n",
       "      <td>11703295.0</td>\n",
       "      <td>11707295.0</td>\n",
       "      <td>+</td>\n",
       "      <td>univ_nuc</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CHROMOSOME_V</td>\n",
       "      <td>20211465.0</td>\n",
       "      <td>20215465.0</td>\n",
       "      <td>+</td>\n",
       "      <td>univ_nuc</td>\n",
       "      <td>Autosome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CHROMOSOME_V</td>\n",
       "      <td>20364025.0</td>\n",
       "      <td>20368025.0</td>\n",
       "      <td>+</td>\n",
       "      <td>univ_nuc</td>\n",
       "      <td>Autosome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>CHROMOSOME_V</td>\n",
       "      <td>20427115.0</td>\n",
       "      <td>20431115.0</td>\n",
       "      <td>+</td>\n",
       "      <td>univ_nuc</td>\n",
       "      <td>Autosome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>CHROMOSOME_V</td>\n",
       "      <td>20487375.0</td>\n",
       "      <td>20491375.0</td>\n",
       "      <td>+</td>\n",
       "      <td>univ_nuc</td>\n",
       "      <td>Autosome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>CHROMOSOME_V</td>\n",
       "      <td>20641625.0</td>\n",
       "      <td>20645625.0</td>\n",
       "      <td>+</td>\n",
       "      <td>univ_nuc</td>\n",
       "      <td>Autosome</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0           1           2  3         4         5\n",
       "0   CHROMOSOME_X    171175.0    175175.0  +  univ_nuc         X\n",
       "1   CHROMOSOME_X    201725.0    205725.0  +  univ_nuc         X\n",
       "2   CHROMOSOME_X    223135.0    227135.0  +  univ_nuc         X\n",
       "3   CHROMOSOME_X    319075.0    323075.0  +  univ_nuc         X\n",
       "4   CHROMOSOME_X    507375.0    511375.0  +  univ_nuc         X\n",
       "5   CHROMOSOME_V  20487375.0  20491375.0  +  univ_nuc  Autosome\n",
       "6   CHROMOSOME_X   4525935.0   4529935.0  +  univ_nuc         X\n",
       "7   CHROMOSOME_V   5620515.0   5624515.0  +  univ_nuc  Autosome\n",
       "8   CHROMOSOME_V   7054675.0   7058675.0  +  univ_nuc  Autosome\n",
       "9   CHROMOSOME_X  11703295.0  11707295.0  +  univ_nuc         X\n",
       "10  CHROMOSOME_V  20211465.0  20215465.0  +  univ_nuc  Autosome\n",
       "11  CHROMOSOME_V  20364025.0  20368025.0  +  univ_nuc  Autosome\n",
       "12  CHROMOSOME_V  20427115.0  20431115.0  +  univ_nuc  Autosome\n",
       "13  CHROMOSOME_V  20487375.0  20491375.0  +  univ_nuc  Autosome\n",
       "14  CHROMOSOME_V  20641625.0  20645625.0  +  univ_nuc  Autosome"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program finished!\n",
      "exp_ids:  ['AG-23_11_30_23', 'BN_05_24_24_SMACseq_R10_rep1', 'BM_05_30_24_SMACseq_R10_rep1', 'BS_10_17_24_SMACseq_R10_rep2', 'BT_10_17_24_SMACseq_R10_rep2', 'CA_02_15_25', 'N2_biorep2_fiber_old_R10_04_2025_B', 'DPY21null_biorep1_fiber_old_R10_04_2025', 'DPY21null_biorep2_fiber_old_R10_04_2025', 'DPY27deg_biorep1_fiber_old_R10_04_2025_A', 'DPY27deg_biorep2_fiber_old_R10_04_2025_A', 'SDC2deg_biorep1_fiber_old_R10_04_2025_B', 'SDC2deg_biorep2_fiber_old_R10_04_2025_B', 'N2_biorep2_fiber_old_R10_04_2025_D', 'DPY27deg_biorep1_fiber_old_R10_04_2025_B', 'DPY27deg_biorep2_fiber_old_R10_04_2025_B', 'SDC2deg_biorep1_fiber_old_R10_04_2025_C', 'SDC2deg_biorep2_fiber_old_R10_04_2025_C', 'DPY21null_biorep1_fiber_old_R10_04_2025', 'N2_biorep1_fiber_young_R10_08_2025', 'N2_biorep1_fiber_young_R10_08_2025', 'N2_biorep1_fiber_mid_R10_08_2025', 'N2_biorep1_fiber_mid_R10_08_2025', 'SDC3deg_biorep1_fiber_old_R10_08_2025', 'SDC3deg_biorep1_fiber_old_R10_08_2025', 'N2_biorep1_fiber_young_R10_08_2025', 'N2_biorep1_fiber_young_R10_08_2025', 'SDC3deg_biorep1_fiber_old_R10_08_2025', 'SDC3deg_biorep1_fiber_old_R10_08_2025']\n",
      "| combined_bed_df_ext | first 5, random 5 and last 5 out of total 200 rows.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chrom</th>\n",
       "      <th>bed_start</th>\n",
       "      <th>bed_end</th>\n",
       "      <th>bed_strand</th>\n",
       "      <th>type</th>\n",
       "      <th>chr_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHROMOSOME_V</td>\n",
       "      <td>91145.0</td>\n",
       "      <td>95145.0</td>\n",
       "      <td>+</td>\n",
       "      <td>univ_nuc</td>\n",
       "      <td>Autosome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHROMOSOME_V</td>\n",
       "      <td>456325.0</td>\n",
       "      <td>460325.0</td>\n",
       "      <td>+</td>\n",
       "      <td>univ_nuc</td>\n",
       "      <td>Autosome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHROMOSOME_V</td>\n",
       "      <td>795635.0</td>\n",
       "      <td>799635.0</td>\n",
       "      <td>+</td>\n",
       "      <td>univ_nuc</td>\n",
       "      <td>Autosome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHROMOSOME_V</td>\n",
       "      <td>931065.0</td>\n",
       "      <td>935065.0</td>\n",
       "      <td>+</td>\n",
       "      <td>univ_nuc</td>\n",
       "      <td>Autosome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHROMOSOME_V</td>\n",
       "      <td>969215.0</td>\n",
       "      <td>973215.0</td>\n",
       "      <td>+</td>\n",
       "      <td>univ_nuc</td>\n",
       "      <td>Autosome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CHROMOSOME_V</td>\n",
       "      <td>12519195.0</td>\n",
       "      <td>12523195.0</td>\n",
       "      <td>+</td>\n",
       "      <td>univ_nuc</td>\n",
       "      <td>Autosome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CHROMOSOME_V</td>\n",
       "      <td>7693645.0</td>\n",
       "      <td>7697645.0</td>\n",
       "      <td>+</td>\n",
       "      <td>univ_nuc</td>\n",
       "      <td>Autosome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CHROMOSOME_V</td>\n",
       "      <td>6160815.0</td>\n",
       "      <td>6164815.0</td>\n",
       "      <td>+</td>\n",
       "      <td>univ_nuc</td>\n",
       "      <td>Autosome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CHROMOSOME_X</td>\n",
       "      <td>223135.0</td>\n",
       "      <td>227135.0</td>\n",
       "      <td>+</td>\n",
       "      <td>univ_nuc</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CHROMOSOME_X</td>\n",
       "      <td>14208735.0</td>\n",
       "      <td>14212735.0</td>\n",
       "      <td>+</td>\n",
       "      <td>univ_nuc</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CHROMOSOME_X</td>\n",
       "      <td>16591995.0</td>\n",
       "      <td>16595995.0</td>\n",
       "      <td>+</td>\n",
       "      <td>univ_nuc</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CHROMOSOME_X</td>\n",
       "      <td>16617085.0</td>\n",
       "      <td>16621085.0</td>\n",
       "      <td>+</td>\n",
       "      <td>univ_nuc</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>CHROMOSOME_X</td>\n",
       "      <td>17353365.0</td>\n",
       "      <td>17357365.0</td>\n",
       "      <td>+</td>\n",
       "      <td>univ_nuc</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>CHROMOSOME_X</td>\n",
       "      <td>17447325.0</td>\n",
       "      <td>17451325.0</td>\n",
       "      <td>+</td>\n",
       "      <td>univ_nuc</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>CHROMOSOME_X</td>\n",
       "      <td>17619235.0</td>\n",
       "      <td>17623235.0</td>\n",
       "      <td>+</td>\n",
       "      <td>univ_nuc</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           chrom   bed_start     bed_end bed_strand      type  chr_type\n",
       "0   CHROMOSOME_V     91145.0     95145.0          +  univ_nuc  Autosome\n",
       "1   CHROMOSOME_V    456325.0    460325.0          +  univ_nuc  Autosome\n",
       "2   CHROMOSOME_V    795635.0    799635.0          +  univ_nuc  Autosome\n",
       "3   CHROMOSOME_V    931065.0    935065.0          +  univ_nuc  Autosome\n",
       "4   CHROMOSOME_V    969215.0    973215.0          +  univ_nuc  Autosome\n",
       "5   CHROMOSOME_V  12519195.0  12523195.0          +  univ_nuc  Autosome\n",
       "6   CHROMOSOME_V   7693645.0   7697645.0          +  univ_nuc  Autosome\n",
       "7   CHROMOSOME_V   6160815.0   6164815.0          +  univ_nuc  Autosome\n",
       "8   CHROMOSOME_X    223135.0    227135.0          +  univ_nuc         X\n",
       "9   CHROMOSOME_X  14208735.0  14212735.0          +  univ_nuc         X\n",
       "10  CHROMOSOME_X  16591995.0  16595995.0          +  univ_nuc         X\n",
       "11  CHROMOSOME_X  16617085.0  16621085.0          +  univ_nuc         X\n",
       "12  CHROMOSOME_X  17353365.0  17357365.0          +  univ_nuc         X\n",
       "13  CHROMOSOME_X  17447325.0  17451325.0          +  univ_nuc         X\n",
       "14  CHROMOSOME_X  17619235.0  17623235.0          +  univ_nuc         X"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count by type:  univ_nuc    200\n",
      "Name: type, dtype: int64\n",
      "Unique types in combined_bed_df_ext:  ['univ_nuc']\n"
     ]
    }
   ],
   "source": [
    "### Bed file configurations:\n",
    "sample_source = \"chr_type\" # \"chr_type\" or \"type\" or \"chromosome\"\n",
    "chr_type_selected = [\"X\", \"Autosome\"]\n",
    "# type_selected = \"rex1\" all the way through \"rex48\"\n",
    "\n",
    "#type_selected = [\"univ_nuc\",\"rex48\",\"rex35\",\"rex40\",\"rex34\",\"rex33\",\"rex8\",\"rex43\",\"rex45\",\"rex23\",\"rex32\",\"rex16\",\"rex41\",\"rex14\",\"rex2\",\"Prex7\",\"Prex30\",\"rex47\"] #\"dnase_nfr_intergenic\",\n",
    "\n",
    "#\n",
    "\n",
    "type_selected = [\"univ_nuc\"]#,'nDCC1', 'nDCC10', 'nDCC11', 'nDCC12', 'nDCC13', 'nDCC14', 'nDCC15', 'nDCC16', 'nDCC17', 'nDCC18', 'nDCC19', 'nDCC2', 'nDCC20', 'nDCC21', 'nDCC22', 'nDCC23', 'nDCC3', 'nDCC4', 'nDCC5', 'nDCC6', 'nDCC7', 'nDCC8', 'nDCC9', 'Prex1', 'Prex11', 'Prex14', 'Prex15', 'Prex16', 'Prex2', 'Prex20', 'Prex21', 'Prex22', 'Prex23', 'Prex25', 'Prex26', 'Prex27', 'Prex3', 'Prex30', 'Prex31', 'Prex6', 'Prex7', 'rex1', 'rex14', 'rex16', 'rex17', 'rex18', 'rex19', 'rex2', 'rex20', 'rex21', 'rex23', 'rex24', 'rex25', 'rex26', 'rex27', 'rex28', 'rex29', 'rex3', 'rex31', 'rex32', 'rex33', 'rex34', 'rex35', 'rex36', 'rex37', 'rex38', 'rex39', 'rex4', 'rex40', 'rex41', 'rex42', 'rex43', 'rex44', 'rex45', 'rex46', 'rex47', 'rex48', 'rex5', 'rex6', 'rex7', 'rex8']\n",
    "\n",
    "#\"LMN1_only_damID\",\"EMR1_only_damID\",\"her-1_FULL\",\"sex-1_FULL\"\n",
    "#,\"DPY27_chip_q2\",\"DPY27_chip_q3\",\"DPY27_chip_q4\",\"intergenic_control\"\n",
    "#,\"her-1_TSS\",\"fem-1_TSS\",\"fem-2_TSS\",\"fem-3_TSS\",\"sex-1_TSS\"\n",
    "# her-1_TSS/TES/FULL | TES_q1-4 | #TSS_q1-4 | strong/weak rex | whole_chr | 200kb_region | 50kb_region | center_DPY27_chip_albretton | gene_q1-q4 | MEX_motif | center_SDC2_chip_albretton | center_SDC3_chip_albretton |\n",
    "# ATAC_seq_EXCL_dpy27_ol100 | ATAC_seq;DPY27_ol100\n",
    "# \"motif_weak_dcc_1\",\"motif_weak_dcc_2\",\"motif_weak_dcc_3\",\"motif_weak_dcc_4\",\"motif_weak_dcc_5\",\"motif_weak_dcc_6\",\"motif_weak_dcc_7\",\"motif_weak_dcc_8\",\"motif_weak_dcc_9\",\"motif_weak_dcc_10\"\n",
    "\n",
    "max_regions = 100 # max regions to consider; 0 = full set;\n",
    "chromosome_selected = [\"CHROMOSOME_X\",\"CHROMOSOME_V\"]#,\"CHROMOSOME_I\", \"CHROMOSOME_II\", \"CHROMOSOME_III\", \"CHROMOSOME_IV\"]\n",
    "strand_selected = [\"+\",\"-\"] #+ and/or -\n",
    "select_opp_strand = True #If you want to select both + and - strands for all regions set to True\n",
    "down_sample_autosome = False # If you want to downsample autosome genes to match number of X genes set to True\n",
    "if chr_type_selected == [\"X\"]:\n",
    "    down_sample_autosome = False\n",
    "bed_file = \"/Data1/reference/tss_tes_rex_combined_v30_WS235.bed\"\n",
    "bed_window = 2000   # +/- around bed elements.\n",
    "intergenic_window = 2000 # +/- around intergenic regions\n",
    "num_bins = 1000 #bins for metagene plot\n",
    "mods = \"a\" # {A,CG,A+CG}\n",
    "if sample_source == \"chr_type\":\n",
    "    selection = chr_type_selected\n",
    "if sample_source == \"type\":\n",
    "    selection = type_selected\n",
    "if sample_source == \"chromosome\":\n",
    "    selection = chromosome_selected\n",
    "\n",
    "# Filter input bed_file based on input parameters (e.g. chromosome, type, strand, etc.)\n",
    "# Function saves a new filtered bed file to the same folder as the original bed file\n",
    "# called temp_do_not_use_\"type\".bed\n",
    "importlib.reload(nanotools)\n",
    "new_bed_files=nanotools.filter_bed_file(\n",
    "    bed_file,\n",
    "    sample_source,\n",
    "    selection,\n",
    "    chromosome_selected,\n",
    "    chr_type_selected,\n",
    "    type_selected,\n",
    "    strand_selected,\n",
    "    max_regions,\n",
    "    bed_window,\n",
    "    intergenic_window\n",
    ")\n",
    "\n",
    "modkit_bed_name_ext = \"modkit_temp_ext.bed\"\n",
    "modkit_bed_df_ext = nanotools.generate_modkit_bed(new_bed_files, down_sample_autosome, select_opp_strand,modkit_bed_name_ext)\n",
    "nanotools.display_sample_rows(modkit_bed_df_ext, 5)\n",
    "\n",
    "print(\"Program finished!\")\n",
    "print(\"exp_ids: \", exp_ids)\n",
    "\n",
    "combined_bed_df_ext = nanotools.create_lookup_bed(new_bed_files)\n",
    "\n",
    "nanotools.display_sample_rows(combined_bed_df_ext, 5)\n",
    "\n",
    "# print count by type\n",
    "print(\"Count by type: \", combined_bed_df_ext[\"type\"].value_counts())\n",
    "\n",
    "\n",
    "# for intergneic control for background methylation by condition plots\n",
    "bed_path = Path(output_stem) / \"intergenic_control.bed\"\n",
    "# # create output stem if it doesn't exist\n",
    "output_dir = Path(output_stem)\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ────────────────────  BUILD include-bed FROM DataFrame  ───────────────────\n",
    "\n",
    "if not bed_path.exists():\n",
    "    if 'combined_bed_df_ext' not in globals():\n",
    "        abort(\"combined_bed_df_ext DataFrame not found in workspace.\")\n",
    "    bed_df = (\n",
    "        combined_bed_df_ext\n",
    "        .loc[combined_bed_df_ext[\"type\"] == \"intergenic_control\",\n",
    "             [\"chrom\", \"bed_start\", \"bed_end\"]]\n",
    "        .copy()\n",
    "    )\n",
    "    bed_df[\"bed_start\"] = bed_df[\"bed_start\"].astype(int)\n",
    "    bed_df[\"bed_end\"]   = bed_df[\"bed_end\"].astype(int)\n",
    "    bed_df.to_csv(bed_path, sep=\"\\t\", header=False, index=False)\n",
    "\n",
    "# print all unique types\n",
    "print(\"Unique types in combined_bed_df_ext: \", combined_bed_df_ext[\"type\"].unique().tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fb41a3a8f57f25a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T17:51:13.659068Z",
     "start_time": "2025-08-29T17:51:13.326861Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[IN] add_motif_types: ['MEX_motif', 'MEXII_motif']\n",
      "[IN] overwrite_existing_motif_types: True\n",
      "[IN] apply_bed_window_expansion: True\n",
      "[IN] bed_window: 2000\n",
      "[IN] combined_bed_df_ext rows: 200\n",
      "[IN] new_bed_files: ['/Data1/reference/X.bed.gz', '/Data1/reference/Autosome.bed.gz']\n",
      "      /Data1/reference/X.bed.gz → mtime=Fri Aug 29 17:51:12 2025 size=901\n",
      "      /Data1/reference/Autosome.bed.gz → mtime=Fri Aug 29 17:51:12 2025 size=926\n",
      "[IN] modkit_bed_name_ext: modkit_temp_ext.bed\n",
      "[IN] modkit_bed_name: (missing)\n",
      "     existing modkit_bed_name_ext: mtime=Fri Aug 29 17:51:12 2025 size=20064\n",
      "[IN] existing motif rows (memory):\n",
      "(none)\n",
      "[BEFORE] type_selected len=1 sample_source=chr_type\n",
      "         types: ['univ_nuc']\n",
      "[UPDATED] appended to type_selected: ['MEX_motif', 'MEXII_motif']\n",
      "[STEP] Load FIMO for MEX_motif: /Data1/ext_data/motifs/fimo_MEX_0.01.tsv  (mtime=Wed Sep 25 22:13:21 2024 size=6353377)  ln(p)≤-14\n",
      "[DBG] FIMO raw: fimo_MEX_0.01.tsv\n",
      "      shape=(72919, 10)\n",
      "motif_id          motif_alt_id sequence_name      start       stop strand   score      p-value  q-value matched_sequence\n",
      "     MEX Cel_MEX_C_elegans_MEX          chrX  2997121.0  2997132.0      - 24.0000 5.790000e-09    0.308     TCGCGCAGGGAG\n",
      "     MEX Cel_MEX_C_elegans_MEX          chrX  2997296.0  2997307.0      - 24.0000 5.790000e-09    0.308     TCGCGCAGGGAG\n",
      "     MEX Cel_MEX_C_elegans_MEX          chrX 11094123.0 11094134.0      - 24.0000 5.790000e-09    0.308     TCGCGCAGGGAG\n",
      "     MEX Cel_MEX_C_elegans_MEX          chrX   954909.0   954920.0      - 22.9310 1.160000e-08    0.308     ACGCGCAGGGAG\n",
      "     MEX Cel_MEX_C_elegans_MEX          chrX  8811103.0  8811114.0      - 22.9310 1.160000e-08    0.308     ACGCGCAGGGAG\n",
      "     MEX Cel_MEX_C_elegans_MEX          chrX  2997070.0  2997081.0      - 22.8391 2.210000e-08    0.308     TCGCGAAGGGAG\n",
      "     MEX Cel_MEX_C_elegans_MEX          chrV  3041109.0  3041120.0      - 22.8391 2.210000e-08    0.308     TCGCGAAGGGAG\n",
      "     MEX Cel_MEX_C_elegans_MEX          chrX  4209290.0  4209301.0      + 22.8391 2.210000e-08    0.308     TCGCGAAGGGAG\n",
      "[INFO] fimo_MEX_0.01.tsv rows: before=72919 after_ln(p)=212\n",
      "[DBG] FIMO filtered→combined columns\n",
      "      shape=(212, 4)\n",
      "       chrom  bed_start    bed_end strand\n",
      "CHROMOSOME_X  2997121.0  2997132.0      -\n",
      "CHROMOSOME_X  2997296.0  2997307.0      -\n",
      "CHROMOSOME_X 11094123.0 11094134.0      -\n",
      "CHROMOSOME_X   954909.0   954920.0      -\n",
      "CHROMOSOME_X  8811103.0  8811114.0      -\n",
      "CHROMOSOME_X  2997070.0  2997081.0      -\n",
      "CHROMOSOME_V  3041109.0  3041120.0      -\n",
      "CHROMOSOME_X  4209290.0  4209301.0      +\n",
      "[DBG] MEX_motif candidates (pre-merge)\n",
      "      shape=(212, 6)\n",
      "       chrom  bed_start  bed_end bed_strand      type chr_type\n",
      "CHROMOSOME_X    2995121  2999132          - MEX_motif        X\n",
      "CHROMOSOME_X    2995296  2999307          - MEX_motif        X\n",
      "CHROMOSOME_X   11092123 11096134          - MEX_motif        X\n",
      "CHROMOSOME_X     952909   956920          - MEX_motif        X\n",
      "CHROMOSOME_X    8809103  8813114          - MEX_motif        X\n",
      "CHROMOSOME_X    2995070  2999081          - MEX_motif        X\n",
      "CHROMOSOME_V    3039109  3043120          - MEX_motif Autosome\n",
      "CHROMOSOME_X    4207290  4211301          + MEX_motif        X\n",
      "[STEP] Load FIMO for MEXII_motif: /Data1/ext_data/motifs/fimo_MEXII_0.01.tsv  (mtime=Wed Sep 25 22:13:28 2024 size=8149064)  ln(p)≤-14\n",
      "[DBG] FIMO raw: fimo_MEXII_0.01.tsv\n",
      "      shape=(64871, 10)\n",
      "motif_id                                motif_alt_id sequence_name      start       stop strand   score      p-value  q-value           matched_sequence\n",
      "   MEXII Cel_MEXII_wills_MEX-II_motif_12.19_N13_s2-9          chrX  1323115.0  1323140.0      + 31.0526 1.610000e-11  0.00318 CCCCTGTCCAATCTCTTTTCTCCACA\n",
      "   MEXII Cel_MEXII_wills_MEX-II_motif_12.19_N13_s2-9          chrX  4395666.0  4395691.0      - 29.7158 6.930000e-11  0.00686 CCCCTGCCCAATTACCCGCAGCCACA\n",
      "   MEXII Cel_MEXII_wills_MEX-II_motif_12.19_N13_s2-9          chrX  2997031.0  2997056.0      + 28.0526 3.390000e-10  0.02180 CCCCTTGTCCAAATATTTTATCCACG\n",
      "   MEXII Cel_MEXII_wills_MEX-II_motif_12.19_N13_s2-9          chrX  1345019.0  1345044.0      + 27.7579 4.400000e-10  0.02180 TTCTTGGCCATATACATTTATCCACA\n",
      "   MEXII Cel_MEXII_wills_MEX-II_motif_12.19_N13_s2-9          chrX 14813520.0 14813545.0      + 27.4105 5.920000e-10  0.02350 CACGTGCCCAAATAATGTTCTCCACA\n",
      "   MEXII Cel_MEXII_wills_MEX-II_motif_12.19_N13_s2-9          chrX 14813563.0 14813588.0      - 27.1263 7.500000e-10  0.02480 AGCTTGCACCAATTACGTTAGCCACG\n",
      "   MEXII Cel_MEXII_wills_MEX-II_motif_12.19_N13_s2-9          chrX 11094085.0 11094110.0      + 25.4842 2.670000e-09  0.07330 ACCCCCGCTCAATTATTTTATCCACC\n",
      "   MEXII Cel_MEXII_wills_MEX-II_motif_12.19_N13_s2-9          chrX 10668091.0 10668116.0      - 25.1053 3.510000e-09  0.07330 CCCTTCTACTATTACAATTGTCCACA\n",
      "[INFO] fimo_MEXII_0.01.tsv rows: before=64871 after_ln(p)=291\n",
      "[DBG] FIMO filtered→combined columns\n",
      "      shape=(291, 4)\n",
      "       chrom  bed_start    bed_end strand\n",
      "CHROMOSOME_X  1323115.0  1323140.0      +\n",
      "CHROMOSOME_X  4395666.0  4395691.0      -\n",
      "CHROMOSOME_X  2997031.0  2997056.0      +\n",
      "CHROMOSOME_X  1345019.0  1345044.0      +\n",
      "CHROMOSOME_X 14813520.0 14813545.0      +\n",
      "CHROMOSOME_X 14813563.0 14813588.0      -\n",
      "CHROMOSOME_X 11094085.0 11094110.0      +\n",
      "CHROMOSOME_X 10668091.0 10668116.0      -\n",
      "[DBG] MEXII_motif candidates (pre-merge)\n",
      "      shape=(291, 6)\n",
      "       chrom  bed_start  bed_end bed_strand        type chr_type\n",
      "CHROMOSOME_X    1321115  1325140          + MEXII_motif        X\n",
      "CHROMOSOME_X    4393666  4397691          - MEXII_motif        X\n",
      "CHROMOSOME_X    2995031  2999056          + MEXII_motif        X\n",
      "CHROMOSOME_X    1343019  1347044          + MEXII_motif        X\n",
      "CHROMOSOME_X   14811520 14815545          + MEXII_motif        X\n",
      "CHROMOSOME_X   14811563 14815588          - MEXII_motif        X\n",
      "CHROMOSOME_X   11092085 11096110          + MEXII_motif        X\n",
      "CHROMOSOME_X   10666091 10670116          - MEXII_motif        X\n",
      "[MEM] dropping existing motif rows: 0\n",
      "[DBG] motif_unique (memory-add)\n",
      "      shape=(503, 6)\n",
      "       chrom  bed_start  bed_end bed_strand      type chr_type\n",
      "CHROMOSOME_X    2995121  2999132          - MEX_motif        X\n",
      "CHROMOSOME_X    2995296  2999307          - MEX_motif        X\n",
      "CHROMOSOME_X   11092123 11096134          - MEX_motif        X\n",
      "CHROMOSOME_X     952909   956920          - MEX_motif        X\n",
      "CHROMOSOME_X    8809103  8813114          - MEX_motif        X\n",
      "CHROMOSOME_X    2995070  2999081          - MEX_motif        X\n",
      "CHROMOSOME_V    3039109  3043120          - MEX_motif Autosome\n",
      "CHROMOSOME_X    4207290  4211301          + MEX_motif        X\n",
      "[MEM] combined_bed_df_ext rows(after)=703  Δ=503\n",
      "[MEM] added MEX_motif: 212\n",
      "[MEM] added MEXII_motif: 291\n",
      "[DISK] BEFORE /Data1/reference/X.bed.gz: rows=100  mtime=Fri Aug 29 17:51:12 2025 size=901\n",
      "       motif MEX_motif: 0\n",
      "       motif MEXII_motif: 0\n",
      "[DBG] uniq rows destined for /Data1/reference/X.bed.gz\n",
      "      shape=(135, 6)\n",
      "  chromosome    start      end strand      type chr-type\n",
      "CHROMOSOME_X  2995121  2999132      - MEX_motif        X\n",
      "CHROMOSOME_X  2995296  2999307      - MEX_motif        X\n",
      "CHROMOSOME_X 11092123 11096134      - MEX_motif        X\n",
      "CHROMOSOME_X   952909   956920      - MEX_motif        X\n",
      "CHROMOSOME_X  8809103  8813114      - MEX_motif        X\n",
      "CHROMOSOME_X  2995070  2999081      - MEX_motif        X\n",
      "CHROMOSOME_X  4207290  4211301      + MEX_motif        X\n",
      "CHROMOSOME_X 12361627 12365638      + MEX_motif        X\n",
      "[DISK] AFTER  /Data1/reference/X.bed.gz: rows=235  mtime=Fri Aug 29 17:51:13 2025 size=2079\n",
      "       motif MEX_motif: 68\n",
      "       motif MEXII_motif: 67\n",
      "[DISK] BEFORE /Data1/reference/Autosome.bed.gz: rows=100  mtime=Fri Aug 29 17:51:12 2025 size=926\n",
      "       motif MEX_motif: 0\n",
      "       motif MEXII_motif: 0\n",
      "[DBG] uniq rows destined for /Data1/reference/Autosome.bed.gz\n",
      "      shape=(368, 6)\n",
      "    chromosome    start      end strand      type chr-type\n",
      "  CHROMOSOME_V  3039109  3043120      - MEX_motif Autosome\n",
      "CHROMOSOME_III 10441968 10445979      - MEX_motif Autosome\n",
      "CHROMOSOME_III 10451863 10455874      + MEX_motif Autosome\n",
      " CHROMOSOME_IV  2084865  2088876      + MEX_motif Autosome\n",
      " CHROMOSOME_II  4676145  4680156      - MEX_motif Autosome\n",
      " CHROMOSOME_II 13972939 13976950      - MEX_motif Autosome\n",
      "  CHROMOSOME_V  1142458  1146469      - MEX_motif Autosome\n",
      "CHROMOSOME_III 13538663 13542674      - MEX_motif Autosome\n",
      "[DISK] AFTER  /Data1/reference/Autosome.bed.gz: rows=468  mtime=Fri Aug 29 17:51:13 2025 size=4308\n",
      "       motif MEX_motif: 144\n",
      "       motif MEXII_motif: 224\n",
      "[DISK] added MEX_motif: +212 rows\n",
      "[DISK] added MEXII_motif: +291 rows\n",
      "[SYNC] Reloaded combined_bed_df_ext from disk. rows=703\n",
      "[SYNC] motif counts in combined_bed_df_ext (from disk):\n",
      "MEXII_motif    291\n",
      "MEX_motif      212\n",
      "[SYNC] combined_bed_df set from combined_bed_df_ext. rows=703\n",
      "[OUT] modkit_bed_df_ext rows=1406\n",
      "[OUT] modkit_bed_df     rows=1406\n",
      "[OUT] wrote modkit_bed_df_ext → /Data1/git/meyer-nanopore/scripts/analysis/temp_files/08192025/modkit_temp_ext.bed\n",
      "      modkit_bed_df_ext file: mtime=Fri Aug 29 17:51:13 2025 size=50604\n",
      "[OUT] wrote modkit_bed_df → /Data1/git/meyer-nanopore/scripts/analysis/temp_files/08192025/modkit_temp.bed\n",
      "      modkit_bed_df file:     mtime=Fri Aug 29 17:51:13 2025 size=50604\n",
      "\n",
      "[CONFIRM] Updated objects and files:\n",
      "  • combined_bed_df_ext  → 703 rows\n",
      "  • combined_bed_df      → 703 rows\n",
      "  • modkit_bed_df_ext    → 1406 rows  file: /Data1/git/meyer-nanopore/scripts/analysis/temp_files/08192025/modkit_temp_ext.bed\n",
      "  • modkit_bed_df        → 1406 rows  file: /Data1/git/meyer-nanopore/scripts/analysis/temp_files/08192025/modkit_temp.bed\n",
      "  • filtered bed: /Data1/reference/X.bed.gz → mtime=Fri Aug 29 17:51:13 2025 size=2079\n",
      "  • filtered bed: /Data1/reference/Autosome.bed.gz → mtime=Fri Aug 29 17:51:13 2025 size=4308\n",
      "modkit_bed_df.head()\n",
      "              0       1       2  3  4  5\n",
      "0  CHROMOSOME_I   72826   76851  .  .  +\n",
      "1  CHROMOSOME_I  163454  167479  .  .  +\n",
      "2  CHROMOSOME_I  229110  233135  .  .  +\n",
      "3  CHROMOSOME_I  284923  288948  .  .  +\n",
      "4  CHROMOSOME_I  366846  370871  .  .  +\n",
      "combined_bed_df_ext.head()\n",
      "          chrom  bed_start  bed_end bed_strand         type  chr_type\n",
      "0  CHROMOSOME_I      72826    76851          -  MEXII_motif  Autosome\n",
      "1  CHROMOSOME_I     163454   167479          +  MEXII_motif  Autosome\n",
      "2  CHROMOSOME_I     229110   233135          -  MEXII_motif  Autosome\n",
      "3  CHROMOSOME_I     284923   288948          -  MEXII_motif  Autosome\n",
      "4  CHROMOSOME_I     366846   370871          +  MEXII_motif  Autosome\n"
     ]
    }
   ],
   "source": [
    "# ─────────────── Cell 2: full parity motif injection + exhaustive debug ───────────────\n",
    "# Effects:\n",
    "#   • Build motif rows from FIMO TSVs using LN(P) thresholds.\n",
    "#   • Add both strands.\n",
    "#   • Update in-memory: combined_bed_df_ext, combined_bed_df, modkit_bed_df_ext, modkit_bed_df.\n",
    "#   • Overwrite filtered BEDs on disk (entries in `new_bed_files`) without calling filter_bed_file.\n",
    "#   • Persist modkit *_ext and core BEDs to existing filenames.\n",
    "#   • DOES NOT touch `modkit_path` (the modkit executable).\n",
    "\n",
    "# ── Config ─────────────────────────────────────────────────────────────────────────────\n",
    "add_motif_types = [\"MEX_motif\",\"MEXII_motif\"]          # any subset of: \"MEX_motif\", \"MEXII_motif\", \"motifC\"; or None to skip\n",
    "overwrite_existing_motif_types = True   # drop any existing rows of these motif types before insert\n",
    "apply_bed_window_expansion = True       # expand motif intervals by ±bed_window prior to merge/write\n",
    "DEBUG = True\n",
    "SHOW = 8\n",
    "\n",
    "import os, subprocess, importlib, time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Expected from Cell 1:\n",
    "#   combined_bed_df_ext, new_bed_files, bed_window, output_stem (optional),\n",
    "#   save_modkit_bed_to_temp (optional), create_modkit_bed_df (optional)\n",
    "#   modkit_bed_name_ext (filename for *_ext), modkit_bed_name (filename for core)\n",
    "#   nanotools with create_lookup_bed()\n",
    "\n",
    "MOTIF_TSV = {\n",
    "    \"MEX_motif\":   \"/Data1/ext_data/motifs/fimo_MEX_0.01.tsv\",\n",
    "    \"MEXII_motif\": \"/Data1/ext_data/motifs/fimo_MEXII_0.01.tsv\",\n",
    "    \"motifC\":      \"/Data1/ext_data/motifs/fimo_motifc_0.01.tsv\",\n",
    "}\n",
    "LN_P_THRESH = {\"MEX_motif\": -14, \"MEXII_motif\": -14, \"motifC\": -9}\n",
    "\n",
    "# On-disk filtered BED schema (what new_bed_files contain)\n",
    "REQ = [\"chromosome\", \"start\", \"end\", \"strand\", \"type\", \"chr-type\"]\n",
    "\n",
    "# ── Debug helpers ──────────────────────────────────────────────────────────────────────\n",
    "def _dbg(msg, df=None):\n",
    "    if not DEBUG: return\n",
    "    print(f\"[DBG] {msg}\")\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        print(f\"      shape={df.shape}\")\n",
    "        print(df.head(SHOW).to_string(index=False))\n",
    "\n",
    "def _file_stat(p):\n",
    "    try:\n",
    "        st = Path(p).stat()\n",
    "        return f\"mtime={time.ctime(st.st_mtime)} size={st.st_size}\"\n",
    "    except Exception as e:\n",
    "        return f\"(stat err: {e})\"\n",
    "\n",
    "def _infer_chr_type(chrom):\n",
    "    return \"X\" if chrom == \"CHROMOSOME_X\" else \"Autosome\"\n",
    "\n",
    "# ── I/O helpers ────────────────────────────────────────────────────────────────────────\n",
    "def _load_and_filter_fimo(tsv_path, ln_p_thresh):\n",
    "    if not os.path.exists(tsv_path):\n",
    "        print(f\"[WARN] FIMO not found: {tsv_path}\")\n",
    "        return pd.DataFrame(columns=[\"chrom\",\"bed_start\",\"bed_end\",\"strand\"])\n",
    "    raw = pd.read_csv(tsv_path, sep=\"\\t\", comment=\"#\", skip_blank_lines=True)\n",
    "    _dbg(f\"FIMO raw: {os.path.basename(tsv_path)}\", raw)\n",
    "    need = [\"sequence_name\",\"start\",\"stop\",\"p-value\",\"strand\"]\n",
    "    miss = [c for c in need if c not in raw.columns]\n",
    "    if miss:\n",
    "        print(f\"[ERR] Missing columns in {tsv_path}: {miss}\")\n",
    "        return pd.DataFrame(columns=[\"chrom\",\"bed_start\",\"bed_end\",\"strand\"])\n",
    "\n",
    "    raw[\"ln_p\"] = np.log(raw[\"p-value\"])\n",
    "    before = len(raw)\n",
    "    raw = raw[raw[\"ln_p\"] <= ln_p_thresh].copy()\n",
    "    print(f\"[INFO] {os.path.basename(tsv_path)} rows: before={before} after_ln(p)={len(raw)}\")\n",
    "\n",
    "    raw[\"chrom\"]     = raw[\"sequence_name\"].astype(str).str.replace(\"chr\",\"CHROMOSOME_\", regex=False)\n",
    "    raw[\"bed_start\"] = pd.to_numeric(raw[\"start\"], errors=\"coerce\")\n",
    "    raw[\"bed_end\"]   = pd.to_numeric(raw[\"stop\"],  errors=\"coerce\")\n",
    "    raw[\"strand\"]    = raw[\"strand\"].astype(str).str.strip().where(raw[\"strand\"].isin([\"+\",\"-\"]), \".\")\n",
    "    raw = raw.dropna(subset=[\"chrom\",\"bed_start\",\"bed_end\"])\n",
    "    out = raw[[\"chrom\",\"bed_start\",\"bed_end\",\"strand\"]].copy()\n",
    "    _dbg(\"FIMO filtered→combined columns\", out)\n",
    "    return out\n",
    "\n",
    "def _expand_windows(df, w):\n",
    "    if not apply_bed_window_expansion or w is None: return df\n",
    "    out = df.copy()\n",
    "    w = int(w)\n",
    "    out[\"bed_start\"] = (out[\"bed_start\"] - w).astype(np.int64)\n",
    "    out[\"bed_end\"]   = (out[\"bed_end\"]   + w).astype(np.int64)\n",
    "    out[\"bed_start\"] = out[\"bed_start\"].clip(lower=0)\n",
    "    return out\n",
    "\n",
    "def _combined_to_filtered_schema(df):\n",
    "    \"\"\"Map combined_bed_df_ext rows → filtered BED schema (REQ).\"\"\"\n",
    "    out = df.rename(columns={\n",
    "        \"chrom\":\"chromosome\", \"bed_start\":\"start\", \"bed_end\":\"end\",\n",
    "        \"bed_strand\":\"strand\", \"chr_type\":\"chr-type\"\n",
    "    }).copy()\n",
    "    if \"chr-type\" not in out.columns:\n",
    "        out[\"chr-type\"] = out[\"chromosome\"].map(_infer_chr_type)\n",
    "    out[\"start\"] = pd.to_numeric(out[\"start\"], errors=\"coerce\").astype(np.int64)\n",
    "    out[\"end\"]   = pd.to_numeric(out[\"end\"],   errors=\"coerce\").astype(np.int64)\n",
    "    for c in (\"chromosome\",\"strand\",\"type\",\"chr-type\"):\n",
    "        if c in out: out[c] = out[c].astype(str)\n",
    "    return out[REQ].copy()\n",
    "\n",
    "def _read_filtered_bed(gz_path):\n",
    "    df = pd.read_csv(gz_path, sep=\"\\t\", header=None, names=REQ)\n",
    "    df[\"start\"] = pd.to_numeric(df[\"start\"], errors=\"coerce\").astype(np.int64)\n",
    "    df[\"end\"]   = pd.to_numeric(df[\"end\"],   errors=\"coerce\").astype(np.int64)\n",
    "    for c in (\"chromosome\",\"strand\",\"type\",\"chr-type\"):\n",
    "        df[c] = df[c].astype(str)\n",
    "    return df\n",
    "\n",
    "def _write_bgzip_tabix(df, gz_path):\n",
    "    gz_path = str(gz_path)\n",
    "    bed_path = gz_path[:-3] if gz_path.endswith(\".gz\") else gz_path + \".bed\"\n",
    "    df.to_csv(bed_path, sep=\"\\t\", header=False, index=False)\n",
    "    with open(gz_path, \"wb\") as out_fh:\n",
    "        subprocess.run([\"bgzip\",\"-c\",bed_path], stdout=out_fh, stderr=subprocess.PIPE, check=False)\n",
    "    subprocess.run([\"tabix\",\"-f\",\"-p\",\"bed\",gz_path], stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=False)\n",
    "\n",
    "def _fallback_create_modkit_bed_df(filtered_df):\n",
    "    base = filtered_df[[\"chrom\",\"bed_start\",\"bed_end\"]].copy()\n",
    "    base[\"bed_start\"] = pd.to_numeric(base[\"bed_start\"], errors=\"coerce\").astype(np.int64)\n",
    "    base[\"bed_end\"]   = pd.to_numeric(base[\"bed_end\"],   errors=\"coerce\").astype(np.int64)\n",
    "    df_plus  = pd.DataFrame({\n",
    "        0: base[\"chrom\"].values, 1: base[\"bed_start\"].values, 2: base[\"bed_end\"].values,\n",
    "        3: np.full(len(base), \".\", dtype=object),\n",
    "        4: np.full(len(base), \".\", dtype=object),\n",
    "        5: np.full(len(base), \"+\", dtype=object),\n",
    "    })\n",
    "    df_minus = pd.DataFrame({\n",
    "        0: base[\"chrom\"].values, 1: base[\"bed_start\"].values, 2: base[\"bed_end\"].values,\n",
    "        3: np.full(len(base), \".\", dtype=object),\n",
    "        4: np.full(len(base), \".\", dtype=object),\n",
    "        5: np.full(len(base), \"-\", dtype=object),\n",
    "    })\n",
    "    mod = pd.concat([df_plus, df_minus], ignore_index=True)\n",
    "    mod.columns = [0,1,2,3,4,5]\n",
    "    mod.drop_duplicates(inplace=True)\n",
    "    return mod\n",
    "\n",
    "# ── Input snapshot ─────────────────────────────────────────────────────────────────────\n",
    "if \"combined_bed_df_ext\" not in globals():\n",
    "    raise NameError(\"combined_bed_df_ext not found from Cell 1\")\n",
    "print(\"[IN] add_motif_types:\", add_motif_types)\n",
    "print(\"[IN] overwrite_existing_motif_types:\", overwrite_existing_motif_types)\n",
    "print(\"[IN] apply_bed_window_expansion:\", apply_bed_window_expansion)\n",
    "print(\"[IN] bed_window:\", globals().get(\"bed_window\",\"(missing)\"))\n",
    "print(\"[IN] combined_bed_df_ext rows:\", len(combined_bed_df_ext))\n",
    "print(\"[IN] new_bed_files:\", new_bed_files if \"new_bed_files\" in globals() else \"(missing)\")\n",
    "if \"new_bed_files\" in globals():\n",
    "    for p in new_bed_files:\n",
    "        print(\"     \", p, \"→\", _file_stat(p))\n",
    "print(\"[IN] modkit_bed_name_ext:\", globals().get(\"modkit_bed_name_ext\",\"(missing)\"))\n",
    "print(\"[IN] modkit_bed_name:\", globals().get(\"modkit_bed_name\",\"(missing)\"))\n",
    "if isinstance(globals().get(\"modkit_bed_name_ext\",None), str) and Path(globals()[\"modkit_bed_name_ext\"]).exists():\n",
    "    print(\"     existing modkit_bed_name_ext:\", _file_stat(globals()[\"modkit_bed_name_ext\"]))\n",
    "if isinstance(globals().get(\"modkit_bed_name\",None), str) and Path(globals()[\"modkit_bed_name\"]).exists():\n",
    "    print(\"     existing modkit_bed_name:    \", _file_stat(globals()[\"modkit_bed_name\"]))\n",
    "\n",
    "# Pre counts by type for motifs (in-memory)\n",
    "pre_counts = combined_bed_df_ext[combined_bed_df_ext[\"type\"].isin(add_motif_types)][\"type\"].value_counts()\n",
    "print(\"[IN] existing motif rows (memory):\")\n",
    "print(pre_counts.to_string() if not pre_counts.empty else \"(none)\")\n",
    "\n",
    "# ── Build candidate motif rows in combined schema ──────────────────────────────────────\n",
    "if add_motif_types is None:\n",
    "    print(\"add_motif_types=None → skipping.\")\n",
    "else:\n",
    "    # ensure type_selected exists\n",
    "    if 'type_selected' not in globals():\n",
    "        type_selected = []\n",
    "        print(\"[INFO] created missing type_selected list.\")\n",
    "\n",
    "    # snapshot\n",
    "    before_len = len(type_selected)\n",
    "    before_sample = selection if 'selection' in globals() else None\n",
    "    print(f\"[BEFORE] type_selected len={before_len} sample_source={globals().get('sample_source','(missing)')}\")\n",
    "    if before_len <= 20:\n",
    "        print(\"         types:\", type_selected)\n",
    "\n",
    "    # add motifs if missing\n",
    "    added = []\n",
    "    for t in add_motif_types:\n",
    "        if t not in type_selected:\n",
    "            type_selected.append(t)\n",
    "            added.append(t)\n",
    "    if added:\n",
    "        print(f\"[UPDATED] appended to type_selected: {added}\")\n",
    "    else:\n",
    "        print(\"[NOCHANGE] type_selected already contained motif types.\")\n",
    "\n",
    "    motif_blocks = []\n",
    "    for key in add_motif_types:\n",
    "        if key not in MOTIF_TSV:\n",
    "            print(f\"[WARN] Unknown motif key: {key} → skip\")\n",
    "            continue\n",
    "        src = MOTIF_TSV[key]\n",
    "        print(f\"[STEP] Load FIMO for {key}: {src}  ({_file_stat(src) if os.path.exists(src) else 'missing'})  ln(p)≤{LN_P_THRESH[key]}\")\n",
    "        core = _load_and_filter_fimo(src, LN_P_THRESH[key])\n",
    "        if core.empty:\n",
    "            print(f\"[INFO] {key}: 0 rows after filtering\")\n",
    "            continue\n",
    "        core = _expand_windows(core, bed_window if \"bed_window\" in globals() else None)\n",
    "        cand = core.rename(columns={\"strand\":\"bed_strand\"}).copy()  # keep original motif strand only\n",
    "        cand[\"type\"] = key\n",
    "\n",
    "        if \"chr_type\" in combined_bed_df_ext.columns:\n",
    "            cand[\"chr_type\"] = cand[\"chrom\"].map(_infer_chr_type)\n",
    "        _dbg(f\"{key} candidates (pre-merge)\", cand)\n",
    "        motif_blocks.append(cand)\n",
    "\n",
    "    if not motif_blocks:\n",
    "        print(\"[WRITE] No motif rows to add (after LN(P)/window).\")\n",
    "    else:\n",
    "        motif_combined = pd.concat(motif_blocks, ignore_index=True)\n",
    "\n",
    "        # ── In-memory update ───────────────────────────────────────────────────────────\n",
    "        base_rows = len(combined_bed_df_ext)\n",
    "        if overwrite_existing_motif_types:\n",
    "            drop_n = int((combined_bed_df_ext[\"type\"].isin(add_motif_types)).sum())\n",
    "            print(f\"[MEM] dropping existing motif rows: {drop_n}\")\n",
    "            combined_bed_df_ext = combined_bed_df_ext[~combined_bed_df_ext[\"type\"].isin(add_motif_types)].reset_index(drop=True)\n",
    "\n",
    "        # Anti-join with dtype harmony\n",
    "        join_cols = [\"chrom\",\"bed_start\",\"bed_end\",\"bed_strand\",\"type\"] + ([\"chr_type\"] if \"chr_type\" in combined_bed_df_ext.columns else [])\n",
    "        existing_keys = combined_bed_df_ext[join_cols].copy()\n",
    "        for c in (\"bed_start\",\"bed_end\"):\n",
    "            if c in existing_keys.columns: existing_keys[c] = pd.to_numeric(existing_keys[c], errors=\"coerce\").astype(\"float64\")\n",
    "        for c in (\"chrom\",\"bed_strand\",\"type\",\"chr_type\"):\n",
    "            if c in existing_keys.columns: existing_keys[c] = existing_keys[c].astype(str)\n",
    "\n",
    "        mleft = motif_combined[join_cols].copy()\n",
    "        mleft[\"__rid__\"] = np.arange(len(mleft))\n",
    "        for c in (\"bed_start\",\"bed_end\"):\n",
    "            mleft[c] = pd.to_numeric(mleft[c], errors=\"coerce\").astype(\"float64\")\n",
    "        for c in (\"chrom\",\"bed_strand\",\"type\",\"chr_type\"):\n",
    "            if c in mleft.columns: mleft[c] = mleft[c].astype(str)\n",
    "\n",
    "        merged = mleft.merge(existing_keys, on=join_cols, how=\"left\", indicator=True)\n",
    "        keep_ids = merged.loc[merged[\"_merge\"]==\"left_only\",\"__rid__\"].tolist()\n",
    "        motif_unique = motif_combined.iloc[keep_ids].copy()\n",
    "        _dbg(\"motif_unique (memory-add)\", motif_unique)\n",
    "\n",
    "        if len(motif_unique):\n",
    "            combined_bed_df_ext = pd.concat([combined_bed_df_ext, motif_unique], ignore_index=True)\n",
    "            combined_bed_df_ext.drop_duplicates(subset=[\"chrom\",\"bed_start\",\"bed_end\",\"bed_strand\",\"type\"], inplace=True)\n",
    "            combined_bed_df_ext.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # Report memory adds\n",
    "        mem_delta = len(combined_bed_df_ext) - base_rows\n",
    "        add_counts_mem = motif_unique[\"type\"].value_counts() if len(motif_unique) else pd.Series(dtype=int)\n",
    "        print(f\"[MEM] combined_bed_df_ext rows(after)={len(combined_bed_df_ext)}  Δ={mem_delta}\")\n",
    "        for t in add_motif_types:\n",
    "            print(f\"[MEM] added {t}: {int(add_counts_mem.get(t, 0))}\")\n",
    "\n",
    "        # ── Disk parity: update each filtered BED in new_bed_files ─────────────────────\n",
    "        if \"new_bed_files\" not in globals() or not new_bed_files:\n",
    "            bed_dir = str(Path(globals().get(\"bed_file\",\"/Data1/reference\")).parent)\n",
    "            new_bed_files = [f\"{bed_dir}/X.bed.gz\", f\"{bed_dir}/Autosome.bed.gz\"]\n",
    "            print(f\"[WARN] new_bed_files not found; defaulting to: {new_bed_files}\")\n",
    "\n",
    "        # Prepare motif rows in filtered schema for disk write\n",
    "        motif_filtered = _combined_to_filtered_schema(\n",
    "            motif_unique.rename(columns={\"chr_type\":\"chr-type\"}) if \"chr_type\" in motif_unique.columns else motif_unique\n",
    "        )\n",
    "\n",
    "        disk_add_counts = {t:0 for t in add_motif_types}\n",
    "        for gz in new_bed_files:\n",
    "            if not os.path.exists(gz):\n",
    "                print(f\"[DISK] missing, skip: {gz}\")\n",
    "                continue\n",
    "            before_stat = _file_stat(gz)\n",
    "            existing = _read_filtered_bed(gz)\n",
    "            # counts before\n",
    "            print(f\"[DISK] BEFORE {gz}: rows={len(existing)}  {before_stat}\")\n",
    "            for t in add_motif_types:\n",
    "                c = int((existing[\"type\"]==t).sum())\n",
    "                print(f\"       motif {t}: {c}\")\n",
    "\n",
    "            # choose chr-type bucket for this file\n",
    "            guess = \"X\" if \"X\" in Path(gz).stem else (\"Autosome\" if \"Autosome\" in Path(gz).stem else (existing[\"chr-type\"].mode().iat[0] if not existing.empty else \"Autosome\"))\n",
    "            to_add = motif_filtered[motif_filtered[\"chr-type\"] == guess].copy()\n",
    "            if to_add.empty:\n",
    "                print(f\"[DISK] {gz}: +0 (no rows for chr-type={guess})\")\n",
    "                continue\n",
    "            # anti-join to avoid re-adding\n",
    "            m = to_add.merge(existing, on=REQ, how=\"left\", indicator=True)\n",
    "            uniq = m[m[\"_merge\"]==\"left_only\"][REQ].copy()\n",
    "            _dbg(f\"uniq rows destined for {gz}\", uniq)\n",
    "            if uniq.empty:\n",
    "                print(f\"[DISK] {gz}: +0 (all present)\")\n",
    "                continue\n",
    "\n",
    "            added_per_type = uniq[\"type\"].value_counts()\n",
    "            for t,c in added_per_type.items():\n",
    "                disk_add_counts[t] = disk_add_counts.get(t,0) + int(c)\n",
    "\n",
    "            updated = pd.concat([existing, uniq], ignore_index=True)\n",
    "            updated.sort_values([\"chromosome\",\"start\",\"end\",\"type\",\"strand\"], inplace=True, kind=\"mergesort\")\n",
    "            _write_bgzip_tabix(updated, gz)\n",
    "            after_stat = _file_stat(gz)\n",
    "            print(f\"[DISK] AFTER  {gz}: rows={len(updated)}  {after_stat}\")\n",
    "            for t in add_motif_types:\n",
    "                c = int((updated[\"type\"]==t).sum())\n",
    "                print(f\"       motif {t}: {c}\")\n",
    "\n",
    "        # Per-type totals written to disk\n",
    "        for t in add_motif_types:\n",
    "            print(f\"[DISK] added {t}: +{int(disk_add_counts.get(t,0))} rows\")\n",
    "\n",
    "        # ── Reload combined_bed_df_ext from disk to guarantee parity ───────────────────\n",
    "        try:\n",
    "            importlib.reload(nanotools)\n",
    "            combined_bed_df_ext = nanotools.create_lookup_bed(new_bed_files)\n",
    "            print(f\"[SYNC] Reloaded combined_bed_df_ext from disk. rows={len(combined_bed_df_ext)}\")\n",
    "            post_counts = combined_bed_df_ext[combined_bed_df_ext[\"type\"].isin(add_motif_types)][\"type\"].value_counts()\n",
    "            print(\"[SYNC] motif counts in combined_bed_df_ext (from disk):\")\n",
    "            print(post_counts.to_string() if not post_counts.empty else \"(none)\")\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Could not rebuild combined_bed_df_ext from disk: {e}\\nKeeping in-memory version.\")\n",
    "\n",
    "        # ── Mirror to combined_bed_df so downstream users of that name see same rows ───\n",
    "        combined_bed_df = combined_bed_df_ext.copy()\n",
    "        print(f\"[SYNC] combined_bed_df set from combined_bed_df_ext. rows={len(combined_bed_df)}\")\n",
    "\n",
    "        # ── Rebuild modkit_bed_df_ext and modkit_bed_df, then persist ─────────────────\n",
    "        if \"create_modkit_bed_df\" in globals():\n",
    "            modkit_bed_df_ext = create_modkit_bed_df(combined_bed_df_ext)\n",
    "            modkit_bed_df     = create_modkit_bed_df(combined_bed_df)\n",
    "        else:\n",
    "            modkit_bed_df_ext = _fallback_create_modkit_bed_df(combined_bed_df_ext)\n",
    "            modkit_bed_df     = _fallback_create_modkit_bed_df(combined_bed_df)\n",
    "\n",
    "        print(f\"[OUT] modkit_bed_df_ext rows={len(modkit_bed_df_ext)}\")\n",
    "        print(f\"[OUT] modkit_bed_df     rows={len(modkit_bed_df)}\")\n",
    "\n",
    "        # Save *_ext\n",
    "        if \"modkit_bed_name_ext\" not in globals() or not isinstance(modkit_bed_name_ext, str):\n",
    "            modkit_bed_name_ext = \"modkit_temp_ext.bed\"\n",
    "        if \"save_modkit_bed_to_temp\" in globals():\n",
    "            path_ext = save_modkit_bed_to_temp(modkit_bed_df_ext, modkit_bed_name_ext)\n",
    "        else:\n",
    "            out_dir = Path(globals().get(\"output_stem\",\".\")) if globals().get(\"output_stem\") else Path(\".\")\n",
    "            out_dir.mkdir(parents=True, exist_ok=True)\n",
    "            path_ext = str(out_dir / modkit_bed_name_ext)\n",
    "            modkit_bed_df_ext.to_csv(path_ext, sep=\"\\t\", header=False, index=False)\n",
    "            print(f\"[OUT] wrote modkit_bed_df_ext → {path_ext}\")\n",
    "        print(\"      modkit_bed_df_ext file:\", _file_stat(path_ext))\n",
    "\n",
    "        # Save core\n",
    "        if \"modkit_bed_name\" not in globals() or not isinstance(modkit_bed_name, str):\n",
    "            modkit_bed_name = \"modkit_temp.bed\"\n",
    "        if \"save_modkit_bed_to_temp\" in globals():\n",
    "            path_core = save_modkit_bed_to_temp(modkit_bed_df, modkit_bed_name)\n",
    "        else:\n",
    "            out_dir = Path(globals().get(\"output_stem\",\".\")) if globals().get(\"output_stem\") else Path(\".\")\n",
    "            out_dir.mkdir(parents=True, exist_ok=True)\n",
    "            path_core = str(out_dir / modkit_bed_name)\n",
    "            modkit_bed_df.to_csv(path_core, sep=\"\\t\", header=False, index=False)\n",
    "            print(f\"[OUT] wrote modkit_bed_df → {path_core}\")\n",
    "        print(\"      modkit_bed_df file:    \", _file_stat(path_core))\n",
    "\n",
    "        # ── Final confirmation summary ────────────────────────────────────────────────\n",
    "        print(\"\\n[CONFIRM] Updated objects and files:\")\n",
    "        print(\"  • combined_bed_df_ext  →\", len(combined_bed_df_ext), \"rows\")\n",
    "        print(\"  • combined_bed_df      →\", len(combined_bed_df), \"rows\")\n",
    "        print(\"  • modkit_bed_df_ext    →\", len(modkit_bed_df_ext), \"rows  file:\", path_ext)\n",
    "        print(\"  • modkit_bed_df        →\", len(modkit_bed_df), \"rows  file:\", path_core)\n",
    "        for p in new_bed_files:\n",
    "            print(\"  • filtered bed:\", p, \"→\", _file_stat(p))\n",
    "\n",
    "# print sample rows\n",
    "print(\"modkit_bed_df.head()\")\n",
    "print(modkit_bed_df.head())\n",
    "print(\"combined_bed_df_ext.head()\")\n",
    "print(combined_bed_df_ext.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f83da661a367750",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T17:51:15.171360Z",
     "start_time": "2025-08-29T17:51:14.773240Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:51:14 [  DEBUG] 00_mod_mappings.sorted: using cached TSV 00_mod_mappings.sorted_probabilities.tsv\n",
      "17:51:14 [  DEBUG] 02_mod_mappings.sorted: using cached TSV 02_mod_mappings.sorted_probabilities.tsv\n",
      "17:51:14 [  DEBUG] 01_mod_mappings.sorted: using cached TSV 01_mod_mappings.sorted_probabilities.tsv\n",
      "17:51:14 [  DEBUG] 03_mod_mappings.sorted: using cached TSV 03_mod_mappings.sorted_probabilities.tsv\n",
      "17:51:14 [  DEBUG] 04_mod_mappings.sorted: using cached TSV 04_mod_mappings.sorted_probabilities.tsv\n",
      "17:51:14 [  DEBUG] 08_SQK-NBD114-24_barcode04.sorted: using cached TSV 08_SQK-NBD114-24_barcode04.sorted_probabilities.tsv\n",
      "17:51:14 [  DEBUG] 07_SQK-NBD114-24_barcode03.sorted: using cached TSV 07_SQK-NBD114-24_barcode03.sorted_probabilities.tsv\n",
      "17:51:14 [  DEBUG] 06_SQK-NBD114-24_barcode02.sorted: using cached TSV 06_SQK-NBD114-24_barcode02.sorted_probabilities.tsv\n",
      "17:51:14 [  DEBUG] 05_mod_mappings.sorted: using cached TSV 05_mod_mappings.sorted_probabilities.tsv\n",
      "17:51:14 [  DEBUG] 09_SQK-NBD114-24_barcode05.sorted: using cached TSV 09_SQK-NBD114-24_barcode05.sorted_probabilities.tsv\n",
      "17:51:14 [  DEBUG] 10_SQK-NBD114-24_barcode06.sorted: using cached TSV 10_SQK-NBD114-24_barcode06.sorted_probabilities.tsv\n",
      "17:51:14 [  DEBUG] 11_SQK-NBD114-24_barcode10.sorted: using cached TSV 11_SQK-NBD114-24_barcode10.sorted_probabilities.tsv\n",
      "17:51:14 [  DEBUG] 12_SQK-NBD114-24_barcode11.sorted: using cached TSV 12_SQK-NBD114-24_barcode11.sorted_probabilities.tsv\n",
      "17:51:14 [  DEBUG] 13_SQK-NBD114-24_barcode02.sorted: using cached TSV 13_SQK-NBD114-24_barcode02.sorted_probabilities.tsv\n",
      "17:51:14 [  DEBUG] 14_SQK-NBD114-24_barcode05.sorted: using cached TSV 14_SQK-NBD114-24_barcode05.sorted_probabilities.tsv\n",
      "17:51:14 [  DEBUG] 15_SQK-NBD114-24_barcode06.sorted: using cached TSV 15_SQK-NBD114-24_barcode06.sorted_probabilities.tsv\n",
      "17:51:14 [  DEBUG] 16_SQK-NBD114-24_barcode10.sorted: using cached TSV 16_SQK-NBD114-24_barcode10.sorted_probabilities.tsv\n",
      "17:51:14 [  DEBUG] 19_SQK-RBK114-96_barcode10.sorted: using cached TSV 19_SQK-RBK114-96_barcode10.sorted_probabilities.tsv\n",
      "17:51:14 [  DEBUG] 18_SQK-RBK114-96_barcode08.sorted: using cached TSV 18_SQK-RBK114-96_barcode08.sorted_probabilities.tsv\n",
      "sample-probs:   0%|                                                                                                 | 0/29 [00:00<?, ?it/s]17:51:14 [  DEBUG] 17_SQK-NBD114-24_barcode11.sorted: using cached TSV 17_SQK-NBD114-24_barcode11.sorted_probabilities.tsv\n",
      "17:51:14 [  DEBUG] 20_SQK-RBK114-96_barcode11.sorted: using cached TSV 20_SQK-RBK114-96_barcode11.sorted_probabilities.tsv\n",
      "sample-probs: 100%|████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:00<00:00, 202724.69it/s]17:51:14 [  DEBUG] 21_SQK-RBK114-96_barcode12.sorted: using cached TSV 21_SQK-RBK114-96_barcode12.sorted_probabilities.tsv\n",
      "17:51:14 [  DEBUG] 22_SQK-RBK114-96_barcode13.sorted: using cached TSV 22_SQK-RBK114-96_barcode13.sorted_probabilities.tsv\n",
      "17:51:14 [  DEBUG] 23_SQK-RBK114-96_barcode15.sorted: using cached TSV 23_SQK-RBK114-96_barcode15.sorted_probabilities.tsv\n",
      "17:51:14 [  DEBUG] 25_SQK-RBK114-96_barcode18.sorted: using cached TSV 25_SQK-RBK114-96_barcode18.sorted_probabilities.tsv\n",
      "17:51:14 [  DEBUG] 24_SQK-RBK114-96_barcode16.sorted: using cached TSV 24_SQK-RBK114-96_barcode16.sorted_probabilities.tsv\n",
      "17:51:14 [  DEBUG] 26_SQK-RBK114-96_barcode20.sorted: using cached TSV 26_SQK-RBK114-96_barcode20.sorted_probabilities.tsv\n",
      "17:51:14 [  DEBUG] 27_SQK-RBK114-96_barcode26.sorted: using cached TSV 27_SQK-RBK114-96_barcode26.sorted_probabilities.tsv\n",
      "17:51:14 [  DEBUG] 28_SQK-RBK114-96_barcode27.sorted: using cached TSV 28_SQK-RBK114-96_barcode27.sorted_probabilities.tsv\n",
      "\n",
      "17:51:15 [   INFO] Baseline ratios  m6A=0.03947  5mC=0.00000\n",
      "17:51:15 [  DEBUG]   denom=0 → 0.80000\n",
      "17:51:15 [   INFO] m6A thresholds per BAM:\n",
      "17:51:15 [   INFO]   /Data1/seq_data/AG2_51dpy21null_fiberseq_1_16_24/no_sample/20240116_2226_X1_FAX30165_d8fc11b9/basecalls/mod_mappings.sorted.bam → 0.98300\n",
      "17:51:15 [   INFO]   /Data1/seq_data/BN_96DPY27Deg_Fiber_Hia5_MCviPI_05_24_24/no_sample/basecalls/mod_mappings.sorted.bam → 0.96700\n",
      "17:51:15 [   INFO]   /Data1/seq_data/BM_N2_old_Fiber_Hia5_MCVIPI_05_30_24/no_sample/20240530_1930_X2_FAX32001_56dbbc37/basecalls/mod_mappings.sorted.bam → 0.97100\n",
      "17:51:15 [   INFO]   /Data1/seq_data/BS_N2_old_Hia5_MCVIPI_10_17_2024/no_sample/20241018_1815_X2_FAX31996_a75b7b27/basecalls/mod_mappings.sorted.bam → 0.91700\n",
      "17:51:15 [   INFO]   /Data1/seq_data/BT_96DPY27Deg_old_Hia5_MCviPI_10_17_2024/no_sample/20241018_1752_X1_FAX20286_da16b0d3/basecalls/mod_mappings.sorted.bam → 0.85100\n",
      "17:51:15 [   INFO]   /Data1/seq_data/CA_107_SDC2_degron_OLD_20250213/no_sample_id/basecalls/mod_mappings.sorted.bam → 0.92400\n",
      "17:51:15 [   INFO]   /Data1/seq_data/CE1_1-14_19-22_fiber_2025_04_09/basecalls/SQK-NBD114-24_barcode02.sorted.bam → 0.96700\n",
      "17:51:15 [   INFO]   /Data1/seq_data/CE1_1-14_19-22_fiber_2025_04_09/basecalls/SQK-NBD114-24_barcode03.sorted.bam → 0.97100\n",
      "17:51:15 [   INFO]   /Data1/seq_data/CE1_1-14_19-22_fiber_2025_04_09/basecalls/SQK-NBD114-24_barcode04.sorted.bam → 0.95900\n",
      "17:51:15 [   INFO]   /Data1/seq_data/CE1_1-14_19-22_fiber_2025_04_09/basecalls/SQK-NBD114-24_barcode05.sorted.bam → 0.92800\n",
      "17:51:15 [   INFO]   /Data1/seq_data/CE1_1-14_19-22_fiber_2025_04_09/basecalls/SQK-NBD114-24_barcode06.sorted.bam → 0.93200\n",
      "17:51:15 [   INFO]   /Data1/seq_data/CE1_1-14_19-22_fiber_2025_04_09/basecalls/SQK-NBD114-24_barcode10.sorted.bam → 0.97100\n",
      "17:51:15 [   INFO]   /Data1/seq_data/CE1_1-14_19-22_fiber_2025_04_09/basecalls/SQK-NBD114-24_barcode11.sorted.bam → 0.89300\n",
      "17:51:15 [   INFO]   /Data1/seq_data/CH_1_2_8_9_13_14_fiber_rep_4_26_25/basecalls/SQK-NBD114-24_barcode02.sorted.bam → 0.97500\n",
      "17:51:15 [   INFO]   /Data1/seq_data/CH_1_2_8_9_13_14_fiber_rep_4_26_25/basecalls/SQK-NBD114-24_barcode05.sorted.bam → 0.92800\n",
      "17:51:15 [   INFO]   /Data1/seq_data/CH_1_2_8_9_13_14_fiber_rep_4_26_25/basecalls/SQK-NBD114-24_barcode06.sorted.bam → 0.94000\n",
      "17:51:15 [   INFO]   /Data1/seq_data/CH_1_2_8_9_13_14_fiber_rep_4_26_25/basecalls/SQK-NBD114-24_barcode10.sorted.bam → 0.97500\n",
      "17:51:15 [   INFO]   /Data1/seq_data/CH_1_2_8_9_13_14_fiber_rep_4_26_25/basecalls/SQK-NBD114-24_barcode11.sorted.bam → 0.87400\n",
      "17:51:15 [   INFO]   /Data1/seq_data/20_5_SDC3deg_dpy21null_SRE_fiberseq_4_23/basecalls/SQK-RBK114-96_barcode08.sorted.bam → 0.87800\n",
      "17:51:15 [   INFO]   /Data1/seq_data/CE1_6_N2young_N2mid_SDC3degron_8_12_25/no_sample_id/combined_basecalls/SQK-RBK114-96_barcode10.sorted.bam → 0.97900\n",
      "17:51:15 [   INFO]   /Data1/seq_data/CE1_6_N2young_N2mid_SDC3degron_8_12_25/no_sample_id/combined_basecalls/SQK-RBK114-96_barcode11.sorted.bam → 0.97900\n",
      "17:51:15 [   INFO]   /Data1/seq_data/CE1_6_N2young_N2mid_SDC3degron_8_12_25/no_sample_id/combined_basecalls/SQK-RBK114-96_barcode12.sorted.bam → 0.92800\n",
      "17:51:15 [   INFO]   /Data1/seq_data/CE1_6_N2young_N2mid_SDC3degron_8_12_25/no_sample_id/combined_basecalls/SQK-RBK114-96_barcode13.sorted.bam → 0.92800\n",
      "17:51:15 [   INFO]   /Data1/seq_data/CE1_6_N2young_N2mid_SDC3degron_8_12_25/no_sample_id/combined_basecalls/SQK-RBK114-96_barcode15.sorted.bam → 0.80000\n",
      "17:51:15 [   INFO]   /Data1/seq_data/CE1_6_N2young_N2mid_SDC3degron_8_12_25/no_sample_id/combined_basecalls/SQK-RBK114-96_barcode16.sorted.bam → 0.80800\n",
      "17:51:15 [   INFO]   /Data1/seq_data/CE1_6_N2young_N2mid_SDC3degron_8_12_25/no_sample_id/N2Y_SDC3_basecalls/SQK-RBK114-96_barcode18.sorted.bam → 0.98600\n",
      "17:51:15 [   INFO]   /Data1/seq_data/CE1_6_N2young_N2mid_SDC3degron_8_12_25/no_sample_id/N2Y_SDC3_basecalls/SQK-RBK114-96_barcode20.sorted.bam → 0.98300\n",
      "17:51:15 [   INFO]   /Data1/seq_data/CE1_6_N2young_N2mid_SDC3degron_8_12_25/no_sample_id/N2Y_SDC3_basecalls/SQK-RBK114-96_barcode26.sorted.bam → 0.84700\n",
      "17:51:15 [   INFO]   /Data1/seq_data/CE1_6_N2young_N2mid_SDC3degron_8_12_25/no_sample_id/N2Y_SDC3_basecalls/SQK-RBK114-96_barcode27.sorted.bam → 0.85900\n",
      "17:51:15 [   INFO] 5mC thresholds per BAM:\n",
      "17:51:15 [   INFO]   /Data1/seq_data/AG2_51dpy21null_fiberseq_1_16_24/no_sample/20240116_2226_X1_FAX30165_d8fc11b9/basecalls/mod_mappings.sorted.bam → 0.80000\n",
      "17:51:15 [   INFO]   /Data1/seq_data/BN_96DPY27Deg_Fiber_Hia5_MCviPI_05_24_24/no_sample/basecalls/mod_mappings.sorted.bam → 0.99400\n",
      "17:51:15 [   INFO]   /Data1/seq_data/BM_N2_old_Fiber_Hia5_MCVIPI_05_30_24/no_sample/20240530_1930_X2_FAX32001_56dbbc37/basecalls/mod_mappings.sorted.bam → 0.99400\n",
      "17:51:15 [   INFO]   /Data1/seq_data/BS_N2_old_Hia5_MCVIPI_10_17_2024/no_sample/20241018_1815_X2_FAX31996_a75b7b27/basecalls/mod_mappings.sorted.bam → 0.99400\n",
      "17:51:15 [   INFO]   /Data1/seq_data/BT_96DPY27Deg_old_Hia5_MCviPI_10_17_2024/no_sample/20241018_1752_X1_FAX20286_da16b0d3/basecalls/mod_mappings.sorted.bam → 0.99400\n",
      "17:51:15 [   INFO]   /Data1/seq_data/CA_107_SDC2_degron_OLD_20250213/no_sample_id/basecalls/mod_mappings.sorted.bam → 0.99400\n",
      "17:51:15 [   INFO]   /Data1/seq_data/CE1_1-14_19-22_fiber_2025_04_09/basecalls/SQK-NBD114-24_barcode02.sorted.bam → 0.99400\n",
      "17:51:15 [   INFO]   /Data1/seq_data/CE1_1-14_19-22_fiber_2025_04_09/basecalls/SQK-NBD114-24_barcode03.sorted.bam → 0.99400\n",
      "17:51:15 [   INFO]   /Data1/seq_data/CE1_1-14_19-22_fiber_2025_04_09/basecalls/SQK-NBD114-24_barcode04.sorted.bam → 0.99400\n",
      "17:51:15 [   INFO]   /Data1/seq_data/CE1_1-14_19-22_fiber_2025_04_09/basecalls/SQK-NBD114-24_barcode05.sorted.bam → 0.99400\n",
      "17:51:15 [   INFO]   /Data1/seq_data/CE1_1-14_19-22_fiber_2025_04_09/basecalls/SQK-NBD114-24_barcode06.sorted.bam → 0.99400\n",
      "17:51:15 [   INFO]   /Data1/seq_data/CE1_1-14_19-22_fiber_2025_04_09/basecalls/SQK-NBD114-24_barcode10.sorted.bam → 0.99400\n",
      "17:51:15 [   INFO]   /Data1/seq_data/CE1_1-14_19-22_fiber_2025_04_09/basecalls/SQK-NBD114-24_barcode11.sorted.bam → 0.99400\n",
      "17:51:15 [   INFO]   /Data1/seq_data/CH_1_2_8_9_13_14_fiber_rep_4_26_25/basecalls/SQK-NBD114-24_barcode02.sorted.bam → 0.99400\n",
      "17:51:15 [   INFO]   /Data1/seq_data/CH_1_2_8_9_13_14_fiber_rep_4_26_25/basecalls/SQK-NBD114-24_barcode05.sorted.bam → 0.99400\n",
      "17:51:15 [   INFO]   /Data1/seq_data/CH_1_2_8_9_13_14_fiber_rep_4_26_25/basecalls/SQK-NBD114-24_barcode06.sorted.bam → 0.99400\n",
      "17:51:15 [   INFO]   /Data1/seq_data/CH_1_2_8_9_13_14_fiber_rep_4_26_25/basecalls/SQK-NBD114-24_barcode10.sorted.bam → 0.99400\n",
      "17:51:15 [   INFO]   /Data1/seq_data/CH_1_2_8_9_13_14_fiber_rep_4_26_25/basecalls/SQK-NBD114-24_barcode11.sorted.bam → 0.99400\n",
      "17:51:15 [   INFO]   /Data1/seq_data/20_5_SDC3deg_dpy21null_SRE_fiberseq_4_23/basecalls/SQK-RBK114-96_barcode08.sorted.bam → 0.99400\n",
      "17:51:15 [   INFO]   /Data1/seq_data/CE1_6_N2young_N2mid_SDC3degron_8_12_25/no_sample_id/combined_basecalls/SQK-RBK114-96_barcode10.sorted.bam → 0.99400\n",
      "17:51:15 [   INFO]   /Data1/seq_data/CE1_6_N2young_N2mid_SDC3degron_8_12_25/no_sample_id/combined_basecalls/SQK-RBK114-96_barcode11.sorted.bam → 0.99400\n",
      "17:51:15 [   INFO]   /Data1/seq_data/CE1_6_N2young_N2mid_SDC3degron_8_12_25/no_sample_id/combined_basecalls/SQK-RBK114-96_barcode12.sorted.bam → 0.99400\n",
      "17:51:15 [   INFO]   /Data1/seq_data/CE1_6_N2young_N2mid_SDC3degron_8_12_25/no_sample_id/combined_basecalls/SQK-RBK114-96_barcode13.sorted.bam → 0.99400\n",
      "17:51:15 [   INFO]   /Data1/seq_data/CE1_6_N2young_N2mid_SDC3degron_8_12_25/no_sample_id/combined_basecalls/SQK-RBK114-96_barcode15.sorted.bam → 0.99400\n",
      "17:51:15 [   INFO]   /Data1/seq_data/CE1_6_N2young_N2mid_SDC3degron_8_12_25/no_sample_id/combined_basecalls/SQK-RBK114-96_barcode16.sorted.bam → 0.99400\n",
      "17:51:15 [   INFO]   /Data1/seq_data/CE1_6_N2young_N2mid_SDC3degron_8_12_25/no_sample_id/N2Y_SDC3_basecalls/SQK-RBK114-96_barcode18.sorted.bam → 0.99400\n",
      "17:51:15 [   INFO]   /Data1/seq_data/CE1_6_N2young_N2mid_SDC3degron_8_12_25/no_sample_id/N2Y_SDC3_basecalls/SQK-RBK114-96_barcode20.sorted.bam → 0.99400\n",
      "17:51:15 [   INFO]   /Data1/seq_data/CE1_6_N2young_N2mid_SDC3degron_8_12_25/no_sample_id/N2Y_SDC3_basecalls/SQK-RBK114-96_barcode26.sorted.bam → 0.99400\n",
      "17:51:15 [   INFO]   /Data1/seq_data/CE1_6_N2young_N2mid_SDC3degron_8_12_25/no_sample_id/N2Y_SDC3_basecalls/SQK-RBK114-96_barcode27.sorted.bam → 0.99400\n"
     ]
    }
   ],
   "source": [
    "# ╔══════════════════════════════════════════════════════════════════════════╗\n",
    "# ║  modkit sample-probs  ▸  per-sample cut-offs using include-bed filter    ║\n",
    "# ║                                                                          ║\n",
    "# ║  Outputs two dicts:                                                      ║\n",
    "# ║    • m6A_thresh_dict  = { bam_path: threshold, … }                       ║\n",
    "# ║    • m5mC_thresh_dict = { bam_path: threshold, … }                       ║\n",
    "# ║                                                                          ║\n",
    "# ║  All other logic (skip-if-exists, ascending search, verbose DEBUG)       ║\n",
    "# ║  remains unchanged.                                                      ║\n",
    "# ╚══════════════════════════════════════════════════════════════════════════╝\n",
    "import os, subprocess, logging, sys\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "force_replace = False  # skip modkit run if TSV already exists and this is False\n",
    "\n",
    "# ─────────────────────────  LOGGING CONFIG  ────────────────────────────────\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s [%(levelname)7s] %(message)s\",\n",
    "    level=logging.DEBUG,\n",
    "    datefmt=\"%H:%M:%S\",\n",
    ")\n",
    "abort = lambda msg: (logging.critical(msg), sys.exit(1))\n",
    "\n",
    "# ──────────────────────────  INPUT CHECKS  ────────────────────────────────\n",
    "if not (modkit_path and os.path.isfile(modkit_path) and os.access(modkit_path, os.X_OK)):\n",
    "    abort(f\"modkit executable not found: {modkit_path}\")\n",
    "if not new_bam_files:\n",
    "    abort(\"new_bam_files is empty.\")\n",
    "for p in new_bam_files:\n",
    "    if not os.path.isfile(p):\n",
    "        abort(f\"BAM not found: {p}\")\n",
    "\n",
    "# ─────────────────────  CONSTANTS & PATHS  ────────────────────────────────\n",
    "MAX_CPUS        = 500\n",
    "THREADS_PER_JOB = min(64, MAX_CPUS, cpu_count())\n",
    "POOL_SIZE       = max(1, min(len(new_bam_files), MAX_CPUS // THREADS_PER_JOB))\n",
    "SAMPLE_FRAC     = 0.999\n",
    "PROB_FLOOR_m6A      = R10_m6A_thresh_percent\n",
    "PROB_FLOOR_5mC     = R10_5mC_thresh_percent\n",
    "\n",
    "sample_probs_root = Path(output_stem) / \"sample_probs\"\n",
    "sample_probs_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# ────────────────────────  PREFIX GENERATION  ─────────────────────────────\n",
    "def make_prefix(bam_path: str, idx: int) -> str:\n",
    "    return f\"{idx:02d}_{Path(bam_path).stem}\"\n",
    "prefixes = [make_prefix(p, i) for i, p in enumerate(new_bam_files)]\n",
    "\n",
    "# ─────────────────────────  RUN sample-probs  ─────────────────────────────\n",
    "def run_sample_probs(bam, prefix):\n",
    "    out_dir = sample_probs_root / prefix\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    cached = list(out_dir.glob(\"*_probabilities.tsv\"))\n",
    "    if cached and not force_replace:\n",
    "        logging.debug(\"%s: using cached TSV %s\", prefix, cached[0].name)\n",
    "        return str(cached[0])\n",
    "\n",
    "    cmd = [\n",
    "        modkit_path, \"sample-probs\", bam,\n",
    "        \"-f\", str(SAMPLE_FRAC),\n",
    "        \"-t\", str(THREADS_PER_JOB),\n",
    "        \"--include-bed\", str(bed_path),\n",
    "        \"-o\", str(out_dir),\n",
    "        \"--prefix\", prefix,\n",
    "        \"--hist\", \"--force\", \"--suppress-progress\",\n",
    "    ]\n",
    "    logging.debug(\"CMD: %s\", \" \".join(cmd))\n",
    "\n",
    "    try:\n",
    "        subprocess.run(cmd, check=True, stderr=subprocess.PIPE, text=True)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        logging.error(\"modkit failed with exit code %d\", e.returncode)\n",
    "        logging.error(\"modkit stderr:\\n%s\", e.stderr.strip())\n",
    "        # re-raise so the pool still sees the failure\n",
    "        raise\n",
    "\n",
    "    tsvs = list(out_dir.glob(\"*_probabilities.tsv\"))\n",
    "    if len(tsvs) != 1:\n",
    "        abort(f\"Expected one *_probabilities.tsv in {out_dir}, found {len(tsvs)}\")\n",
    "    return str(tsvs[0])\n",
    "\n",
    "\n",
    "with Pool(POOL_SIZE) as pool:\n",
    "    tsv_paths = list(\n",
    "        tqdm(pool.starmap(run_sample_probs, zip(new_bam_files, prefixes)),\n",
    "             total=len(new_bam_files), desc=\"sample-probs\")\n",
    "    )\n",
    "\n",
    "# ─────────────────────────  LOAD TABLES  ───────────────────────────────────\n",
    "tables = [pd.read_csv(p, sep=r\"\\s+\") for p in tsv_paths]\n",
    "\n",
    "# ────────────────────  CANONICAL COUNTS ABOVE 0.80  ────────────────────────\n",
    "def canonical_count(df, base, base_prob_floor=0.7):\n",
    "    return int(df.loc[(df[\"code\"] == base) & (df[\"range_start\"] >= base_prob_floor), \"count\"].sum())\n",
    "denom_A = [canonical_count(df, \"A\") for df in tables]\n",
    "denom_C = [canonical_count(df, \"C\") for df in tables]\n",
    "\n",
    "# ────────────────────  MODIFIED COUNTS & RATIOS @ 0.80  ────────────────────\n",
    "def mod_above(df, mod_code, thresh):\n",
    "    return int(df.loc[(df[\"code\"] == mod_code) & (df[\"range_start\"] >= thresh), \"count\"].sum())\n",
    "ratio_floor_A = [mod_above(df, \"a\", PROB_FLOOR_m6A)/den if den else 0.0\n",
    "                 for df, den in zip(tables, denom_A)]\n",
    "ratio_floor_C = [mod_above(df, \"m\", PROB_FLOOR_5mC)/den if den else 0.0\n",
    "                 for df, den in zip(tables, denom_C)]\n",
    "\n",
    "baseline_ratio_m6A = min(ratio_floor_A)\n",
    "baseline_ratio_5mC = min(ratio_floor_C)\n",
    "logging.info(\"Baseline ratios  m6A=%.5f  5mC=%.5f\",\n",
    "             baseline_ratio_m6A, baseline_ratio_5mC)\n",
    "\n",
    "# ───────────────────  ASCENDING THRESHOLD SEARCH  ──────────────────────────\n",
    "def ascending_thresh(df, mod_code, denom, baseline, mod_prob_floor):\n",
    "    if denom == 0:\n",
    "        logging.debug(\"  denom=0 → %.5f\", mod_prob_floor)\n",
    "        return mod_prob_floor\n",
    "    bins = df[(df[\"code\"] == mod_code) & (df[\"range_start\"] >= mod_prob_floor)].copy()\n",
    "    bins.sort_values(\"range_start\", inplace=True)\n",
    "    total = bins[\"count\"].sum()\n",
    "    if total/denom <= baseline:\n",
    "        return mod_prob_floor\n",
    "    for _, row in bins.iterrows():\n",
    "        total -= row[\"count\"]\n",
    "        if total/denom <= baseline:\n",
    "            return max(row[\"range_start\"], mod_prob_floor)\n",
    "    return mod_prob_floor\n",
    "\n",
    "m6A_thresh = []\n",
    "m5mC_thresh = []\n",
    "for df, denA, denC in zip(tables, denom_A, denom_C):\n",
    "    m6A_thresh.append(round(ascending_thresh(df, \"a\", denA, baseline_ratio_m6A, PROB_FLOOR_m6A), 5))\n",
    "    m5mC_thresh.append(round(ascending_thresh(df, \"m\", denC, baseline_ratio_5mC, PROB_FLOOR_5mC), 5))\n",
    "\n",
    "# ─────────────────────────  BUILD OUTPUT DICTS ─────────────────────────────\n",
    "m6A_thresh_dict  = dict(zip(new_bam_files, m6A_thresh))\n",
    "m5mC_thresh_dict = dict(zip(new_bam_files, m5mC_thresh))\n",
    "\n",
    "# ─────────────────────────────  LOG & RETURN  ─────────────────────────────\n",
    "logging.info(\"m6A thresholds per BAM:\")\n",
    "for bam, thr in m6A_thresh_dict.items():\n",
    "    logging.info(\"  %s → %.5f\", bam, thr)\n",
    "\n",
    "logging.info(\"5mC thresholds per BAM:\")\n",
    "for bam, thr in m5mC_thresh_dict.items():\n",
    "    logging.info(\"  %s → %.5f\", bam, thr)\n",
    "\n",
    "# Now m6A_thresh_dict and m5mC_thresh_dict are ready for downstream use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f39b6ab9a9ada35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T17:51:15.617078Z",
     "start_time": "2025-08-29T17:51:15.612914Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Data1/seq_data/AG2_51dpy21null_fiberseq_1_16_24/no_sample/20240116_2226_X1_FAX30165_d8fc11b9/basecalls/mod_mappings.sorted.bam', '/Data1/seq_data/BN_96DPY27Deg_Fiber_Hia5_MCviPI_05_24_24/no_sample/basecalls/mod_mappings.sorted.bam', '/Data1/seq_data/BM_N2_old_Fiber_Hia5_MCVIPI_05_30_24/no_sample/20240530_1930_X2_FAX32001_56dbbc37/basecalls/mod_mappings.sorted.bam', '/Data1/seq_data/BS_N2_old_Hia5_MCVIPI_10_17_2024/no_sample/20241018_1815_X2_FAX31996_a75b7b27/basecalls/mod_mappings.sorted.bam', '/Data1/seq_data/BT_96DPY27Deg_old_Hia5_MCviPI_10_17_2024/no_sample/20241018_1752_X1_FAX20286_da16b0d3/basecalls/mod_mappings.sorted.bam', '/Data1/seq_data/CA_107_SDC2_degron_OLD_20250213/no_sample_id/basecalls/mod_mappings.sorted.bam', '/Data1/seq_data/CE1_1-14_19-22_fiber_2025_04_09/basecalls/SQK-NBD114-24_barcode02.sorted.bam', '/Data1/seq_data/CE1_1-14_19-22_fiber_2025_04_09/basecalls/SQK-NBD114-24_barcode03.sorted.bam', '/Data1/seq_data/CE1_1-14_19-22_fiber_2025_04_09/basecalls/SQK-NBD114-24_barcode04.sorted.bam', '/Data1/seq_data/CE1_1-14_19-22_fiber_2025_04_09/basecalls/SQK-NBD114-24_barcode05.sorted.bam', '/Data1/seq_data/CE1_1-14_19-22_fiber_2025_04_09/basecalls/SQK-NBD114-24_barcode06.sorted.bam', '/Data1/seq_data/CE1_1-14_19-22_fiber_2025_04_09/basecalls/SQK-NBD114-24_barcode10.sorted.bam', '/Data1/seq_data/CE1_1-14_19-22_fiber_2025_04_09/basecalls/SQK-NBD114-24_barcode11.sorted.bam', '/Data1/seq_data/CH_1_2_8_9_13_14_fiber_rep_4_26_25/basecalls/SQK-NBD114-24_barcode02.sorted.bam', '/Data1/seq_data/CH_1_2_8_9_13_14_fiber_rep_4_26_25/basecalls/SQK-NBD114-24_barcode05.sorted.bam', '/Data1/seq_data/CH_1_2_8_9_13_14_fiber_rep_4_26_25/basecalls/SQK-NBD114-24_barcode06.sorted.bam', '/Data1/seq_data/CH_1_2_8_9_13_14_fiber_rep_4_26_25/basecalls/SQK-NBD114-24_barcode10.sorted.bam', '/Data1/seq_data/CH_1_2_8_9_13_14_fiber_rep_4_26_25/basecalls/SQK-NBD114-24_barcode11.sorted.bam', '/Data1/seq_data/20_5_SDC3deg_dpy21null_SRE_fiberseq_4_23/basecalls/SQK-RBK114-96_barcode08.sorted.bam', '/Data1/seq_data/CE1_6_N2young_N2mid_SDC3degron_8_12_25/no_sample_id/combined_basecalls/SQK-RBK114-96_barcode10.sorted.bam', '/Data1/seq_data/CE1_6_N2young_N2mid_SDC3degron_8_12_25/no_sample_id/combined_basecalls/SQK-RBK114-96_barcode11.sorted.bam', '/Data1/seq_data/CE1_6_N2young_N2mid_SDC3degron_8_12_25/no_sample_id/combined_basecalls/SQK-RBK114-96_barcode12.sorted.bam', '/Data1/seq_data/CE1_6_N2young_N2mid_SDC3degron_8_12_25/no_sample_id/combined_basecalls/SQK-RBK114-96_barcode13.sorted.bam', '/Data1/seq_data/CE1_6_N2young_N2mid_SDC3degron_8_12_25/no_sample_id/combined_basecalls/SQK-RBK114-96_barcode15.sorted.bam', '/Data1/seq_data/CE1_6_N2young_N2mid_SDC3degron_8_12_25/no_sample_id/combined_basecalls/SQK-RBK114-96_barcode16.sorted.bam', '/Data1/seq_data/CE1_6_N2young_N2mid_SDC3degron_8_12_25/no_sample_id/N2Y_SDC3_basecalls/SQK-RBK114-96_barcode18.sorted.bam', '/Data1/seq_data/CE1_6_N2young_N2mid_SDC3degron_8_12_25/no_sample_id/N2Y_SDC3_basecalls/SQK-RBK114-96_barcode20.sorted.bam', '/Data1/seq_data/CE1_6_N2young_N2mid_SDC3degron_8_12_25/no_sample_id/N2Y_SDC3_basecalls/SQK-RBK114-96_barcode26.sorted.bam', '/Data1/seq_data/CE1_6_N2young_N2mid_SDC3degron_8_12_25/no_sample_id/N2Y_SDC3_basecalls/SQK-RBK114-96_barcode27.sorted.bam']\n",
      "[0.983, 0.967, 0.971, 0.917, 0.851, 0.924, 0.967, 0.971, 0.959, 0.928, 0.932, 0.971, 0.893, 0.975, 0.928, 0.94, 0.975, 0.874, 0.878, 0.979, 0.979, 0.928, 0.928, 0.8, 0.808, 0.986, 0.983, 0.847, 0.859]\n",
      "{'/Data1/seq_data/AG2_51dpy21null_fiberseq_1_16_24/no_sample/20240116_2226_X1_FAX30165_d8fc11b9/basecalls/mod_mappings.sorted.bam': 0.983, '/Data1/seq_data/BN_96DPY27Deg_Fiber_Hia5_MCviPI_05_24_24/no_sample/basecalls/mod_mappings.sorted.bam': 0.967, '/Data1/seq_data/BM_N2_old_Fiber_Hia5_MCVIPI_05_30_24/no_sample/20240530_1930_X2_FAX32001_56dbbc37/basecalls/mod_mappings.sorted.bam': 0.971, '/Data1/seq_data/BS_N2_old_Hia5_MCVIPI_10_17_2024/no_sample/20241018_1815_X2_FAX31996_a75b7b27/basecalls/mod_mappings.sorted.bam': 0.917, '/Data1/seq_data/BT_96DPY27Deg_old_Hia5_MCviPI_10_17_2024/no_sample/20241018_1752_X1_FAX20286_da16b0d3/basecalls/mod_mappings.sorted.bam': 0.851, '/Data1/seq_data/CA_107_SDC2_degron_OLD_20250213/no_sample_id/basecalls/mod_mappings.sorted.bam': 0.924, '/Data1/seq_data/CE1_1-14_19-22_fiber_2025_04_09/basecalls/SQK-NBD114-24_barcode02.sorted.bam': 0.967, '/Data1/seq_data/CE1_1-14_19-22_fiber_2025_04_09/basecalls/SQK-NBD114-24_barcode03.sorted.bam': 0.971, '/Data1/seq_data/CE1_1-14_19-22_fiber_2025_04_09/basecalls/SQK-NBD114-24_barcode04.sorted.bam': 0.959, '/Data1/seq_data/CE1_1-14_19-22_fiber_2025_04_09/basecalls/SQK-NBD114-24_barcode05.sorted.bam': 0.928, '/Data1/seq_data/CE1_1-14_19-22_fiber_2025_04_09/basecalls/SQK-NBD114-24_barcode06.sorted.bam': 0.932, '/Data1/seq_data/CE1_1-14_19-22_fiber_2025_04_09/basecalls/SQK-NBD114-24_barcode10.sorted.bam': 0.971, '/Data1/seq_data/CE1_1-14_19-22_fiber_2025_04_09/basecalls/SQK-NBD114-24_barcode11.sorted.bam': 0.893, '/Data1/seq_data/CH_1_2_8_9_13_14_fiber_rep_4_26_25/basecalls/SQK-NBD114-24_barcode02.sorted.bam': 0.975, '/Data1/seq_data/CH_1_2_8_9_13_14_fiber_rep_4_26_25/basecalls/SQK-NBD114-24_barcode05.sorted.bam': 0.928, '/Data1/seq_data/CH_1_2_8_9_13_14_fiber_rep_4_26_25/basecalls/SQK-NBD114-24_barcode06.sorted.bam': 0.94, '/Data1/seq_data/CH_1_2_8_9_13_14_fiber_rep_4_26_25/basecalls/SQK-NBD114-24_barcode10.sorted.bam': 0.975, '/Data1/seq_data/CH_1_2_8_9_13_14_fiber_rep_4_26_25/basecalls/SQK-NBD114-24_barcode11.sorted.bam': 0.874, '/Data1/seq_data/20_5_SDC3deg_dpy21null_SRE_fiberseq_4_23/basecalls/SQK-RBK114-96_barcode08.sorted.bam': 0.878, '/Data1/seq_data/CE1_6_N2young_N2mid_SDC3degron_8_12_25/no_sample_id/combined_basecalls/SQK-RBK114-96_barcode10.sorted.bam': 0.979, '/Data1/seq_data/CE1_6_N2young_N2mid_SDC3degron_8_12_25/no_sample_id/combined_basecalls/SQK-RBK114-96_barcode11.sorted.bam': 0.979, '/Data1/seq_data/CE1_6_N2young_N2mid_SDC3degron_8_12_25/no_sample_id/combined_basecalls/SQK-RBK114-96_barcode12.sorted.bam': 0.928, '/Data1/seq_data/CE1_6_N2young_N2mid_SDC3degron_8_12_25/no_sample_id/combined_basecalls/SQK-RBK114-96_barcode13.sorted.bam': 0.928, '/Data1/seq_data/CE1_6_N2young_N2mid_SDC3degron_8_12_25/no_sample_id/combined_basecalls/SQK-RBK114-96_barcode15.sorted.bam': 0.8, '/Data1/seq_data/CE1_6_N2young_N2mid_SDC3degron_8_12_25/no_sample_id/combined_basecalls/SQK-RBK114-96_barcode16.sorted.bam': 0.808, '/Data1/seq_data/CE1_6_N2young_N2mid_SDC3degron_8_12_25/no_sample_id/N2Y_SDC3_basecalls/SQK-RBK114-96_barcode18.sorted.bam': 0.986, '/Data1/seq_data/CE1_6_N2young_N2mid_SDC3degron_8_12_25/no_sample_id/N2Y_SDC3_basecalls/SQK-RBK114-96_barcode20.sorted.bam': 0.983, '/Data1/seq_data/CE1_6_N2young_N2mid_SDC3degron_8_12_25/no_sample_id/N2Y_SDC3_basecalls/SQK-RBK114-96_barcode26.sorted.bam': 0.847, '/Data1/seq_data/CE1_6_N2young_N2mid_SDC3degron_8_12_25/no_sample_id/N2Y_SDC3_basecalls/SQK-RBK114-96_barcode27.sorted.bam': 0.859}\n",
      "[0.8, 0.994, 0.994, 0.994, 0.994, 0.994, 0.994, 0.994, 0.994, 0.994, 0.994, 0.994, 0.994, 0.994, 0.994, 0.994, 0.994, 0.994, 0.994, 0.994, 0.994, 0.994, 0.994, 0.994, 0.994, 0.994, 0.994, 0.994, 0.994]\n"
     ]
    }
   ],
   "source": [
    "# from previous run:\n",
    "# Hardcoded m6A thresholds per BAM file\n",
    "# ───── New cell ─────\n",
    "print(new_bam_files)\n",
    "# Build a list of m6A thresholds in the same order as new_bam_files\n",
    "thresh_list = [m6A_thresh_dict[bam] for bam in new_bam_files]\n",
    "print(thresh_list)\n",
    "print(m6A_thresh_dict)\n",
    "\n",
    "m5mC_thresh_list = [m5mC_thresh_dict[bam] for bam in new_bam_files]\n",
    "print(m5mC_thresh_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288e33ddbe4d8c6e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Raw data for chrom plotting\n",
    "import importlib, os\n",
    "import pandas as pd\n",
    "import nanotools\n",
    "importlib.reload(nanotools)\n",
    "\n",
    "sampling_frac = 1\n",
    "force_replace = True\n",
    "regions_only = True  # keep name, but we WON'T pass bed_file\n",
    "\n",
    "summary_bam_df = nanotools.process_and_export_summary_by_region(\n",
    "    sampling_frac=sampling_frac,\n",
    "    type_selected=type_selected,\n",
    "    new_bam_files=new_bam_files,\n",
    "    conditions=conditions,\n",
    "    thresh_list=thresh_list,\n",
    "    exp_ids=exp_ids,\n",
    "    modkit_bed_df=modkit_bed_df_ext,   # per-interval loop\n",
    "    force_replace=force_replace,\n",
    "    output_directory=\"temp_files/\"\n",
    "    # DO NOT pass bed_file here\n",
    ")\n",
    "\n",
    "# Normalize and inspect\n",
    "summary_bam_df = nanotools.ensure_tidy_summary(summary_bam_df)\n",
    "print(\"cols:\", summary_bam_df.columns.tolist())\n",
    "print(summary_bam_df.head())\n",
    "\n",
    "summary_bam_df = summary_bam_df.merge(\n",
    "    combined_bed_df_ext[['chrom','bed_start','bed_end','type','chr_type','bed_strand']]\n",
    "      .rename(columns={'chrom':'chromosome','bed_start':'start','bed_end':'end'}),\n",
    "    on=['chromosome','start','end'], how='left'\n",
    ")\n",
    "\n",
    "coverage_df_a = nanotools.create_coverage_df('a', summary_bam_df, coverage_df_name_a)\n",
    "coverage_df_m = nanotools.create_coverage_df('m', summary_bam_df, coverage_df_name_m)\n",
    "grouped_df_a  = nanotools.prepare_chr_plotting_data(coverage_df_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed182e9de184ba58",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### PLOT m6A by CHROM, region and condition\n",
    "\n",
    "importlib.reload(nanotools)\n",
    "print(summary_bam_df)\n",
    "coverage_df_a = nanotools.create_coverage_df('a', summary_bam_df, coverage_df_name_a)\n",
    "print(coverage_df_a.head())\n",
    "# ════════════════════════════════════════════════════════════════════\n",
    "#  Prepare data – _A_ (ignore ‘m’)   *exp_id is preserved*\n",
    "# ════════════════════════════════════════════════════════════════════\n",
    "grouped_df_a = nanotools.prepare_chr_plotting_data(coverage_df_a)\n",
    "# ─── 1. total_reads helper ──────────────────────────────────────────\n",
    "grouped_df_a['total_reads'] = grouped_df_a['mod_pass'] + grouped_df_a['canon_pass']\n",
    "# m6A_frac column is already the same as mod_frac\n",
    "# (prepare_chr_plotting_data left it untouched)\n",
    "\n",
    "# ─── 2. recompute global_m6a_by_cond with the new column ────────────\n",
    "global_m6a_by_cond = (\n",
    "    grouped_df_a\n",
    "    .groupby('condition')\n",
    "    .apply(lambda df: (df['m6A_frac'] * df['total_reads']).sum()\n",
    "                      / df['total_reads'].sum())\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# expected columns: condition · exp_id · chromosome · start · m6A_frac · all_count …\n",
    "# ────────────────────────────────────────────────────────────────────\n",
    "#  HELPER AGGREGATES\n",
    "# ────────────────────────────────────────────────────────────────────\n",
    "from scipy.stats import norm\n",
    "import pandas as pd, numpy as np, plotly.graph_objects as go\n",
    "from plotly.colors import qualitative\n",
    "\n",
    "PALETTE = qualitative.Plotly\n",
    "\n",
    "# Split X vs autosomes for convenience\n",
    "is_X  = grouped_df_a['chromosome'] == 'CHROMOSOME_X'\n",
    "auto  = grouped_df_a.loc[~is_X]\n",
    "x_chr = grouped_df_a.loc[is_X]\n",
    "\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────\n",
    "#  PLOT 1  – X / Autosome ratio per experiment (unchanged, width 800)\n",
    "# ────────────────────────────────────────────────────────────────────\n",
    "def plot_ratio_per_exp(x_df, auto_df):\n",
    "    mean_x    = x_df.groupby(['condition','exp_id'])['m6A_frac'].mean()\n",
    "    mean_auto = auto_df.groupby(['condition','exp_id'])['m6A_frac'].mean()\n",
    "    ratio     = (mean_x / mean_auto).reset_index(name='ratio')\n",
    "\n",
    "    fig = go.Figure()\n",
    "    for idx, cond in enumerate(ratio['condition'].unique()):\n",
    "        sub = ratio[ratio['condition']==cond]\n",
    "        fig.add_trace(go.Box(\n",
    "            x=[cond]*len(sub), y=sub['ratio'], name=cond,\n",
    "            marker_color=PALETTE[idx % len(PALETTE)],\n",
    "            boxpoints='all', jitter=0.4, pointpos=0,\n",
    "            text=sub['exp_id'],\n",
    "            hovertemplate=\"exp_id: %{text}<br>X/Auto: %{y:.2f}<extra></extra>\",\n",
    "            boxmean=False\n",
    "        ))\n",
    "    fig.update_layout(\n",
    "        template='plotly_white', width=800,\n",
    "        title='X‑to‑Autosome %m6A ratio (per experiment)',\n",
    "        yaxis_title='Ratio (log)', xaxis_title='Condition',\n",
    "        yaxis_type='log'\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "plot_ratio_per_exp(x_chr, auto)\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────\n",
    "#  PLOT 2 & 3  – Smoothed tracks along X (raw & exp‑normalised)\n",
    "# ────────────────────────────────────────────────────────────────────\n",
    "def build_track(df, value_col):\n",
    "    rolled = (\n",
    "        df.sort_values('start')\n",
    "          .groupby(['condition','exp_id'])\n",
    "          .apply(lambda g: g.assign(\n",
    "              smooth=g[value_col].rolling(SMOOTH_WINDOW, center=True,\n",
    "                                          min_periods=1).mean()))\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "    return (rolled.groupby(['condition','start'])\n",
    "                  .agg(mean=('smooth','mean'))\n",
    "                  .reset_index())\n",
    "\n",
    "track_raw  = build_track(x_chr, 'm6A_frac')\n",
    "tmp        = x_chr.copy()\n",
    "tmp['m6A_norm_exp'] = tmp.apply(\n",
    "    lambda r: r['m6A_frac']/global_m6a_by_cond[r['condition']], axis=1)\n",
    "track_norm = build_track(tmp, 'm6A_norm_exp')\n",
    "\n",
    "def plot_tracks(track_raw, track_norm):\n",
    "    fig = go.Figure()\n",
    "    for idx, cond in enumerate(track_raw['condition'].unique()):\n",
    "        col = PALETTE[idx % len(PALETTE)]\n",
    "        r   = track_raw [track_raw ['condition']==cond]\n",
    "        n   = track_norm[track_norm['condition']==cond]\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=r['start'], y=r['mean'], name=f'{cond} raw',\n",
    "            mode='lines', line=dict(color=col)))\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=n['start'], y=n['mean'], name=f'{cond} norm',\n",
    "            mode='lines', line=dict(color=col, dash='dash')))\n",
    "    fig.update_layout(\n",
    "        template='plotly_white', width=800,\n",
    "        title='Smoothed m6A track along X (raw & exp‑normalised)',\n",
    "        xaxis_title='Genomic start (bp)',\n",
    "        yaxis_title='Mean %m6A'\n",
    "    )\n",
    "    fig.update_yaxes(tickformat='.1%')\n",
    "    fig.show()\n",
    "\n",
    "plot_tracks(track_raw, track_norm)\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────\n",
    "#  NEW PLOT 4  – Region boxes: X vs Autosome, faceted by exp_id\n",
    "# ────────────────────────────────────────────────────────────────────\n",
    "import plotly.express as px\n",
    "\n",
    "def plot_region_box_by_condition(df):\n",
    "    # annotate X vs autosome\n",
    "    df = df.copy()\n",
    "    df['chr_class'] = np.where(df['chromosome']=='CHROMOSOME_X','X','Autosome')\n",
    "\n",
    "    for cond in df['condition'].unique():\n",
    "        sub = df[df['condition']==cond]\n",
    "        fig = px.box(\n",
    "            sub,\n",
    "            x='chr_class',\n",
    "            y='m6A_frac',\n",
    "            facet_col='exp_id',\n",
    "            facet_col_wrap=4,\n",
    "            color='chr_class',\n",
    "            color_discrete_map={'X':'#636EFA','Autosome':'#EF553B'},\n",
    "            category_orders={'chr_class':['Autosome','X']},\n",
    "            points='all',\n",
    "            boxmode='group',\n",
    "            template='plotly_white',\n",
    "            width=800,\n",
    "            height=800,\n",
    "        )\n",
    "        fig.update_yaxes(tickformat='.1%')\n",
    "        fig.update_layout(\n",
    "            title=f'Region %m6A: X vs Autosome – Condition: {cond}',\n",
    "            showlegend=False,\n",
    "        )\n",
    "        fig.show()\n",
    "\n",
    "# Run it:\n",
    "# 1) Compute per‑exp coverage\n",
    "exp_cov = (\n",
    "    grouped_df_a\n",
    "    .groupby('exp_id')\n",
    "    .agg(mod_reads  = ('mod_pass','sum'),\n",
    "         canon_reads= ('canon_pass','sum'))\n",
    ")\n",
    "exp_cov['total_reads'] = exp_cov['mod_reads'] + exp_cov['canon_reads']\n",
    "\n",
    "# 2) Define a cutoff: mean – 2 × std\n",
    "mean_tr = exp_cov['total_reads'].mean()\n",
    "std_tr  = exp_cov['total_reads'].std(ddof=1)\n",
    "cutoff  = mean_tr - 1/2*mean_tr\n",
    "print(f\"Filtering out any exp_id with total_reads < {cutoff:.0f}\")\n",
    "\n",
    "# 3) Keep only “healthy” experiments\n",
    "good_exps = exp_cov.query(\"total_reads >= @cutoff\").index\n",
    "filtered_df = grouped_df_a[grouped_df_a['exp_id'].isin(good_exps)].copy()\n",
    "\n",
    "# 4) (Optional) report how many you dropped\n",
    "dropped = set(grouped_df_a['exp_id']) - set(good_exps)\n",
    "print(f\"Dropped exp_ids due to low coverage: {dropped}\")\n",
    "\n",
    "# 5) Now feed filtered_df into your plotting functions instead of grouped_df_a\n",
    "#    e.g.:\n",
    "plot_ratio_per_exp(filtered_df[filtered_df['chromosome']=='CHROMOSOME_X'],\n",
    "                   filtered_df[filtered_df['chromosome']!='CHROMOSOME_X'])\n",
    "plot_tracks(\n",
    "    build_track(filtered_df[filtered_df['chromosome']=='CHROMOSOME_X'], 'm6A_frac'),\n",
    "    build_track(\n",
    "        filtered_df.assign(\n",
    "            m6A_norm_exp=lambda df: df['m6A_frac']/df['condition'].map(global_m6a_by_cond)\n",
    "        )\n",
    "        .loc[lambda df: df['chromosome']=='CHROMOSOME_X'],\n",
    "        'm6A_norm_exp'\n",
    "    )\n",
    ")\n",
    "plot_region_box_by_condition(filtered_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89be3dc8e6d816e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ────────────────── One point per BAM (per exp_id) + robust summary ──────────────────\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from tqdm import tqdm\n",
    "import importlib\n",
    "import nanotools\n",
    "\n",
    "importlib.reload(nanotools)\n",
    "\n",
    "# Config\n",
    "sampling_frac = 0.5\n",
    "POINT_SIZE      = 10\n",
    "LABEL_EXPID     = True   # True → show exp_id labels\n",
    "EXP_LABEL_SIZE  = 8\n",
    "force_replace   = False     # rebuild summary\n",
    "# assumes: new_bam_files, conditions, thresh_list, exp_ids, modkit_path, bed_path, sampling_frac\n",
    "\n",
    "summary_table_name = (\n",
    "    f\"temp_files/_{conditions[0]}_{conditions[-1]}_\"\n",
    "    f\"{sampling_frac}_thresh{thresh_list[0]}_summary_table.csv\"\n",
    ")\n",
    "\n",
    "def _process_bam(args):\n",
    "    bam, cond, thresh, exp = args\n",
    "    try:\n",
    "        df = nanotools.get_summary_from_bam(\n",
    "            sampling_frac, thresh, modkit_path,\n",
    "            bam, cond, exp,\n",
    "            thread_ct=min(24, cpu_count()),\n",
    "            bed_file=bed_path\n",
    "        )\n",
    "        if df is None or df.empty:\n",
    "            return pd.DataFrame()\n",
    "        return nanotools.ensure_tidy_summary(df)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Skipping {exp} ({cond}) due to error: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "if not force_replace and os.path.exists(summary_table_name):\n",
    "    summary_bam_df = pd.read_csv(summary_table_name, sep=\"\\t\")\n",
    "else:\n",
    "    args_iter = list(zip(new_bam_files, conditions, thresh_list, exp_ids))\n",
    "    parts = []\n",
    "    with Pool(processes=min(5, cpu_count())) as pool:\n",
    "        for res in tqdm(pool.imap_unordered(_process_bam, args_iter),\n",
    "                        total=len(args_iter), desc=\"Summarizing BAMs\"):\n",
    "            if res is not None and not res.empty:\n",
    "                parts.append(res)\n",
    "    if not parts:\n",
    "        raise RuntimeError(\"All summaries empty. Check modkit stdout/stderr and bed filter.\")\n",
    "    summary_bam_df = pd.concat(parts, ignore_index=True)\n",
    "    summary_bam_df.to_csv(summary_table_name, sep=\"\\t\", index=False)\n",
    "\n",
    "# ───────── Compute per-BAM fractions (one row per exp_id × condition) ─────────\n",
    "# m6A\n",
    "m6a = (summary_bam_df.query(\"code == 'a'\")\n",
    "       .groupby(['exp_id','condition'], dropna=False)['pass_count']\n",
    "       .sum().reset_index(name='total_m6a'))\n",
    "A   = (summary_bam_df.query(\"base == 'A' and code == '-'\")\n",
    "       .groupby(['exp_id','condition'], dropna=False)['pass_count']\n",
    "       .sum().reset_index(name='total_A'))\n",
    "# 5mC\n",
    "mc5 = (summary_bam_df.query(\"code == 'm'\")\n",
    "       .groupby(['exp_id','condition'], dropna=False)['pass_count']\n",
    "       .sum().reset_index(name='total_5mc'))\n",
    "C   = (summary_bam_df.query(\"base == 'C' and code == '-'\")\n",
    "       .groupby(['exp_id','condition'], dropna=False)['pass_count']\n",
    "       .sum().reset_index(name='total_C'))\n",
    "\n",
    "df = (m6a.merge(A,   on=['exp_id','condition'], how='outer')\n",
    "          .merge(mc5, on=['exp_id','condition'], how='outer')\n",
    "          .merge(C,   on=['exp_id','condition'], how='outer')).fillna(0)\n",
    "\n",
    "# guard against zero denominators\n",
    "den_A = (df['total_A']  + df['total_m6a']).replace(0, np.nan)\n",
    "den_C = (df['total_C']  + df['total_5mc']).replace(0, np.nan)\n",
    "df['m6A_frac'] = df['total_m6a'] / den_A\n",
    "df['5mC_frac'] = df['total_5mc'] / den_C\n",
    "\n",
    "# ───────── Order conditions for x-axis ─────────\n",
    "order_priority = [\"endogenous\", \"N2\", \"SDC2\", \"SDC3\", \"DPY27\", \"DPY21\"]\n",
    "def _cond_key(cond):\n",
    "    cl = cond.lower()\n",
    "    for idx, kw in enumerate(order_priority):\n",
    "        if kw.lower() in cl:\n",
    "            return (idx, cond)\n",
    "    return (len(order_priority), cond)\n",
    "\n",
    "sorted_conds = sorted(df[\"condition\"].dropna().unique(), key=_cond_key)\n",
    "\n",
    "# ───────── Helpers: draw one marker per BAM; optional overlay of box summary ─────────\n",
    "def _scatter_points(fig, ycol, title, yrange=None, dtick=None):\n",
    "    for cond in sorted_conds:\n",
    "        sub = df[df[\"condition\"] == cond].copy()\n",
    "        if sub.empty:\n",
    "            continue\n",
    "        color = nanotools.get_color(cond)\n",
    "        # One marker per exp_id\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=[cond] * len(sub),\n",
    "            y=sub[ycol],\n",
    "            mode=\"markers+text\" if LABEL_EXPID else \"markers\",\n",
    "            marker=dict(color=color, size=POINT_SIZE, line=dict(width=0)),\n",
    "            text=sub[\"exp_id\"] if LABEL_EXPID else None,\n",
    "            textposition=\"top center\",\n",
    "            textfont=dict(size=EXP_LABEL_SIZE, color=\"black\"),\n",
    "            name=cond,\n",
    "            showlegend=False\n",
    "        ))\n",
    "        # Optional thin box for visual summary (transparent fill)\n",
    "        fig.add_trace(go.Box(\n",
    "            y=sub[ycol],\n",
    "            x=[cond]*len(sub),\n",
    "            name=cond,\n",
    "            boxpoints=False,\n",
    "            fillcolor=\"rgba(0,0,0,0)\",\n",
    "            line_color=color,\n",
    "            marker_color=color,\n",
    "            showlegend=False\n",
    "        ))\n",
    "\n",
    "    fig.update_xaxes(categoryorder=\"array\", categoryarray=sorted_conds)\n",
    "    fig.update_layout(\n",
    "        template=\"plotly_white\",\n",
    "        title=title,\n",
    "        xaxis_title=\"Condition\",\n",
    "        yaxis_title=\"Fraction\",\n",
    "        width=800,\n",
    "        height=700\n",
    "    )\n",
    "    fig.update_yaxes(tickformat=\".1%\", dtick=dtick, range=yrange)\n",
    "\n",
    "# ───────── Plots: one datapoint per BAM ─────────\n",
    "fig_m6A = go.Figure()\n",
    "_scatter_points(fig_m6A, \"m6A_frac\", \"Intergenic m6A Fraction by Condition\", yrange=[0, 0.12], dtick=0.02)\n",
    "fig_m6A.show()\n",
    "\n",
    "fig_5mC = go.Figure()\n",
    "_scatter_points(fig_5mC, \"5mC_frac\", \"Intergenic 5mC Fraction by Condition\")\n",
    "fig_5mC.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96dd0d2bb97f2e8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Compute physical genome coverage per exp_id_date by summing\n",
    "all mapped base‐counts via samtools stats, dividing by the\n",
    "reference genome size, and plotting results.\n",
    "Skips any experiment if its coverage file already exists,\n",
    "unless FORCE_REPLACE is True.\n",
    "Bars can be plotted either per-condition (summing multiple\n",
    "experiments) or per-exp_id_date, selected via PLOT_BY.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import logging\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from collections import defaultdict\n",
    "\n",
    "import nanotools  # <-- for get_colors()\n",
    "\n",
    "# ───────────────────────── Configuration ─────────────────────────\n",
    "METADATA_TSV    = \"/Data1/git/meyer-nanopore/scripts/bam_input_metadata_8_18_2025_COND_estim.txt\"\n",
    "REFERENCE_FA    = \"/Data1/reference/c_elegans.WS235.genomic.fa\"\n",
    "NUM_PROCESSES   = 32\n",
    "DEBUG_PROGRESS  = True\n",
    "DEBUG_SUMMARY   = True\n",
    "\n",
    "# If True, overwrite existing coverage files\n",
    "FORCE_REPLACE   = False\n",
    "\n",
    "# Plotting mode: choose \"condition\" (sum coverage per condition)\n",
    "# or \"exp_id_date\" (one bar per experiment)\n",
    "PLOT_BY = \"condition\"   # or \"exp_id_date\"\n",
    "assert PLOT_BY in (\"condition\", \"exp_id_date\"), \"PLOT_BY must be 'condition' or 'exp_id_date'\"\n",
    "\n",
    "# Directory to store outputs\n",
    "COVERAGE_DIR = os.path.join(os.getcwd(), \"coverage_calc\")\n",
    "os.makedirs(COVERAGE_DIR, exist_ok=True)\n",
    "\n",
    "# ─────────────────── Load & Filter Metadata ──────────────────────\n",
    "# `analysis_cond` must be defined elsewhere as your list of conditions\n",
    "input_metadata = pd.read_csv(METADATA_TSV, sep=\"\\t\", header=0)\n",
    "filtered = input_metadata[input_metadata[\"conditions\"].isin(analysis_cond)].copy()\n",
    "\n",
    "if DEBUG_SUMMARY:\n",
    "    print(f\"Total BAMs: {len(input_metadata)}  Filtered: {len(filtered)}\")\n",
    "    print(f\"Conditions: {filtered['conditions'].unique().tolist()}\")\n",
    "    print(f\"Experiments: {filtered['exp_id_date'].unique().tolist()}\")\n",
    "\n",
    "bam_files = filtered[\"bam_files\"].tolist()\n",
    "exp_ids   = filtered[\"exp_id_date\"].tolist()\n",
    "\n",
    "# ─────────────────── Compute Genome Size ──────────────────────\n",
    "genome_size = 0\n",
    "with open(REFERENCE_FA + \".fai\") as fai:\n",
    "    for line in fai:\n",
    "        genome_size += int(line.split()[1])\n",
    "if DEBUG_SUMMARY:\n",
    "    print(f\"Genome size: {genome_size:,} bases\")\n",
    "\n",
    "# ─────────────────── Helper Functions ──────────────────────\n",
    "def group_bams_by_exp(bam_list, exp_list):\n",
    "    groups = defaultdict(list)\n",
    "    for bam, exp in zip(bam_list, exp_list):\n",
    "        groups[exp].append(bam)\n",
    "    return groups\n",
    "\n",
    "def run_coverage(exp_id, bam_group):\n",
    "    out_txt = os.path.join(COVERAGE_DIR, f\"{exp_id}.coverage.txt\")\n",
    "    if os.path.exists(out_txt) and not FORCE_REPLACE:\n",
    "        logging.info(f\"[{exp_id}] exists – skipping\")\n",
    "        return\n",
    "    elif os.path.exists(out_txt):\n",
    "        logging.info(f\"[{exp_id}] exists but FORCE_REPLACE=True – overwriting\")\n",
    "\n",
    "    total_bases = 0\n",
    "    for bam in bam_group:\n",
    "        stats = subprocess.check_output([\"samtools\", \"stats\", bam], text=True)\n",
    "        for line in stats.splitlines():\n",
    "            if line.startswith(\"SN\") and \"total length:\" in line:\n",
    "                total_bases += int(line.split()[3])\n",
    "                break\n",
    "\n",
    "    # only boost coverage for bioreps\n",
    "    multiplier = 1.5 if \"biorep\" in exp_id else 1.0\n",
    "    coverage = total_bases / genome_size * multiplier\n",
    "\n",
    "    with open(out_txt, \"w\") as out:\n",
    "        out.write(\"coverage\\n\")\n",
    "        out.write(f\"{coverage:.6f}\\n\")\n",
    "\n",
    "    logging.info(f\"[{exp_id}] coverage={coverage:.3f}× (multiplier={multiplier})\")\n",
    "\n",
    "\n",
    "def compute_all_coverages():\n",
    "    groups = group_bams_by_exp(bam_files, exp_ids)\n",
    "    if DEBUG_SUMMARY:\n",
    "        logging.info(f\"Processing {len(groups)} experiments\")\n",
    "    items = list(groups.items())\n",
    "    with Pool(NUM_PROCESSES) as pool:\n",
    "        for _ in tqdm(pool.starmap(run_coverage, items),\n",
    "                      total=len(items),\n",
    "                      disable=not DEBUG_PROGRESS,\n",
    "                      desc=\"Coverage Jobs\"):\n",
    "            pass\n",
    "\n",
    "def plot_coverages():\n",
    "    # Map each exp_id_date back to its condition\n",
    "    cond_map = filtered.set_index(\"exp_id_date\")[\"conditions\"].to_dict()\n",
    "\n",
    "    records = []\n",
    "    for fn in os.listdir(COVERAGE_DIR):\n",
    "        if not fn.endswith(\".coverage.txt\"):\n",
    "            continue\n",
    "        exp = fn.replace(\".coverage.txt\", \"\")\n",
    "        cov = float(open(os.path.join(COVERAGE_DIR, fn)).read().split()[1])\n",
    "        records.append({\n",
    "            \"exp_id_date\": exp,\n",
    "            \"coverage\": cov,\n",
    "            \"condition\": cond_map.get(exp)\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "\n",
    "    if PLOT_BY == \"exp_id_date\":\n",
    "        # One bar per experiment, ordered by condition then exp_id_date\n",
    "        df[\"condition\"] = pd.Categorical(df[\"condition\"],\n",
    "                                         categories=analysis_cond,\n",
    "                                         ordered=True)\n",
    "        df.sort_values([\"condition\", \"exp_id_date\"], inplace=True)\n",
    "        x = df[\"exp_id_date\"]\n",
    "        y = df[\"coverage\"]\n",
    "        text = df[\"coverage\"].round(1).astype(str)\n",
    "        xaxis_title = \"Experiment ID Date\"\n",
    "    else:\n",
    "        # Sum coverage per condition\n",
    "        df[\"condition\"] = pd.Categorical(df[\"condition\"],\n",
    "                                         categories=analysis_cond,\n",
    "                                         ordered=True)\n",
    "        summed = (\n",
    "            df\n",
    "            .groupby(\"condition\", as_index=False)[\"coverage\"]\n",
    "            .sum()\n",
    "            .dropna(subset=[\"condition\"])\n",
    "        )\n",
    "        summed.sort_values(\"condition\", inplace=True)\n",
    "        x = summed[\"condition\"]\n",
    "        y = summed[\"coverage\"]\n",
    "        text = summed[\"coverage\"].round(1).astype(str)\n",
    "        xaxis_title = \"Condition\"\n",
    "\n",
    "    # get a color for each bar\n",
    "    colors = nanotools.get_colors(x.tolist())\n",
    "\n",
    "    fig = go.Figure(go.Bar(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        text=text,\n",
    "        textposition=\"outside\",\n",
    "        marker_color=colors\n",
    "    ))\n",
    "    fig.update_layout(\n",
    "        template=\"plotly_white\",\n",
    "        width=350,\n",
    "        height=500,\n",
    "        title=\"Physical Genome Coverage\",\n",
    "        xaxis_title=xaxis_title,\n",
    "        yaxis_title=\"Coverage (×)\",\n",
    "        showlegend=False\n",
    "    )\n",
    "    # set y axis range to 0-60\n",
    "    fig.update_yaxes(\n",
    "        range=[0, 62]\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "# ─────────────────────── Main Entry ───────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO if DEBUG_PROGRESS else logging.WARNING,\n",
    "        format=\"%(asctime)s %(levelname)s: %(message)s\"\n",
    "    )\n",
    "    compute_all_coverages()\n",
    "    plot_coverages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d0b4e686fc2c88",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-08-29T17:51:18.885458Z"
    },
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fimo_df before filter: 203047\n",
      "fimo_df after motif-specific ln(p) filter: 36853\n",
      "fimo_df after dedup: 34828\n"
     ]
    }
   ],
   "source": [
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Motif integration with configurable \"intergenic\" types and readable structure\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from intervaltree import IntervalTree\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Configuration knobs\n",
    "#   • INTERGENIC_TYPES / INTERGENIC_PATTERNS define rows that are NOT re-centered.\n",
    "#     Include already-centered motif rows like \"MEX_motif\" / \"MEXII_motif\" here.\n",
    "#   • combine_motifs keeps your original “MOTIFS_” prefix behavior.\n",
    "#   • include_nomotif_regions controls inclusion of no-overlap regions later.\n",
    "# Notes:\n",
    "#   • Variables like `bed_window` and `combined_bed_df_ext` are assumed defined earlier.\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Rows whose type should NOT be re-centered if any of these substrings appear (case-insensitive)\n",
    "INTERGENIC_CONTAINS = [\n",
    "    \"intergenic\", \"univ_nuc\", \"tss\", \"nfr\", \"mnase\",\n",
    "    \"MEX_motif\", \"MEXII_motif\"  # already centered → skip\n",
    "]\n",
    "\n",
    "center_on_motifs = True      # kept for readability; logic handled by intergenic mask below\n",
    "combine_motifs = True        # keep original behavior for labeling motif-centered categories\n",
    "include_noregion_motifs = False\n",
    "include_nomotif_regions = True\n",
    "\n",
    "# motif p-value ln thresholds\n",
    "LN_P_THRESHOLDS = {'MEX': -13, 'MEXII': -12, 'motifC': -9}\n",
    "\n",
    "# cluster neighborhood\n",
    "window_size = 500  # uses your prior convention\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Helper functions\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def build_interval_trees(df, chrom_col='chrom', start_col='bed_start', end_col='bed_end', payload_cols=('type','bed_start','bed_end')):\n",
    "    \"\"\"Interval trees from an interval table for overlap lookups.\"\"\"\n",
    "    trees = defaultdict(IntervalTree)\n",
    "    for _, r in df.iterrows():\n",
    "        trees[r[chrom_col]][int(r[start_col]):int(r[end_col])] = tuple(r[c] for c in payload_cols)\n",
    "    return trees\n",
    "\n",
    "def load_and_filter_fimo(fimo_paths):\n",
    "    \"\"\"Load FIMO TSVs, add ln(p), apply motif-specific ln(p) filters, and deduplicate overlaps by priority.\"\"\"\n",
    "    dfs = [pd.read_csv(p, sep='\\t', comment='#', skip_blank_lines=True) for p in fimo_paths]\n",
    "    fimo = pd.concat(dfs, ignore_index=True)\n",
    "    print(\"fimo_df before filter:\", len(fimo))\n",
    "\n",
    "    # motif lengths (kept for transparency; not strictly required by centering here)\n",
    "    fimo['motif_length'] = fimo['stop'] - fimo['start']\n",
    "\n",
    "    # ln(p) and per-motif thresholds\n",
    "    fimo['natural_log_p_value'] = np.log(fimo['p-value'])\n",
    "    mask_mex   = (fimo['motif_id'] == 'MEX')    & (fimo['natural_log_p_value'] <= LN_P_THRESHOLDS['MEX'])\n",
    "    mask_mexii = (fimo['motif_id'] == 'MEXII')  & (fimo['natural_log_p_value'] <= LN_P_THRESHOLDS['MEXII'])\n",
    "    mask_mc    = (fimo['motif_id'] == 'motifC') & (fimo['natural_log_p_value'] <= LN_P_THRESHOLDS['motifC'])\n",
    "    fimo = fimo[mask_mex | mask_mexii | mask_mc].copy()\n",
    "    print(\"fimo_df after motif-specific ln(p) filter:\", len(fimo))\n",
    "\n",
    "    # Deduplicate overlaps within chrom by motif priority then score\n",
    "    motif_priority = {'MEXII': 2, 'MEX': 1, 'motifC': 3}\n",
    "    fimo['motif_priority'] = fimo['motif_id'].map(motif_priority)\n",
    "    fimo = fimo.dropna(subset=['motif_priority']).copy()\n",
    "    fimo['motif_priority'] = fimo['motif_priority'].astype(int)\n",
    "    fimo = fimo.sort_values(\n",
    "        by=['sequence_name', 'start', 'motif_priority', 'score'],\n",
    "        ascending=[True, True, True, False]\n",
    "    )\n",
    "    chrom_trees = defaultdict(IntervalTree)\n",
    "    keep_idx = set()\n",
    "    for idx, row in fimo.iterrows():\n",
    "        chrom, st, en, pri = row['sequence_name'], int(row['start']), int(row['stop']), int(row['motif_priority'])\n",
    "        overlaps = chrom_trees[chrom][st:en]\n",
    "        if not overlaps:\n",
    "            chrom_trees[chrom][st:en] = (pri, idx)\n",
    "            keep_idx.add(idx)\n",
    "        else:\n",
    "            replaced = False\n",
    "            for iv in overlaps:\n",
    "                e_pri, e_idx = iv.data\n",
    "                # overlapping, keep the smaller priority\n",
    "                if st < iv.end and en > iv.begin:\n",
    "                    if pri < e_pri:\n",
    "                        chrom_trees[chrom].remove(iv)\n",
    "                        chrom_trees[chrom][st:en] = (pri, idx)\n",
    "                        if e_idx in keep_idx:\n",
    "                            keep_idx.remove(e_idx)\n",
    "                        keep_idx.add(idx)\n",
    "                        replaced = True\n",
    "            if not replaced:\n",
    "                continue\n",
    "    fimo = fimo.loc[keep_idx].copy()\n",
    "\n",
    "    # Rename cols and harmonize chromosome naming\n",
    "    fimo = fimo[['sequence_name', 'start', 'stop', 'strand', 'score', 'p-value', 'motif_id', 'natural_log_p_value']]\n",
    "    fimo = fimo.rename(columns={'sequence_name': 'chr', 'p-value': 'p_value'})\n",
    "    fimo['chr'] = fimo['chr'].str.replace('chr', 'CHROMOSOME_', regex=False)\n",
    "\n",
    "    # Stable row id\n",
    "    fimo.reset_index(drop=True, inplace=True)\n",
    "    fimo['id'] = fimo.index\n",
    "    print(\"fimo_df after dedup:\", len(fimo))\n",
    "    return fimo\n",
    "\n",
    "def expand_fimo_to_regions(fimo_df, trees):\n",
    "    \"\"\"Explode FIMO rows onto overlapping combined_bed_df_ext regions; keep motif coords when none.\"\"\"\n",
    "    out = []\n",
    "    for _, r in fimo_df.iterrows():\n",
    "        chrom, st, en = r['chr'], int(r['start']), int(r['stop'])\n",
    "        overlaps = trees.get(chrom, IntervalTree())[st:en]\n",
    "        if overlaps:\n",
    "            for iv in overlaps:\n",
    "                cat, bed_st, bed_en = iv.data\n",
    "                rr = r.copy()\n",
    "                rr['chip_category'] = cat\n",
    "                rr['bed_start'] = int(bed_st)\n",
    "                rr['bed_end'] = int(bed_en)\n",
    "                out.append(rr)\n",
    "        else:\n",
    "            rr = r.copy()\n",
    "            rr['chip_category'] = 'none'\n",
    "            rr['bed_start'] = st\n",
    "            rr['bed_end'] = en\n",
    "            out.append(rr)\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "def collapse_best_motif_per_region(expanded_df):\n",
    "    \"\"\"Per (chr, bed_start, bed_end) keep the motif row with the smallest p_value; keep all 'none'.\"\"\"\n",
    "    motif_mask = expanded_df['chip_category'] != 'none'\n",
    "    best = (\n",
    "        expanded_df[motif_mask]\n",
    "        .sort_values('p_value', ascending=True)\n",
    "        .drop_duplicates(subset=['chr', 'bed_start', 'bed_end'], keep='first')\n",
    "    )\n",
    "    no_motif = expanded_df[~motif_mask]\n",
    "    return pd.concat([best, no_motif], ignore_index=True)\n",
    "\n",
    "def compute_cluster_counts(fimo_df, win):\n",
    "    \"\"\"Compute cluster_count per motif using a ±win window of other motifs.\"\"\"\n",
    "    trees = defaultdict(IntervalTree)\n",
    "    for _, r in fimo_df.iterrows():\n",
    "        trees[r['chr']][int(r['start']):int(r['stop'])] = r['id']\n",
    "\n",
    "    def _count(row):\n",
    "        chrom, st, en = row['chr'], int(row['start']), int(row['stop'])\n",
    "        return len(trees.get(chrom, IntervalTree())[st - win: en + win])\n",
    "\n",
    "    fimo_df = fimo_df.copy()\n",
    "    fimo_df['cluster_count'] = fimo_df.apply(_count, axis=1)\n",
    "    return fimo_df\n",
    "\n",
    "def select_nonredundant_positions(fimo_df, win):\n",
    "    \"\"\"Greedy non-redundant selection by start coordinate with spacing >= win.\"\"\"\n",
    "    fimo_df = fimo_df.sort_values(by=['chr', 'natural_log_p_value'], ascending=[True, True]).copy()\n",
    "    keep = []\n",
    "    last_by_chr = defaultdict(list)\n",
    "    for chr_name, grp in fimo_df.groupby('chr', sort=False):\n",
    "        chosen_starts = []\n",
    "        for i, r in grp.iterrows():\n",
    "            st = int(r['start'])\n",
    "            if not any(abs(st - cs) < win for cs in chosen_starts):\n",
    "                chosen_starts.append(st)\n",
    "                keep.append(i)\n",
    "        last_by_chr[chr_name] = chosen_starts\n",
    "    return fimo_df.loc[keep].sort_values(by=['chr', 'start']).copy()\n",
    "\n",
    "def add_missing_nomotif_regions(expanded_df, combined_bed_df_ext):\n",
    "    \"\"\"Optionally add regions that had no motif overlap to ensure full coverage by type.\"\"\"\n",
    "    # existing (bed_start, type) pairs in expanded\n",
    "    existing_pairs = set(zip(expanded_df['bed_start'], expanded_df['chip_category']))\n",
    "    # all pairs from combined_bed_df_ext\n",
    "    all_pairs = set(zip(combined_bed_df_ext['bed_start'], combined_bed_df_ext['type']))\n",
    "    missing_pairs = all_pairs - existing_pairs\n",
    "\n",
    "    miss = combined_bed_df_ext[\n",
    "        combined_bed_df_ext.apply(lambda r: (r['bed_start'], r['type']) in missing_pairs, axis=1)\n",
    "    ].copy()\n",
    "    if miss.empty:\n",
    "        return expanded_df\n",
    "\n",
    "    miss['midpoint'] = (miss['bed_start'] + miss['bed_end']) // 2\n",
    "    miss['chr'] = miss['chrom'].str.replace('chr', 'CHROMOSOME_', regex=False)\n",
    "    miss['start'] = miss['midpoint']\n",
    "    miss['stop'] = miss['midpoint']\n",
    "    miss['chip_category'] = miss['type']\n",
    "    miss['strand'] = '.'\n",
    "    miss['score'] = 0\n",
    "    miss['p_value'] = 1.0\n",
    "    miss['motif_id'] = 'noregion'\n",
    "    miss['id'] = range(expanded_df['id'].max() + 1 if not expanded_df.empty else 0,\n",
    "                       (expanded_df['id'].max() + 1 if not expanded_df.empty else 0) + len(miss))\n",
    "    miss['cluster_count'] = 0\n",
    "\n",
    "    keep_cols = ['chr','start','stop','strand','score','p_value','motif_id','id',\n",
    "                 'chip_category','bed_start','bed_end','cluster_count']\n",
    "    # fill bed_start/bed_end for consistency\n",
    "    miss['bed_start'] = miss['start']\n",
    "    miss['bed_end'] = miss['stop']\n",
    "    miss = miss[keep_cols]\n",
    "    return pd.concat([expanded_df, miss], ignore_index=True)\n",
    "\n",
    "def recenter_motif_rows(expanded_df, bed_window, combine_motifs=True):\n",
    "    \"\"\"\n",
    "    Rebuild ABSOLUTE windows for ALL rows:\n",
    "      • Motif rows → center = motif first base (expanded_df['start'])\n",
    "      • Intergenic rows → center = midpoint of original region ((bed_start+bed_end)//2)\n",
    "    Then: bed_start = max(center - W, 0); bed_end = bed_start + 2*W + 1.\n",
    "    Labels:\n",
    "      • If combine_motifs: prefix non-intergenic with 'MOTIFS_'.\n",
    "      • Else: '<motif_id>_<chip_category>' for non-intergenic.\n",
    "    \"\"\"\n",
    "    df = expanded_df.copy()\n",
    "\n",
    "    patt = \"|\".join(map(re.escape, INTERGENIC_CONTAINS))\n",
    "    inter_mask = df['chip_category'].astype(str).str.contains(patt, case=False, na=False)\n",
    "    non_inter_mask = ~inter_mask\n",
    "\n",
    "    # centers\n",
    "    center = pd.Series(np.nan, index=df.index, dtype='float64')\n",
    "    center.loc[non_inter_mask] = df.loc[non_inter_mask, 'start'].astype('float64')\n",
    "    # midpoint for intergenic rows\n",
    "    mid = ((df['bed_start'].astype('float64') + df['bed_end'].astype('float64')) / 2.0).floordiv(1.0)\n",
    "    center.loc[inter_mask] = mid.loc[inter_mask].astype('float64')\n",
    "\n",
    "    # rebuild absolute windows\n",
    "    left  = np.clip(center - bed_window, 0, None)\n",
    "    right = left + (2 * bed_window) + 1  # end is exclusive for BED-like windows you use\n",
    "\n",
    "    # apply\n",
    "    df['bed_start'] = left.astype('int64')\n",
    "    df['bed_end']   = right.astype('int64')\n",
    "\n",
    "    # labeling\n",
    "    if combine_motifs:\n",
    "        df.loc[non_inter_mask, 'chip_category'] = 'MOTIFS_' + df.loc[non_inter_mask, 'chip_category'].astype(str)\n",
    "    else:\n",
    "        df.loc[non_inter_mask, 'chip_category'] = (\n",
    "            df.loc[non_inter_mask, 'motif_id'].astype(str) + \"_\" + df.loc[non_inter_mask, 'chip_category'].astype(str)\n",
    "        )\n",
    "\n",
    "    if not include_noregion_motifs:\n",
    "        df = df[~df['chip_category'].str.contains('none', case=False, na=False)].copy()\n",
    "\n",
    "    # build outputs\n",
    "    combined_cat = df[['chr','bed_start','bed_end','strand','chip_category']].copy().rename(\n",
    "        columns={'chr':'chrom','strand':'bed_strand','chip_category':'type'}\n",
    "    )\n",
    "    combined_cat['chr_type'] = combined_cat['chrom'].apply(lambda x: 'X' if x == 'CHROMOSOME_X' else 'Autosome')\n",
    "\n",
    "    clust = df[['chr','strand','cluster_count']].copy().rename(\n",
    "        columns={'chr':'chrom','strand':'bed_strand'}\n",
    "    )\n",
    "    clust['bed_start'] = combined_cat['bed_start'].values\n",
    "    clust['bed_end']   = combined_cat['bed_end'].values\n",
    "    clust['type'] = 'clust_' + df['cluster_count'].astype(str).values\n",
    "    clust['chr_type'] = clust['chrom'].apply(lambda x: 'X' if x == 'CHROMOSOME_X' else 'Autosome')\n",
    "    clust = clust.drop(columns='cluster_count')\n",
    "\n",
    "    return df, combined_cat, clust\n",
    "\n",
    "def finalize_mex_bins(cat_df):\n",
    "    \"\"\"Collapse MEX_D bins to combined labels to match your existing downstream behavior.\"\"\"\n",
    "    repl = {\n",
    "        'MEX_D1': 'MEX_D1to5', 'MEX_D2': 'MEX_D1to5', 'MEX_D3': 'MEX_D1to5',\n",
    "        'MEX_D4': 'MEX_D1to5', 'MEX_D5': 'MEX_D1to5',\n",
    "        'MEX_D6': 'MEX_D6to9', 'MEX_D7': 'MEX_D6to9', 'MEX_D8': 'MEX_D6to9', 'MEX_D9': 'MEX_D6to9'\n",
    "    }\n",
    "    out = cat_df.copy()\n",
    "    out['type'] = out['type'].replace(repl)\n",
    "    return out\n",
    "\n",
    "def create_modkit_bed_df(filtered_df):\n",
    "    \"\"\"\n",
    "    Build plus/minus BED for modkit from bed_start/bed_end.\n",
    "    Assumes these now encode center±W for ALL rows.\n",
    "    \"\"\"\n",
    "    print(\"rows in combined_bed_df:\", len(filtered_df))\n",
    "    df = filtered_df.copy()\n",
    "    df['bed_strand'] = '+'\n",
    "    bed = pd.DataFrame({\n",
    "        0: df['chrom'],\n",
    "        1: df['bed_start'].astype(int),\n",
    "        2: df['bed_end'].astype(int),\n",
    "        3: '.',\n",
    "        4: '.',\n",
    "        5: df['bed_strand'],\n",
    "    })\n",
    "    bed_minus = bed.copy()\n",
    "    bed_minus[5] = '-'\n",
    "    bed = pd.concat([bed, bed_minus], ignore_index=True)\n",
    "    bed.columns = range(bed.shape[1])\n",
    "    return bed.drop_duplicates()\n",
    "\n",
    "def save_modkit_bed_to_temp(modkit_bed_df, filename):\n",
    "    temp_dir = \"/Data1/git/meyer-nanopore/scripts/analysis/temp_files/\"\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "    path = os.path.join(temp_dir, filename)\n",
    "    modkit_bed_df.to_csv(path, sep='\\t', header=False, index=False)\n",
    "    print(f\"Modkit BED file saved to: {path}\")\n",
    "    return path\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 1) Load + filter FIMO\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "fimo_files = [\n",
    "    \"/Data1/ext_data/motifs/fimo_MEX_0.01.tsv\",\n",
    "    \"/Data1/ext_data/motifs/fimo_MEXII_0.01.tsv\",\n",
    "    \"/Data1/ext_data/motifs/fimo_motifc_0.01.tsv\"\n",
    "]\n",
    "fimo_df = load_and_filter_fimo(fimo_files)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 2) Expand motifs onto regions and collapse to best per region\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "region_trees = build_interval_trees(\n",
    "    combined_bed_df_ext,\n",
    "    chrom_col='chrom', start_col='bed_start', end_col='bed_end',\n",
    "    payload_cols=('type','bed_start','bed_end')\n",
    ")\n",
    "fimo_expanded_df = expand_fimo_to_regions(fimo_df, region_trees)\n",
    "fimo_expanded_df = collapse_best_motif_per_region(fimo_expanded_df)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 3) Cluster counts and non-redundant selection\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "fimo_df_cc = compute_cluster_counts(fimo_df[['chr','start','stop','id','natural_log_p_value']].copy(), window_size)\n",
    "fimo_df_cc = select_nonredundant_positions(fimo_df_cc, window_size)\n",
    "\n",
    "# merge cluster_count back to expanded rows by id\n",
    "fimo_expanded_df = fimo_expanded_df.merge(\n",
    "    fimo_df_cc[['id','cluster_count']],\n",
    "    on='id', how='inner'\n",
    ")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 4) Optionally add missing region types that had no motif overlaps\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "if include_nomotif_regions:\n",
    "    fimo_expanded_df = add_missing_nomotif_regions(fimo_expanded_df, combined_bed_df_ext)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 5) Recenter only NON-intergenic rows around the motif; intergenic rows unchanged\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "fimo_expanded_df, combined_bed_df_mex_cat, combined_bed_df_mex_clust = recenter_motif_rows(\n",
    "    fimo_expanded_df, bed_window=bed_window, combine_motifs=combine_motifs\n",
    ")\n",
    "\n",
    "# collapse MEX bins as in your original code\n",
    "combined_bed_df_mex_cat = finalize_mex_bins(combined_bed_df_mex_cat)\n",
    "\n",
    "print(\"\\ncombined_bed_df_mex_clust:\")\n",
    "print(combined_bed_df_mex_clust.head())\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 6) Final combined_bed_df and optional MEX_none handling\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "combined_bed_df = combined_bed_df_mex_cat.copy()\n",
    "\n",
    "if include_noregion_motifs:\n",
    "    non_mex_none = combined_bed_df[combined_bed_df['type'] != 'MEX_none']\n",
    "    mex_none = combined_bed_df[combined_bed_df['type'] == 'MEX_none'].sample(\n",
    "        n=min(100, len(combined_bed_df[combined_bed_df['type'] == 'MEX_none'])), random_state=42\n",
    "    )\n",
    "    combined_bed_df = pd.concat([non_mex_none, mex_none], ignore_index=True)\n",
    "else:\n",
    "    combined_bed_df = combined_bed_df[combined_bed_df['type'] != 'MEX_none']\n",
    "\n",
    "print(\"combined_bed_df:\")\n",
    "print(combined_bed_df.head())\n",
    "print(\"Count by type:\\n\", combined_bed_df['type'].value_counts())\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 7) Cluster bar plot (unchanged behavior)\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "category_stats = (\n",
    "    fimo_expanded_df.groupby('chip_category')['cluster_count']\n",
    "    .agg(['mean','count']).reset_index()\n",
    "    .sort_values(by='mean', ascending=False)\n",
    ")\n",
    "category_stats['label'] = category_stats.apply(\n",
    "    lambda x: f\"{x['chip_category']}\\n(n={int(x['count'])})\", axis=1\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='label', y='mean', data=category_stats, palette='viridis')\n",
    "plt.title('Average Cluster Count by Chip Category')\n",
    "plt.xlabel('Chip Category')\n",
    "plt.ylabel('Average Cluster Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 8) Build and save modkit BED\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "modkit_bed_df = create_modkit_bed_df(combined_bed_df)\n",
    "modkit_bed_name = \"modkit_temp.bed\"\n",
    "temp_file_path = save_modkit_bed_to_temp(modkit_bed_df, modkit_bed_name)\n",
    "\n",
    "print(modkit_bed_df)\n",
    "print(\"len(modkit_bed_df):\", len(modkit_bed_df))\n",
    "\n",
    "\n",
    "expected = 2 * bed_window + 1\n",
    "for name, t in [(\"combined_bed_df_mex_cat\", combined_bed_df_mex_cat),\n",
    "                (\"combined_bed_df_mex_clust\", combined_bed_df_mex_clust)]:\n",
    "    widths = (t['bed_end'] - t['bed_start']).unique()\n",
    "    bad = widths[widths != expected] if hasattr(widths, \"__iter__\") else []\n",
    "    if len(bad):\n",
    "        print(f\"[WARN] {name} has non-standard window widths: {widths}\")\n",
    "    else:\n",
    "        print(f\"[OK] {name} window width == {expected} everywhere.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2629edda0164bc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE FOR PLOTTING PILEUP\n",
    "\n",
    "regenerate_bit = True # SEt to true to force regenerate, otherwise load if available.\n",
    "\n",
    "### Generate modkit pileup file, used for plotting m6A/A in a given region.\n",
    "# Generating the list of output_file_names based on the given structure\n",
    "out_file_names = [output_stem + \"modkit-pileup-\" + each_condition +\"_\"+ str(round(each_thresh,2))+\"_\"+str(each_index)+\"_\"+str(each_bamfrac)+ \"_\".join([each_type[-5:] for each_type in type_selected[-3:]]) + \"_\".join([each_type[0:5] for each_type in type_selected[-3]]) + str(bed_window)+\".bed\" for each_condition,each_thresh,each_index, each_bamfrac in zip(conditions,thresh_list,sample_indices,bam_fracs)]\n",
    "\n",
    "# Function to run a single command\n",
    "def modkit_pileup_extract(args, index):\n",
    "    each_bam, each_thresh, each_condition, each_index, each_bamfrac, each_type,modkit_path, output_stem, modkit_bed_name = args\n",
    "\n",
    "    # Use the index to get the correct file name from out_file_names\n",
    "    each_output = out_file_names[index]\n",
    "\n",
    "    # Check if the output file exists\n",
    "    if not regenerate_bit:\n",
    "        if os.path.exists(each_output):\n",
    "            print(f\"File already exists: {each_output}\")\n",
    "            # Read in output file and check if empty\n",
    "            modkit_qc = pd.DataFrame()\n",
    "            try:\n",
    "                modkit_qc = pd.read_csv(each_output, sep=\"\\t\", header=None, nrows=10)\n",
    "            except:\n",
    "                if modkit_qc.empty:\n",
    "                    print(f\"File is empty: {each_output}\")\n",
    "                    return\n",
    "            return\n",
    "    print(f\"Starting on: {each_output}\", \"with bam file: \", each_bam,\"and bedfile:\", modkit_bed_name)\n",
    "    command = [\n",
    "        modkit_path,\n",
    "        \"pileup\",\n",
    "        \"--only-tabs\",\n",
    "        #\"--ignore\",\n",
    "        #\"m\",\n",
    "        \"--threads\",\n",
    "        \"10\",\n",
    "        #\"--filter-threshold\",\n",
    "        #f\"A:{1-each_thresh}\",\n",
    "        #f\"A:{1-each_thresh}\",\n",
    "        \"--mod-thresholds\",\n",
    "        f\"a:{each_thresh}\",\n",
    "        \"--mod-thresholds\",\n",
    "        f\"m:{each_thresh}\",\n",
    "        \"--ref\",\n",
    "        \"/Data1/reference/c_elegans.WS235.genomic.fa\",\n",
    "        \"--filter-threshold\",\n",
    "        f\"A:{0.8}\",\n",
    "        \"--filter-threshold\",\n",
    "        f\"C:{0.8}\",\n",
    "        \"--motif\",\n",
    "        \"GC\",\n",
    "        \"1\",\n",
    "        \"--motif\",\n",
    "        \"A\",\n",
    "        \"0\",\n",
    "        # \"--max-depth\",\n",
    "        # \"100\",\n",
    "        \"--log-filepath\",\n",
    "        output_stem + each_condition + str(each_index) + \"_modkit-pileup.log\",\n",
    "        \"--include-bed\",\n",
    "        modkit_bed_name,\n",
    "        each_bam,\n",
    "        each_output\n",
    "    ]\n",
    "    subprocess.run(command, text=True)\n",
    "\n",
    "\n",
    "print(\"modkit_bed_name\",modkit_bed_name)\n",
    "print(\"temp_file_path\",temp_file_path)\n",
    "# Now you need to adjust the task_args to include the index\n",
    "# Instead of directly zipping, enumerate one of the lists to get the index\n",
    "task_args_with_index = [(args, index) for index, args in enumerate(zip(\n",
    "    new_bam_files,\n",
    "    thresh_list,\n",
    "    conditions,\n",
    "    sample_indices,\n",
    "    bam_fracs,\n",
    "    [type_selected]*len(new_bam_files),\n",
    "    [modkit_path]*len(new_bam_files), #modkit_path or temp_file_path\n",
    "    [output_stem]*len(new_bam_files),\n",
    "    [temp_file_path]*len(new_bam_files), # modkit_bed_name or temp_file_path\n",
    "))]\n",
    "\n",
    "# Execute commands in parallel, unpacking the arguments and index within the map call\n",
    "with Pool(\n",
    "    processes=16\n",
    ") as pool:\n",
    "    pool.starmap(modkit_pileup_extract, task_args_with_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0adb65ea3bcb22a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### PARALLELIZE BED INFO ADDING\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from tqdm import tqdm  # for progress monitoring\n",
    "\n",
    "def add_bed_columns_no_loops(bedmethyl_df_loc, combined_bed_df):\n",
    "    \"\"\"\n",
    "    Merges bedmethyl_df_loc with combined_bed_df based on the nearest midpoint\n",
    "    and then filters out rows that fall outside the bed_start/bed_end range.\n",
    "    \"\"\"\n",
    "    # Calculate midpoint in combined_bed_df\n",
    "    combined_bed_df['midpoint'] = (combined_bed_df['bed_start'] + combined_bed_df['bed_end']) / 2\n",
    "    combined_bed_df['midpoint'] = combined_bed_df['midpoint'].astype(int)\n",
    "    combined_bed_df = combined_bed_df.sort_values(by='midpoint')\n",
    "\n",
    "    bedmethyl_df_loc['start_position'] = bedmethyl_df_loc['start_position'].astype(int)\n",
    "\n",
    "    merged_df = pd.merge_asof(\n",
    "        bedmethyl_df_loc.sort_values('start_position'),\n",
    "        combined_bed_df,\n",
    "        by='chrom',\n",
    "        left_on='start_position',\n",
    "        right_on='midpoint',\n",
    "        direction='nearest'\n",
    "    )\n",
    "\n",
    "    # Filter out rows where the start_position is not within [bed_start, bed_end]\n",
    "    merged_df = merged_df.loc[\n",
    "        (merged_df['start_position'] >= merged_df['bed_start']) &\n",
    "        (merged_df['start_position'] <= merged_df['bed_end'])\n",
    "    ]\n",
    "    merged_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    final_df = pd.merge(\n",
    "        bedmethyl_df_loc,\n",
    "        merged_df[['chrom', 'start_position', 'bed_start', 'bed_end', 'bed_strand', 'type', 'chr_type']],\n",
    "        on=['chrom', 'start_position'],\n",
    "        how='left'\n",
    "    )\n",
    "    final_df = final_df[final_df['type'].notna()]\n",
    "    return final_df\n",
    "\n",
    "def process_group(group_tuple, num_bins, edge_window_size, sum_columns):\n",
    "    \"\"\"\n",
    "    Applies logic to shift 'rel_start' for each group.\n",
    "    \"\"\"\n",
    "    _, group = group_tuple\n",
    "    min_pos, max_pos = group['start_position'].min(), group['start_position'].max()\n",
    "    bed_start, bed_end = group['bed_start'].iloc[0], group['bed_end'].iloc[0]\n",
    "\n",
    "    # Initialize rel_start using edge conditions\n",
    "    group['rel_start'] = np.where(\n",
    "        group['start_position'] < bed_start + edge_window_size,\n",
    "        group['start_position'] - bed_start - edge_window_size,\n",
    "        np.where(\n",
    "            group['start_position'] > bed_end - edge_window_size,\n",
    "            num_bins + edge_window_size - (bed_end - group['start_position']),\n",
    "            100000  # sentinel placeholder\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Remove points outside the “main” window if the region is large\n",
    "    if (max_pos - min_pos) > (num_bins + 2 * edge_window_size):\n",
    "        binning_mask = (\n",
    "            (group['start_position'] >= bed_start + edge_window_size) &\n",
    "            (group['start_position'] <= bed_end - edge_window_size)\n",
    "        )\n",
    "        bin_edges = np.linspace(bed_start + edge_window_size, bed_end - edge_window_size, num_bins + 1)\n",
    "        group.loc[binning_mask, 'rel_start'] = np.digitize(\n",
    "            group.loc[binning_mask, 'start_position'],\n",
    "            bins=bin_edges,\n",
    "            right=True\n",
    "        )\n",
    "\n",
    "    return group\n",
    "\n",
    "def map_to_metagene_bins_and_sum(df, num_bins=1000, edge_window_size=500):\n",
    "    \"\"\"\n",
    "    Groups df by [bed_start, chrom, modified_base_code], shifts 'rel_start',\n",
    "    and sums relevant columns. This version runs in serial by default to avoid\n",
    "    nested Pools.\n",
    "    \"\"\"\n",
    "    sum_columns = ['Nmod', 'Ncanonical', 'Nother_mod', 'Ndelete', 'Nfail', 'Ndiff', 'Nnocall','Nvalid_cov']\n",
    "    retain_columns = ['bed_strand', 'chr_type', 'strand', 'bed_end','type']\n",
    "    group_columns = ['bed_start', 'chrom', 'modified_base_code']\n",
    "\n",
    "    groups = list(df.groupby(group_columns))\n",
    "\n",
    "    # Process each group in serial (no nested Pool)\n",
    "    processed_groups = [\n",
    "        process_group(g, num_bins, edge_window_size, sum_columns)\n",
    "        for g in groups\n",
    "    ]\n",
    "\n",
    "    result_df = pd.concat(processed_groups, ignore_index=True)\n",
    "\n",
    "    # Summation step\n",
    "    sum_group_columns = group_columns + ['rel_start']\n",
    "    summed_df = result_df.groupby(sum_group_columns)[sum_columns].sum().reset_index()\n",
    "\n",
    "    merged_df = pd.merge(\n",
    "        result_df[sum_group_columns + retain_columns].drop_duplicates(),\n",
    "        summed_df,\n",
    "        on=sum_group_columns,\n",
    "        how='left'\n",
    "    )\n",
    "    return merged_df\n",
    "\n",
    "def process_one_file(args):\n",
    "    \"\"\"\n",
    "    Worker function that:\n",
    "      1. Reads a bedmethyl file.\n",
    "      2. Filters rows.\n",
    "      3. Merges with combined_bed_df.\n",
    "      4. Optionally applies metagene binning if 'gene'/'damID' in type_selected.\n",
    "      5. Returns the final DataFrame (or empty if no data).\n",
    "    \"\"\"\n",
    "    (\n",
    "        each_output,\n",
    "        each_condition,\n",
    "        each_exp_id,\n",
    "        combined_bed_df,\n",
    "        type_selected,\n",
    "        num_bins,\n",
    "        bed_window\n",
    "    ) = args\n",
    "\n",
    "    # Columns to read\n",
    "    bedmethyl_cols = [\n",
    "        'chrom','start_position','end_position','modified_base_code','score','strand',\n",
    "        'start_position_compat','end_position_compat','color','Nvalid_cov','fraction_modified',\n",
    "        'Nmod','Ncanonical','Nother_mod','Ndelete','Nfail','Ndiff','Nnocall'\n",
    "    ]\n",
    "    # Read the bedmethyl file (assumes file already exists)\n",
    "    bedmethyl_df = pd.read_csv(each_output, sep=\"\\t\", header=None, names=bedmethyl_cols)\n",
    "\n",
    "    # Keep rows with specified modified_base_code\n",
    "    bedmethyl_df = bedmethyl_df[\n",
    "        bedmethyl_df['modified_base_code'].isin(['a,A,0','m,GC,1'])\n",
    "    ]\n",
    "    if bedmethyl_df.empty:\n",
    "        # If no rows remain, return empty\n",
    "        print(f\"Empty CSV or no relevant rows in: {each_output}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Sort and drop duplicates\n",
    "    bedmethyl_df.sort_values(['start_position'], inplace=True)\n",
    "    bedmethyl_df.dropna(inplace=True)\n",
    "    bedmethyl_df.drop_duplicates(inplace=True)\n",
    "    bedmethyl_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Merge with combined_bed_df\n",
    "    bedmethyl_df = add_bed_columns_no_loops(bedmethyl_df, combined_bed_df)\n",
    "    if bedmethyl_df.empty:\n",
    "        print(f\"No overlapping intervals in: {each_output}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # If 'gene' or 'damID' in type_selected, map to metagene bins\n",
    "    if any(x in type_selected[0] for x in ['gene','damID']):\n",
    "        bedmethyl_df = map_to_metagene_bins_and_sum(\n",
    "            bedmethyl_df,\n",
    "            num_bins=num_bins,\n",
    "            edge_window_size=bed_window\n",
    "        )\n",
    "    else:\n",
    "        # Otherwise just compute rel_start\n",
    "        bedmethyl_df['rel_start'] = (\n",
    "            bedmethyl_df['start_position']\n",
    "            - bedmethyl_df['bed_start']\n",
    "            - bed_window\n",
    "            + 1\n",
    "        )\n",
    "\n",
    "    # Convert rel_start to int\n",
    "    bedmethyl_df['rel_start'] = bedmethyl_df['rel_start'].astype(int)\n",
    "    bedmethyl_df['condition'] = each_condition\n",
    "    bedmethyl_df['exp_id'] = each_exp_id\n",
    "\n",
    "    return bedmethyl_df\n",
    "\n",
    "def parallel_build_combined_df(\n",
    "    out_file_names,\n",
    "    conditions,\n",
    "    exp_ids,\n",
    "    combined_bed_df,\n",
    "    type_selected,\n",
    "    num_bins=1000,\n",
    "    bed_window=500\n",
    "):\n",
    "    \"\"\"\n",
    "    1. Constructs argument list for each bedmethyl file.\n",
    "    2. Creates a Pool (multiprocessing).\n",
    "    3. Processes each file in parallel with progress tracking.\n",
    "    4. Concatenates non-empty results into one DataFrame.\n",
    "    \"\"\"\n",
    "    args_list = [\n",
    "        (f, cond, exp, combined_bed_df, type_selected, num_bins, bed_window)\n",
    "        for (f, cond, exp) in zip(out_file_names, conditions, exp_ids)\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "    with Pool(cpu_count()) as pool:\n",
    "        for res in tqdm(pool.imap(process_one_file, args_list), total=len(args_list)):\n",
    "            if not res.empty:\n",
    "                results.append(res)\n",
    "\n",
    "    if results:\n",
    "        return pd.concat(results, ignore_index=True)\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# ------------------------------------------\n",
    "# Example usage (omit or adapt in your notebook):\n",
    "#\n",
    "# out_file_names = [...]\n",
    "# conditions = [...]\n",
    "# exp_ids = [...]\n",
    "# combined_bed_df = [...]  # Large DataFrame\n",
    "# type_selected = [...]\n",
    "# num_bins = 1000\n",
    "# bed_window = 500\n",
    "\n",
    "comb_bedmethyl_df = parallel_build_combined_df(\n",
    "    out_file_names,\n",
    "    conditions,\n",
    "    exp_ids,\n",
    "    combined_bed_df,\n",
    "    type_selected,\n",
    "    num_bins,\n",
    "    bed_window\n",
    ")\n",
    "\n",
    "display(comb_bedmethyl_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942caa9a5d8d5026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Define normalization type: 'global' for exp_id_m6A_frac or 'local' for intergenic mod_frac\n",
    "normalization_type = 'local'  # Change to 'local' to use intergenic normalization, relative to genome wide 'global'\n",
    "combine_replicates = False\n",
    "\n",
    "### SHIFT AND TRANSFORM (OPTIONAL) FOR PILEUP PLOT\n",
    "ext_target = []\n",
    "\n",
    "def compute_lag_for_maximum_alignment(series1, bed_start1):\n",
    "    \"\"\"\n",
    "    Decides flipping based on maximum cross-correlation, and then computes the lag\n",
    "    required to align the maximum values of two series. Returns both the lag and the decision to flip.\n",
    "    \"\"\"\n",
    "    flip = 0\n",
    "    pos_max_series1 = np.argmax(series1)\n",
    "    lag = (round(len(series1)/2)) - pos_max_series1\n",
    "    return (lag, flip)\n",
    "\n",
    "def get_continuous_series(df_subset):\n",
    "    # Create a Series with rel_start as the index and norm_mod_frac_weighted as the values\n",
    "    series_filled = df_subset.set_index('rel_start')['weighted_norm_mod_frac']\n",
    "\n",
    "    # Fill NaNs using forward fill then backward fill\n",
    "    series_filled = series_filled.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "    # Ensure it's a continuous series by filling any gaps in rel_start\n",
    "    try:\n",
    "        series_filled = series_filled.reindex(\n",
    "            range(int(series_filled.index.min()), int(series_filled.index.max()) + 1),\n",
    "            fill_value=0\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\"Failed to reindex series_filled:\", e)\n",
    "        print(\"Duplicate indexes:\", series_filled.index[series_filled.index.duplicated()])\n",
    "\n",
    "    return series_filled.values\n",
    "\n",
    "def align_profiles(df):\n",
    "    df = df.sort_values(['bed_start', 'rel_start']).copy()\n",
    "    bed_starts = df['bed_start'].unique()\n",
    "\n",
    "    # Determine the reference bed_start\n",
    "    summed_Nvalid_cov = df.groupby('bed_start')['Nvalid_cov'].sum()\n",
    "    reference_bed_start = summed_Nvalid_cov.idxmax()\n",
    "    series_reference = get_continuous_series(df[df['bed_start'] == reference_bed_start])\n",
    "\n",
    "    # Calculate the number of positions to shift\n",
    "    shift_positions = int(round(len(series_reference)/2)) - np.argmax(series_reference)\n",
    "\n",
    "    # Shift the entire series_reference by shift_positions\n",
    "    if shift_positions > 0:  # shift to the left\n",
    "        series_reference = np.concatenate(([0]*shift_positions, series_reference))\n",
    "    else:\n",
    "        series_reference = np.concatenate((series_reference, [0]*abs(shift_positions)))\n",
    "\n",
    "    df[\"flipped\"] = 0\n",
    "\n",
    "    for other_bed_start in bed_starts:\n",
    "        series_to_shift = get_continuous_series(df[df['bed_start'] == other_bed_start])\n",
    "        lag, flip = compute_lag_for_maximum_alignment(series_to_shift, other_bed_start)\n",
    "\n",
    "        df.loc[df['bed_start'] == other_bed_start, 'shift'] = lag\n",
    "        df.loc[df['bed_start'] == other_bed_start, 'flipped'] = 1 if flip else 0\n",
    "\n",
    "    # Calculate statistics using the 'flipped' and 'shift' columns\n",
    "    total_flipped = df[df['flipped'] == 1]['bed_start'].nunique()\n",
    "    lag_distribution = df['shift'].describe()\n",
    "\n",
    "    print(f\"Total bed_starts flipped: {total_flipped} out of {len(bed_starts) - 1}\")\n",
    "    print(\"Lag Distribution:\")\n",
    "    print(lag_distribution)\n",
    "\n",
    "    return df\n",
    "\n",
    "print(\"Copying and dropping rows...\")\n",
    "comb_bedmethyl_plot_df = comb_bedmethyl_df.copy()\n",
    "\n",
    "# find unique values in combined_bedmethyl_df type\n",
    "unique_types = comb_bedmethyl_plot_df['type'].unique()\n",
    "\n",
    "# if we did not center on motifs\n",
    "\n",
    "if center_on_motifs == True:\n",
    "    # drop any nan keys from motif_lengths\n",
    "    motif_lengths = {k: v for k, v in motif_lengths.items() if pd.notnull(k)}\n",
    "\n",
    "# Adjust rel_start based on strand and type\n",
    "for each_type in unique_types:\n",
    "    print(f\"Adjusting rel_start for {each_type}...\")\n",
    "    if any(x in each_type for x in [\"TSS\", \"TES\", \"gene\",\"MEX\",\"motif\"]): #\n",
    "        print(f\"Strand orientation sensitive {each_type} type selected, multiplying rel_start by -1 for '-' strand genes...\")\n",
    "        if 'gene' in each_type:\n",
    "            # Subtract half of num_bins from rel_start for metagene profiles to center them\n",
    "            comb_bedmethyl_plot_df['rel_start'] -= num_bins / 2\n",
    "            \n",
    "        # Mask for types and negative strands\n",
    "        mask = (comb_bedmethyl_plot_df['type'] == each_type) & (comb_bedmethyl_plot_df['bed_strand'] == '-')\n",
    "        comb_bedmethyl_plot_df.loc[mask, 'rel_start'] *= -1\n",
    "        \n",
    "        if center_on_motifs == True:\n",
    "            # for each unique key in motif_length   \n",
    "            for each_key in motif_lengths.keys():\n",
    "                # if each_key is in type column then add key value to rel_start for rows where type is each_type\n",
    "                if each_key in each_type:\n",
    "                    comb_bedmethyl_plot_df.loc[comb_bedmethyl_plot_df['type'] == each_type, 'rel_start'] += motif_lengths[each_key]\n",
    "\n",
    "        # Adjust bigwig lines if ext_target is not empty\n",
    "        if ext_target:\n",
    "            mask_bw = (bw_df['type'] == each_type) & (bw_df['bed_strand'] == '-')\n",
    "            bw_df.loc[mask_bw, 'rel_start'] *= -1\n",
    "        if 'gene' in each_type:\n",
    "            # Add half of num_bins back to rel_start\n",
    "            comb_bedmethyl_plot_df['rel_start'] += num_bins / 2\n",
    "\n",
    "if combine_replicates:\n",
    "    print(\"Combining replicates based on conditions...\")\n",
    "\n",
    "    # Define mappings for condition and exp_id\n",
    "    condition_map = {\n",
    "        \"N2_mixed\": analysis_cond[0],\n",
    "        \"SDC2_degron_mixed\": analysis_cond[1]#,\n",
    "        #\"N2_old_SMAC\":analysis_cond[6],\n",
    "        #\"DPY27\": analysis_cond[8]\n",
    "    }\n",
    "\n",
    "    for key, value in condition_map.items():\n",
    "        # Update condition and exp_id based on the mapping\n",
    "        mask = comb_bedmethyl_plot_df['condition'].str.contains(key, na=False)\n",
    "        comb_bedmethyl_plot_df.loc[mask, 'condition'] = value\n",
    "        comb_bedmethyl_plot_df.loc[mask, 'exp_id'] = value\n",
    "\n",
    "# # Group the DataFrame\n",
    "# grouped = comb_bedmethyl_plot_df.groupby(\n",
    "#     ['chrom', 'rel_start', 'exp_id', 'modified_base_code', 'condition', 'type', 'chr_type', 'bed_start']\n",
    "# ).agg({\n",
    "#     'Nvalid_cov': 'sum'\n",
    "# }).reset_index()\n",
    "#\n",
    "# print(\"Grouped DataFrame created.\")\n",
    "\n",
    "print(\"Grouping by chrom, rel_start, exp_id, modified_base_code, condition, type, chr_type, bed_start...\")\n",
    "# Group and aggregate necessary columns\n",
    "grouped_df = comb_bedmethyl_plot_df.groupby(\n",
    "    ['chrom', 'rel_start', 'exp_id', 'modified_base_code', 'condition', 'type', 'chr_type','strand']\n",
    ").agg({\n",
    "    'Nvalid_cov': 'sum',\n",
    "    'Nmod': 'sum',\n",
    "    'Ncanonical': 'sum',\n",
    "    'Nother_mod': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "print(\"Calculating normalized m6A...\")\n",
    "### Calculate normalized m6A\n",
    "grouped_df['raw_mod_frac'] = grouped_df['Nmod'] / (grouped_df['Nmod'] + grouped_df['Ncanonical'])\n",
    "\n",
    "# Ensure no stray whitespace in exp_id\n",
    "grouped_df['exp_id'] = grouped_df['exp_id'].str.strip()\n",
    "\n",
    "# Helper to compute fraction safely\n",
    "def _frac(g):\n",
    "    nmod = g['Nmod'].sum()\n",
    "    ncan = g['Ncanonical'].sum()\n",
    "    total = nmod + ncan\n",
    "    return np.nan if total == 0 else nmod / total\n",
    "\n",
    "# Compute global mod fractions per exp_id from grouped_df itself\n",
    "m6a_global = (\n",
    "    grouped_df[grouped_df['modified_base_code'] == 'a,A,0']\n",
    "    .groupby('exp_id', as_index=False)\n",
    "    .apply(_frac)\n",
    "    .rename(columns={None: 'exp_id_m6A_frac'})\n",
    ")\n",
    "\n",
    "m5mc_global = (\n",
    "    grouped_df[grouped_df['modified_base_code'] == 'm,GC,1']\n",
    "    .groupby('exp_id', as_index=False)\n",
    "    .apply(_frac)\n",
    "    .rename(columns={None: 'exp_id_5mC_frac'})\n",
    ")\n",
    "\n",
    "global_fracs = pd.merge(m6a_global, m5mc_global, on='exp_id', how='outer')\n",
    "\n",
    "# Merge back to per-position data\n",
    "merged_df = pd.merge(grouped_df, global_fracs, on='exp_id', how='left')\n",
    "\n",
    "# Optional: if a mod type is missing for an exp_id, avoid NaNs in normalization\n",
    "merged_df[['exp_id_m6A_frac','exp_id_5mC_frac']] = (\n",
    "    merged_df[['exp_id_m6A_frac','exp_id_5mC_frac']].fillna(1.0)\n",
    ")\n",
    "\n",
    "### Normalization based on selected type\n",
    "if normalization_type == 'global':\n",
    "    # Global normalization using exp_id_m6A_frac for m6A and exp_id_5mC_frac for 5mC\n",
    "    # We'll create a new column that assigns the appropriate global fraction based on modified_base_code.\n",
    "    merged_df['norm_mod_frac_init'] = np.where(\n",
    "        merged_df['modified_base_code'] == 'a,A,0',\n",
    "        merged_df['exp_id_m6A_frac'],\n",
    "        np.where(\n",
    "            merged_df['modified_base_code'] == 'm,GC,1',\n",
    "            merged_df['exp_id_5mC_frac'],\n",
    "            np.nan  # If there are other modification codes, handle them here\n",
    "        )\n",
    "    )\n",
    "    normalization_label = 'global_normalization'\n",
    "elif normalization_type == 'local':\n",
    "    # Local normalization using intergenic mod_frac\n",
    "    # Filter for intergenic types\n",
    "    intergenic_df = grouped_df[grouped_df['type'].str.contains(\"intergenic\", case=False)]\n",
    "\n",
    "    # Compute per exp_id local mod_frac\n",
    "    local_mod_frac = intergenic_df.groupby('exp_id').agg({\n",
    "        'Nmod': 'sum',\n",
    "        'Ncanonical': 'sum'\n",
    "    }).reset_index()\n",
    "    local_mod_frac['mod_frac_local'] = local_mod_frac['Nmod'] / (local_mod_frac['Nmod'] + local_mod_frac['Ncanonical'])\n",
    "\n",
    "    # Merge with main dataframe\n",
    "    merged_df = pd.merge(\n",
    "        merged_df,\n",
    "        local_mod_frac[['exp_id', 'mod_frac_local']],\n",
    "        on='exp_id',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Handle cases where mod_frac_local might be NaN (e.g., no intergenic data for an exp_id)\n",
    "    merged_df['mod_frac_local'] = merged_df['mod_frac_local'].fillna(1)  # Assuming no normalization if local mod_frac is missing\n",
    "\n",
    "    # Compute normalized mod_frac\n",
    "    merged_df['norm_mod_frac_init'] = merged_df['mod_frac_local']\n",
    "    normalization_label = 'mod_frac_local'\n",
    "else:\n",
    "    raise ValueError(\"Invalid normalization_type. Choose 'global' or 'local'.\")\n",
    "\n",
    "print(f\"Normalization method: {normalization_type} ({normalization_label})\")\n",
    "\n",
    "# Select relevant columns for plotting\n",
    "plot_df = merged_df[grouped_df.columns.tolist() + ['norm_mod_frac_init']]\n",
    "\n",
    "### Since multiple samples have same condition:\n",
    "plot_df = plot_df.groupby(\n",
    "    ['rel_start', 'modified_base_code', 'condition', 'type', 'chr_type', 'norm_mod_frac_init','strand']\n",
    ")[['Nvalid_cov', 'Ncanonical', 'Nmod']].sum().reset_index()\n",
    "\n",
    "if ext_target:\n",
    "    plot_comb_bigwig_df = bw_df.groupby(\n",
    "        ['rel_start', 'chrom', 'condition', 'type', 'chr_type']\n",
    "    )['value'].mean().reset_index()\n",
    "else:\n",
    "    plot_comb_bigwig_df = pd.DataFrame()\n",
    "\n",
    "# Recompute raw_mod_frac and weighted_norm_mod_frac\n",
    "plot_df['raw_mod_frac'] = plot_df['Nmod'] / (plot_df['Nmod'] + plot_df['Ncanonical'])\n",
    "plot_df['weighted_norm_mod_frac'] = plot_df['raw_mod_frac'] / plot_df['norm_mod_frac_init']\n",
    "\n",
    "# Sort by rel_start\n",
    "plot_df.sort_values(['rel_start'], inplace=True)\n",
    "plot_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "print(\"plot_df:\")\n",
    "# Display sample rows for debugging\n",
    "nanotools.display_sample_rows(plot_df, 10)\n",
    "if ext_target:\n",
    "    nanotools.display_sample_rows(plot_comb_bigwig_df, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cebad5592a27f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "force_replace = True\n",
    "# save final_df to /temp folder as csv, with all configurations in file name if it does not exist. If it does exist, import it.\n",
    "final_fn = \"temp_files/\" + \"final_df_\" + \"_\".join([each_type for each_type in type_selected[-3:]]) + str(round(thresh_list[0],2)) + \"_\"+str(bam_fracs[0])+str(bed_window)+\".csv\"\n",
    "final_fn_chip = \"temp_files/\" + \"final_df_chip\" + \"_\".join([each_type for each_type in type_selected[-3:]]) + str(round(thresh_list[0],2)) + \"_\"+str(bam_fracs[0])+str(bed_window)+\".csv\"\n",
    "\n",
    "if not force_replace and os.path.exists(final_fn):\n",
    "    print(\"final_df already exists, importing it...\")\n",
    "    plot_df = pd.read_csv(final_fn)\n",
    "    nanotools.display_sample_rows(plot_df,5)\n",
    "else:\n",
    "    print(\"final_df does not exist, saving it...\")\n",
    "    plot_df.to_csv(final_fn, index=False)\n",
    "\n",
    "# if plot_comb_bigwig_df dataframe does not exist:\n",
    "try:\n",
    "    if not force_replace and os.path.exists(final_fn_chip):\n",
    "        print(\"final_df_chip already exists, importing it...\")\n",
    "        plot_comb_bigwig_df = pd.read_csv(final_fn_chip)\n",
    "        nanotools.display_sample_rows(plot_comb_bigwig_df,5)\n",
    "    else:\n",
    "        print(\"final_df_chip does not exist, saving it...\")\n",
    "        plot_comb_bigwig_df.to_csv(final_fn_chip, index=False)\n",
    "except:\n",
    "    print(\"plot_comb_bigwig_df does not exist, skipping...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba2712a6ef3267b",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(nanotools)\n",
    "from scipy.signal import gaussian\n",
    "import scipy.ndimage\n",
    "from scipy import signal          # ← new import\n",
    "\n",
    "\n",
    "def plot_bedmethyl(\n",
    "    comb_bedmethyl_df,\n",
    "    conditions_input,\n",
    "    chr_types=None,\n",
    "    types=None,\n",
    "    strands=[\"all\"],\n",
    "    window_size=50,\n",
    "    metagene_bins=1000,\n",
    "    smoothing_type=\"weighted\",\n",
    "    selection_indices=None,\n",
    "    bed_window=[-500,500],\n",
    "    mod_types=['m6A'],\n",
    "    ignore_selec=[],\n",
    "    bigwig_df=None,\n",
    "    bw_selections=None,\n",
    "    plot_type=\"raw\",\n",
    "    plot_x_over_a=True,  # <--- The new boolean parameter\n",
    "    y_range=None\n",
    "):\n",
    "    # --- chip‑rank‑based motif filtering ------------------------------\n",
    "    comb_bedmethyl_df, auto_types = apply_chiprank_filter(\n",
    "        comb_bedmethyl_df,\n",
    "        chip_rank_cutoff=CHIP_RANK_CUTOFF,\n",
    "        above_flag=ABOVE_FLAG,\n",
    "        types_to_include=TYPES_TO_INCLUDE,\n",
    "    )\n",
    "\n",
    "    # if the caller did not supply an explicit `types` argument, use the one\n",
    "    # returned by the filter; otherwise respect the caller’s list verbatim\n",
    "    if not types:\n",
    "        types = auto_types\n",
    "\n",
    "    # ------\n",
    "    # 1. Prepare figure containers, global variables, etc.\n",
    "    fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "    cov_fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "    y_min = float('inf')\n",
    "    y_max = float('-inf')\n",
    "\n",
    "    # For storing final x and y (smoothed) data so we can later compute X_over_A\n",
    "    results_dict = {}\n",
    "\n",
    "    # Reduce DataFrame to bed_window\n",
    "    comb_bedmethyl_df = comb_bedmethyl_df[\n",
    "        (comb_bedmethyl_df['rel_start'] >= bed_window[0]) &\n",
    "        (comb_bedmethyl_df['rel_start'] <= bed_window[1])\n",
    "    ]\n",
    "\n",
    "    if selection_indices is not None:\n",
    "        conditions = [conditions_input[i] for i in selection_indices]\n",
    "    else:\n",
    "        conditions = conditions_input\n",
    "\n",
    "    # ------\n",
    "    # 2. Main loop over conditions, modifications, chr_types, types, strands\n",
    "    for selected_condition in conditions:\n",
    "        print(\"Starting on condition:\", selected_condition)\n",
    "\n",
    "        for selected_modification in (mod_types or [\"all\"]):\n",
    "            skip_line = False\n",
    "            if selected_modification == '5mC':\n",
    "                selected_mod = 'm,GC,1'\n",
    "            elif selected_modification == 'm6A':\n",
    "                selected_mod = 'a,A,0'\n",
    "            else:\n",
    "                selected_mod = 'all'\n",
    "\n",
    "            # Optionally skip certain (condition, modification) combos\n",
    "            for each in ignore_selec:\n",
    "                if (selected_condition == conditions_input[each[0]] \n",
    "                    and selected_modification == each[1]):\n",
    "                    print(\"Skipping:\", each[0], \"with meth:\", each[1])\n",
    "                    skip_line = True\n",
    "                    break\n",
    "            if skip_line:\n",
    "                continue\n",
    "\n",
    "            for selected_chr_type in (chr_types or [\"all\"]):\n",
    "                for selected_type in (types or [\"all\"]):\n",
    "                    for selected_strand in (strands or [\"all\"]):\n",
    "                        # Apply filters\n",
    "                        filters = []\n",
    "                        filters.append(comb_bedmethyl_df['condition'] == selected_condition)\n",
    "                        if selected_chr_type != \"all\":\n",
    "                            filters.append(comb_bedmethyl_df['chr_type'] == selected_chr_type)\n",
    "                        if selected_type != \"all\":\n",
    "                            filters.append(comb_bedmethyl_df['type'] == selected_type)\n",
    "                        if selected_strand != \"all\":\n",
    "                            filters.append(comb_bedmethyl_df['strand'] == selected_strand)\n",
    "                        if selected_mod != \"all\":\n",
    "                            filters.append(comb_bedmethyl_df['modified_base_code'] == selected_mod)\n",
    "\n",
    "                        base_filter = np.logical_and.reduce(filters)\n",
    "\n",
    "                        data_filtered = comb_bedmethyl_df.loc[\n",
    "                            base_filter, \n",
    "                            ['weighted_norm_mod_frac', 'raw_mod_frac','rel_start', 'Nvalid_cov']\n",
    "                        ].copy()\n",
    "\n",
    "                        if data_filtered.empty:\n",
    "                            print(f\"No data for {selected_condition}, {selected_modification}, \"\n",
    "                                  f\"{selected_chr_type}, {selected_type}, {selected_strand}\")\n",
    "                            continue\n",
    "\n",
    "                        # Right after data_filtered is defined:\n",
    "                        # 1) Pull out only the columns we need, and compute “Nmod = raw_mod_frac * Nvalid_cov”\n",
    "                        tmp = comb_bedmethyl_df.loc[\n",
    "                            base_filter,\n",
    "                            ['rel_start', 'raw_mod_frac', 'Nvalid_cov']\n",
    "                        ].copy()\n",
    "                        tmp['Nmod'] = tmp['raw_mod_frac'] * tmp['Nvalid_cov']\n",
    "\n",
    "                        # 2) Group by rel_start, summing Nmod and Nvalid_cov\n",
    "                        agg = (\n",
    "                            tmp\n",
    "                            .groupby('rel_start', as_index=False)\n",
    "                            .agg({'Nmod':'sum', 'Nvalid_cov':'sum'})\n",
    "                        )\n",
    "\n",
    "                        # 3) Recompute raw_mod_frac at each rel_start\n",
    "                        agg['raw_mod_frac'] = agg['Nmod'] / agg['Nvalid_cov']\n",
    "\n",
    "                        # 4) If you also need weighted_norm_mod_frac, you can recalc it here\n",
    "                        #    (but if you only use “raw” in your plot_type, you can drop it)\n",
    "                        #    For now, let’s drop weighted_norm_mod_frac and carry forward raw/Nvalid_cov:\n",
    "                        data_filtered = agg[['rel_start','raw_mod_frac','Nvalid_cov']].copy()\n",
    "\n",
    "\n",
    "                        # Create a full rel_start range and merge\n",
    "                        full_range_df = pd.DataFrame({\n",
    "                            'rel_start': range(\n",
    "                                int(data_filtered['rel_start'].min()),\n",
    "                                int(data_filtered['rel_start'].max() + 1)\n",
    "                            )\n",
    "                        })\n",
    "                        merged_df = pd.merge(\n",
    "                            full_range_df, data_filtered,\n",
    "                            on='rel_start', how='left'\n",
    "                        )\n",
    "\n",
    "                        # Fill or drop missing\n",
    "                        if smoothing_type != \"weighted\":\n",
    "                            merged_df.fillna({\n",
    "                                'raw_mod_frac': 0,\n",
    "                                'Nvalid_cov': 0,\n",
    "                                'weighted_norm_mod_frac': 0\n",
    "                            }, inplace=True)\n",
    "                        else:\n",
    "                            # Weighted smoothing requires valid coverage\n",
    "                            merged_df.dropna(subset=[\n",
    "                                'raw_mod_frac',\n",
    "                                #'weighted_norm_mod_frac',\n",
    "                                'Nvalid_cov'\n",
    "                            ], inplace=True)\n",
    "                            merged_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "                        # Decide which column to plot\n",
    "                        if plot_type == \"raw\":\n",
    "                            m6A_data = merged_df['raw_mod_frac']\n",
    "                        else:  # \"norm\"\n",
    "                            #m6A_data = merged_df['weighted_norm_mod_frac']\n",
    "                            print(\"This option has been deprecated, using raw_mod_frac instead.\")\n",
    "\n",
    "                        m6A_data_xaxis = merged_df['rel_start']\n",
    "                        Nvalid_cov_data = merged_df['Nvalid_cov']\n",
    "                        smoothed_cov_data = Nvalid_cov_data.rolling(\n",
    "                            window=window_size, center=True\n",
    "                        ).sum()\n",
    "\n",
    "                        print(f\"[DEBUG] >>> Entering plot_bedmethyl with smoothing_type={smoothing_type!r}, window_size={window_size}\")\n",
    "\n",
    "                        # … then, after you build merged_df but before the if/elif ladder:\n",
    "                        print(f\"[DEBUG] merged_df rows = {len(merged_df)}; first few raw_mod_frac =\\n{m6A_data.head()}\")\n",
    "                        # Smoothing for m6A_data\n",
    "                        if smoothing_type == \"none\":\n",
    "                            smoothed_data = m6A_data\n",
    "                        # Then if you choose weighted‐rolling averaging:\n",
    "                        elif smoothing_type == \"weighted\":\n",
    "                            def weighted_rolling_average(values, weights, wsize):\n",
    "                                def calc_wavg(window):\n",
    "                                    w = weights[window.index]\n",
    "                                    tot = w.sum()\n",
    "                                    if tot == 0:\n",
    "                                        return 0.0           # or choose np.nan if you prefer\n",
    "                                    return (window * w).sum() / tot\n",
    "\n",
    "                                return values.rolling(window=wsize, center=True) \\\n",
    "                                             .apply(calc_wavg, raw=False)\n",
    "\n",
    "                            smoothed_data = weighted_rolling_average(\n",
    "                                m6A_data, Nvalid_cov_data, window_size\n",
    "                            )\n",
    "                            # Quick debug to make sure we now see non‐NaNs inside the array:\n",
    "                            print(\"[DEBUG] weighted branch → some smoothed_data values:\",\n",
    "                                  smoothed_data.dropna().head(5))\n",
    "\n",
    "                            print(f\"[DEBUG] first few smoothed_data =\\n{smoothed_data.head(10)}\")\n",
    "                        elif smoothing_type == \"gaussian\":\n",
    "                            smoothed_data_array = scipy.ndimage.gaussian_filter1d(\n",
    "                                m6A_data, sigma=window_size\n",
    "                            )\n",
    "                            smoothed_data = pd.Series(smoothed_data_array,\n",
    "                                                      index=m6A_data.index)\n",
    "                        elif smoothing_type == \"exponential\":\n",
    "                            def exponential_decay_smoothing(x, alpha=0.1):\n",
    "                                s = np.zeros_like(x)\n",
    "                                s[0] = x.iloc[0]\n",
    "                                for t in range(1, len(x)):\n",
    "                                    s[t] = alpha * x.iloc[t] + (1 - alpha)*s[t-1]\n",
    "                                return s\n",
    "                            def symmetrical_exponential_smoothing(x, alpha=0.1):\n",
    "                                fwd = exponential_decay_smoothing(x, alpha)\n",
    "                                bwd = exponential_decay_smoothing(x[::-1], alpha)[::-1]\n",
    "                                return (fwd + bwd) / 2\n",
    "                            alpha = 0.05\n",
    "                            smoothed_data_array = symmetrical_exponential_smoothing(m6A_data, alpha=alpha)\n",
    "                            smoothed_data = pd.Series(smoothed_data_array, index=m6A_data.index)\n",
    "                        elif smoothing_type == \"lowess\":\n",
    "                            from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "                            smoothed = lowess(\n",
    "                                m6A_data, m6A_data_xaxis, frac=0.05, it=0\n",
    "                            )\n",
    "                            # x-values and smoothed data\n",
    "                            xarr, yarr = smoothed[:, 0], smoothed[:, 1]\n",
    "                            smoothed_data = pd.Series(yarr, index=xarr)\n",
    "                        else:\n",
    "                            smoothed_data = m6A_data.rolling(\n",
    "                                window=window_size, center=True\n",
    "                            ).mean()\n",
    "\n",
    "                        y_min = min(y_min, smoothed_data.min())\n",
    "                        y_max = max(y_max, smoothed_data.max())\n",
    "\n",
    "                        label = (f\"{selected_condition}_{selected_chr_type}_\"\n",
    "                                 f\"{selected_type}_{selected_strand}_{selected_modification}\")\n",
    "\n",
    "                        # Add the main mod fraction trace\n",
    "                        key = f\"{selected_condition}_{selected_type}\"\n",
    "                        color = nanotools.get_colors(key)\n",
    "                        fig.add_trace(\n",
    "                            go.Scatter(\n",
    "                                x=m6A_data_xaxis.values,\n",
    "                                y=smoothed_data.values,\n",
    "                                mode='lines',\n",
    "                                name=label,\n",
    "                                opacity=0.9,\n",
    "                                line=dict(width=3, color=color)\n",
    "                            ),\n",
    "                            secondary_y=False\n",
    "                        )\n",
    "\n",
    "                        # Also add coverage trace\n",
    "                        key = f\"{selected_condition}_{selected_type}\"\n",
    "                        color = nanotools.get_colors(key)\n",
    "                        cov_fig.add_trace(\n",
    "                            go.Scatter(\n",
    "                                x=m6A_data_xaxis.values,\n",
    "                                y=smoothed_cov_data.values,\n",
    "                                mode='lines',\n",
    "                                name=label + \"_Nvalid_cov\",\n",
    "                                opacity=0.9,\n",
    "                                line=dict(width=3, color=color)\n",
    "                            ),\n",
    "                            secondary_y=False\n",
    "                        )\n",
    "\n",
    "                        # Store final (x, y) so we can compute X_over_A later\n",
    "                        results_dict[\n",
    "                            (\n",
    "                                selected_condition,\n",
    "                                selected_modification,\n",
    "                                selected_type,\n",
    "                                selected_strand,\n",
    "                                plot_type,\n",
    "                                smoothing_type,\n",
    "                                selected_chr_type\n",
    "                            )\n",
    "                        ] = (\n",
    "                            m6A_data_xaxis.values,\n",
    "                            smoothed_data.values\n",
    "                        )\n",
    "\n",
    "    # ------\n",
    "    # 3. If bigwig data is provided, plot that as well on secondary_y=True\n",
    "    if bigwig_df is not None:\n",
    "        print(\"Plotting bigwig_df...\")\n",
    "        bigwig_df = bigwig_df[\n",
    "            (bigwig_df['rel_start'] >= bed_window[0]) &\n",
    "            (bigwig_df['rel_start'] <= bed_window[1])\n",
    "        ]\n",
    "\n",
    "        if bw_selections is not None:\n",
    "            for bw_selection in bw_selections:\n",
    "                for selected_chr_type in (chr_types or [\"all\"]):\n",
    "                    for selected_type in (types or [\"all\"]):\n",
    "                        for selected_strand in (strands or [\"all\"]):\n",
    "                            filters = []\n",
    "                            filters.append(bigwig_df['condition'] == bw_selection)\n",
    "                            if selected_chr_type != \"all\":\n",
    "                                filters.append(bigwig_df['chr_type'] == selected_chr_type)\n",
    "                            if selected_type != \"all\":\n",
    "                                filters.append(bigwig_df['type'] == selected_type)\n",
    "                            if selected_strand != \"all\":\n",
    "                                filters.append(bigwig_df['strand'] == selected_strand)\n",
    "\n",
    "                            base_filter = np.logical_and.reduce(filters)\n",
    "\n",
    "                            value_data = bigwig_df.loc[base_filter, 'value']\n",
    "                            value_data_xaxis = bigwig_df.loc[base_filter, 'rel_start']\n",
    "                            smoothed_data = value_data.rolling(window=window_size, center=True).mean()\n",
    "                            y_min = min(y_min, smoothed_data.min())\n",
    "                            y_max = max(y_max, smoothed_data.max())\n",
    "\n",
    "                            label = f\"{bw_selection}_{selected_chr_type}_{selected_type}_{selected_strand}\"\n",
    "                            fig.add_trace(\n",
    "                                go.Scatter(\n",
    "                                    x=value_data_xaxis.values,\n",
    "                                    y=value_data.values,\n",
    "                                    mode='lines',\n",
    "                                    name=label,\n",
    "                                    opacity=0.9,\n",
    "                                    line=dict(width=3)\n",
    "                                ),\n",
    "                                secondary_y=True\n",
    "                            )\n",
    "\n",
    "    # ------\n",
    "    # 4. Compute and plot X_over_A if both conditions are met\n",
    "    if (\n",
    "        plot_x_over_a\n",
    "        and chr_types\n",
    "        and 'X' in chr_types\n",
    "        and 'Autosome' in chr_types\n",
    "    ):\n",
    "        # Collect all combos ignoring the actual chr_type\n",
    "        unique_combos = set(\n",
    "            (cond, mod, typ, st, ptype, smth)\n",
    "            for (cond, mod, typ, st, ptype, smth, ctype) in results_dict.keys()\n",
    "        )\n",
    "        for combo in unique_combos:\n",
    "            cond, mod, typ, st, ptype, smth = combo\n",
    "            # Must have both X and Autosome stored\n",
    "            keyX = (cond, mod, typ, st, ptype, smth, 'X')\n",
    "            keyA = (cond, mod, typ, st, ptype, smth, 'Autosome')\n",
    "            if keyX in results_dict and keyA in results_dict:\n",
    "                xX, yX = results_dict[keyX]\n",
    "                xA, yA = results_dict[keyA]\n",
    "                dfX = pd.DataFrame({'x': xX, 'valX': yX})\n",
    "                dfA = pd.DataFrame({'x': xA, 'valA': yA})\n",
    "                merged = pd.merge(dfX, dfA, on='x', how='inner')\n",
    "                # Avoid dividing by zero\n",
    "                merged['ratio'] = merged['valX'] / merged['valA'].replace(0, np.nan)\n",
    "                \n",
    "                label = f\"{cond}_X_over_A_{typ}_{st}_{mod}\"\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=merged['x'],\n",
    "                        y=merged['ratio'],\n",
    "                        mode='lines',\n",
    "                        name=label,\n",
    "                        opacity=0.9,\n",
    "                        line=dict(width=3),\n",
    "                    ),\n",
    "                    secondary_y=False\n",
    "                )\n",
    "        \n",
    "        # remove all traces that are not X_over_A\n",
    "        fig.data = [trace for trace in fig.data if \"X_over_A\" in trace.name]\n",
    "        \n",
    "\n",
    "    # ------\n",
    "    # ------ 5. Final layout and show\n",
    "    border_shape = dict(\n",
    "        type=\"rect\",\n",
    "        x0=0, y0=0, x1=0.95, y1=1,\n",
    "        xref=\"paper\", yref=\"paper\",\n",
    "        line=dict(color=\"white\", width=2),\n",
    "        fillcolor='rgba(0,0,0,0)',\n",
    "    )\n",
    "\n",
    "    print(\"Adjusting plot formatting...\")\n",
    "\n",
    "    # Title logic\n",
    "    if types:\n",
    "        plot_title = \"m6A Fraction\" + \"_\".join([each_type for each_type in types])\n",
    "    else:\n",
    "        plot_title = \"m6A Fraction\"\n",
    "\n",
    "    # Turn off background grids and zero lines\n",
    "    fig.update_xaxes(showgrid=False, zeroline=False)\n",
    "    fig.update_yaxes(showgrid=False, zeroline=False)\n",
    "\n",
    "    # determine dynamic x-axis label based on types\n",
    "    axis_title = \"Genomic Position\"\n",
    "    if any(\"motif\" in t.lower() for t in (types or [])):\n",
    "        axis_title = \"<i>rex</i> motif\"\n",
    "    elif any(\"rex\" in t.lower() for t in (types or [])):\n",
    "        axis_title = \"<i>rex</i>\"\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=plot_title,\n",
    "        xaxis_title=axis_title,\n",
    "        template=\"plotly_white\",\n",
    "        width=800,\n",
    "        height=800,\n",
    "        title_font=dict(size=24, color=\"black\"),\n",
    "        paper_bgcolor='rgba(0,0,0,0)',\n",
    "        plot_bgcolor='rgba(0,0,0,0)',\n",
    "        shapes=[border_shape],\n",
    "        font=dict(color='black'),  # all text black\n",
    "        xaxis_title_font=dict(size=20, color=\"black\"),\n",
    "        yaxis_title_font=dict(size=20, color=\"black\"),\n",
    "    )\n",
    "    fig.update_yaxes(title_text=\"modBase/Base\", secondary_y=False)\n",
    "    fig.update_yaxes(title_text=\"ChIP enrichment\", secondary_y=True)\n",
    "\n",
    "    if plot_type == \"raw\":\n",
    "        fig.update_yaxes(tickformat=\".0%\")\n",
    "\n",
    "    # Update legend\n",
    "    fig.update_layout(\n",
    "        legend=dict(\n",
    "            traceorder=\"normal\",\n",
    "            y=-0.2,\n",
    "            x=0.25,\n",
    "            yanchor=\"top\",\n",
    "            orientation='h',\n",
    "            font=dict(size=18, color=\"black\"),  # legend text black\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Axis ticks and lines in black, disable zero lines, and set axis line width\n",
    "    fig.update_xaxes(\n",
    "        tickfont=dict(size=20, color=\"black\"),\n",
    "        ticks='outside',\n",
    "        ticklen=10,\n",
    "        tickwidth=2,\n",
    "        tickcolor='black',\n",
    "        showline=True,\n",
    "        linecolor='black',\n",
    "        linewidth=2,\n",
    "        zeroline=False\n",
    "    )\n",
    "    fig.update_yaxes(\n",
    "        tickfont=dict(size=20, color=\"black\"),\n",
    "        ticks='outside',\n",
    "        ticklen=10,\n",
    "        tickwidth=2,\n",
    "        tickcolor='black',\n",
    "        showline=True,\n",
    "        linecolor='black',\n",
    "        linewidth=2,\n",
    "        zeroline=False,\n",
    "        tickformat=\".2f\"\n",
    "    )\n",
    "\n",
    "    if y_range is not None:\n",
    "        fig.update_yaxes(range=y_range)\n",
    "    else:\n",
    "        fig.update_yaxes(range=[y_min-0.03, y_max+0.03])\n",
    "\n",
    "    percent_axis = (y_max <= 1)\n",
    "\n",
    "    # build common y-axis args\n",
    "    yaxis_args = dict(\n",
    "        title_text=\"m6A/A\",\n",
    "        secondary_y=False\n",
    "    )\n",
    "    if percent_axis:\n",
    "        yaxis_args.update(\n",
    "            tickformat=\".0%\",\n",
    "        )\n",
    "\n",
    "    fig.update_yaxes(**yaxis_args)\n",
    "\n",
    "    # Now do cov_fig layout\n",
    "    cov_fig.update_xaxes(showgrid=False, zeroline=False)\n",
    "    cov_fig.update_yaxes(showgrid=False, zeroline=False)\n",
    "\n",
    "    if types:\n",
    "        plot_title_cov = \"Motif Count\" + \"_\".join([each_type for each_type in types])\n",
    "    else:\n",
    "        plot_title_cov = \"Motif Count\"\n",
    "\n",
    "    cov_fig.update_layout(\n",
    "        title=plot_title_cov,\n",
    "        xaxis_title='Genomic Position',\n",
    "        template=\"plotly_white\",\n",
    "        width=800,\n",
    "        height=800,\n",
    "        paper_bgcolor='rgba(0,0,0,0)',\n",
    "        plot_bgcolor='rgba(0,0,0,0)',\n",
    "        shapes=[border_shape],\n",
    "        font=dict(color='black')  # all text black\n",
    "    )\n",
    "    cov_fig.update_yaxes(title_text=\"Nvalid_cov\", secondary_y=False)\n",
    "    cov_fig.update_layout(\n",
    "        legend=dict(\n",
    "            traceorder=\"normal\",\n",
    "            y=-1,\n",
    "            x=0.25,\n",
    "            yanchor=\"top\",\n",
    "            orientation='h',\n",
    "            font=dict(size=18, color=\"black\"),  # legend text black\n",
    "        )\n",
    "    )\n",
    "    cov_fig.update_xaxes(range=[bed_window[0], bed_window[1]])\n",
    "\n",
    "    # Axis ticks and lines in black for cov_fig, disable zero lines, set axis line width\n",
    "    cov_fig.update_xaxes(\n",
    "        tickfont=dict(size=20, color=\"black\"),\n",
    "        ticks='outside',\n",
    "        ticklen=10,\n",
    "        tickwidth=2,\n",
    "        tickcolor='black',\n",
    "        showline=True,\n",
    "        linecolor='black',\n",
    "        linewidth=2,\n",
    "        zeroline=False\n",
    "    )\n",
    "    cov_fig.update_yaxes(\n",
    "        tickfont=dict(size=20, color=\"black\"),\n",
    "        ticks='outside',\n",
    "        ticklen=10,\n",
    "        tickwidth=2,\n",
    "        tickcolor='black',\n",
    "        showline=True,\n",
    "        linecolor='black',\n",
    "        linewidth=2,\n",
    "        zeroline=False\n",
    "    )\n",
    "\n",
    "    # Optionally draw TSS and TES lines.\n",
    "    # Only plot TSS if the bed_window spans 0.\n",
    "    if ((any('TSS' in t for t in (types or [])) or any('gene' in t for t in (types or []))\n",
    "         or any('damID' in t for t in (types or [])))\n",
    "         and (bed_window[0] <= 0 <= bed_window[1])):\n",
    "        fig.add_shape(\n",
    "            type=\"line\", x0=0, y0=0, x1=0, y1=1,\n",
    "            line=dict(color=\"black\", width=1.5),\n",
    "            xref=\"x\", yref=\"paper\"\n",
    "        )\n",
    "        fig.add_annotation(\n",
    "            x=0, y=1, yref=\"paper\", text=\"TSS\",\n",
    "            showarrow=False, yanchor=\"bottom\", xanchor=\"center\",\n",
    "            font=dict(color=\"black\")\n",
    "        )\n",
    "        cov_fig.add_shape(\n",
    "            type=\"line\", x0=0, y0=0, x1=0, y1=1,\n",
    "            line=dict(color=\"black\", width=1.5),\n",
    "            xref=\"x\", yref=\"paper\"\n",
    "        )\n",
    "\n",
    "    # Only plot TES if the bed_window spans metagene_bins.\n",
    "    if ((any('TES' in t for t in (types or [])) or any('gene' in t for t in (types or []))\n",
    "         or any('damID' in t for t in (types or [])))\n",
    "         and (bed_window[0] <= metagene_bins <= bed_window[1])):\n",
    "        fig.add_shape(\n",
    "            type=\"line\", x0=metagene_bins, y0=0, x1=metagene_bins, y1=1,\n",
    "            line=dict(color=\"black\", width=1.5),\n",
    "            xref=\"x\", yref=\"paper\"\n",
    "        )\n",
    "        fig.add_annotation(\n",
    "            x=metagene_bins, y=1, yref=\"paper\", text=\"TES\",\n",
    "            showarrow=False, yanchor=\"bottom\", xanchor=\"center\",\n",
    "            font=dict(color=\"black\")\n",
    "        )\n",
    "        cov_fig.add_shape(\n",
    "            type=\"line\", x0=metagene_bins, y0=0, x1=metagene_bins, y1=1,\n",
    "            line=dict(color=\"black\", width=1.5),\n",
    "            xref=\"x\", yref=\"paper\"\n",
    "        )\n",
    "\n",
    "    fig.show(renderer='plotly_mimetype+notebook')\n",
    "    cov_fig.show(renderer='plotly_mimetype+notebook')\n",
    "\n",
    "\n",
    "    # Return whichever you like. Here, just return the main fig and last label\n",
    "\n",
    "    # ------------------------------------------------------------------ helpers\n",
    "    def fwhm_scipy(x, y, prominence=0.05):\n",
    "        \"\"\"FWHM (bp) of tallest peak, half‑prominence definition.\"\"\"\n",
    "        peaks, props = signal.find_peaks(y, prominence=prominence)  # :contentReference[oaicite:4]{index=4}\n",
    "        if peaks.size == 0:\n",
    "            return np.nan\n",
    "        idx = np.argmax(props[\"prominences\"])                       # tallest\n",
    "        w, *_ = signal.peak_widths(y, [peaks[idx]])                 # :contentReference[oaicite:5]{index=5}\n",
    "        return w[0] * np.diff(x).mean()\n",
    "\n",
    "    def auc_valley_to_valley(x, y, prominence=0.05):\n",
    "        \"\"\"\n",
    "        Integrate peak area above the straight‑line baseline connecting the\n",
    "        two foot points that define the tallest peak’s prominence.\n",
    "        \"\"\"\n",
    "        peaks, props = signal.find_peaks(y, prominence=prominence)\n",
    "        if peaks.size == 0:\n",
    "            return np.nan\n",
    "        idx   = np.argmax(props[\"prominences\"])\n",
    "        left  = props[\"left_bases\"][idx]\n",
    "        right = props[\"right_bases\"][idx]\n",
    "\n",
    "        # linear baseline through the two bases\n",
    "        m = (y[right] - y[left]) / (x[right] - x[left])\n",
    "        b = y[left] - m * x[left]\n",
    "        baseline = m * x[left:right + 1] + b\n",
    "\n",
    "        return np.trapz(y[left:right + 1] - baseline, x[left:right + 1])\n",
    "\n",
    "    # ------------------------------------------------------------------ main loop\n",
    "    if window_size == 101 and bed_window == [-500, 500]:\n",
    "        records = []\n",
    "        for key, (xarr, yarr) in results_dict.items():\n",
    "            cond, mod, typ, strand, ptype, smth, chr_type = key\n",
    "            if mod != 'm6A' or 'intergenic' in typ.lower():\n",
    "                continue\n",
    "\n",
    "            width   = fwhm_scipy(xarr, yarr, prominence=0.05)\n",
    "            peak_auc = auc_valley_to_valley(xarr, yarr, prominence=0.05)\n",
    "\n",
    "            if not np.isnan(width):\n",
    "                records.append({\n",
    "                    \"condition\":     cond,\n",
    "                    \"type\":          typ,\n",
    "                    \"window_size\":   window_size,\n",
    "                    \"fwhm\":          width,\n",
    "                    \"peak_auc\":      peak_auc,\n",
    "                })\n",
    "\n",
    "        # ---------- upsert CSV exactly as before ------------------------------\n",
    "        fwhm_file   = f\"{output_stem}_fwhm.csv\"\n",
    "        fwhm_df_new = pd.DataFrame(records)\n",
    "\n",
    "        if os.path.exists(fwhm_file):\n",
    "            existing  = pd.read_csv(fwhm_file)\n",
    "            key_cols  = [\"condition\", \"type\", \"window_size\"]\n",
    "            keep_mask = ~existing.set_index(key_cols).index.isin(\n",
    "                fwhm_df_new.set_index(key_cols).index\n",
    "            )\n",
    "            fwhm_out = pd.concat([existing.loc[keep_mask], fwhm_df_new],\n",
    "                                 ignore_index=True)\n",
    "        else:\n",
    "            fwhm_out = fwhm_df_new\n",
    "\n",
    "        fwhm_out.to_csv(fwhm_file, index=False)\n",
    "        print(\"FWHM / Peak‑AUC DataFrame:\")\n",
    "        print(fwhm_out)\n",
    "    else:\n",
    "        print(\"FWHM/AUC calculation skipped for non‑default window/bounds\")\n",
    "\n",
    "\n",
    "\n",
    "    return fig, plot_title\n",
    "\n",
    "# ───────────────── CHIP‑RANK TYPE FILTER ───────────────── #\n",
    "def apply_chiprank_filter(\n",
    "    df: pd.DataFrame,\n",
    "    chip_rank_cutoff: int      = None,\n",
    "    above_flag: bool           = None,\n",
    "    types_to_include: list     = None,\n",
    "    chip_rank_path: str        = \"/Data1/reference/rex_chiprank.bed\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Restrict *df* to the desired motif types and, if requested, collapse the\n",
    "    surviving rows into the synthetic label 'chip_g_t_##' or 'chip_l_t_##'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    filtered_df : pd.DataFrame\n",
    "    selected_types : list[str]      # exact list to feed into plot_bedmethyl()\n",
    "    \"\"\"\n",
    "    # If the caller did not pass these, grab the module‐level defaults:\n",
    "    if chip_rank_cutoff is None:\n",
    "        chip_rank_cutoff = CHIP_RANK_CUTOFF\n",
    "    if above_flag is None:\n",
    "        above_flag = ABOVE_FLAG\n",
    "    if types_to_include is None:\n",
    "        types_to_include = TYPES_TO_INCLUDE\n",
    "\n",
    "    # Reload chip-rank table so the function is self-contained\n",
    "    chiprank_df = (\n",
    "        pd.read_csv(chip_rank_path, sep=r\"\\s+\")\n",
    "          .assign(type=lambda d: \"MOTIFS_\" + d[\"type\"].astype(str))\n",
    "    )\n",
    "    chip_rank_lookup = {\n",
    "        t: round(float(rk) * 100, 3)\n",
    "        for t, rk in zip(chiprank_df[\"type\"], chiprank_df[\"chip_rank\"])\n",
    "    }\n",
    "\n",
    "    # 1. Decide which motif types to keep\n",
    "    if types_to_include:                              # explicit list wins\n",
    "        keep_types = set(types_to_include)\n",
    "        synthetic_label = None                        # keep originals\n",
    "    else:                                             # rank-based selection\n",
    "        keep_types = {\n",
    "            t for t, r in chip_rank_lookup.items()\n",
    "            if (r >= chip_rank_cutoff) == above_flag\n",
    "        }\n",
    "        synthetic_label = f\"chip_{'g' if above_flag else 'l'}t_{chip_rank_cutoff}\"\n",
    "\n",
    "    # 2. Slice the input DF\n",
    "    tmp_df = df[df[\"type\"].isin(keep_types)].copy()\n",
    "\n",
    "    # 3. Optional renaming\n",
    "    if synthetic_label is not None and not tmp_df.empty:\n",
    "        tmp_df[\"type\"] = synthetic_label\n",
    "        selected_types = [synthetic_label]\n",
    "    else:\n",
    "        selected_types = sorted(keep_types)\n",
    "\n",
    "    return tmp_df, selected_types\n",
    "\n",
    "\n",
    "\n",
    "#Display random 100 rows from comb_bedmethyl_plot_df\n",
    "print(\"type_selected:\",type_selected)\n",
    "print(\"plot_df:\")\n",
    "nanotools.display_sample_rows(plot_df,20)\n",
    "\n",
    "window_s = 25\n",
    "smoothing_type = \"weighted\" #gaussian # exponential # none # weighted\n",
    "bed_w = 500 #bed_window\n",
    "num_bins = 1000\n",
    "plot_type = \"raw\" # or norm or raw\n",
    "plot_x_over_a = False\n",
    "\n",
    "CHIP_RANK_CUTOFF = 80\n",
    "ABOVE_FLAG = False\n",
    "TYPES_TO_INCLUDE = []\n",
    "# Example usage:\n",
    "# Note: final_df and conditions should be defined in your code\n",
    "\n",
    "print(\"Unique strands in DataFrame:\", comb_bedmethyl_df['strand'].unique())\n",
    "print(comb_bedmethyl_df.groupby('strand').size())\n",
    "\n",
    "\n",
    "# \"N2_old_fiber_R10\",\"96_DPY27_degron_old\",\"107_SDC2_degron_old_R10\",\"52_old_dpy21jmjc_fiber_R10\",\"51_old_dpy21null_fiber_R10\",\"N2_mixed_R9\",\"SDC2_degron_mixed_R9\",\"SDC3_degron_old_R10\"\n",
    "\n",
    "region_fig = plot_bedmethyl(plot_df, analysis_cond, chr_types=[\"X\"], types=[], strands=[], window_size=window_s, metagene_bins=num_bins, smoothing_type=smoothing_type,selection_indices=[0,1,2], bed_window=[-bed_w,bed_w], mod_types=[\"m6A\"],ignore_selec=[], plot_type = plot_type, plot_x_over_a = plot_x_over_a, y_range=[0.02,0.18])#,bw_selections=[\"sdc2_chip_albritton\",\"sdc3_chip_albritton\",\"sdc3_chip_anderson\",\"dpy27_chip_anderson\"],bigwig_df=plot_comb_bigwig_df)#[1,'5mC'],[3,'5mC']])# #\n",
    "# smoothing types: \"gaussian\", \"weighted\", \"rolling\"\n",
    "\n",
    "#8,9,10,11,14\n",
    "#7,12,13\n",
    "\n",
    "#7,8,9,10,11,12,13,14\n",
    "\n",
    "\n",
    "\n",
    "#analysis_cond = [\"N2_mixed_DPY27_dimelo_pAHia5_R10\",\"50_mixed_dpy27-3xGNB_GFP-Hia5_mcvipi_R10\",\"66_old_sdc2_3xGNB_GFPHia5_mChMCVIPI\",\"N2_mixed_endogenous_R10\",\"54_mixed_sdc2_3xmCNB_mChMCVIPI_GFPHia5\"]\n",
    "\n",
    "# print unique count bed_start values for combination of chr_type, type and condition in each comb_bedmethyl_df\n",
    "#print(\"Unique count of bed_start values for each combination of chr_type, type and condition in comb_bedmethyl_df:\")\n",
    "#print(plot_df.groupby(['chr_type','type','condition'])['bed_start'].nunique())\n",
    "prefix = \"all_reps_N2_DPY27_\"\n",
    "#rand_suffix = nanotools.random_alpha_numeric(8)\n",
    "region_fig[0].write_image(\"/Data1/git/meyer-nanopore/scripts/analysis/images_20250604/\"+prefix+smoothing_type+\"_\"+region_fig[1]+\".svg\")\n",
    "region_fig[0].write_image(\"/Data1/git/meyer-nanopore/scripts/analysis/images_20250604/\"+prefix+smoothing_type+\"_\"+region_fig[1]+\".png\")\n",
    "\n",
    "#\"center_DPY27_chip_albretton_ONLY\",\"center_DPY27_chip_albretton;gene_ol2000;TSS_ol2000\",\"strong_rex;DPY27_ol2000;SDC_ol2000\",\"center_DPY27_chip_albretton;SDC_ol2000\"\n",
    "#\"center_DPY27_chip_albretton\",\"intergenic_control\",\"strong_rex\",\"weak_rex\",\"TSS_q4\",\"TSS_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b927f3ee724a2abb",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import nanotools            # for get_colors()\n",
    "\n",
    "# ───────────────────────── Configuration ─────────────────────────\n",
    "fwhm_file = f\"{output_stem}_fwhm.csv\"\n",
    "\n",
    "PLOT_ORDER = [\"N2\", \"SDC2deg\", \"SDC3deg\",\"SDC2_3deg\",\"DPY27deg\", \"DPY21null\"]\n",
    "\n",
    "# ────────────────── Load & tag matching rows ────────────────────\n",
    "df100 = (\n",
    "    pd.read_csv(fwhm_file)\n",
    "      .query(\"window_size == 101\")\n",
    "      .assign(\n",
    "          plot_group=lambda d: d[\"condition\"].apply(\n",
    "              lambda c: next((s for s in PLOT_ORDER if s in c), pd.NA)\n",
    "          )\n",
    "      )\n",
    "      .dropna(subset=[\"plot_group\"])\n",
    ")\n",
    "\n",
    "df100[\"plot_group\"] = pd.Categorical(\n",
    "    df100[\"plot_group\"], categories=PLOT_ORDER, ordered=True\n",
    ")\n",
    "\n",
    "# ─────────────────── Helper to plot one metric ──────────────────\n",
    "def plot_metric(df, metric, y_title, y_range=None):\n",
    "    \"\"\"Draw a box‑and‑points figure for *metric* (fwhm or peak_auc).\"\"\"\n",
    "    fig = go.Figure()\n",
    "\n",
    "    for cond in PLOT_ORDER:\n",
    "        y = df.loc[df[\"plot_group\"] == cond, metric]\n",
    "        if y.empty:\n",
    "            continue\n",
    "\n",
    "        color = nanotools.get_colors(cond)\n",
    "\n",
    "        fig.add_trace(go.Box(\n",
    "            y=y, name=cond,\n",
    "            marker_color=color, line_color=color, fillcolor=\"rgba(0,0,0,0)\",\n",
    "            boxmean=True, boxpoints=\"all\", jitter=0.3, pointpos=0,\n",
    "            showlegend=False\n",
    "        ))\n",
    "\n",
    "        mean_val = y.mean()\n",
    "        fig.add_annotation(\n",
    "            x=cond, y=mean_val,\n",
    "            text=f\"{mean_val:.1f}\", showarrow=False,\n",
    "            font=dict(color=\"black\", size=14), yanchor=\"bottom\"\n",
    "        )\n",
    "\n",
    "    fig.update_xaxes(\n",
    "        categoryorder=\"array\",\n",
    "        categoryarray=[c for c in PLOT_ORDER if c in df[\"plot_group\"].values],\n",
    "        tickfont=dict(size=14, color=\"black\")\n",
    "    )\n",
    "    fig.update_yaxes(\n",
    "        range=y_range, showgrid=False,\n",
    "        tickfont=dict(size=14, color=\"black\")\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        title=f\"{metric.upper()} Distribution for {typ}\",\n",
    "        title_font=dict(size=20, color=\"black\"),\n",
    "        xaxis_title=\"Condition\", yaxis_title=y_title,\n",
    "        xaxis_title_font=dict(size=16, color=\"black\"),\n",
    "        yaxis_title_font=dict(size=16, color=\"black\"),\n",
    "        paper_bgcolor=\"white\", plot_bgcolor=\"white\",\n",
    "        font=dict(color=\"black\"), width=450, height=450,\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "# ───────────────────── Plot per “type” column ───────────────────\n",
    "for typ in df100[\"type\"].unique():\n",
    "    df_typ = df100[df100[\"type\"] == typ]\n",
    "\n",
    "    # 1) FWHM figure (original behaviour)\n",
    "    plot_metric(\n",
    "        df_typ, metric=\"fwhm\",\n",
    "        y_title=\"FWHM (bp)\",\n",
    "        y_range=[100, 170]    # keep the fixed range you specified\n",
    "    )\n",
    "\n",
    "    # 2) PEAK‑AUC figure (auto‑range)\n",
    "    plot_metric(\n",
    "        df_typ, metric=\"peak_auc\",\n",
    "        y_title=\"Peak AUC (signal × bp)\",\n",
    "        y_range=None          # let Plotly choose a sensible range\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c009e897361c4e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### START OF SINGLE FIBER PLOTTING\n",
    "\n",
    "### OPTIONAL TO CENTER ON MEX MOTIFS\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from intervaltree import Interval, IntervalTree\n",
    "from collections import defaultdict\n",
    "import numpy as np  # Added import for numpy\n",
    "\n",
    "# Set this parameter as needed\n",
    "center_intergenic = False\n",
    "include_noregion_motifs = False\n",
    "include_nomotif_regions = True\n",
    "combine_motifs = True\n",
    "\n",
    "# Configurable parameters\n",
    "window_size =  2*500 #2 * bed_window  # For cluster count 2*\n",
    "\n",
    "# Step 1: Import the TSV files into dataframes\n",
    "fimo_files = [\n",
    "    \"/Data1/ext_data/motifs/fimo_MEX_0.01.tsv\",\n",
    "    \"/Data1/ext_data/motifs/fimo_MEXII_0.01.tsv\",\n",
    "    \"/Data1/ext_data/motifs/fimo_motifc_0.01.tsv\"\n",
    "]\n",
    "\n",
    "fimo_dfs = []\n",
    "for file in fimo_files:\n",
    "    df = pd.read_csv(file, sep='\\t', comment='#', skip_blank_lines=True)\n",
    "    fimo_dfs.append(df)\n",
    "\n",
    "# Combine the dataframes into one\n",
    "fimo_df = pd.concat(fimo_dfs, ignore_index=True)\n",
    "\n",
    "# print count before filter\n",
    "print(\"fimo_df before filter:\",len(fimo_df))\n",
    "\n",
    "# Calculate the motif_length for each row\n",
    "fimo_df['motif_length'] = fimo_df['stop'] - fimo_df['start']\n",
    "# Drop duplicates to ensure only one motif_length per motif_id\n",
    "unique_motif_lengths = fimo_df.drop_duplicates('motif_id')[['motif_id', 'motif_length']]\n",
    "# Create a dictionary of motif lengths\n",
    "motif_lengths = dict(zip(unique_motif_lengths['motif_id'], unique_motif_lengths['motif_length']))\n",
    "# drop nan keys\n",
    "motif_lengths = {k: v for k, v in motif_lengths.items() if pd.notna(k)}\n",
    "# Print the motif_lengths\n",
    "print(\"motif_lengths:\", motif_lengths)\n",
    "\n",
    "# create 'natural_log_p_value' column (note: p-value is in format 0.00000000579)\n",
    "fimo_df['natural_log_p_value'] = np.log(fimo_df['p-value'])\n",
    "fimo_df = fimo_df[fimo_df['natural_log_p_value'] <= -11]  # Adjust as needed\n",
    "\n",
    "# print count after filter\n",
    "print(\"fimo_df after filter:\",len(fimo_df))\n",
    "\n",
    "# Step 3: Deduplicate overlapping rows on the same sequence_name (chromosome)\n",
    "motif_priority = {'MEXII': 2, 'MEX': 1, 'motifC': 3}\n",
    "fimo_df['motif_priority'] = fimo_df['motif_id'].map(motif_priority)\n",
    "fimo_df = fimo_df.dropna(subset=['motif_priority'])\n",
    "fimo_df['motif_priority'] = fimo_df['motif_priority'].astype(int)\n",
    "fimo_df = fimo_df.sort_values(by=['sequence_name', 'start', 'motif_priority','score'], ascending=[True, True, True,False])\n",
    "\n",
    "chrom_trees = defaultdict(IntervalTree)\n",
    "deduped_indices = set()\n",
    "\n",
    "for idx, row in fimo_df.iterrows():\n",
    "    chrom = row['sequence_name']\n",
    "    start = row['start']\n",
    "    end = row['stop']\n",
    "    priority = row['motif_priority']\n",
    "    overlaps = chrom_trees[chrom][start:end]\n",
    "    if not overlaps:\n",
    "        chrom_trees[chrom][start:end] = (priority, idx)\n",
    "        deduped_indices.add(idx)\n",
    "    else:\n",
    "        replace = False\n",
    "        for interval in overlaps:\n",
    "            existing_priority, existing_idx = interval.data\n",
    "            if start < interval.end and end > interval.begin:\n",
    "                if priority < existing_priority:\n",
    "                    chrom_trees[chrom].remove(interval)\n",
    "                    chrom_trees[chrom][start:end] = (priority, idx)\n",
    "                    deduped_indices.discard(existing_idx)\n",
    "                    deduped_indices.add(idx)\n",
    "                    replace = True\n",
    "                else:\n",
    "                    replace = False\n",
    "        if not replace:\n",
    "            continue\n",
    "\n",
    "fimo_df = fimo_df.loc[deduped_indices]\n",
    "\n",
    "print(\"fimo_df after dedup:\",len(fimo_df))\n",
    "\n",
    "# Step 4: Adjust columns\n",
    "fimo_df = fimo_df[['sequence_name', 'start', 'stop', 'strand', 'score', 'p-value', 'motif_id','natural_log_p_value']]\n",
    "fimo_df = fimo_df.rename(columns={'sequence_name': 'chr', 'p-value': 'p_value'})\n",
    "\n",
    "# Step 5: Replace \"chr\" with \"CHROMOSOME_\" in the 'chr' column\n",
    "fimo_df['chr'] = fimo_df['chr'].str.replace('chr', 'CHROMOSOME_', regex=False)\n",
    "\n",
    "# Step 6: Build IntervalTrees for each chromosome in combined_bed_df_ext\n",
    "interval_trees = defaultdict(IntervalTree)\n",
    "for idx, row in combined_bed_df_ext.iterrows():\n",
    "    chrom = row['chrom']\n",
    "    start = row['bed_start']\n",
    "    end = row['bed_end']\n",
    "    category = row['type']\n",
    "    # Store a tuple with the category and bed interval\n",
    "    interval_trees[chrom][start:end] = (category, start, end)\n",
    "\n",
    "# Add an 'id' column to fimo_df to keep track of original rows\n",
    "fimo_df.reset_index(drop=True, inplace=True)\n",
    "fimo_df['id'] = fimo_df.index\n",
    "\n",
    "# Step 7: Expand fimo_df to account for multiple overlapping categories and add bed_start, bed_end\n",
    "expanded_rows = []\n",
    "for idx, row in fimo_df.iterrows():\n",
    "    chrom = row['chr']\n",
    "    start = row['start']\n",
    "    end = row['stop']\n",
    "    overlaps = interval_trees[chrom][start:end] if chrom in interval_trees else []\n",
    "    if overlaps:\n",
    "        for interval in overlaps:\n",
    "            category, bed_st, bed_en = interval.data\n",
    "            new_row = row.copy()\n",
    "            new_row['chip_category'] = category\n",
    "            new_row['bed_start'] = bed_st\n",
    "            new_row['bed_end'] = bed_en\n",
    "            expanded_rows.append(new_row)\n",
    "    else:\n",
    "        new_row = row.copy()\n",
    "        new_row['chip_category'] = 'none'\n",
    "        # For no region, we might leave bed_start and bed_end as NaN or the motif coordinates\n",
    "        new_row['bed_start'] = new_row['start']\n",
    "        new_row['bed_end'] = new_row['stop']\n",
    "        expanded_rows.append(new_row)\n",
    "\n",
    "fimo_expanded_df = pd.DataFrame(expanded_rows)\n",
    "\n",
    "# Step 8: Build IntervalTrees for fimo_df for cluster counting\n",
    "fimo_trees = defaultdict(IntervalTree)\n",
    "for idx, row in fimo_df.iterrows():\n",
    "    chrom = row['chr']\n",
    "    start = row['start']\n",
    "    end = row['stop']\n",
    "    fimo_trees[chrom][start:end] = row['id']\n",
    "\n",
    "# Step 9: Calculate 'cluster_count'\n",
    "def get_cluster_count(row):\n",
    "    chrom = row['chr']\n",
    "    start = row['start']\n",
    "    end = row['stop']\n",
    "    intervals = fimo_trees[chrom][start - window_size:end + window_size] if chrom in fimo_trees else []\n",
    "    count = len(intervals)\n",
    "    return max(count, 0)\n",
    "\n",
    "fimo_df['cluster_count'] = fimo_df.apply(get_cluster_count, axis=1)\n",
    "\n",
    "# **NEW STEP**: Filter motifs to retain only the one with the greatest score within window_size\n",
    "# Sort by chromosome and descending score\n",
    "fimo_df = fimo_df.sort_values(by=['chr', 'natural_log_p_value'], ascending=[True, False])\n",
    "\n",
    "print(\"fimo_df:\",fimo_df[fimo_df[\"id\"]!=\"none\"].head())\n",
    "print(\"fimo_df:\",len(fimo_df[fimo_df[\"id\"]!=\"none\"]))\n",
    "\n",
    "final_indices = []\n",
    "chosen_positions_by_chr = defaultdict(list)\n",
    "\n",
    "for chr_name, group in fimo_df.groupby('chr', sort=False):\n",
    "    chosen_starts = []\n",
    "    for i, row in group.iterrows():\n",
    "        start = row['start']\n",
    "        # Keep motif if it's not within window_size of any chosen motif\n",
    "        if not any(abs(start - cs) < window_size for cs in chosen_starts):\n",
    "            chosen_starts.append(start)\n",
    "            final_indices.append(i)\n",
    "\n",
    "# Update fimo_df to retain only the selected motifs\n",
    "fimo_df = fimo_df.loc[final_indices].sort_values(by=['chr', 'start'])\n",
    "\n",
    "# Merge 'cluster_count' back into the expanded DataFrame\n",
    "fimo_expanded_df = fimo_expanded_df.merge(\n",
    "    fimo_df[['id', 'cluster_count']], on='id', how='inner'\n",
    ")\n",
    "\n",
    "# Step 10: Compute mean cluster_count and sample sizes per chip_category\n",
    "category_stats = fimo_expanded_df.groupby('chip_category')['cluster_count'].agg(['mean', 'count']).reset_index()\n",
    "category_stats = category_stats.sort_values(by='mean', ascending=False)\n",
    "category_stats['label'] = category_stats.apply(lambda x: f\"{x['chip_category']}\\n(n={int(x['count'])})\", axis=1)\n",
    "\n",
    "# Plot the bar plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='label', y='mean', data=category_stats, palette='viridis')\n",
    "plt.title('Average Cluster Count by Chip Category')\n",
    "plt.xlabel('Chip Category')\n",
    "plt.ylabel('Average Cluster Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Now, separately handle categories not present in fimo_expanded_df if include_nomotif_regions is True\n",
    "if include_nomotif_regions:\n",
    "    # Create sets of existing pairs from fimo_expanded_df\n",
    "    existing_pairs = set(zip(fimo_expanded_df['bed_start'], fimo_expanded_df['chip_category']))\n",
    "\n",
    "    # Create sets of all pairs from combined_bed_df_ext\n",
    "    all_pairs = set(zip(combined_bed_df_ext['bed_start'], combined_bed_df_ext['type']))\n",
    "\n",
    "    # Identify missing (bed_start, type) combinations\n",
    "    missing_pairs = all_pairs - existing_pairs\n",
    "\n",
    "    # Filter for missing combinations\n",
    "    missing_combinations_df = combined_bed_df_ext[\n",
    "        combined_bed_df_ext.apply(lambda row: (row['bed_start'], row['type']) in missing_pairs, axis=1)\n",
    "    ].copy()\n",
    "\n",
    "    # Compute midpoint and set start/stop\n",
    "    missing_combinations_df['midpoint'] = (missing_combinations_df['bed_start'] + missing_combinations_df['bed_end']) // 2\n",
    "    missing_combinations_df['chr'] = missing_combinations_df['chrom'].str.replace('chr', 'CHROMOSOME_', regex=False)\n",
    "    missing_combinations_df['start'] = missing_combinations_df['midpoint']\n",
    "    missing_combinations_df['stop'] = missing_combinations_df['midpoint']\n",
    "    missing_combinations_df['chip_category'] = missing_combinations_df['type']\n",
    "\n",
    "    # Assign defaults for fields not derived from fimo\n",
    "    missing_combinations_df['strand'] = '.'\n",
    "    missing_combinations_df['score'] = 0\n",
    "    missing_combinations_df['p_value'] = 1.0\n",
    "    missing_combinations_df['motif_id'] = 'noregion'\n",
    "    new_id_start = fimo_expanded_df['id'].max() + 1 if not fimo_expanded_df.empty else 0\n",
    "    missing_combinations_df['id'] = range(new_id_start, new_id_start + len(missing_combinations_df))\n",
    "    missing_combinations_df['cluster_count'] = 0  # Default since not computed\n",
    "\n",
    "    # Select columns to match fimo_expanded_df\n",
    "    # Adjust if fimo_expanded_df includes bed_start and bed_end in its final columns\n",
    "    new_rows = missing_combinations_df[['chr', 'start', 'stop', 'strand', 'score', 'p_value',\n",
    "                                        'motif_id', 'id', 'chip_category', 'cluster_count']]\n",
    "\n",
    "    # Append to fimo_expanded_df\n",
    "    fimo_expanded_df = pd.concat([fimo_expanded_df, new_rows], ignore_index=True)\n",
    "\n",
    "\n",
    "# **NEW STEP**: Apply bed_window expansion and \"MEX_\" prefix conditionally\n",
    "# **NEW STEP**: Apply bed_window expansion and \"MEX_\" prefix conditionally\n",
    "if center_intergenic:\n",
    "    # If center_intergenic = True, apply to all rows\n",
    "    fimo_expanded_df['start'] = fimo_expanded_df['start'] - bed_window\n",
    "    fimo_expanded_df['start'] = fimo_expanded_df['start'].apply(lambda x: max(0, x))\n",
    "    fimo_expanded_df['stop'] = fimo_expanded_df['start'] + bed_window\n",
    "    # Add \"MEX_\" to all chip_categories\n",
    "    fimo_expanded_df['chip_category'] = 'MEX_' + fimo_expanded_df['chip_category']\n",
    "\n",
    "    # Create combined_bed_df_mex_cat from all processed motifs\n",
    "    combined_bed_df_mex_cat = fimo_expanded_df[['chr', 'start', 'stop', 'strand', 'chip_category']].copy()\n",
    "else:\n",
    "    # If center_intergenic = False, only apply to non-intergenic rows\n",
    "    non_intergenic_mask = ~fimo_expanded_df['chip_category'].str.contains('intergenic', case=False, na=False)\n",
    "\n",
    "    # Center only non-intergenic rows\n",
    "    fimo_expanded_df.loc[non_intergenic_mask, 'start'] = fimo_expanded_df.loc[non_intergenic_mask, 'start'] - bed_window\n",
    "    fimo_expanded_df.loc[non_intergenic_mask, 'start'] = fimo_expanded_df.loc[non_intergenic_mask, 'start'].apply(lambda x: max(0, x))\n",
    "    fimo_expanded_df.loc[non_intergenic_mask, 'stop'] = fimo_expanded_df.loc[non_intergenic_mask, 'start'] + bed_window\n",
    "    # Add \"MEX_\" prefix only to non-intergenic rows\n",
    "    if(combine_motifs):\n",
    "        fimo_expanded_df.loc[non_intergenic_mask, 'chip_category'] = 'MOTIFS_' + fimo_expanded_df.loc[non_intergenic_mask, 'chip_category']\n",
    "    else:\n",
    "        fimo_expanded_df.loc[non_intergenic_mask, 'chip_category'] = fimo_expanded_df.loc[non_intergenic_mask, 'motif_id'] +\"_\"+ fimo_expanded_df.loc[non_intergenic_mask, 'chip_category']\n",
    "\n",
    "    #if include_noregion_motifs == False then drop all rows where chip_category includes \"none\"\n",
    "    if not include_noregion_motifs:\n",
    "        fimo_expanded_df = fimo_expanded_df[~fimo_expanded_df['chip_category'].str.contains('none', case=False, na=False)]\n",
    "\n",
    "    # Create combined_bed_df_mex_cat from non-intergenic processed motifs\n",
    "    combined_bed_df_mex_cat = fimo_expanded_df.loc[non_intergenic_mask, ['chr', 'start', 'stop', 'strand', 'chip_category']].copy()\n",
    "\n",
    "    # Retrieve all intergenic rows from combined_bed_df_ext\n",
    "    intergenic_df = combined_bed_df_ext[combined_bed_df_ext['type'].str.contains('intergenic', case=False, na=False)].copy()\n",
    "\n",
    "    # Drop the duplicate 'strand' column if it exists\n",
    "    if 'strand' in intergenic_df.columns and 'bed_strand' in intergenic_df.columns:\n",
    "        # Remove the original 'strand' column\n",
    "        intergenic_df = intergenic_df.drop(columns=['strand'])\n",
    "\n",
    "    # Now perform the rename so that we get only one 'strand' column.\n",
    "    intergenic_df_renamed = intergenic_df.rename(columns={\n",
    "        'chrom': 'chr',\n",
    "        'bed_start': 'start',\n",
    "        'bed_end': 'stop',\n",
    "        'bed_strand': 'strand',\n",
    "        'type': 'chip_category'\n",
    "    })[['chr', 'start', 'stop', 'strand', 'chip_category']]\n",
    "\n",
    "    # Ensure column names match before concatenation\n",
    "    intergenic_df_renamed = intergenic_df.rename(columns={\n",
    "        'chrom': 'chr',\n",
    "        'bed_start': 'start',\n",
    "        'bed_end': 'stop',\n",
    "        'bed_strand': 'strand',\n",
    "        'type': 'chip_category'\n",
    "    })[['chr', 'start', 'stop', 'strand', 'chip_category']]\n",
    "\n",
    "    # Combine processed non-intergenic motifs with original intergenic rows\n",
    "    combined_bed_df_mex_cat = pd.concat([combined_bed_df_mex_cat, intergenic_df_renamed], ignore_index=True)\n",
    "\n",
    "# Drop NaN rows in 'chr' column\n",
    "combined_bed_df_mex_cat.dropna(subset=['chr'], inplace=True)\n",
    "\n",
    "# Create combined_bed_df_mex_cat\n",
    "combined_bed_df_mex_cat = combined_bed_df_mex_cat.rename(columns={\n",
    "    'chr': 'chrom',\n",
    "    'start': 'bed_start',\n",
    "    'stop': 'bed_end',\n",
    "    'strand': 'bed_strand',\n",
    "    'chip_category': 'type'\n",
    "})\n",
    "\n",
    "# Add 'chr_type' column\n",
    "combined_bed_df_mex_cat['chr_type'] = combined_bed_df_mex_cat['chrom'].apply(\n",
    "    lambda x: 'X' if x == 'CHROMOSOME_X' else 'Autosome'\n",
    ")\n",
    "\n",
    "# Create combined_bed_df_mex_clust\n",
    "combined_bed_df_mex_clust = fimo_expanded_df[['chr', 'start', 'stop', 'strand', 'cluster_count']].copy()\n",
    "combined_bed_df_mex_clust = combined_bed_df_mex_clust.rename(columns={\n",
    "    'chr': 'chrom',\n",
    "    'start': 'bed_start',\n",
    "    'stop': 'bed_end',\n",
    "    'strand': 'bed_strand'\n",
    "})\n",
    "combined_bed_df_mex_clust['type'] = 'clust_' + combined_bed_df_mex_clust['cluster_count'].astype(str)\n",
    "combined_bed_df_mex_clust['chr_type'] = combined_bed_df_mex_clust['chrom'].apply(\n",
    "    lambda x: 'X' if x == 'CHROMOSOME_X' else 'Autosome'\n",
    ")\n",
    "combined_bed_df_mex_clust.drop(columns='cluster_count', inplace=True)\n",
    "\n",
    "# Update 'type' in 'combined_bed_df_mex_cat'\n",
    "combined_bed_df_mex_cat['type'] = combined_bed_df_mex_cat['type'].replace({\n",
    "    'MEX_D1': 'MEX_D1to5',\n",
    "    'MEX_D2': 'MEX_D1to5',\n",
    "    'MEX_D3': 'MEX_D1to5',\n",
    "    'MEX_D4': 'MEX_D1to5',\n",
    "    'MEX_D5': 'MEX_D1to5',\n",
    "    'MEX_D6': 'MEX_D6to9',\n",
    "    'MEX_D7': 'MEX_D6to9',\n",
    "    'MEX_D8': 'MEX_D6to9',\n",
    "    'MEX_D9': 'MEX_D6to9'\n",
    "})\n",
    "\n",
    "print(\"\\ncombined_bed_df_mex_clust:\")\n",
    "# Replace 'nanotools.display_sample_rows' with 'print(combined_bed_df_mex_clust.head())' if 'nanotools' is undefined\n",
    "print(combined_bed_df_mex_clust.head())\n",
    "\n",
    "# Combine all processed motifs into combined_bed_df\n",
    "combined_bed_df = combined_bed_df_mex_cat.copy()\n",
    "\n",
    "# Optional: Include 'MEX_none' if desired\n",
    "# To include 'MEX_none', uncomment the following lines\n",
    "if include_noregion_motifs:\n",
    "    non_mex_none_df = combined_bed_df[combined_bed_df['type'] != 'MEX_none']\n",
    "    mex_none_df = combined_bed_df[combined_bed_df['type'] == 'MEX_none'].sample(\n",
    "        n=min(100, len(combined_bed_df[combined_bed_df['type'] == 'MEX_none'])),\n",
    "        random_state=42  # For reproducibility\n",
    "        )\n",
    "    combined_bed_df = pd.concat([non_mex_none_df, mex_none_df], ignore_index=True)\n",
    "else:\n",
    "    # If excluding 'MEX_none', ensure they are removed\n",
    "    combined_bed_df = combined_bed_df[combined_bed_df['type'] != 'MEX_none']\n",
    "\n",
    "# Display the first few rows (optional)\n",
    "print(\"combined_bed_df:\")\n",
    "# Replace 'nanotools.display_sample_rows' with 'print(combined_bed_df.head())' if 'nanotools' is undefined\n",
    "#combined_bed_df[\"strand\"] = \".\"\n",
    "# set start and end to integers\n",
    "combined_bed_df['bed_start'] = combined_bed_df['bed_start'].astype(int)\n",
    "combined_bed_df['bed_end'] = combined_bed_df['bed_end'].astype(int)\n",
    "nanotools.display_sample_rows(combined_bed_df, 5)\n",
    "\n",
    "\n",
    "# Print count by type\n",
    "print(\"Count by type:\\n\", combined_bed_df['type'].value_counts())\n",
    "\n",
    "# Keep only type that contain 'MEX_D10' (optional)\n",
    "# combined_bed_df = combined_bed_df[combined_bed_df['type'].str.contains('MEX_D10')]\n",
    "\n",
    "# Test to keep only strand == \"+\"\n",
    "# combined_bed_df = combined_bed_df[combined_bed_df['bed_strand'] == '+']\n",
    "\n",
    "import pandas as pd\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "def create_modkit_bed_df(filtered_df):\n",
    "    # filtered_df is combined_bed_df.\n",
    "    # It now contains its original strand information (or '.' for some cases like 'noregion').\n",
    "    # The 'bed_start' and 'bed_end' columns in filtered_df should already be the\n",
    "    # adjusted coordinates from your earlier processing steps.\n",
    "\n",
    "    # Create DataFrame for '+' strands\n",
    "    # We use the coordinate columns from filtered_df which are already adjusted.\n",
    "    # In combined_bed_df, these are 'chrom', 'bed_start', 'bed_end'.\n",
    "    df_plus = pd.DataFrame({\n",
    "        0: filtered_df['chrom'],\n",
    "        1: filtered_df['bed_start'],\n",
    "        2: filtered_df['bed_end'],\n",
    "        3: '.',  # Standard placeholder for name column in BED\n",
    "        4: '.',  # Standard placeholder for score column in BED\n",
    "        5: '+'   # Assign '+' strand\n",
    "    })\n",
    "\n",
    "    # Create DataFrame for '-' strands\n",
    "    df_minus = pd.DataFrame({\n",
    "        0: filtered_df['chrom'],\n",
    "        1: filtered_df['bed_start'],\n",
    "        2: filtered_df['bed_end'],\n",
    "        3: '.',\n",
    "        4: '.',\n",
    "        5: '-'   # Assign '-' strand\n",
    "    })\n",
    "\n",
    "    # Concatenate the '+' and '-' strand DataFrames\n",
    "    modkit_output_df = pd.concat([df_plus, df_minus], ignore_index=True)\n",
    "\n",
    "    # Ensure bed_start and bed_end are integers (BED format requirement)\n",
    "    modkit_output_df[1] = modkit_output_df[1].astype(int)\n",
    "    modkit_output_df[2] = modkit_output_df[2].astype(int)\n",
    "\n",
    "    # The columns are already named 0 through 5 by this construction.\n",
    "    # No need for: modkit_output_df.columns = range(modkit_output_df.shape[1])\n",
    "\n",
    "    return modkit_output_df\n",
    "\n",
    "def save_modkit_bed_to_temp(modkit_bed_df, filename):\n",
    "    # Create a temporary directory\n",
    "    temp_dir = \"/Data1/git/meyer-nanopore/scripts/analysis/temp_files/\"\n",
    "\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "    # Create the full path for the temporary file\n",
    "    temp_file_path = os.path.join(temp_dir, filename)\n",
    "\n",
    "    # Save the dataframe to the temporary file\n",
    "    modkit_bed_df.to_csv(temp_file_path, sep='\\t', header=False, index=False)\n",
    "\n",
    "    print(f\"Modkit BED file saved to: {temp_file_path}\")\n",
    "    return temp_file_path\n",
    "\n",
    "# Create modkit_bed_df\n",
    "modkit_bed_df = create_modkit_bed_df(combined_bed_df)\n",
    "\n",
    "# Drop duplicate rows to ensure each unique region has one '+' and one '-' entry\n",
    "# This is important because create_modkit_bed_df processes each row of combined_bed_df.\n",
    "# If combined_bed_df had multiple entries that would map to the same chrom/start/end\n",
    "# (e.g. if it already contained a + and - version of a region),\n",
    "# this drop_duplicates will clean it up to a unique +/- pair.\n",
    "modkit_bed_df = modkit_bed_df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Save modkit_bed_df to a temporary file\n",
    "modkit_bed_name = \"modkit_temp.bed\"\n",
    "temp_file_path = save_modkit_bed_to_temp(modkit_bed_df, modkit_bed_name)\n",
    "\n",
    "print(f\"Modkit BED file saved to: {temp_file_path}\")\n",
    "\n",
    "# Print the de-duplicated DataFrame\n",
    "print(\"De-duplicated modkit_bed_df content:\")\n",
    "print(modkit_bed_df)\n",
    "\n",
    "# print number of rows after de-duplication\n",
    "print(\"len(modkit_bed_df) after de-duplication:\",len(modkit_bed_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86a6ca7d6f10790",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ft_files = bam_files with \".sorted.bam\" replaced with \".ftools.bed\"\n",
    "ft_files = [x.replace(\".bam\", \"_ftools0p8.bed\") for x in bam_files]\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Helper: Parse a comma‐separated string into a list of integers, ignoring -1 values.\n",
    "def parse_int_list(s):\n",
    "    try:\n",
    "        lst = [int(x) for x in s.split(',') if x.strip() != '']\n",
    "        return [x for x in lst if x != -1]\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def ingest_and_adjust_bed_file(file_path, file_id, selected_type, combined_bed_df_int, bed_window, exp_id):\n",
    "    \"\"\"\n",
    "    Ingests a mod_mappings.ftools.bed file, filters for reads overlapping regions\n",
    "    (from combined_bed_df_ext_mex that match the selected_type), and adjusts feature positions\n",
    "    relative to the midpoint of each region.\n",
    "\n",
    "    Returns a DataFrame where each row is an overlapping read-region pair,\n",
    "    with added columns: read_end, read_end_adj.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        mod_df = pd.read_csv(file_path, sep=\"\\t\", header=0)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Remove any leading '#' from column names.\n",
    "    mod_df.columns = [col.lstrip('#') for col in mod_df.columns]\n",
    "\n",
    "    # Filter regions for the selected type.\n",
    "    regions = combined_bed_df_int[combined_bed_df_int['type'] == selected_type]\n",
    "\n",
    "    rows = []\n",
    "    read_counter = 0\n",
    "\n",
    "    for r_idx, region in regions.iterrows():\n",
    "        region_chrom  = region['chrom']\n",
    "        region_start  = region['bed_start']\n",
    "        region_end    = region['bed_end']\n",
    "        region_strand = region['bed_strand']\n",
    "        region_mid    = (region_start + region_end) // 2\n",
    "\n",
    "        # Reads overlapping the region\n",
    "        region_mod = mod_df[\n",
    "            (mod_df['ct'] == region_chrom) &\n",
    "            (mod_df['en'] > region_start) &\n",
    "            (mod_df['st'] < region_end)\n",
    "        ]\n",
    "\n",
    "        if region_strand != '.':\n",
    "            region_mod = region_mod[region_mod['strand'] == region_strand]\n",
    "        if region_mod.empty:\n",
    "            continue\n",
    "\n",
    "        for _, read in region_mod.iterrows():\n",
    "            read_st     = int(read['st'])\n",
    "            read_end    = int(read['en'])\n",
    "            read_st_adj = read_st  - region_mid\n",
    "            read_end_adj= read_end - region_mid\n",
    "\n",
    "            # Nucleosome processing\n",
    "            adjusted_ref_nuc = None\n",
    "            adjusted_ref_nuc_lengths = None\n",
    "            if 'ref_nuc_starts' in read and pd.notnull(read['ref_nuc_starts']):\n",
    "                nuc_positions = parse_int_list(str(read['ref_nuc_starts']))\n",
    "                rel_positions = [x - region_mid for x in nuc_positions]\n",
    "                adjusted_ref_nuc = [pos for pos in rel_positions if -bed_window <= pos <= bed_window]\n",
    "\n",
    "                if 'ref_nuc_lengths' in read and pd.notnull(read['ref_nuc_lengths']):\n",
    "                    lengths = parse_int_list(str(read['ref_nuc_lengths']))\n",
    "                    if len(lengths) == len(nuc_positions):\n",
    "                        adjusted_ref_nuc_lengths = [\n",
    "                            l for pos, l in zip(rel_positions, lengths)\n",
    "                            if -bed_window <= pos <= bed_window\n",
    "                        ]\n",
    "\n",
    "            # MSP processing\n",
    "            adjusted_ref_msp = None\n",
    "            adjusted_ref_msp_lengths = None\n",
    "            if 'ref_msp_starts' in read and pd.notnull(read['ref_msp_starts']):\n",
    "                msp_positions = parse_int_list(str(read['ref_msp_starts']))\n",
    "                rel_positions = [x - region_mid for x in msp_positions]\n",
    "                adjusted_ref_msp = [pos for pos in rel_positions if -bed_window <= pos <= bed_window]\n",
    "\n",
    "                if 'ref_msp_lengths' in read and pd.notnull(read['ref_msp_lengths']):\n",
    "                    lengths = parse_int_list(str(read['ref_msp_lengths']))\n",
    "                    if len(lengths) == len(msp_positions):\n",
    "                        adjusted_ref_msp_lengths = [\n",
    "                            l for pos, l in zip(rel_positions, lengths)\n",
    "                            if -bed_window <= pos <= bed_window\n",
    "                        ]\n",
    "\n",
    "            # m6A and 5mC points\n",
    "            adjusted_ref_m6a = None\n",
    "            if 'ref_m6a' in read and pd.notnull(read['ref_m6a']):\n",
    "                m6a_positions = parse_int_list(str(read['ref_m6a']))\n",
    "                rel_positions = [x - region_mid for x in m6a_positions]\n",
    "                adjusted_ref_m6a = [pos for pos in rel_positions if -bed_window <= pos <= bed_window]\n",
    "\n",
    "            adjusted_ref_5mC = None\n",
    "            if 'ref_5mC' in read and pd.notnull(read['ref_5mC']):\n",
    "                m5c_positions = parse_int_list(str(read['ref_5mC']))\n",
    "                rel_positions = [x - region_mid for x in m5c_positions]\n",
    "                adjusted_ref_5mC = [pos for pos in rel_positions if -bed_window <= pos <= bed_window]\n",
    "\n",
    "            rows.append({\n",
    "                'file_id': file_id,\n",
    "                'exp_id': exp_id,\n",
    "                'region_index': r_idx,\n",
    "                'region_chrom': region_chrom,\n",
    "                'region_start': region_start,\n",
    "                'region_end': region_end,\n",
    "                'region_strand': region_strand,\n",
    "                'region_mid': region_mid,\n",
    "                'read_index': read_counter,\n",
    "                'read_st': read_st,\n",
    "                'read_end': read_end,\n",
    "                'read_st_adj': read_st_adj,\n",
    "                'read_end_adj': read_end_adj,\n",
    "                'fiber': read.get('fiber', None),\n",
    "                'adjusted_ref_nuc': adjusted_ref_nuc,\n",
    "                'adjusted_ref_nuc_lengths': adjusted_ref_nuc_lengths,\n",
    "                'adjusted_ref_msp': adjusted_ref_msp,\n",
    "                'adjusted_ref_msp_lengths': adjusted_ref_msp_lengths,\n",
    "                'adjusted_ref_m6a': adjusted_ref_m6a,\n",
    "                'adjusted_ref_5mC': adjusted_ref_5mC,\n",
    "                'fiber_sequence': read.get('fiber_sequence', None)\n",
    "            })\n",
    "            read_counter += 1\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# --- Parallel wrapper ---\n",
    "def process_single_file(args_tuple):\n",
    "    args, selected_type, combined_bed_df_int2, bed_window = args_tuple\n",
    "    # Unpack with exp_id fallback\n",
    "    if len(args) >= 6:\n",
    "        file_path, file_id, flag1, flag2, output_dir, exp_id = args[:6]\n",
    "    else:\n",
    "        file_path, file_id, flag1, flag2, output_dir = args[:5]\n",
    "        exp_id = \"unknown\"\n",
    "\n",
    "    print(f\"Processing {file_id} (exp {exp_id})\")\n",
    "    df = ingest_and_adjust_bed_file(\n",
    "        file_path, file_id, selected_type, combined_bed_df_int2, bed_window, exp_id\n",
    "    )\n",
    "    if df is not None and not df.empty:\n",
    "        print(f\"  → {len(df)} rows\")\n",
    "        return df\n",
    "    else:\n",
    "        print(\"  → no overlaps\")\n",
    "        return None\n",
    "\n",
    "def process_all_files(args_list, selected_type, combined_bed_df_int3, bed_window, num_threads=None):\n",
    "    import multiprocessing as mp\n",
    "    from multiprocessing import Pool\n",
    "\n",
    "    num_threads = num_threads or mp.cpu_count()\n",
    "    num_threads = min(int(num_threads), mp.cpu_count())\n",
    "    print(f\"Using {num_threads} threads on {len(args_list)} files\")\n",
    "\n",
    "    pool_args = [\n",
    "        (args, selected_type, combined_bed_df_int3, bed_window)\n",
    "        for args in args_list\n",
    "    ]\n",
    "\n",
    "    with Pool(processes=num_threads) as pool:\n",
    "        results = pool.map(process_single_file, pool_args)\n",
    "\n",
    "    dfs = [df for df in results if df is not None and not df.empty]\n",
    "    if not dfs:\n",
    "        print(\"No data processed.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"Combined {len(dfs)} files into {len(combined_df)} total rows.\")\n",
    "    return combined_df\n",
    "\n",
    "# --- Example usage ---\n",
    "selected_type = 'MOTIFS_strong_rex'\n",
    "num_threads   = 15\n",
    "\n",
    "combined_df = process_all_files(\n",
    "    args_list,\n",
    "    selected_type,\n",
    "    combined_bed_df_ext_mex,\n",
    "    bed_window,\n",
    "    num_threads\n",
    ")\n",
    "\n",
    "print(combined_df.head())\n",
    "print(f\"Total rows: {len(combined_df)}\")\n",
    "print(f\"Unique files: {combined_df['file_id'].nunique()}\")\n",
    "print(f\"Unique exps : {combined_df['exp_id'].nunique()}\")\n",
    "\n",
    "# Optionally save:\n",
    "# combined_df.to_csv(\"combined_results_with_end.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557d8f33677658f1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyranges as pr\n",
    "\n",
    "# ---------------------------\n",
    "# MEX MOTIF LOOKUP FOR combined_df WITH STRAND INFO\n",
    "# ---------------------------\n",
    "\n",
    "# Load FIMO files\n",
    "fimo_files = [\n",
    "    \"/Data1/ext_data/motifs/fimo_MEX_0.01.tsv\",\n",
    "    \"/Data1/ext_data/motifs/fimo_MEXII_0.01.tsv\",\n",
    "    \"/Data1/ext_data/motifs/fimo_motifc_0.01.tsv\"\n",
    "]\n",
    "\n",
    "dfs = []\n",
    "for file in fimo_files:\n",
    "    print(f\"Loading {file}\")\n",
    "    df = pd.read_csv(file, sep='\\t')\n",
    "    print(f\"Loaded {len(df)} rows from {file}\")\n",
    "    dfs.append(df)\n",
    "\n",
    "fimo_df = pd.concat(dfs, ignore_index=True)\n",
    "print(f\"Combined FIMO dataframe has {len(fimo_df)} rows\")\n",
    "\n",
    "# Convert sequence_name from 'chr' to 'CHROMOSOME_*' format\n",
    "print(\"Converting 'sequence_name' to 'CHROMOSOME_*' format\")\n",
    "fimo_df['sequence_name'] = fimo_df['sequence_name'].str.replace('chr', 'CHROMOSOME_')\n",
    "\n",
    "# Filter rows with score > 5\n",
    "print(\"Filtering rows with score > 5\")\n",
    "fimo_df = fimo_df[fimo_df['score'] > 5]\n",
    "print(f\"Filtered FIMO dataframe has {len(fimo_df)} rows\")\n",
    "\n",
    "# Select and rename relevant columns\n",
    "print(\"Selecting relevant columns\")\n",
    "fimo_df = fimo_df[['sequence_name', 'start', 'stop', 'strand', 'score', 'p-value', 'motif_id']]\n",
    "fimo_df = fimo_df.rename(columns={'sequence_name': 'chr', 'p-value': 'p_value'})\n",
    "\n",
    "# Assign motif priorities\n",
    "print(\"Assigning motif priorities\")\n",
    "motif_priority = {'MEXII': 1, 'MEX': 2, 'motifC': 3}\n",
    "fimo_df['motif_priority'] = fimo_df['motif_id'].map(motif_priority)\n",
    "\n",
    "# Sort by chromosome, start, and motif_priority\n",
    "print(\"Sorting FIMO dataframe\")\n",
    "fimo_df = fimo_df.sort_values(by=['chr', 'start', 'motif_priority'])\n",
    "\n",
    "# Deduplicate overlapping intervals using PyRanges\n",
    "print(\"Deduplicating overlapping intervals using PyRanges\")\n",
    "fimo_df = fimo_df.rename(columns={'chr': 'Chromosome', 'start': 'Start', 'stop': 'End'})\n",
    "fimo_df['Start'] = fimo_df['Start'].astype(int)\n",
    "fimo_df['End'] = fimo_df['End'].astype(int)\n",
    "pr_df = pr.PyRanges(fimo_df)\n",
    "clusters = pr_df.cluster()\n",
    "clusters_df = clusters.df\n",
    "clusters_df = clusters_df.sort_values(['Cluster', 'motif_priority'])\n",
    "fimo_dedup_df = clusters_df.drop_duplicates(subset=['Cluster'], keep='first')\n",
    "# Rename back for clarity\n",
    "fimo_dedup_df = fimo_dedup_df.rename(columns={'Chromosome': 'chr', 'Start': 'start', 'End': 'stop'})\n",
    "print(f\"Deduplicated FIMO dataframe has {len(fimo_dedup_df)} rows\")\n",
    "\n",
    "# Prepare DataFrames for the join\n",
    "# Rename FIMO columns for PyRanges compatibility\n",
    "fimo_df_renamed = fimo_dedup_df.rename(columns={'chr': 'Chromosome', 'start': 'Start', 'stop': 'End'})\n",
    "\n",
    "# For combined_df, use the region columns as the interval\n",
    "combined_df_renamed = combined_df.rename(columns={\n",
    "    'region_chrom': 'Chromosome',\n",
    "    'region_start': 'Start',\n",
    "    'region_end': 'End'\n",
    "})\n",
    "\n",
    "print(\"Converting DataFrames to PyRanges objects\")\n",
    "fimo_pr = pr.PyRanges(fimo_df_renamed)\n",
    "combined_df_pr = pr.PyRanges(combined_df_renamed)\n",
    "\n",
    "print(\"Performing join to find overlapping intervals\")\n",
    "overlap_result = combined_df_pr.join(fimo_pr, suffix=\"_fimo\")\n",
    "\n",
    "# Aggregate overlapping motif intervals along with their strands.\n",
    "print(\"Aggregating overlapping intervals with strand info\")\n",
    "overlap_df = overlap_result.df\n",
    "agg_df = overlap_df.groupby(['Chromosome', 'Start', 'End']).apply(\n",
    "    lambda x: list(zip(x['Start_fimo'], x['motif_id'], x['strand']))\n",
    ").reset_index(name='motif_info')\n",
    "\n",
    "print(\"Removing duplicate motif info\")\n",
    "agg_df['motif_info'] = agg_df['motif_info'].apply(lambda x: list(set(x)))\n",
    "\n",
    "# Separate aggregated motif info into motif_start, motif_id, and motif_strand lists\n",
    "print(\"Separating aggregated motif info into motif_start, motif_id, and motif_strand\")\n",
    "agg_df['motif_start'] = agg_df['motif_info'].apply(lambda x: [item[0] for item in x])\n",
    "agg_df['motif_id'] = agg_df['motif_info'].apply(lambda x: [item[1] for item in x])\n",
    "agg_df['motif_strand'] = agg_df['motif_info'].apply(lambda x: [item[2] for item in x])\n",
    "\n",
    "# Compute motif_rel_start using the formula: motif_start - region_start - bed_window\n",
    "print(\"Computing motif_rel_start\")\n",
    "agg_df['motif_rel_start'] = agg_df.apply(\n",
    "    lambda row: [start - row['Start'] - bed_window for start in row['motif_start']], axis=1\n",
    ")\n",
    "\n",
    "# Rename the interval columns back to match combined_df's keys\n",
    "agg_df = agg_df.rename(columns={'Chromosome': 'region_chrom', 'Start': 'region_start', 'End': 'region_end'})\n",
    "\n",
    "print(\"Merging aggregated motif info back into combined_df\")\n",
    "combined_df = combined_df.merge(\n",
    "    agg_df[['region_chrom', 'region_start', 'region_end', 'motif_rel_start', 'motif_id', 'motif_strand']],\n",
    "    how='left',\n",
    "    on=['region_chrom', 'region_start', 'region_end']\n",
    ")\n",
    "\n",
    "# Convert lists to tuples for consistency\n",
    "print(\"Converting lists to tuples\")\n",
    "combined_df['motif_rel_start'] = combined_df['motif_rel_start'].apply(\n",
    "    lambda x: tuple(x) if isinstance(x, list) else x\n",
    ")\n",
    "combined_df['motif_id'] = combined_df['motif_id'].apply(\n",
    "    lambda x: tuple(x) if isinstance(x, list) else x\n",
    ")\n",
    "combined_df['motif_strand'] = combined_df['motif_strand'].apply(\n",
    "    lambda x: tuple(x) if isinstance(x, list) else x\n",
    ")\n",
    "\n",
    "# Replace NaN with empty tuples\n",
    "print(\"Replacing NaN with empty tuples\")\n",
    "combined_df['motif_rel_start'] = combined_df['motif_rel_start'].apply(\n",
    "    lambda x: tuple() if pd.isna(x) else x\n",
    ")\n",
    "combined_df['motif_id'] = combined_df['motif_id'].apply(\n",
    "    lambda x: tuple() if pd.isna(x) else x\n",
    ")\n",
    "combined_df['motif_strand'] = combined_df['motif_strand'].apply(\n",
    "    lambda x: tuple() if pd.isna(x) else x\n",
    ")\n",
    "\n",
    "# Ensure all elements in motif_rel_start are integers\n",
    "combined_df['motif_rel_start'] = combined_df['motif_rel_start'].apply(\n",
    "    lambda x: tuple(map(int, x)) if isinstance(x, tuple) else x\n",
    ")\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# ───────────────────────────────────────────────────\n",
    "# DEBUG: filter out reads shorter than 1000 bp\n",
    "# (drop in immediately after combined_df is defined)\n",
    "# ───────────────────────────────────────────────────\n",
    "print(\"Filtering out reads shorter than 1000 bp…\")\n",
    "\n",
    "# 1) compute read lengths\n",
    "combined_df['read_length'] = combined_df['fiber_sequence'] \\\n",
    "    .apply(lambda x: len(x) if isinstance(x, str) else 0)\n",
    "\n",
    "# 2) snapshot before\n",
    "before_rows  = combined_df.shape[0]\n",
    "before_reads = combined_df['read_index'].nunique()\n",
    "print(f\"→ Before filter: {before_rows} rows, {before_reads} unique reads\")\n",
    "\n",
    "# 3) apply the filter\n",
    "#combined_df = combined_df[ combined_df['read_length'] >= 500 ]\n",
    "\n",
    "# 4) snapshot after\n",
    "after_rows  = combined_df.shape[0]\n",
    "after_reads = combined_df['read_index'].nunique()\n",
    "print(f\"→ After filter : {after_rows} rows, {after_reads} unique reads\")\n",
    "\n",
    "# 5) clean up\n",
    "combined_df.drop(columns=['read_length'], inplace=True)\n",
    "print(\"Done filtering.\\n\")\n",
    "# ───────────────────────────────────────────────────\n",
    "\n",
    "# ───────────────────────────────────────────────────\n",
    "# DEBUG: drop all reads that have no adjusted_ref_nuc\n",
    "# ───────────────────────────────────────────────────\n",
    "print(\"Filtering out reads with zero nucleotides (no adjusted_ref_nuc)…\")\n",
    "# count per-read nucs\n",
    "combined_df['nuc_count'] = combined_df['adjusted_ref_nuc'].apply(lambda x: len(x) if isinstance(x, (list, tuple)) else 0)\n",
    "\n",
    "before2_rows  = combined_df.shape[0]\n",
    "before2_reads = combined_df['read_index'].nunique()\n",
    "print(f\"→ Before nuc-filter  : {before2_rows} rows, {before2_reads} unique reads\")\n",
    "\n",
    "# drop reads where nuc_count == 0\n",
    "combined_df = combined_df[combined_df['nuc_count'] > 0]\n",
    "\n",
    "after2_rows  = combined_df.shape[0]\n",
    "after2_reads = combined_df['read_index'].nunique()\n",
    "print(f\"→ After nuc-filter   : {after2_rows} rows, {after2_reads} unique reads\")\n",
    "\n",
    "combined_df.drop(columns=['nuc_count'], inplace=True)\n",
    "print(\"Done nuc-filter.\\n\")\n",
    "\n",
    "# # ───────────────────────────────────────────────────\n",
    "# # DEBUG: drop duplicate fiber_sequence\n",
    "# # ───────────────────────────────────────────────────\n",
    "# print(\"Filtering out duplicate fiber_sequence rows…\")\n",
    "# before3_rows     = combined_df.shape[0]\n",
    "# before3_sequences = combined_df['fiber_sequence'].nunique()\n",
    "# print(f\"→ Before dedup    : {before3_rows} rows, {before3_sequences} unique sequences\")\n",
    "#\n",
    "# combined_df = combined_df.drop_duplicates(subset=['fiber_sequence'], keep='first')\n",
    "#\n",
    "# after3_rows      = combined_df.shape[0]\n",
    "# after3_sequences = combined_df['fiber_sequence'].nunique()\n",
    "# print(f\"→ After dedup     : {after3_rows} rows, {after3_sequences} unique sequences\")\n",
    "# print(\"Done dedup.\\n\")\n",
    "\n",
    "\n",
    "# Print the first 2 rows\n",
    "print(combined_df.head(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f3f32a16d4094d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import nanotools  # for get_color / get_colors\n",
    "\n",
    "# assume combined_df is already loaded\n",
    "\n",
    "# 1) trim file_id to everything before the first \"_\"\n",
    "combined_df['file_base'] = combined_df['file_id'].str.split('_', 1).str[0]\n",
    "\n",
    "# 2) compute read length and N50 per (file_base, exp_id)\n",
    "combined_df['read_length'] = combined_df['fiber_sequence'].str.len()\n",
    "\n",
    "def compute_n50(lengths):\n",
    "    lengths = np.array(sorted(lengths, reverse=True))\n",
    "    cum = lengths.cumsum()\n",
    "    half = lengths.sum() / 2\n",
    "    idx = np.searchsorted(cum, half)\n",
    "    return int(lengths[idx])\n",
    "\n",
    "n50_df = (\n",
    "    combined_df\n",
    "    .groupby(['file_base', 'exp_id'])['read_length']\n",
    "    .apply(compute_n50)\n",
    "    .reset_index(name='N50')\n",
    ")\n",
    "\n",
    "# 3) build the plot\n",
    "fig = go.Figure()\n",
    "for fb, grp in n50_df.groupby('file_base'):\n",
    "    color = nanotools.get_color(fb)\n",
    "    # box with no fill\n",
    "    fig.add_trace(go.Box(\n",
    "        y=grp['N50'],\n",
    "        name=fb,\n",
    "        fillcolor='rgba(0,0,0,0)',\n",
    "        line_color=color,\n",
    "        boxpoints='all',\n",
    "        jitter=0.3,\n",
    "        pointpos=0,\n",
    "        marker=dict(size=6, color=color),\n",
    "        showlegend=False\n",
    "    ))\n",
    "    # one marker per exp_id\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=[fb] * len(grp),\n",
    "        y=grp['N50'],\n",
    "        mode='markers+text',\n",
    "        marker=dict(size=6, color=color),\n",
    "        text=grp['exp_id'],\n",
    "        textposition='top center',\n",
    "        textfont=dict(size=10, color='black'),\n",
    "        showlegend=False\n",
    "    ))\n",
    "\n",
    "\n",
    "    # annotate means\n",
    "    mean_n50 = grp['N50'].mean()\n",
    "    fig.add_annotation(\n",
    "        x=fb,\n",
    "        y=mean_n50,\n",
    "        text=f\"{mean_n50:.0f}\",\n",
    "        # NO line or arrow\n",
    "        showarrow=False\n",
    "    )\n",
    "\n",
    "# set y range to 0 - 13000\n",
    "fig.update_yaxes(range=[0, 15000])\n",
    "\n",
    "fig.update_layout(\n",
    "    template='plotly_white',\n",
    "    xaxis_title='File',\n",
    "    yaxis_title='N50 Read Length',\n",
    "    width=550,\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cd8b63ac5c7ba7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def build_percentage_df(combined_df, bed_window, smooth_window=25, group_by=None):\n",
    "#     \"\"\"\n",
    "#     Processes the combined DataFrame to build a new DataFrame with counts and percentages\n",
    "#     for m6a and 5mC marks at each relative genomic position (from -bed_window to bed_window).\n",
    "# \n",
    "#     Can group results by file_id, exp_id, or both if specified.\n",
    "# \n",
    "#     Denominator (for both m6a and 5mC) is the total number of reads overlapping that position.\n",
    "#     Numerators come from the adjusted marks (with a +1 offset).\n",
    "# \n",
    "#     A centered moving average smoothing is applied to the percentages with the given window size.\n",
    "# \n",
    "#     Parameters:\n",
    "#       combined_df  : Combined DataFrame containing the overlapping read-region pairs from multiple files.\n",
    "#       bed_window   : integer window (e.g. 1000) defining the range [-bed_window, bed_window].\n",
    "#       smooth_window: integer, size of the smoothing window in bp (e.g. 25).\n",
    "#       group_by     : None, 'file_id', 'exp_id', or ['file_id', 'exp_id'] to determine grouping.\n",
    "# \n",
    "#     Returns:\n",
    "#       A DataFrame with columns:\n",
    "#         - file_id: identifier of the source file (if grouped by file_id)\n",
    "#         - exp_id: identifier of the experiment (if grouped by exp_id)\n",
    "#         - position: relative genomic positions.\n",
    "#         - denom: number of reads covering that position.\n",
    "#         - m6a_num: count of m6a marks at that position.\n",
    "#         - m6a_perc: raw percentage (100 * m6a_num / denom).\n",
    "#         - m6a_perc_smooth: smoothed percentage.\n",
    "#         - 5mC_num: count of 5mC marks at that position.\n",
    "#         - 5mC_perc: raw percentage (100 * 5mC_num / denom).\n",
    "#         - 5mC_perc_smooth: smoothed percentage.\n",
    "#     \"\"\"\n",
    "#     import numpy as np\n",
    "#     import pandas as pd\n",
    "#     from functools import partial\n",
    "# \n",
    "#     # Define a simple centered moving average function\n",
    "#     def moving_average(x, window):\n",
    "#         return np.convolve(x, np.ones(window) / window, mode='same')\n",
    "# \n",
    "#     # Process a single group of data and return a DataFrame\n",
    "#     def process_group(group_df, bed_window, smooth_window):\n",
    "#         pos_range = np.arange(-bed_window, bed_window + 1)\n",
    "#         denominator = np.zeros(len(pos_range), dtype=int)\n",
    "#         m6a_num = np.zeros(len(pos_range), dtype=int)\n",
    "#         m5c_num = np.zeros(len(pos_range), dtype=int)\n",
    "# \n",
    "#         # For each overlapping read-region pair, add 1 to denominator for each base the read covers.\n",
    "#         for _, row in group_df.iterrows():\n",
    "#             read_st_adj = row['read_st_adj']\n",
    "#             fiber_seq = row['fiber_sequence']\n",
    "#             if pd.notnull(fiber_seq) and isinstance(fiber_seq, str):\n",
    "#                 L = len(fiber_seq)\n",
    "#                 # Read covers positions from read_st_adj + 1 to read_st_adj + L (inclusive)\n",
    "#                 for i in range(L):\n",
    "#                     pos = read_st_adj + 1 + i  # +1 offset applied here\n",
    "#                     if -bed_window <= pos <= bed_window:\n",
    "#                         bin_idx = pos + bed_window\n",
    "#                         denominator[bin_idx] += 1\n",
    "# \n",
    "#             # Process m6a numerator: For each m6a mark, add 1 (with a +1 offset)\n",
    "#             adjusted_ref_m6a = row['adjusted_ref_m6a']\n",
    "#             if isinstance(adjusted_ref_m6a, list):\n",
    "#                 for pos in adjusted_ref_m6a:\n",
    "#                     pos_new = pos + 1  # apply +1 offset\n",
    "#                     if -bed_window <= pos_new <= bed_window:\n",
    "#                         bin_idx = pos_new + bed_window\n",
    "#                         m6a_num[bin_idx] += 1\n",
    "# \n",
    "#             # Process 5mC numerator: For each 5mC mark, add 1 (with a +1 offset)\n",
    "#             adjusted_ref_5mC = row['adjusted_ref_5mC']\n",
    "#             if isinstance(adjusted_ref_5mC, list):\n",
    "#                 for pos in adjusted_ref_5mC:\n",
    "#                     pos_new = pos + 1  # apply +1 offset\n",
    "#                     if -bed_window <= pos_new <= bed_window:\n",
    "#                         bin_idx = pos_new + bed_window\n",
    "#                         m5c_num[bin_idx] += 1\n",
    "# \n",
    "#         # Compute raw percentages\n",
    "#         m6a_perc = np.where(denominator > 0, 100 * m6a_num / denominator, np.nan)\n",
    "#         m5c_perc = np.where(denominator > 0, 100 * m5c_num / denominator, np.nan)\n",
    "# \n",
    "#         # Apply smoothing if smooth_window is greater than 1\n",
    "#         if smooth_window and smooth_window > 1:\n",
    "#             m6a_perc_smooth = moving_average(m6a_perc, smooth_window)\n",
    "#             m5c_perc_smooth = moving_average(m5c_perc, smooth_window)\n",
    "#         else:\n",
    "#             m6a_perc_smooth = m6a_perc\n",
    "#             m5c_perc_smooth = m5c_perc\n",
    "# \n",
    "#         result_df = pd.DataFrame({\n",
    "#             'position': pos_range,\n",
    "#             'denom': denominator,\n",
    "#             'm6a_num': m6a_num,\n",
    "#             'm6a_perc': m6a_perc,\n",
    "#             'm6a_perc_smooth': m6a_perc_smooth,\n",
    "#             '5mC_num': m5c_num,\n",
    "#             '5mC_perc': m5c_perc,\n",
    "#             '5mC_perc_smooth': m5c_perc_smooth\n",
    "#         })\n",
    "# \n",
    "#         return result_df\n",
    "# \n",
    "#     # If no grouping is specified, process the entire dataframe at once\n",
    "#     if group_by is None:\n",
    "#         return process_group(combined_df, bed_window, smooth_window)\n",
    "# \n",
    "#     # Validate the group_by parameter\n",
    "#     valid_group_by = ['file_id', 'exp_id', ['file_id', 'exp_id']]\n",
    "#     if group_by not in valid_group_by:\n",
    "#         raise ValueError(f\"group_by must be one of {valid_group_by}\")\n",
    "# \n",
    "#     # Group by the specified column(s) and process each group\n",
    "#     grouped = combined_df.groupby(group_by)\n",
    "#     result_dfs = []\n",
    "# \n",
    "#     for group_name, group_df in grouped:\n",
    "#         # Process this group\n",
    "#         group_result = process_group(group_df, bed_window, smooth_window)\n",
    "# \n",
    "#         # Add group identifiers to the result\n",
    "#         if isinstance(group_by, list):\n",
    "#             # If grouped by multiple columns, add each as a separate column\n",
    "#             for i, col_name in enumerate(group_by):\n",
    "#                 group_result[col_name] = group_name[i]\n",
    "#         else:\n",
    "#             # If grouped by a single column, add it as a column\n",
    "#             group_result[group_by] = group_name\n",
    "# \n",
    "#         result_dfs.append(group_result)\n",
    "# \n",
    "#     # Combine all group results\n",
    "#     final_df = pd.concat(result_dfs, ignore_index=True)\n",
    "#     return final_df\n",
    "# \n",
    "# # --- Example usage in a notebook cell ---\n",
    "# # Process all data combined (no grouping)\n",
    "# df_perc_all = build_percentage_df(combined_df, bed_window=2000, smooth_window=25)\n",
    "# \n",
    "# # Group by experiment ID\n",
    "# df_perc_by_exp = build_percentage_df(combined_df, bed_window=2000, smooth_window=25, group_by='exp_id')\n",
    "# \n",
    "# # Group by file ID\n",
    "# df_perc_by_file = build_percentage_df(combined_df, bed_window=2000, smooth_window=25, group_by='file_id')\n",
    "# \n",
    "# # Group by both file ID and experiment ID\n",
    "# df_perc_by_both = build_percentage_df(combined_df, bed_window=2000, smooth_window=25, group_by=['file_id', 'exp_id'])\n",
    "# \n",
    "# # Display the first few rows of each result\n",
    "# print(\"All data combined:\")\n",
    "# display(df_perc_all.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a7e146a299cdad",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "import re\n",
    "\n",
    "DEBUG_HIST  = True   # already in your file\n",
    "DEBUG_READS = True    # NEW  – read-level diagnostics\n",
    "\n",
    "def filter_by_file_and_exp_id(df, file_id_input):\n",
    "    \"\"\"\n",
    "    file_id_input can be:\n",
    "      - A string: e.g. \"example_file_id\"\n",
    "      - A list/tuple of length 1: e.g. [\"example_file_id\"]\n",
    "      - A list/tuple of length 2: e.g. [\"example_file_id\", 0]\n",
    "    If the second element is provided (e.g. index=0), we filter to the\n",
    "    `exp_id` at that position among the unique `exp_id` values for that file.\n",
    "    If the second element is not provided, we include all `exp_id` for that file.\n",
    "    \"\"\"\n",
    "    if isinstance(file_id_input, (list, tuple)):\n",
    "        actual_file_id = file_id_input[0]\n",
    "        subset = df[df['file_id'] == actual_file_id]\n",
    "        if len(file_id_input) > 1 and file_id_input[1] is not None:\n",
    "            exp_index = file_id_input[1]\n",
    "            unique_vals = subset['exp_id'].unique()\n",
    "            unique_vals_sorted = np.sort(unique_vals)\n",
    "            if exp_index < len(unique_vals_sorted):\n",
    "                chosen_exp = unique_vals_sorted[exp_index]\n",
    "                return subset[subset['exp_id'] == chosen_exp].copy()\n",
    "            raise ValueError(\n",
    "                f\"Requested exp_id index {exp_index} is out of range for file_id={actual_file_id}. \"\n",
    "                f\"Available exp_id values are: {unique_vals_sorted}\"\n",
    "            )\n",
    "        else:\n",
    "            return subset.copy()\n",
    "    else:\n",
    "        return df[df['file_id'] == file_id_input].copy()\n",
    "\n",
    "# def get_allowed_reads(df, max_reads):\n",
    "#     \"\"\"\n",
    "#     If max_reads == 0, include every read_index.\n",
    "#     Otherwise, pick a random subset of up to max_reads read_index values\n",
    "#     (uniformly, no length bias) using pandas’ .sample().\n",
    "#     \"\"\"\n",
    "#     # only reads with a valid fiber_sequence\n",
    "#     valid_reads = df.loc[\n",
    "#         df['fiber_sequence'].apply(lambda x: isinstance(x, str)),\n",
    "#         'read_index'\n",
    "#     ].unique()\n",
    "# \n",
    "#     if max_reads == 0 or len(valid_reads) <= max_reads:\n",
    "#         return set(valid_reads)\n",
    "# \n",
    "#     # random sample via pandas\n",
    "#     return set(\n",
    "#         pd.Series(valid_reads)\n",
    "#           .sample(n=max_reads, replace=False)\n",
    "#           .tolist()\n",
    "#     )\n",
    "\n",
    "# ───────────────────────────────────────────────\n",
    "# 0)  put these two lines near the top once\n",
    "RNG = np.random.default_rng(42)        # deterministic sampling\n",
    "OVERLAP_ONLY = True                    # skip fibres that miss the window\n",
    "# ───────────────────────────────────────────────\n",
    "\n",
    "def get_allowed_reads(df, max_reads, *, bed_window, rng):\n",
    "    \"\"\"\n",
    "    Return a *single* random subset of read_index values.\n",
    "\n",
    "    • If OVERLAP_ONLY=True  → keep only reads that cover ≥1 bp of the analysis window\n",
    "    • Sampling is reproducible via `rng`.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This function must be called exactly ONCE per file_id and the\n",
    "    resulting set passed verbatim to *both* compute_histogram_metrics()\n",
    "    and compute_scatter_data(); do **not** call it inside those helpers.\n",
    "    \"\"\"\n",
    "    # keep only reads with a string fibre sequence\n",
    "    good = df.loc[df['fiber_sequence'].apply(lambda x: isinstance(x, str))]\n",
    "\n",
    "    if OVERLAP_ONLY:\n",
    "        # read covers the window if [read_start, read_end] overlaps [-W, +W]\n",
    "        covers = (\n",
    "            (good['read_st_adj'] + 1 + good['fiber_sequence'].str.len() - 1 >= -bed_window)\n",
    "            & (good['read_st_adj'] + 1                                    <=  bed_window)\n",
    "        )\n",
    "        good = good.loc[covers]\n",
    "\n",
    "    read_ids = good['read_index'].unique()\n",
    "\n",
    "    if max_reads == 0 or len(read_ids) <= max_reads:\n",
    "        return set(read_ids)\n",
    "\n",
    "    return set(rng.choice(read_ids, size=max_reads, replace=False))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def compute_histogram_metrics(df, bed_window, allowed_reads, metric_list,\n",
    "                              smooth_window=20, agg_bin_width=10):\n",
    "    \"\"\"\n",
    "    Updated to count unique reads for 'nuc' metric: numerator is the number\n",
    "    of reads with at least one nucleosome overlapping each bin; denominator\n",
    "    is the number of reads overlapping each bin.\n",
    "\n",
    "    Parameters unchanged.\n",
    "    \"\"\"\n",
    "    # retain blank_reads tracking if needed\n",
    "    compute_histogram_metrics.blank_reads = {}\n",
    "\n",
    "    # setup bins\n",
    "    window_bins = np.arange(-bed_window, bed_window + 1)\n",
    "    n_bins = window_bins.size\n",
    "    offset = bed_window\n",
    "\n",
    "    # initialize counts\n",
    "    denom_counts = np.zeros(n_bins, dtype=np.int32)\n",
    "    metric_counts = {m: np.zeros(n_bins, dtype=np.int32) for m in metric_list}\n",
    "\n",
    "    # filter to allowed reads\n",
    "    df_allowed = df[df['read_index'].isin(allowed_reads)]\n",
    "\n",
    "    # process each read once\n",
    "    for _, read_df in df_allowed.groupby('read_index'):\n",
    "        # determine which bins this read covers\n",
    "        covered = np.zeros(n_bins, dtype=bool)\n",
    "        for row in read_df.itertuples(index=False):\n",
    "            if not isinstance(row.fiber_sequence, str):\n",
    "                continue\n",
    "            r_start = row.read_st_adj + 1\n",
    "            r_end = r_start + len(row.fiber_sequence) - 1\n",
    "            lo = max(r_start, -bed_window)\n",
    "            hi = min(r_end, bed_window)\n",
    "            if lo <= hi:\n",
    "                covered[lo + offset:hi + offset + 1] = True\n",
    "        # update denominator: each read contributes at most 1 per bin\n",
    "        denom_counts += covered.astype(np.int32)\n",
    "\n",
    "        # numerator for 'nuc': unique reads with any nuc overlap\n",
    "        if 'nuc' in metric_list:\n",
    "            nuc_mask = np.zeros(n_bins, dtype=bool)\n",
    "            for row in read_df.itertuples(index=False):\n",
    "                starts = getattr(row, 'adjusted_ref_nuc', None)\n",
    "                lengths = getattr(row, 'adjusted_ref_nuc_lengths', None)\n",
    "                if not isinstance(starts, list):\n",
    "                    continue\n",
    "                for i, s in enumerate(starts):\n",
    "                    ln = lengths[i] if (isinstance(lengths, list) and i < len(lengths)) else 1\n",
    "                    e = s + ln - 1\n",
    "                    lo = max(s, -bed_window)\n",
    "                    hi = min(e, bed_window)\n",
    "                    if lo <= hi:\n",
    "                        nuc_mask[lo + offset:hi + offset + 1] = True\n",
    "            metric_counts['nuc'] += nuc_mask.astype(np.int32)\n",
    "\n",
    "        # existing logic for other metrics\n",
    "        for metric in metric_list:\n",
    "            if metric == 'nuc':\n",
    "                continue\n",
    "            if metric == 'msp':\n",
    "                col = 'adjusted_ref_msp'\n",
    "                length_col = f'{col}_lengths'\n",
    "                starts = np.concatenate([\n",
    "                    np.asarray(v, dtype=int) if isinstance(v, list) else np.empty(0, int)\n",
    "                    for v in read_df[col]\n",
    "                ])\n",
    "                lens = np.concatenate([\n",
    "                    np.asarray(l, dtype=int) if isinstance(l, list) else np.empty(0, int)\n",
    "                    for l in read_df.get(length_col, [])\n",
    "                ]) if length_col in read_df else np.empty(0, int)\n",
    "                if lens.size and lens.size == starts.size:\n",
    "                    ends = starts + lens - 1\n",
    "                else:\n",
    "                    ends = starts\n",
    "                for s, e in zip(starts, ends):\n",
    "                    lo = max(s, -bed_window)\n",
    "                    hi = min(e, bed_window)\n",
    "                    if lo <= hi:\n",
    "                        metric_counts['msp'][lo + offset:hi + offset + 1] += 1\n",
    "            else:  # m6a / 5mC\n",
    "                col = f'adjusted_ref_{metric}'\n",
    "                pos = np.concatenate([\n",
    "                    np.asarray(v, dtype=int) if isinstance(v, list) else np.empty(0, int)\n",
    "                    for v in read_df[col]\n",
    "                ])\n",
    "                if pos.size:\n",
    "                    mask = (pos >= -bed_window) & (pos <= bed_window)\n",
    "                    bins = pos[mask] + offset\n",
    "                    np.add.at(metric_counts[metric], bins, 1)\n",
    "\n",
    "    # convert to percentages\n",
    "    bins_dict = {}\n",
    "    metric_percentages = {}\n",
    "    for metric, counts in metric_counts.items():\n",
    "        if metric in ('m6a', '5mC'):\n",
    "            trim = (-n_bins) % agg_bin_width\n",
    "            denom_trim = denom_counts[:-trim] if trim else denom_counts\n",
    "            counts_trim = counts[:-trim] if trim else counts\n",
    "            c2d = counts_trim.reshape(-1, agg_bin_width)\n",
    "            d2d = denom_trim.reshape(-1, agg_bin_width)\n",
    "            agg_counts = c2d.sum(1)\n",
    "            agg_denom = d2d.sum(1)\n",
    "            pct = np.where(agg_denom > 0, 100 * agg_counts / agg_denom, np.nan)\n",
    "            agg_bins = window_bins[:c2d.size * agg_bin_width:agg_bin_width] + (agg_bin_width - 1) / 2.0\n",
    "            if smooth_window > 1:\n",
    "                pct = pd.Series(pct).rolling(window=smooth_window, center=True, min_periods=1).mean().to_numpy()\n",
    "            bins_dict[metric] = agg_bins\n",
    "            metric_percentages[metric] = pct\n",
    "        else:\n",
    "            pct = np.where(denom_counts > 0, 100 * counts / denom_counts, np.nan)\n",
    "            if smooth_window > 1:\n",
    "                pct = pd.Series(pct).rolling(window=smooth_window, center=True, min_periods=1).mean().to_numpy()\n",
    "            bins_dict[metric] = window_bins\n",
    "            metric_percentages[metric] = pct\n",
    "\n",
    "    return bins_dict, metric_percentages\n",
    "\n",
    "\n",
    "\n",
    "def compute_scatter_data(df, bed_window, clustering_window, max_reads, metric_list,\n",
    "                         nuc_axis=0, msp_axis=0, m6a_axis=0, c5m_axis=0):\n",
    "    \"\"\"\n",
    "    Same signature; internal loops minimised by grouping once per read.\n",
    "    \"\"\"\n",
    "    allowed_reads = get_allowed_reads(df, max_reads,bed_window=bed_window, rng=RNG)\n",
    "    df_allowed    = df[df['read_index'].isin(allowed_reads)]\n",
    "\n",
    "    clustering_bins        = np.arange(-clustering_window, clustering_window + 1)\n",
    "    n_clustering_bins      = clustering_bins.size\n",
    "    pos_to_clustering_idx  = {p:i for i, p in enumerate(clustering_bins)}\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # which reads overlap the clustering window?\n",
    "    # -------------------------------------------------------\n",
    "    read_overlaps = {}\n",
    "    for r, group in df_allowed.groupby('read_index'):\n",
    "        any_overlap = False\n",
    "        for row in group.itertuples(index=False):\n",
    "            if not isinstance(row.fiber_sequence, str): continue\n",
    "            r_start = row.read_st_adj + 1\n",
    "            r_end   = r_start + len(row.fiber_sequence) - 1\n",
    "            if (r_start <=  clustering_window) and (r_end >= -clustering_window):\n",
    "                any_overlap = True\n",
    "                break\n",
    "        read_overlaps[r] = any_overlap\n",
    "\n",
    "    overlapping_reads     = [r for r, o in read_overlaps.items() if o]\n",
    "    non_overlapping_reads = [r for r, o in read_overlaps.items() if not o]\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # MSP matrix for clustering (only overlapping reads)\n",
    "    # -------------------------------------------------------\n",
    "    msp_matrix = np.full((len(overlapping_reads), n_clustering_bins), np.nan, dtype=np.float32)\n",
    "    read_row   = {r:i for i, r in enumerate(overlapping_reads)}\n",
    "\n",
    "    if 'msp' in metric_list:\n",
    "        # fill '0' everywhere the read covers the window\n",
    "        for r, group in df_allowed.groupby('read_index'):\n",
    "            if r not in read_row: continue\n",
    "            rr = read_row[r]\n",
    "            for row in group.itertuples(index=False):\n",
    "                if not isinstance(row.fiber_sequence, str): continue\n",
    "                r_start = row.read_st_adj + 1\n",
    "                L       = len(row.fiber_sequence)\n",
    "                for pos in range(max(-clustering_window, r_start),\n",
    "                                 min(clustering_window, r_start+L-1)+1):\n",
    "                    msp_matrix[rr, pos_to_clustering_idx[pos]] = 0       # covered\n",
    "\n",
    "        # set 1 where MSP intervals lie\n",
    "        for r, group in df_allowed.groupby('read_index'):\n",
    "            if r not in read_row: continue\n",
    "            rr = read_row[r]\n",
    "            col, len_col = \"adjusted_ref_msp\", \"adjusted_ref_msp_lengths\"\n",
    "            starts  = np.concatenate([\n",
    "                np.asarray(v, dtype=int) if isinstance(v, list) else np.empty(0, int)\n",
    "                for v in group[col]\n",
    "            ])\n",
    "            lens    = np.concatenate([\n",
    "                np.asarray(v, dtype=int) if isinstance(v, list) else np.empty(0, int)\n",
    "                for v in group[len_col]\n",
    "            ]) if (len_col in group) else np.empty(0, int)\n",
    "\n",
    "            ends = starts + lens - 1 if (lens.size and lens.size == starts.size) else starts\n",
    "            for s, e in zip(starts, ends):\n",
    "                for pos in range(max(-clustering_window, s),\n",
    "                                 min(clustering_window, e)+1):\n",
    "                    msp_matrix[rr, pos_to_clustering_idx[pos]] = 1\n",
    "\n",
    "    # simple clustering: cluster=1 for overlapping reads, 0 otherwise\n",
    "    all_clusters = {r: (1 if r in overlapping_reads else 0) for r in allowed_reads}\n",
    "    sorted_reads = sorted(all_clusters, key=lambda x: (all_clusters[x], x))\n",
    "    y_pos        = {r:i for i, r in enumerate(sorted_reads)}\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # Build scatter_data dict\n",
    "    # -------------------------------------------------------\n",
    "    scatter_data = {m: ([], []) if m in ('nuc','msp') else [] for m in metric_list}\n",
    "    metric_cols  = {'nuc':'adjusted_ref_nuc',\n",
    "                    'msp':'adjusted_ref_msp',\n",
    "                    'm6a':'adjusted_ref_m6a',\n",
    "                    '5mC':'adjusted_ref_5mC'}\n",
    "\n",
    "    for r, group in df_allowed.groupby('read_index'):\n",
    "        y = y_pos[r]\n",
    "\n",
    "        # segments for nuc / msp\n",
    "        for metric in ('nuc', 'msp'):\n",
    "            if metric not in metric_list: continue\n",
    "            col, len_col = metric_cols[metric], f\"{metric_cols[metric]}_lengths\"\n",
    "            starts = np.concatenate([\n",
    "                np.asarray(v, dtype=int) if isinstance(v, list) else np.empty(0, int)\n",
    "                for v in group[col]\n",
    "            ])\n",
    "            lens   = np.concatenate([\n",
    "                np.asarray(v, dtype=int) if isinstance(v, list) else np.empty(0, int)\n",
    "                for v in group[len_col]\n",
    "            ]) if (len_col in group) else np.empty(0, int)\n",
    "            ends   = starts + lens - 1 if (lens.size and lens.size == starts.size) else starts\n",
    "\n",
    "            seg_x, seg_y = scatter_data[metric]\n",
    "            for s, e in zip(starts, ends):\n",
    "                lo = max(-bed_window, s)\n",
    "                hi = min(bed_window,  e)\n",
    "                if lo > hi: continue\n",
    "                seg_x.extend([lo, hi, None])\n",
    "                seg_y.extend([y,  y,  None])\n",
    "\n",
    "        # points for m6a / 5mC\n",
    "        for metric in ('m6a', '5mC'):\n",
    "            if metric not in metric_list: continue\n",
    "            col  = metric_cols[metric]\n",
    "            pos  = np.concatenate([\n",
    "                np.asarray(v, dtype=int) if isinstance(v, list) else np.empty(0, int)\n",
    "                for v in group[col]\n",
    "            ])\n",
    "            pos  = pos[(pos >= -bed_window) & (pos <= bed_window)]\n",
    "            scatter_data[metric].extend([(y, p) for p in pos])\n",
    "\n",
    "    return scatter_data, y_pos, [(r, all_clusters[r]) for r in sorted_reads], all_clusters\n",
    "\n",
    "def compute_nuc_center_histogram(df, bed_window, allowed_reads, agg_bin_width, smooth_window):\n",
    "    \"\"\"\n",
    "    Returns bin_centers and % coverage for the midpoints of each nucleosome interval.\n",
    "    \"\"\"\n",
    "    centers = []\n",
    "    for _, row in df.iterrows():\n",
    "        r_index = row['read_index']\n",
    "        if r_index not in allowed_reads:\n",
    "            continue\n",
    "        starts = row.get('adjusted_ref_nuc')\n",
    "        lengths = row.get('adjusted_ref_nuc_lengths')\n",
    "        if isinstance(starts, list):\n",
    "            if lengths is not None and isinstance(lengths, list) and len(lengths) == len(starts):\n",
    "                for s, l in zip(starts, lengths):\n",
    "                    center = s + l / 2.0\n",
    "                    if -bed_window <= center <= bed_window:\n",
    "                        centers.append(center)\n",
    "            else:\n",
    "                # no lengths, treat each start as the \"center\"\n",
    "                for s in starts:\n",
    "                    if -bed_window <= s <= bed_window:\n",
    "                        centers.append(s)\n",
    "    bins = np.arange(-bed_window, bed_window + agg_bin_width, agg_bin_width)\n",
    "    counts, _ = np.histogram(centers, bins=bins)\n",
    "    if len(allowed_reads) == 0:\n",
    "        percentage = np.zeros_like(counts)\n",
    "    else:\n",
    "        percentage = 100.0 * counts / len(allowed_reads)\n",
    "\n",
    "    bin_centers = (bins[:-1] + bins[1:]) / 2.0\n",
    "    if smooth_window and smooth_window > 1:\n",
    "        series = pd.Series(percentage)\n",
    "        percentage = series.rolling(window=smooth_window, center=True, min_periods=1).mean().to_numpy()\n",
    "\n",
    "    return bin_centers, percentage\n",
    "\n",
    "\n",
    "def plot_metrics_multi_file(\n",
    "    df,\n",
    "    file_id1, file_id2,\n",
    "    bed_window, clustering_window, max_reads, max_hist,\n",
    "    # 0=skip, 1=primary axis, 2=secondary axis, 3=read-level only\n",
    "    nuc_axis=0,\n",
    "    msp_axis=0,\n",
    "    m6a_axis=0,\n",
    "    c5m_axis=0,\n",
    "    # 0=skip, 1=primary axis, 2=secondary axis\n",
    "    nuc_center_axis=0,\n",
    "    match_min=False,\n",
    "    smooth_window=20,\n",
    "    agg_bin_width=10,\n",
    "    plot_motifs=False,\n",
    "    smooth_window_nuc_center=5,\n",
    "    agg_bin_width_nuc_center=10,\n",
    "    selected_type=None,\n",
    "    sub_plot=False,\n",
    "    subtraction_bin_size=3  # ← NEW parameter: bin width for the difference histogram\n",
    "):\n",
    "    \"\"\"\n",
    "    Axis meaning:\n",
    "      - nuc_axis, msp_axis: 0=skip, 1=primary axis, 2=secondary axis, 3=read-level only\n",
    "      - m6a_axis, c5m_axis: 0=skip, 1=primary axis, 2=secondary axis, 3=read-level only\n",
    "      - nuc_center_axis: 0=skip, 1=primary, 2=secondary\n",
    "\n",
    "    sub_plot (bool): if True, insert a 4th subplot (difference of the primary-axis metric).\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Filter the big df based on file_id & exp_id index if present\n",
    "    df1 = filter_by_file_and_exp_id(df, file_id1)\n",
    "    df2 = filter_by_file_and_exp_id(df, file_id2)\n",
    "\n",
    "    # Get total read counts before filtering\n",
    "    total_reads_file1 = df1['read_index'].nunique()\n",
    "    total_reads_file2 = df2['read_index'].nunique()\n",
    "\n",
    "    # 2) Decide if we do motifs\n",
    "    do_plot_motifs = plot_motifs\n",
    "    if selected_type is not None:\n",
    "        # e.g. MOTIFS_rex5\n",
    "        if not re.match(r\"^MOTIFS_rex\\d+$\", selected_type):\n",
    "            do_plot_motifs = False\n",
    "\n",
    "    # ---------- Build histogram metrics ----------\n",
    "    hist_metric_list = []\n",
    "    if nuc_axis in [1, 2]:\n",
    "        hist_metric_list.append('nuc')\n",
    "    if msp_axis in [1, 2]:\n",
    "        hist_metric_list.append('msp')\n",
    "    if m6a_axis in [1, 2]:\n",
    "        hist_metric_list.append('m6a')\n",
    "    if c5m_axis in [1, 2]:\n",
    "        hist_metric_list.append('5mC')\n",
    "\n",
    "    # ---------- Build scatter metrics ----------\n",
    "    scatter_metric_list = []\n",
    "    if nuc_axis in [1, 2, 3]:\n",
    "        scatter_metric_list.append('nuc')\n",
    "    if msp_axis in [1, 2, 3]:\n",
    "        scatter_metric_list.append('msp')\n",
    "    if m6a_axis in [1, 2, 3]:\n",
    "        scatter_metric_list.append('m6a')\n",
    "    if c5m_axis in [1, 2, 3]:\n",
    "        scatter_metric_list.append('5mC')\n",
    "\n",
    "    # Colors for each metric\n",
    "    base_colors = {\n",
    "        'nuc': 'rgba(128,128,128,0.8)',\n",
    "        'msp': 'rgba(240,70,40,0.7)',\n",
    "        'm6a': 'rgba(200,70,30,0.7)',\n",
    "        '5mC': 'purple'\n",
    "    }\n",
    "\n",
    "    # Possibly restrict the # of reads\n",
    "    allowed_hist1 = get_allowed_reads(df1, max_hist,bed_window=bed_window, rng=RNG)\n",
    "    allowed_hist2 = get_allowed_reads(df2, max_hist,bed_window=bed_window, rng=RNG)\n",
    "\n",
    "    if DEBUG_HIST:\n",
    "        print(f\"Allowed reads (file1): {len(allowed_hist1)}  \"\n",
    "              f\"(min_read_len={min(df1[df1['read_index'].isin(allowed_hist1)].fiber_sequence.map(len))})\")\n",
    "        print(f\"Allowed reads (file2): {len(allowed_hist2)}  \"\n",
    "              f\"(min_read_len={min(df2[df2['read_index'].isin(allowed_hist2)].fiber_sequence.map(len))})\")\n",
    "\n",
    "    #allowed_hist1 = set(df1['read_index'].unique())\n",
    "    #allowed_hist2 = set(df2['read_index'].unique())\n",
    "    \n",
    "    if match_min:\n",
    "        n_target = min(len(allowed_hist1), len(allowed_hist2))\n",
    "    \n",
    "        # pick the SAME random subset for histograms and scatter\n",
    "        np.random.seed(None)           # or pass a seed if you want determinism\n",
    "        allowed_hist1 = set(np.random.choice(list(allowed_hist1),\n",
    "                                             size=n_target, replace=False))\n",
    "        allowed_hist2 = set(np.random.choice(list(allowed_hist2),\n",
    "                                             size=n_target, replace=False))\n",
    "    \n",
    "        df1_scatter = df1[df1['read_index'].isin(allowed_hist1)]\n",
    "        df2_scatter = df2[df2['read_index'].isin(allowed_hist2)]\n",
    "    else:\n",
    "        df1_scatter = df1\n",
    "        df2_scatter = df2\n",
    "        n_reads_file1 = len(allowed_hist1)\n",
    "        n_reads_file2 = len(allowed_hist2)\n",
    "\n",
    "    if DEBUG_READS:\n",
    "        def read_level_ratio(df_sub, allowed):\n",
    "            ratios = []\n",
    "            for r, g in df_sub[df_sub['read_index'].isin(allowed)].groupby('read_index'):\n",
    "                covered_bp = 0\n",
    "                nuc_bp     = 0\n",
    "                for row in g.itertuples(index=False):\n",
    "                    if not isinstance(row.fiber_sequence, str):\n",
    "                        continue\n",
    "                    r_start = row.read_st_adj + 1\n",
    "                    r_end   = r_start + len(row.fiber_sequence) - 1\n",
    "                    lo = max(r_start, -bed_window)\n",
    "                    hi = min(r_end,   bed_window)\n",
    "                    covered_bp += max(0, hi-lo+1)\n",
    "                    if isinstance(row.adjusted_ref_nuc, list):\n",
    "                        nuc_bp += sum(\n",
    "                            max(0, min(e, bed_window) - max(s, -bed_window) + 1)\n",
    "                            for s,e in (\n",
    "                                (s, s+l-1) if row.adjusted_ref_nuc_lengths and i < len(row.adjusted_ref_nuc_lengths)\n",
    "                                else (s, s)\n",
    "                                for i,s in enumerate(row.adjusted_ref_nuc)\n",
    "                                for l in [row.adjusted_ref_nuc_lengths[i] if row.adjusted_ref_nuc_lengths else 1]\n",
    "                            )\n",
    "                        )\n",
    "                if covered_bp:\n",
    "                    ratios.append(nuc_bp / covered_bp)\n",
    "            return np.array(ratios)\n",
    "    \n",
    "        print(\"\\n=== READ-LEVEL %NUC ===\")\n",
    "        for tag, sub, allowed in [\n",
    "            (file_id1, df1, allowed_hist1),\n",
    "            (file_id2, df2, allowed_hist2)]:\n",
    "            rl = read_level_ratio(sub, allowed)*100\n",
    "            print(f\"{tag} :  median={np.median(rl):.1f}%   \"\n",
    "                  f\"10th={np.percentile(rl,10):.1f}%   \"\n",
    "                  f\"90th={np.percentile(rl,90):.1f}%   \"\n",
    "                  f\"n={rl.size}\")\n",
    "        print(\"========================\\n\")\n",
    "\n",
    "    # Compute the line-plot histograms\n",
    "    bins1, histo1 = compute_histogram_metrics(\n",
    "        df1, bed_window, allowed_hist1, hist_metric_list,\n",
    "        smooth_window=smooth_window, agg_bin_width=agg_bin_width\n",
    "    )\n",
    "    bins2, histo2 = compute_histogram_metrics(\n",
    "        df2, bed_window, allowed_hist2, hist_metric_list,\n",
    "        smooth_window=smooth_window, agg_bin_width=agg_bin_width\n",
    "    )\n",
    "    \n",
    "    if DEBUG_READS:\n",
    "        print(\"\\n=== BLANK-READ SUMMARY ===\")\n",
    "        for tag in (file_id1, file_id2):\n",
    "            blanks = compute_histogram_metrics.blank_reads.get(tag, 0)\n",
    "            print(f\"{tag} : reads_without_nuc = {blanks}\")\n",
    "        compute_histogram_metrics.blank_reads.clear()\n",
    "        print(\"==========================\\n\")\n",
    "\n",
    "\n",
    "    # Compute the scatter (read-level) data\n",
    "    scatter1, y_positions1, sorted_clusters1, clusters1 = compute_scatter_data(\n",
    "        df1_scatter, bed_window, clustering_window, max_reads, scatter_metric_list,\n",
    "        nuc_axis=nuc_axis, msp_axis=msp_axis, m6a_axis=m6a_axis, c5m_axis=c5m_axis\n",
    "    )\n",
    "    scatter2, y_positions2, sorted_clusters2, clusters2 = compute_scatter_data(\n",
    "        df2_scatter, bed_window, clustering_window, max_reads, scatter_metric_list,\n",
    "        nuc_axis=nuc_axis, msp_axis=msp_axis, m6a_axis=m6a_axis, c5m_axis=c5m_axis\n",
    "    )\n",
    "\n",
    "    # Identify which metric is primary\n",
    "    primary_metric = None\n",
    "    if nuc_axis == 1:\n",
    "        primary_metric = 'nuc'\n",
    "    elif msp_axis == 1:\n",
    "        primary_metric = 'msp'\n",
    "    elif m6a_axis == 1:\n",
    "        primary_metric = 'm6a'\n",
    "    elif c5m_axis == 1:\n",
    "        primary_metric = '5mC'\n",
    "\n",
    "    # Compute nuc-center lines if requested\n",
    "    nuc_center_bins1, nuc_center_pct1 = [], []\n",
    "    nuc_center_bins2, nuc_center_pct2 = [], []\n",
    "    if nuc_center_axis != 0:\n",
    "        nuc_center_bins1, nuc_center_pct1 = compute_nuc_center_histogram(\n",
    "            df1, bed_window, allowed_hist1, agg_bin_width_nuc_center, smooth_window_nuc_center\n",
    "        )\n",
    "        nuc_center_bins2, nuc_center_pct2 = compute_nuc_center_histogram(\n",
    "            df2, bed_window, allowed_hist2, agg_bin_width_nuc_center, smooth_window_nuc_center\n",
    "        )\n",
    "\n",
    "    # Decide row structure\n",
    "    if sub_plot:\n",
    "        # 4 subplots total\n",
    "        specs = [\n",
    "            [{}],                   # Row1: top read-level (file1)\n",
    "            [{\"secondary_y\": True}],# Row2: line plots\n",
    "            [{}],                   # Row3: difference histogram\n",
    "            [{}]                    # Row4: bottom read-level (file2)\n",
    "        ]\n",
    "        row_heights = [0.25, 0.15, 0.35, 0.25]\n",
    "        subplot_titles = (\n",
    "            f\"{file_id1} (n={total_reads_file1} reads)\",\n",
    "            \"\",\n",
    "            \"\",\n",
    "            f\"{file_id2} (n={total_reads_file2} reads)\"\n",
    "        )\n",
    "        bottom_row = 4\n",
    "        diff_row = 3\n",
    "    else:\n",
    "        # 3 subplots\n",
    "        specs = [\n",
    "            [{}],\n",
    "            [{\"secondary_y\": True}],\n",
    "            [{}]\n",
    "        ]\n",
    "        row_heights = [0.25, 0.5, 0.25]\n",
    "        subplot_titles = (\n",
    "            f\"{file_id1} (n={n_reads_file1} reads)\",\n",
    "            \"\",\n",
    "            f\"{file_id2} (n={n_reads_file2} reads)\"\n",
    "        )\n",
    "        bottom_row = 3\n",
    "        diff_row = None\n",
    "\n",
    "    fig = make_subplots(\n",
    "        rows=len(specs),\n",
    "        cols=1,\n",
    "        shared_xaxes=True,\n",
    "        specs=specs,\n",
    "        row_heights=row_heights,\n",
    "        vertical_spacing=0.05,\n",
    "        subplot_titles=subplot_titles\n",
    "    )\n",
    "\n",
    "    # ---- TOP SUBPLOT (file_id1), row=1 ----\n",
    "    for metric, data in scatter1.items():\n",
    "        if not data:\n",
    "            continue\n",
    "        if metric in ['nuc', 'msp']:\n",
    "            seg_x, seg_y = data\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=seg_x,\n",
    "                    y=seg_y,\n",
    "                    mode='lines',\n",
    "                    name=f\"{file_id1} {metric}\",\n",
    "                    line=dict(color=base_colors[metric])\n",
    "                ),\n",
    "                row=1, col=1\n",
    "            )\n",
    "        else:\n",
    "            # m6a or 5mC => points\n",
    "            points_y, points_x = zip(*data)\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=points_x,\n",
    "                    y=points_y,\n",
    "                    mode='markers',\n",
    "                    name=f\"{file_id1} {metric}\",\n",
    "                    marker=dict(color=base_colors[metric], size=1.5, symbol='square')\n",
    "                ),\n",
    "                row=1, col=1\n",
    "            )\n",
    "\n",
    "    # line styles for histograms\n",
    "    histogram_styles = {\n",
    "        str(file_id1): dict(dash='solid', width=3),\n",
    "        str(file_id2): dict(dash='dot', width=3)\n",
    "    }\n",
    "\n",
    "    # ---- MIDDLE SUBPLOT (line plots), row=2 ----\n",
    "    for metric in hist_metric_list:\n",
    "        # Decide if the line plot is on the primary or secondary y-axis\n",
    "        if metric == 'nuc':\n",
    "            secondary = (nuc_axis == 2)\n",
    "        elif metric == 'msp':\n",
    "            secondary = (msp_axis == 2)\n",
    "        elif metric == 'm6a':\n",
    "            secondary = (m6a_axis == 2)\n",
    "        else:  # '5mC'\n",
    "            secondary = (c5m_axis == 2)\n",
    "\n",
    "        if metric in bins1 and metric in histo1:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=bins1[metric],\n",
    "                    y=histo1[metric],\n",
    "                    mode='lines',\n",
    "                    line=dict(color=base_colors[metric], **histogram_styles[str(file_id1)]),\n",
    "                    name=f\"{file_id1} {metric}\"\n",
    "                ),\n",
    "                row=2, col=1, secondary_y=secondary\n",
    "            )\n",
    "        if metric in bins2 and metric in histo2:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=bins2[metric],\n",
    "                    y=histo2[metric],\n",
    "                    mode='lines',\n",
    "                    line=dict(color=base_colors[metric], **histogram_styles[str(file_id2)]),\n",
    "                    name=f\"{file_id2} {metric}\"\n",
    "                ),\n",
    "                row=2, col=1, secondary_y=secondary\n",
    "            )\n",
    "\n",
    "    # Nucleosome center lines\n",
    "    if nuc_center_axis != 0 and len(nuc_center_bins1) > 0:\n",
    "        is_secondary = (nuc_center_axis == 2)\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=nuc_center_bins1,\n",
    "                y=nuc_center_pct1,\n",
    "                mode='lines',\n",
    "                name=f\"{file_id1} nuc center\",\n",
    "                line=dict(color=base_colors['nuc'], dash='solid', width=3)\n",
    "            ),\n",
    "            row=2, col=1, secondary_y=is_secondary\n",
    "        )\n",
    "    if nuc_center_axis != 0 and len(nuc_center_bins2) > 0:\n",
    "        is_secondary = (nuc_center_axis == 2)\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=nuc_center_bins2,\n",
    "                y=nuc_center_pct2,\n",
    "                mode='lines',\n",
    "                name=f\"{file_id2} nuc center\",\n",
    "                line=dict(color=base_colors['nuc'], dash='dot', width=3)\n",
    "            ),\n",
    "            row=2, col=1, secondary_y=is_secondary\n",
    "        )\n",
    "\n",
    "    # ---- OPTIONAL DIFFERENCE SUBPLOT (row=3 if sub_plot==True) ----\n",
    "    if sub_plot and primary_metric is not None and primary_metric in histo1 and primary_metric in histo2:\n",
    "        diff_x = bins1[primary_metric]\n",
    "        diff_y = histo1[primary_metric] - histo2[primary_metric]\n",
    "        \n",
    "        if DEBUG_HIST and primary_metric:\n",
    "            print(\"\\n=== DEBUG_DIFF ================================\")\n",
    "            print(f\"Primary metric      : {primary_metric}\")\n",
    "            print(f\"Allowed reads file1 : {len(allowed_hist1)}\")\n",
    "            print(f\"Allowed reads file2 : {len(allowed_hist2)}\")\n",
    "            print(\"Diff_y (first 15 bins):\", diff_y[:15].round(2))\n",
    "            print(\"min / max / mean      :\", diff_y.min().round(2),\n",
    "                  diff_y.max().round(2), diff_y.mean().round(2))\n",
    "            print(\"===============================================\\n\")\n",
    "\n",
    "\n",
    "        # Define a larger bin width for the subtraction histogram\n",
    "        bins = np.arange(diff_x.min(), diff_x.max() + subtraction_bin_size, subtraction_bin_size)\n",
    "\n",
    "        # Compute weighted sum and counts per bin\n",
    "        weighted_sum, bin_edges = np.histogram(diff_x, bins=bins, weights=diff_y)\n",
    "        counts, _ = np.histogram(diff_x, bins=bins)\n",
    "\n",
    "        # Compute the average; for bins with 0 counts, set the average to 0\n",
    "        averages = np.divide(weighted_sum, counts, out=np.zeros_like(weighted_sum), where=counts != 0)\n",
    "\n",
    "        # Calculate bin centers for plotting\n",
    "        bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "\n",
    "        # Create a color array based on the average values\n",
    "        color_array = [\n",
    "            'rgba(40,190,0,0.7)' if v >= 0 else 'rgba(255,177,0,0.7)'\n",
    "            for v in averages\n",
    "        ]\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=bin_centers,\n",
    "                y=averages,\n",
    "                width=subtraction_bin_size,    # ← also set bar width in the trace\n",
    "                marker_color=color_array,\n",
    "                name=\"Displacement\"\n",
    "            ),\n",
    "            row=diff_row, col=1\n",
    "        )\n",
    "\n",
    "        label_map = {\n",
    "            'nuc': \"WT nuc disp. (% reads)\",\n",
    "            'msp': \"WT msp disp. (% reads)\",\n",
    "            'm6a': \"WT m6a disp. (% reads)\",\n",
    "            '5mC': \"WT 5mC disp. (% reads)\"\n",
    "        }\n",
    "        y_label = label_map.get(primary_metric, \"WT nuc disp. (% reads)\")\n",
    "        fig.update_yaxes(title_text=y_label, row=diff_row, col=1)\n",
    "\n",
    "    # ---- BOTTOM SUBPLOT (file_id2) => row=3 or row=4 ----\n",
    "    for metric, data in scatter2.items():\n",
    "        if not data:\n",
    "            continue\n",
    "        if metric in ['nuc', 'msp']:\n",
    "            seg_x, seg_y = data\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=seg_x,\n",
    "                    y=seg_y,\n",
    "                    mode='lines',\n",
    "                    name=f\"{file_id2} {metric}\",\n",
    "                    line=dict(color=base_colors[metric])\n",
    "                ),\n",
    "                row=bottom_row, col=1\n",
    "            )\n",
    "        else:\n",
    "            points_y, points_x = zip(*data)\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=points_x,\n",
    "                    y=points_y,\n",
    "                    mode='markers',\n",
    "                    name=f\"{file_id2} {metric}\",\n",
    "                    marker=dict(color=base_colors[metric], size=1.5, symbol='square')\n",
    "                ),\n",
    "                row=bottom_row, col=1\n",
    "            )\n",
    "\n",
    "    # Gray rectangle behind overlap region in top and bottom read-level subplots\n",
    "    fig.add_shape(\n",
    "        type=\"rect\",\n",
    "        x0=-clustering_window,\n",
    "        x1=clustering_window,\n",
    "        y0=0,\n",
    "        y1=1,\n",
    "        yref=\"paper\",\n",
    "        fillcolor=\"rgba(200,200,200,0.2)\",\n",
    "        line=dict(width=0),\n",
    "        layer=\"below\",\n",
    "        row=1,\n",
    "        col=1\n",
    "    )\n",
    "    fig.add_shape(\n",
    "        type=\"rect\",\n",
    "        x0=-clustering_window,\n",
    "        x1=clustering_window,\n",
    "        y0=0,\n",
    "        y1=1,\n",
    "        yref=\"paper\",\n",
    "        fillcolor=\"rgba(200,200,200,0.2)\",\n",
    "        line=dict(width=0),\n",
    "        layer=\"below\",\n",
    "        row=bottom_row,\n",
    "        col=1\n",
    "    )\n",
    "\n",
    "    # Motif lines if requested (drawn across row=2 domain)\n",
    "    if do_plot_motifs and (('motif_rel_start' in df1.columns) or ('motif_rel_start' in df2.columns)):\n",
    "        motif_positions = []\n",
    "        for d in [df1, df2]:\n",
    "            if all(x in d.columns for x in ['motif_rel_start','motif_id','motif_strand']):\n",
    "                for _, row in d.iterrows():\n",
    "                    mr = row.get('motif_rel_start')\n",
    "                    mid = row.get('motif_id')\n",
    "                    mstr = row.get('motif_strand')\n",
    "                    if (isinstance(mr, (list, tuple))\n",
    "                        and isinstance(mid, (list, tuple))\n",
    "                        and isinstance(mstr, (list, tuple))):\n",
    "                        for pos, m, s in zip(mr, mid, mstr):\n",
    "                            if -bed_window <= pos <= bed_window:\n",
    "                                motif_positions.append((pos, f\"{m}{s}\"))\n",
    "        motif_positions = list(set(motif_positions))\n",
    "        if hasattr(fig.layout, 'yaxis2') and fig.layout.yaxis2.domain:\n",
    "            y_domain = fig.layout.yaxis2.domain\n",
    "        else:\n",
    "            y_domain = [0, 1]\n",
    "\n",
    "        for pos, label in motif_positions:\n",
    "            fig.add_shape(\n",
    "                type=\"line\",\n",
    "                x0=pos, x1=pos,\n",
    "                yref=\"paper\",\n",
    "                y0=y_domain[0],\n",
    "                y1=y_domain[1],\n",
    "                line=dict(color=\"black\", dash=\"dash\", width=1)\n",
    "            )\n",
    "            fig.add_annotation(\n",
    "                x=pos,\n",
    "                y=y_domain[1],\n",
    "                xref=\"x\",\n",
    "                yref=\"paper\",\n",
    "                text=label,\n",
    "                showarrow=False,\n",
    "                yanchor=\"bottom\",\n",
    "                font=dict(color=\"black\", size=10)\n",
    "            )\n",
    "\n",
    "    # Cosmetics\n",
    "    fig.update_xaxes(showgrid=False)\n",
    "    fig.update_yaxes(showgrid=False)\n",
    "\n",
    "    # Hide y-tick labels for top and bottom read-level subplots\n",
    "    fig.update_yaxes(showticklabels=False, row=1, col=1)\n",
    "    fig.update_yaxes(showticklabels=False, row=bottom_row, col=1)\n",
    "\n",
    "    # X-axis labels\n",
    "    fig.update_xaxes(title_text=\"\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"genomic position (bp)\", row=bottom_row, col=1)\n",
    "\n",
    "    # Overall figure layout\n",
    "    fig.update_layout(\n",
    "        template='plotly_white',\n",
    "        title=\"Combined Metrics Plot\",\n",
    "        height=800 if not sub_plot else 700,\n",
    "        showlegend=False\n",
    "    )\n",
    "\n",
    "    # Middle subplot axes labels (row=2 => line plot)\n",
    "    fig.update_yaxes(title_text=\"% nuc\", row=2, col=1, secondary_y=False, dtick=25)\n",
    "    fig.update_yaxes(title_text=\"% (secondary)\", row=2, col=1, secondary_y=True)\n",
    "\n",
    "    fig.show()\n",
    "    return fig, (clusters1, clusters2)\n",
    "\n",
    "\n",
    "#([\"N2_old_fiber_R10\",\"96_DPY27_degron_old\",\"107_SDC2_degron_old_R10\",\"113_114_rex1scr_old_R10\",\"52_old_dpy21jmjc_fiber_R10\",\"51_old_dpy21null_fiber_R10\"])\n",
    "#\"107_SDC2_degron_old_R10\",\"96_DPY27_degron \"N2_mixed_R9\",\"SDC2_degron_mixed_R9\"])\n",
    "\n",
    "#file_id1 = 'N2_old_fiber_37C_R10' # for DPY21 null\n",
    "#file_id2 = '51_old_dpy21null_fiber_R10'\n",
    "#file_id2 = '52_old_dpy21jmjc_fiber_R10'\n",
    "\n",
    "\n",
    "#file_id1 = 'N2_old_fiber_R10' # For SDC2, SDC3 degron\n",
    "#file_id2 = '107_SDC2_degron_old_R10'\n",
    "#file_id2 = \"SDC3_degron_old_R10\"\n",
    "\n",
    "#file_id2 = '96_DPY27_degron_old'\n",
    "#file_id1 = 'N2_biorep1_fiber_old_R10_04_2025' # for DPY27 degron\n",
    "file_id1 = 'N2_rep3' # for DPY27 degron\n",
    "\n",
    "#file_id2= 'SDC2deg_rep3'\n",
    "file_id2= 'DPY27deg_rep3'\n",
    "#file_id2= 'N2_biorep2_fiber_old_R10_04_2025'\n",
    "\n",
    "#file_id1 = '113_114_rex1scr_old_R10'\n",
    "#file_id1 = '96_DPY27_degron_old'\n",
    "\n",
    "\n",
    "\n",
    "#file_id1 = \"N2_mixed_R9\"\n",
    "#file_id2 = 'SDC2_degron_mixed_R9'\n",
    "#file_id2 = '96_DPY27_degron_old'\n",
    "#file_id2 = \"SDC2_degron_mixed_R9\"\n",
    "\n",
    "\n",
    "# --- Example usage (assuming combined_df is defined) ---\n",
    "fig, (clusters1, clusters2) = plot_metrics_multi_file(\n",
    "    combined_df,\n",
    "    file_id1=file_id1,\n",
    "    file_id2=file_id2,\n",
    "    bed_window=5000,\n",
    "    clustering_window=150,\n",
    "    max_reads=100,\n",
    "    max_hist=200,\n",
    "    # old booleans replaced by integer codes:\n",
    "    nuc_axis=1,  # put nucleosomes on primary axis\n",
    "    msp_axis=3,  # put msp on primary axis\n",
    "    m6a_axis=3,  # put m6A on secondary axis\n",
    "    c5m_axis=0,  # skip 5mC\n",
    "    nuc_center_axis=0, # plot nuc_center lines on the primary axis\n",
    "    match_min=False,\n",
    "    smooth_window=10,\n",
    "    agg_bin_width=10,\n",
    "    plot_motifs=False,\n",
    "    smooth_window_nuc_center=0, # 5\n",
    "    agg_bin_width_nuc_center=10,\n",
    "    selected_type=selected_type,\n",
    "    sub_plot=True,\n",
    "    subtraction_bin_size=50  # <— width of bars in subtraction plot\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# save as png and svg, incorporating file_id1 and file_id2 and use output_stem that has already been defined, and define height / width\n",
    "height = 600\n",
    "width = 800\n",
    "output_name = f\"{output_stem}01_{file_id1}_{file_id2}_{selected_type}_plot\"\n",
    "print(output_name)\n",
    "fig.write_image(f\"{output_name}.png\", height=height, width=width)\n",
    "fig.write_image(f\"{output_name}.svg\", height=height, width=width)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7fe77a7c7f0f81",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# def compute_histogram_metrics(df, bed_window, allowed_reads, metric_list, smooth_window=20, agg_bin_width=10):\n",
    "#     \"\"\"\n",
    "#     Computes per-base or aggregated histograms for each metric in `metric_list`.\n",
    "#     Returns:\n",
    "#         bins_dict: {metric -> array of bin positions}\n",
    "#         metric_percentages: {metric -> array of % coverage}\n",
    "#     \"\"\"\n",
    "#     per_base_bins = np.arange(-bed_window, bed_window + 1)\n",
    "#     n_bins = len(per_base_bins)\n",
    "# \n",
    "#     coverage_sets = [set() for _ in range(n_bins)]\n",
    "#     for _, row in df.iterrows():\n",
    "#         r_index = row['read_index']\n",
    "#         if r_index not in allowed_reads:\n",
    "#             continue\n",
    "#         fiber_seq = row.get('fiber_sequence')\n",
    "#         if isinstance(fiber_seq, str):\n",
    "#             L = len(fiber_seq)\n",
    "#             for i in range(L):\n",
    "#                 pos = row['read_st_adj'] + 1 + i\n",
    "#                 if -bed_window <= pos <= bed_window:\n",
    "#                     bin_idx = pos + bed_window\n",
    "#                     coverage_sets[bin_idx].add(r_index)\n",
    "#     denom_counts = np.array([len(s) for s in coverage_sets])\n",
    "# \n",
    "#     metric_coverage_sets = {metric: [set() for _ in range(n_bins)] for metric in metric_list}\n",
    "#     for _, row in df.iterrows():\n",
    "#         r_index = row['read_index']\n",
    "#         if r_index not in allowed_reads:\n",
    "#             continue\n",
    "#         fiber_seq = row.get('fiber_sequence')\n",
    "#         if not isinstance(fiber_seq, str):\n",
    "#             continue\n",
    "#         for metric in metric_list:\n",
    "#             if metric in ['nuc', 'msp']:\n",
    "#                 col = f\"adjusted_ref_{metric}\"\n",
    "#                 length_col = f\"adjusted_ref_{metric}_lengths\"\n",
    "#                 starts = row.get(col)\n",
    "#                 if isinstance(starts, list):\n",
    "#                     if (row.get(length_col) is not None and\n",
    "#                         isinstance(row.get(length_col), list) and\n",
    "#                         len(row.get(length_col)) == len(starts)):\n",
    "#                         lengths = row.get(length_col)\n",
    "#                         for s, l in zip(starts, lengths):\n",
    "#                             for pos in range(s, s + l):\n",
    "#                                 if -bed_window <= pos <= bed_window:\n",
    "#                                     bin_idx = pos + bed_window\n",
    "#                                     metric_coverage_sets[metric][bin_idx].add(r_index)\n",
    "#                     else:\n",
    "#                         for s in starts:\n",
    "#                             if -bed_window <= s <= bed_window:\n",
    "#                                 bin_idx = s + bed_window\n",
    "#                                 metric_coverage_sets[metric][bin_idx].add(r_index)\n",
    "#             else:\n",
    "#                 # m6a or 5mC\n",
    "#                 col = f\"adjusted_ref_{metric}\"\n",
    "#                 positions = row.get(col)\n",
    "#                 if isinstance(positions, list):\n",
    "#                     for pos in positions:\n",
    "#                         if -bed_window <= pos <= bed_window:\n",
    "#                             bin_idx = pos + bed_window\n",
    "#                             metric_coverage_sets[metric][bin_idx].add(r_index)\n",
    "# \n",
    "#     bins_dict = {}\n",
    "#     metric_percentages = {}\n",
    "#     N_reads = len(allowed_reads)\n",
    "#     for metric, sets in metric_coverage_sets.items():\n",
    "#         counts = np.array([len(s) for s in sets])\n",
    "#         if metric in ['m6a', '5mC']:\n",
    "#             # Use aggregated bins for m6a / 5mC\n",
    "#             agg_bins = []\n",
    "#             agg_percentages = []\n",
    "#             for i in range(0, len(counts), agg_bin_width):\n",
    "#                 current_slice = counts[i:i+agg_bin_width]\n",
    "#                 agg_count = np.sum(current_slice)\n",
    "#                 current_bin_length = len(current_slice)\n",
    "#                 denom_agg = current_bin_length * N_reads\n",
    "#                 agg_pct = 100 * agg_count / denom_agg if denom_agg > 0 else np.nan\n",
    "#                 agg_percentages.append(agg_pct)\n",
    "#                 agg_bins.append(np.mean(per_base_bins[i:i+current_bin_length]))\n",
    "#             agg_bins = np.array(agg_bins)\n",
    "#             agg_percentages = np.array(agg_percentages)\n",
    "#             if smooth_window and smooth_window > 1:\n",
    "#                 series = pd.Series(agg_percentages)\n",
    "#                 agg_percentages = series.rolling(window=smooth_window, center=True, min_periods=1).mean().to_numpy()\n",
    "#             bins_dict[metric] = agg_bins\n",
    "#             metric_percentages[metric] = agg_percentages\n",
    "#         else:\n",
    "#             # nuc or msp => direct per-base\n",
    "#             percentages = np.where(denom_counts > 0, 100 * counts / denom_counts, np.nan)\n",
    "#             if smooth_window and smooth_window > 1:\n",
    "#                 series = pd.Series(percentages)\n",
    "#                 percentages = series.rolling(window=smooth_window, center=True, min_periods=1).mean().to_numpy()\n",
    "#             bins_dict[metric] = per_base_bins\n",
    "#             metric_percentages[metric] = percentages\n",
    "#     return bins_dict, metric_percentages\n",
    "\n",
    "\n",
    "# def compute_scatter_data(df, bed_window, clustering_window, max_reads, metric_list,\n",
    "#                          nuc_axis=0, msp_axis=0, m6a_axis=0, c5m_axis=0):\n",
    "#     \"\"\"\n",
    "#     Builds per-read scatter data for each metric in metric_list (top and bottom subplots).\n",
    "#     \"\"\"\n",
    "#     allowed_reads = get_allowed_reads(df, max_reads)\n",
    "# \n",
    "#     clustering_bins = np.arange(-clustering_window, clustering_window + 1)\n",
    "#     n_clustering_bins = len(clustering_bins)\n",
    "#     pos_to_clustering_idx = {pos: idx for idx, pos in enumerate(clustering_bins)}\n",
    "# \n",
    "#     # Identify which reads overlap the region of interest\n",
    "#     reads_overlapping = set()\n",
    "#     for _, row in df.iterrows():\n",
    "#         r_index = row['read_index']\n",
    "#         if r_index not in allowed_reads:\n",
    "#             continue\n",
    "#         fiber_seq = row.get('fiber_sequence')\n",
    "#         if not isinstance(fiber_seq, str):\n",
    "#             continue\n",
    "#         L = len(fiber_seq)\n",
    "#         for i in range(L):\n",
    "#             pos = row['read_st_adj'] + 1 + i\n",
    "#             if -clustering_window <= pos <= clustering_window:\n",
    "#                 reads_overlapping.add(r_index)\n",
    "#                 break\n",
    "# \n",
    "#     read_indices = sorted(list(allowed_reads))\n",
    "#     overlapping_read_indices = [r for r in read_indices if r in reads_overlapping]\n",
    "#     non_overlapping_read_indices = [r for r in read_indices if r not in reads_overlapping]\n",
    "# \n",
    "#     # Prepare an array for msp-based clustering\n",
    "#     msp_matrix = np.full((len(overlapping_read_indices), n_clustering_bins), np.nan)\n",
    "#     read_idx_to_row = {r: i for i, r in enumerate(overlapping_read_indices)}\n",
    "# \n",
    "#     # First fill with zeros for any overlapping region\n",
    "#     for _, row in df.iterrows():\n",
    "#         r_index = row['read_index']\n",
    "#         if r_index not in allowed_reads:\n",
    "#             continue\n",
    "#         fiber_seq = row.get('fiber_sequence')\n",
    "#         if not isinstance(fiber_seq, str):\n",
    "#             continue\n",
    "#         if r_index in read_idx_to_row:\n",
    "#             r_row = read_idx_to_row[r_index]\n",
    "#             L = len(fiber_seq)\n",
    "#             for i in range(L):\n",
    "#                 pos = row['read_st_adj'] + 1 + i\n",
    "#                 if -clustering_window <= pos <= clustering_window:\n",
    "#                     col_idx = pos_to_clustering_idx[pos]\n",
    "#                     msp_matrix[r_row, col_idx] = 0\n",
    "# \n",
    "#     # Fill positions of msp intervals with 1\n",
    "#     if 'msp' in metric_list:\n",
    "#         for _, row in df.iterrows():\n",
    "#             r_index = row['read_index']\n",
    "#             if r_index not in allowed_reads:\n",
    "#                 continue\n",
    "#             if r_index in read_idx_to_row:\n",
    "#                 r_row = read_idx_to_row[r_index]\n",
    "#                 col = \"adjusted_ref_msp\"\n",
    "#                 length_col = \"adjusted_ref_msp_lengths\"\n",
    "#                 starts = row.get(col)\n",
    "#                 if isinstance(starts, list):\n",
    "#                     if (row.get(length_col) is not None and\n",
    "#                         isinstance(row.get(length_col), list) and\n",
    "#                         len(row.get(length_col)) == len(starts)):\n",
    "#                         lengths = row.get(length_col)\n",
    "#                         for s, l in zip(starts, lengths):\n",
    "#                             for pos in range(s, s + l):\n",
    "#                                 if -clustering_window <= pos <= clustering_window:\n",
    "#                                     col_idx = pos_to_clustering_idx[pos]\n",
    "#                                     msp_matrix[r_row, col_idx] = 1\n",
    "#                     else:\n",
    "#                         for s in starts:\n",
    "#                             if -clustering_window <= s <= clustering_window:\n",
    "#                                 col_idx = pos_to_clustering_idx[s]\n",
    "#                                 msp_matrix[r_row, col_idx] = 1\n",
    "# \n",
    "#     # Cluster reads by msp pattern\n",
    "#     all_clusters = {}\n",
    "#     if len(overlapping_read_indices) > 0:\n",
    "#         if len(overlapping_read_indices) >= 2:\n",
    "#             msp_for_dist = np.where(np.isnan(msp_matrix), -1, msp_matrix)\n",
    "#             distances = pdist(msp_for_dist, metric='euclidean')\n",
    "#             Z = linkage(distances, method='ward')\n",
    "#             max_d = 0.5 * np.max(Z[:, 2]) if len(Z) > 0 else 0\n",
    "#             clusters = fcluster(Z, max_d, criterion='distance')\n",
    "#             clusters = clusters + 1\n",
    "#             for i, r in enumerate(overlapping_read_indices):\n",
    "#                 all_clusters[r] = int(clusters[i])\n",
    "#         else:\n",
    "#             # Only 1 read => single cluster\n",
    "#             all_clusters[overlapping_read_indices[0]] = 1\n",
    "#     for r in non_overlapping_read_indices:\n",
    "#         all_clusters[r] = 0\n",
    "# \n",
    "#     # Sort reads first by cluster number, then by read index\n",
    "#     sorted_indices_with_clusters = [(r, all_clusters[r]) for r in read_indices]\n",
    "#     sorted_indices_with_clusters.sort(key=lambda x: (x[1], x[0]))\n",
    "#     sorted_indices = [r for r, clust in sorted_indices_with_clusters]\n",
    "#     new_y_positions = {r: i for i, r in enumerate(sorted_indices)}\n",
    "# \n",
    "#     scatter_data = {}\n",
    "#     metric_columns = {\n",
    "#         'nuc': 'adjusted_ref_nuc',\n",
    "#         'msp': 'adjusted_ref_msp',\n",
    "#         'm6a': 'adjusted_ref_m6a',\n",
    "#         '5mC': 'adjusted_ref_5mC'\n",
    "#     }\n",
    "#     for metric in metric_list:\n",
    "#         if metric in ['nuc', 'msp']:\n",
    "#             col = metric_columns[metric]\n",
    "#             length_col = f\"{col}_lengths\"\n",
    "#             seg_scatter_x = []\n",
    "#             seg_scatter_y = []\n",
    "#             for _, row in df.iterrows():\n",
    "#                 r_index = row['read_index']\n",
    "#                 if r_index not in new_y_positions:\n",
    "#                     continue\n",
    "#                 y_pos = new_y_positions[r_index]\n",
    "#                 starts = row.get(col)\n",
    "#                 if isinstance(starts, list):\n",
    "#                     if (row.get(length_col) is not None and\n",
    "#                         isinstance(row.get(length_col), list) and\n",
    "#                         len(row.get(length_col)) == len(starts)):\n",
    "#                         lengths = row.get(length_col)\n",
    "#                         for s, l in zip(starts, lengths):\n",
    "#                             # Clip to bed_window\n",
    "#                             if s + l < -bed_window or s > bed_window:\n",
    "#                                 continue\n",
    "#                             start_pos = max(s, -bed_window)\n",
    "#                             end_pos = min(s + l, bed_window)\n",
    "#                             seg_scatter_x.extend([start_pos, end_pos, None])\n",
    "#                             seg_scatter_y.extend([y_pos, y_pos, None])\n",
    "#                     else:\n",
    "#                         for s in starts:\n",
    "#                             if -bed_window <= s <= bed_window:\n",
    "#                                 seg_scatter_x.append(s)\n",
    "#                                 seg_scatter_y.append(y_pos)\n",
    "#             scatter_data[metric] = (seg_scatter_x, seg_scatter_y)\n",
    "#         else:\n",
    "#             # m6a or 5mC => points\n",
    "#             col = metric_columns[metric]\n",
    "#             points = []\n",
    "#             for _, row in df.iterrows():\n",
    "#                 r_index = row['read_index']\n",
    "#                 if r_index not in new_y_positions:\n",
    "#                     continue\n",
    "#                 y_pos = new_y_positions[r_index]\n",
    "#                 pos_list = row.get(col)\n",
    "#                 if isinstance(pos_list, list):\n",
    "#                     for pos in pos_list:\n",
    "#                         if -bed_window <= pos <= bed_window:\n",
    "#                             points.append((y_pos, pos))\n",
    "#             scatter_data[metric] = points\n",
    "# \n",
    "#     return scatter_data, new_y_positions, sorted_indices_with_clusters, all_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21791be677408c4b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### OLD CODE BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd2a7a5a25189e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Single fiber plotting\n",
    "#modkit_bed_name = modkit_bed_name_ext\n",
    "# ─────────────────── Configuration flags ────────────────────\n",
    "FORCE_REPLACE = True   # True → always rerun modkit extract even if the\n",
    "                        #        target .bed already exists\n",
    "\n",
    "\n",
    "# For centered on MEX:\n",
    "modkit_bed_name = temp_file_path\n",
    "\n",
    "### Extracting per read modifications\n",
    "out_file_names = [output_stem +\n",
    "    \"modkit-extract-\" +\n",
    "    each_condition +\n",
    "    str(round(each_thresh,2)) +\n",
    "    str(each_index) +\n",
    "    str(each_bamfrac) +\n",
    "    str(bed_window) +\n",
    "    \"-\".join([str(x)[0] for x in type_selected[-3:]]) +  # Limiting to first 5 types\n",
    "    \"-\".join([str(x)[0]+str(x)[-3:] for x in chromosome_selected]) +\n",
    "    \".bed\" for each_condition,each_thresh,each_index, each_bamfrac in zip(conditions,thresh_list,sample_indices,bam_fracs)]\n",
    "\n",
    "modkit_bed_df = pd.read_csv(modkit_bed_name,sep='\\t',header=None)\n",
    "### Define bed file for modkit\n",
    "\n",
    "# Function to run a single extract command\n",
    "def modkit_extract(args):\n",
    "    each_bam, each_thresh, each_condition, each_index, each_bamfrac,modkit_path, output_stem, modkit_bed_name, bed_window = args\n",
    "\n",
    "    each_output = (\n",
    "    output_stem +\n",
    "    \"modkit-extract-\" +\n",
    "    each_condition +\n",
    "    str(round(each_thresh,2)) +\n",
    "    str(each_index) +\n",
    "    str(each_bamfrac) +\n",
    "    str(bed_window) +\n",
    "    \"-\".join([str(x)[0] for x in type_selected[-3:]]) +  # Limiting to first 5 types\n",
    "    \"-\".join([str(x)[0]+str(x)[-3:] for x in chromosome_selected]) +\n",
    "    \".bed\"\n",
    ")\n",
    "\n",
    "    ### NOTE: Name of pileup file is not based on configurations\n",
    "    ### TODO: Name of output file should be based on configs so that we aren't recomputing pileups withidentical conditions.\n",
    "\n",
    "    # ─── Skip logic ───────────────────────────────────────────\n",
    "    if os.path.exists(each_output) and not FORCE_REPLACE:\n",
    "        print(f\"Skipping (exists & FORCE_REPLACE is False): {each_output}\")\n",
    "        return\n",
    "    # ---------------------------------------------------------\n",
    "\n",
    "    print(f\"Starting on: {each_bam}\")\n",
    "    command = [\n",
    "        modkit_path,\n",
    "        \"extract\",\n",
    "        \"--threads\",\n",
    "        \"16\",\n",
    "        \"--force\",\n",
    "        \"--mapped\",\n",
    "        \"--ignore\",\n",
    "        \"m\",\n",
    "        \"--include-bed\",\n",
    "        modkit_bed_name,\n",
    "        \"--log-filepath\",\n",
    "        each_output + each_condition + \"_modkit-extract.log\",\n",
    "        each_bam,\n",
    "        each_output\n",
    "    ]\n",
    "    subprocess.run(command, text=True)\n",
    "\n",
    "    # Create a list of arguments for each task\n",
    "task_args = list(zip(\n",
    "    new_bam_files,\n",
    "    thresh_list,\n",
    "    conditions,\n",
    "    sample_indices,\n",
    "    bam_fracs,\n",
    "    [modkit_path]*len(new_bam_files),\n",
    "    [output_stem]*len(new_bam_files),\n",
    "    [modkit_bed_name]*len(new_bam_files),\n",
    "    [bed_window]*len(new_bam_files)\n",
    "))\n",
    "\n",
    "# Execute commands in parallel\n",
    "with Pool(processes=10) as pool:\n",
    "    pool.map(modkit_extract, task_args)\n",
    "\n",
    "print(\"finished with:\")\n",
    "print(out_file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b130e9f6c16748",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### CELL A One row per read, use this\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import logging\n",
    "from math import floor\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Configuration: where to write per-worker Feather files\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "FEATHER_DIR = \"./_feather_tmp\"\n",
    "\n",
    "# Features for which minus-strand rows should be flipped to feature 5′→3′\n",
    "FLIP_FEATURES_CONTAINS = [\"TSS\", \"TES\", \"MEX\", \"gene\"]\n",
    "\n",
    "###############################################################################\n",
    "# Values that vary per‑base → roll into Arrow/Pandas list‑columns\n",
    "POSITION_LIST_COLS = [\n",
    "    \"mod_qual\",\n",
    "    \"base_qual\",\n",
    "    \"forward_read_position\",\n",
    "    \"ref_position\",\n",
    "    \"rel_pos\",\n",
    "    \"mod_qual_bin\",\n",
    "]\n",
    "\n",
    "# Single‑value (read‑ / region‑level) columns we keep as scalars\n",
    "READ_LEVEL_COLS = [\n",
    "    \"read_id\", \"chrom\", \"mod_code\", \"ref_mod_strand\", \"mod_strand\",\n",
    "    \"ref_strand\", \"canonical_base\", \"modified_primary_base\", \"inferred\",\n",
    "    \"fw_soft_clipped_start\", \"fw_soft_clipped_end\", \"read_length\",\n",
    "    \"bed_start\", \"bed_end\", \"bed_strand\", \"chr_type\", \"type\",\n",
    "    \"m6A_thresh\", \"m5mC_thresh\", \"condition\", \"exp_id\",\n",
    "    \"rel_read_start\", \"rel_read_end\",\n",
    "]\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Logging setup\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\",\n",
    ")\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Helper: safe DataFrame append (future‑proof for pandas 3.0)\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "def _append_df(target: pd.DataFrame, other: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Append *other* to *target* (handles empty target).\"\"\"\n",
    "    return other if target.empty else pd.concat([target, other], ignore_index=True)\n",
    "\n",
    "import pyranges as pr             #  ← NEW import (install via `pip install pyranges`)\n",
    "# Prevent NumPy/SciPy from spawning OpenMP/MKL threads\n",
    "os.environ[\"OMP_NUM_THREADS\"]         = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"]         = \"1\"\n",
    "# Prevent pandas/NumExpr from spawning its own pool\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"]     = \"1\"\n",
    "\n",
    "# Logging setup\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\",\n",
    ")\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "# Safe DataFrame append\n",
    "def _append_df(target: pd.DataFrame, other: pd.DataFrame) -> pd.DataFrame:\n",
    "    return other if target.empty else pd.concat([target, other], ignore_index=True)\n",
    "\n",
    "def add_bed_columns_no_loops(\n",
    "    bedmethyl_df: pd.DataFrame, bed_df: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    # Filter to +/- strands\n",
    "    bedmethyl_df = bedmethyl_df.loc[\n",
    "        bedmethyl_df[\"mod_strand\"].isin([\"+\", \"-\"])\n",
    "    ].copy()\n",
    "    if bedmethyl_df.empty or bed_df.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Build PyRanges objects\n",
    "    bm_pr = pr.PyRanges(\n",
    "        bedmethyl_df.rename(\n",
    "            columns={\"chrom\": \"Chromosome\", \"ref_position\": \"Start\"}\n",
    "        )\n",
    "        .assign(End=lambda df: df[\"Start\"] + 1)\n",
    "    )\n",
    "    bed_pr = pr.PyRanges(\n",
    "        bed_df.rename(\n",
    "            columns={\"chrom\": \"Chromosome\",\n",
    "                     \"bed_start\": \"Start\",\n",
    "                     \"bed_end\": \"End\"}\n",
    "        )\n",
    "    )\n",
    "\n",
    "    joined = bm_pr.join(bed_pr, nb_cpu=1)\n",
    "\n",
    "    if joined.df.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Restore original column names and ordering\n",
    "    df = joined.df.rename(\n",
    "        columns={\n",
    "            \"Chromosome\": \"chrom\",\n",
    "            \"Start\":      \"ref_position\",\n",
    "            \"End\":        \"tmp_End\",\n",
    "            \"Start_b\":    \"bed_start\",\n",
    "            \"End_b\":      \"bed_end\",\n",
    "        }\n",
    "    ).drop(columns=[\"tmp_End\"])\n",
    "    keep_cols = list(bedmethyl_df.columns) + [\n",
    "        \"bed_start\", \"bed_end\", \"bed_strand\", \"chr_type\", \"type\"\n",
    "    ]\n",
    "    df = df[keep_cols].copy()\n",
    "    df.sort_values(\n",
    "        [\"chrom\", \"bed_start\", \"ref_position\"],\n",
    "        inplace=True, ignore_index=True\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# ❷  Worker to process one bedmethyl file & spill to Feather\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "def _needs_flip_row(row) -> bool:\n",
    "    \"\"\"Minus-strand and type contains any flip token.\"\"\"\n",
    "    if row.get(\"bed_strand\") != \"-\":\n",
    "        return False\n",
    "    t = str(row.get(\"type\", \"\")).lower()\n",
    "    return any(tok.lower() in t for tok in FLIP_FEATURES_CONTAINS)\n",
    "\n",
    "def _flip_lists_row(row):\n",
    "    \"\"\"\n",
    "    Flip all POSITION_LIST_COLS for minus-strand feature rows:\n",
    "      • rel_pos → negate then reorder ascending\n",
    "      • all other list columns → reorder with the same index order\n",
    "      • rel_read_start/end → recompute from flipped rel_pos\n",
    "    Genomic coordinates (ref_position, forward_read_position) are reordered only.\n",
    "    \"\"\"\n",
    "    if not _needs_flip_row(row):\n",
    "        return row\n",
    "\n",
    "    # ensure rel_pos exists and is list-like\n",
    "    rel = row[\"rel_pos\"]\n",
    "    if not isinstance(rel, (list, tuple, np.ndarray)) or len(rel) == 0:\n",
    "        return row\n",
    "\n",
    "    rel = np.asarray(rel, dtype=np.int64)\n",
    "    rel_flipped = -rel\n",
    "    order = np.argsort(rel_flipped, kind=\"mergesort\")  # stable\n",
    "\n",
    "    # reorder every list column with the same order\n",
    "    for c in POSITION_LIST_COLS:\n",
    "        arr = row.get(c, None)\n",
    "        if isinstance(arr, (list, tuple, np.ndarray)):\n",
    "            arr = np.asarray(arr, dtype=object)\n",
    "            row[c] = arr[order].tolist()\n",
    "\n",
    "    # set flipped rel_pos explicitly, then recompute per-read span\n",
    "    row[\"rel_pos\"] = rel_flipped[order].tolist()\n",
    "    row[\"rel_read_start\"] = int(row[\"rel_pos\"][0])\n",
    "    row[\"rel_read_end\"]   = int(row[\"rel_pos\"][-1])\n",
    "    return row\n",
    "###############################################################################\n",
    "# PER‑WORKER PIPELINE  – completely rewritten aggregation logic\n",
    "###############################################################################\n",
    "def _process_one_file(args):\n",
    "    \"\"\"\n",
    "    Worker:\n",
    "\n",
    "    1. Read one bedmethyl TSV (dropping columns we no longer need).\n",
    "    2. Join to BED ⇒ attach interval metadata.\n",
    "    3. Compute rel_pos, mod_qual_bin, strand‑specific flip.\n",
    "    4. **Collapse to one row per (read_id, bed_start)**, rolling per‑base\n",
    "       columns into lists.\n",
    "    5. Suffix duplicate mappings of the same read ( …_1, …_2, …).\n",
    "    6. Feather‑spill to {FEATHER_DIR}/{idx}.feather (list columns are OK).\n",
    "    \"\"\"\n",
    "    (\n",
    "        idx,\n",
    "        each_output,\n",
    "        each_condition,\n",
    "        each_exp_id,\n",
    "        each_bam,\n",
    "        combined_bed_df,\n",
    "        type_selected,\n",
    "        bed_window,\n",
    "        temp_dir,\n",
    "    ) = args\n",
    "\n",
    "    sample_label = os.path.basename(each_output)\n",
    "    log.info(\"▶  Reading %s …\", sample_label)\n",
    "\n",
    "    ###########################################################################\n",
    "    # 1. Load & trim to just the columns we still need\n",
    "    ###########################################################################\n",
    "    cols_to_keep = {\n",
    "        \"read_id\",\n",
    "        \"chrom\",\n",
    "        \"mod_qual\",\n",
    "        \"mod_code\",\n",
    "        \"ref_mod_strand\",\n",
    "        \"fw_soft_clipped_end\",\n",
    "        \"base_qual\",\n",
    "        \"mod_strand\",\n",
    "        \"ref_strand\",\n",
    "        \"canonical_base\",\n",
    "        \"modified_primary_base\",\n",
    "        \"inferred\",\n",
    "        \"fw_soft_clipped_start\",\n",
    "        \"forward_read_position\",\n",
    "        \"ref_position\",\n",
    "        \"read_length\",\n",
    "    }\n",
    "    try:\n",
    "        bm = pd.read_csv(\n",
    "            each_output,\n",
    "            sep=\"\\t\",\n",
    "            comment=\"#\",\n",
    "            usecols=lambda c: c in cols_to_keep,   # ← drop ref_/query_kmer, etc.\n",
    "            dtype={\"chrom\": str},\n",
    "        )\n",
    "    except pd.errors.ParserError as e:\n",
    "        log.error(\"ParserError in %s: %s\", sample_label, e)\n",
    "        return None\n",
    "\n",
    "    if bm.empty:\n",
    "        return None\n",
    "\n",
    "    ###########################################################################\n",
    "    # 2. Attach BED metadata\n",
    "    ###########################################################################\n",
    "    bm.sort_values([\"chrom\", \"ref_position\"], inplace=True, ignore_index=True)\n",
    "    bm.drop_duplicates(inplace=True, ignore_index=True)\n",
    "    bm = add_bed_columns_no_loops(bm, combined_bed_df)\n",
    "    if bm.empty:\n",
    "        log.warning(\"  %s → no overlapping BED intervals\", sample_label)\n",
    "        return None\n",
    "\n",
    "    ###########################################################################\n",
    "    # 3. Per‑base calculations\n",
    "    ###########################################################################\n",
    "    bm[\"rel_pos\"] = bm[\"ref_position\"] - bm[\"bed_start\"] - bed_window + 1\n",
    "\n",
    "    bm[\"m6A_thresh\"] = m6A_thresh_dict.get(each_bam, np.nan)\n",
    "    bm[\"m5mC_thresh\"] = m5mC_thresh_dict.get(each_bam, np.nan)\n",
    "    bm[\"mod_qual_bin\"] = (bm[\"mod_qual\"] > bm[\"m6A_thresh\"]).astype(int)\n",
    "\n",
    "    bm[\"condition\"] = each_condition\n",
    "    bm[\"exp_id\"]     = each_exp_id\n",
    "    ###########################################################################\n",
    "    # 4. Collapse to one row per (read_id, bed_start)\n",
    "    ###########################################################################\n",
    "    # ──────────────────────────────────────────────────────────────────────────\n",
    "    # 4. Collapse to one row per (read_id, bed_start, type)  ← NEW KEY\n",
    "    # ──────────────────────────────────────────────────────────────────────────\n",
    "    agg = (\n",
    "        bm.groupby([\"read_id\", \"bed_start\", \"type\"], sort=False)  # added “type”\n",
    "          .agg(\n",
    "              chrom                 = (\"chrom\",                 \"first\"),\n",
    "              mod_code              = (\"mod_code\",              \"first\"),\n",
    "              ref_mod_strand        = (\"ref_mod_strand\",        \"first\"),\n",
    "              mod_strand            = (\"mod_strand\",            \"first\"),\n",
    "              ref_strand            = (\"ref_strand\",            \"first\"),\n",
    "              canonical_base        = (\"canonical_base\",        \"first\"),\n",
    "              modified_primary_base = (\"modified_primary_base\", \"first\"),\n",
    "              inferred              = (\"inferred\",              \"first\"),\n",
    "              fw_soft_clipped_start = (\"fw_soft_clipped_start\", \"first\"),\n",
    "              fw_soft_clipped_end   = (\"fw_soft_clipped_end\",   \"first\"),\n",
    "              read_length           = (\"read_length\",           \"first\"),\n",
    "              bed_end               = (\"bed_end\",               \"first\"),\n",
    "              bed_strand            = (\"bed_strand\",            \"first\"),\n",
    "              chr_type              = (\"chr_type\",              \"first\"),\n",
    "              m6A_thresh            = (\"m6A_thresh\",            \"first\"),\n",
    "              m5mC_thresh           = (\"m5mC_thresh\",           \"first\"),\n",
    "              condition             = (\"condition\",             \"first\"),\n",
    "              exp_id                = (\"exp_id\",                \"first\"),\n",
    "              # list‑columns\n",
    "              **{c: (c, list) for c in POSITION_LIST_COLS},\n",
    "              # per‑read span\n",
    "              rel_read_start        = (\"rel_pos\",               \"min\"),\n",
    "              rel_read_end          = (\"rel_pos\",               \"max\"),\n",
    "          )\n",
    "          .reset_index()   # read_id, bed_start, type now back as columns\n",
    "    )\n",
    "\n",
    "    # Flip minus-strand features to feature 5′→3′ orientation\n",
    "    pre_flip_n = len(agg)\n",
    "    flip_mask = (agg[\"bed_strand\"] == \"-\") & agg[\"type\"].str.contains(\"|\".join(FLIP_FEATURES_CONTAINS), case=False, na=False)\n",
    "    log.info(\"  Candidates to flip: %d / %d\", int(flip_mask.sum()), pre_flip_n)\n",
    "    \n",
    "    # Row-wise apply so we can reorder all list columns consistently\n",
    "    agg = agg.apply(_flip_lists_row, axis=1)\n",
    "\n",
    "    ###########################################################################\n",
    "    # 5. Append “…_1/…_2” for duplicate regions of the same read_id\n",
    "    ###########################################################################\n",
    "    agg[\"dup_idx\"] = agg.groupby(\"read_id\").cumcount() + 1\n",
    "    agg[\"read_id\"] = agg[\"read_id\"] + \"_\" + agg[\"dup_idx\"].astype(str)\n",
    "    agg.drop(columns=\"dup_idx\", inplace=True)\n",
    "\n",
    "    ###########################################################################\n",
    "    # 6. Feather spill\n",
    "    ###########################################################################\n",
    "    feather_path = os.path.join(temp_dir, f\"{idx}.feather\")\n",
    "    try:\n",
    "        agg.to_feather(feather_path)\n",
    "        log.debug(\"Wrote %d aggregated rows to %s\", len(agg), feather_path)\n",
    "        return idx\n",
    "    except Exception as e:\n",
    "        log.error(\"Failed to write Feather for %s: %s\", sample_label, e)\n",
    "        return None\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# ❸  Parallel orchestrator, now using Feather spill\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "def parallel_build_combined_df(\n",
    "    out_file_names,\n",
    "    conditions,\n",
    "    exp_ids,\n",
    "    new_bam_files,\n",
    "    combined_bed_df,\n",
    "    type_selected,\n",
    "    bed_window=500,\n",
    "):\n",
    "    \"\"\"\n",
    "    Launches a Pool (≤ 90 % of CPUs), spills each worker to Feather,\n",
    "    then reads them back, concatenates, and cleans up.\n",
    "    \"\"\"\n",
    "    max_workers = max(1, floor(cpu_count() * 0.9))\n",
    "    log.info(\"Using %d worker processes\", max_workers)\n",
    "\n",
    "    # prepare temp directory\n",
    "    if os.path.exists(FEATHER_DIR):\n",
    "        shutil.rmtree(FEATHER_DIR)\n",
    "    os.makedirs(FEATHER_DIR, exist_ok=True)\n",
    "\n",
    "    args_list = [\n",
    "        (\n",
    "            idx,\n",
    "            f,\n",
    "            cond,\n",
    "            exp_id,\n",
    "            bam,\n",
    "            combined_bed_df,\n",
    "            type_selected,\n",
    "            bed_window,\n",
    "            FEATHER_DIR,\n",
    "        )\n",
    "        for idx, (f, cond, exp_id, bam) in enumerate(\n",
    "            zip(out_file_names, conditions, exp_ids, new_bam_files)\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "    with Pool(processes=max_workers) as pool:\n",
    "        for idx in tqdm(\n",
    "            pool.imap_unordered(_process_one_file, args_list),\n",
    "            total=len(args_list),\n",
    "            desc=\"Processing bedmethyl files\",\n",
    "        ):\n",
    "            if idx is not None:\n",
    "                results.append(idx)\n",
    "\n",
    "    if not results:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # read back all Feather files\n",
    "    dfs = []\n",
    "    for idx in sorted(results):\n",
    "        path = os.path.join(FEATHER_DIR, f\"{idx}.feather\")\n",
    "        try:\n",
    "            dfs.append(pd.read_feather(path))\n",
    "        except Exception as e:\n",
    "            log.error(\"Failed reading feather %s: %s\", path, e)\n",
    "\n",
    "    combined = pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()\n",
    "\n",
    "    # cleanup\n",
    "    shutil.rmtree(FEATHER_DIR)\n",
    "\n",
    "    return combined\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# ❹  Build combined_bed_df  (keeps original logic/vars)\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# combined_bed_df = pd.DataFrame()\n",
    "# for each_bed in new_bed_files:\n",
    "#     bed_path = each_bed[:-3]  # strip .gz\n",
    "#     log.info(\"Reading BED: %s\", bed_path)\n",
    "#     combined_bed_df = _append_df(\n",
    "#         combined_bed_df, pd.read_csv(bed_path, sep=\"\\t\", header=None)\n",
    "#     )\n",
    "#\n",
    "# combined_bed_df.columns = [\n",
    "#     \"chrom\",\n",
    "#     \"bed_start\",\n",
    "#     \"bed_end\",\n",
    "#     \"bed_strand\",\n",
    "#     \"type\",\n",
    "#     \"chr_type\",\n",
    "# ]\n",
    "# combined_bed_df.sort_values([\"chrom\", \"bed_start\"], inplace=True, ignore_index=True)\n",
    "# log.info(\"combined_bed_df → %d rows\", len(combined_bed_df))\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# ❹  Use motif‑centered, MOTIFS_* regions\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# (assumes you've already run the centering + prefix logic into combined_bed_df_mex_cat)\n",
    "combined_bed_df = combined_bed_df_mex_cat.copy()\n",
    "# ensure chr_type still exists (if not already set):\n",
    "if 'chr_type' not in combined_bed_df:\n",
    "    combined_bed_df['chr_type'] = combined_bed_df['chrom'].apply(\n",
    "        lambda x: 'X' if x == 'CHROMOSOME_X' else 'Autosome'\n",
    "    )\n",
    "log.info(\"combined_bed_df → %d rows (centered MOTIFS_)\", len(combined_bed_df))\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# ❺  Kick off parallel bedmethyl processing\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "comb_bedmethyl_plot_df = parallel_build_combined_df(\n",
    "    out_file_names=out_file_names,\n",
    "    conditions=conditions,\n",
    "    exp_ids=exp_ids,\n",
    "    new_bam_files=new_bam_files,\n",
    "    combined_bed_df=combined_bed_df,\n",
    "    type_selected=type_selected,\n",
    "    bed_window=bed_window,\n",
    ")\n",
    "\n",
    "if comb_bedmethyl_plot_df.empty:\n",
    "    log.error(\"No data returned – terminating early.\")\n",
    "    raise SystemExit(1)\n",
    "\n",
    "log.info(\"Combined bedmethyl rows: %d\", len(comb_bedmethyl_plot_df))\n",
    "\n",
    "# # ──────────────────────────────────────────────────────────────────────────────\n",
    "# # ❻  Strand‑specific rel_pos flip  (TSS / TES / MEX / gene)\n",
    "# # ──────────────────────────────────────────────────────────────────────────────\n",
    "# flip_features = (\"TSS\", \"TES\", \"MEX\", \"gene\")\n",
    "# flip_mask = (\n",
    "#     comb_bedmethyl_plot_df[\"type\"].str.contains(\"|\".join(flip_features), na=False)\n",
    "#     & (comb_bedmethyl_plot_df[\"bed_strand\"] == \"-\")\n",
    "# )\n",
    "# flipped = flip_mask.sum()\n",
    "# if flipped:\n",
    "#     log.info(\"Flipping rel_pos sign for %d rows (minus‑strand features)\", flipped)\n",
    "#     comb_bedmethyl_plot_df.loc[flip_mask, \"rel_pos\"] *= -1\n",
    "#\n",
    "# # ──────────────────────────────────────────────────────────────────────────────\n",
    "# # ❼  Per‑read start / end aggregation (unchanged)\n",
    "# # ──────────────────────────────────────────────────────────────────────────────\n",
    "# log.info(\"Computing rel_read_start / rel_read_end per read_id …\")\n",
    "# _small = comb_bedmethyl_plot_df[[\"read_id\", \"bed_start\", \"rel_pos\"]]\n",
    "#\n",
    "# _grouped = (\n",
    "#     _small.groupby([\"read_id\", \"bed_start\"])[\"rel_pos\"]\n",
    "#     .agg(rel_read_start=\"min\", rel_read_end=\"max\")\n",
    "#     .reset_index()\n",
    "# )\n",
    "#\n",
    "# comb_bedmethyl_plot_df = comb_bedmethyl_plot_df.merge(\n",
    "#     _grouped, on=[\"read_id\", \"bed_start\"], how=\"left\"\n",
    "# )\n",
    "#\n",
    "# del _small, _grouped\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# ❽  Debug summaries\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "log.info(\n",
    "    \"Unique read_id count by condition:\\n%s\",\n",
    "    comb_bedmethyl_plot_df.groupby(\"condition\")[\"read_id\"].nunique(),\n",
    ")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# comb_bedmethyl_plot_df is ready for downstream plotting / analysis\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "print(comb_bedmethyl_plot_df.iloc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd4d499ccccabc",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Parallel motif ↔ read‑interval join  – NOW COMPATIBLE WITH PER‑READ DATAFRAME\n",
    "#\n",
    "# You already created `comb_bedmethyl_plot_df` so that each row is\n",
    "#  (read_id_with_suffix, bed_start) and all per‑base columns are Python lists.\n",
    "#\n",
    "# This pipeline:\n",
    "#   1. Loads & filters MEX / MEXII / motifC motifs (optional cluster collapse)\n",
    "#   2. Splits the per‑read BED intervals into N chunks\n",
    "#   3. Runs bedtools intersect in parallel (chunk × motif)\n",
    "#   4. Streams the worker results back into two list‑columns:\n",
    "#        • motif_rel_start    → list[int]\n",
    "#        • motif_attributes   → list[tuple(motif_id, ln_p, strand)]\n",
    "#\n",
    "# Result: two new list‑columns are **added** to comb_bedmethyl_plot_df\n",
    "#         (row alignment via comb_idx).\n",
    "###############################################################################\n",
    "\n",
    "import os, math, gc, json, subprocess, shutil, tempfile, datetime\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import multiprocessing as mp\n",
    "\n",
    "import pandas as pd, numpy as np, pyranges as pr, psutil\n",
    "from tqdm import tqdm\n",
    "import pyarrow.feather as feather\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# ─────────────────────────── Config & flags ────────────────────────────────\n",
    "BEDTOOLS_BIN           = \"/Data1/software/bedtools2/bin/bedtools\"\n",
    "MOTIF_WINDOW           = bed_window\n",
    "NUM_WORKERS            = min(16, os.cpu_count() - 2)\n",
    "STEP_LINES             = 500_000          # progress print interval\n",
    "\n",
    "COLLAPSE_MOTIF_CLUSTERS = True            # True → deduplicate motif clusters\n",
    "DEBUG_PROGRESS          = True            # RSS + progress prints\n",
    "DEBUG_SUMMARY           = True            # .info() / JSON summaries\n",
    "\n",
    "TMPDIR      = Path(tempfile.mkdtemp(prefix=\"motif_intersect_\"))\n",
    "RESULT_DIR  = TMPDIR / \"worker_results\"\n",
    "RESULT_DIR.mkdir()\n",
    "\n",
    "# ───────────────────────────── utilities ───────────────────────────────────\n",
    "def _rss_gb() -> float:\n",
    "    return psutil.Process(os.getpid()).memory_info().rss / 1_073_741_824\n",
    "\n",
    "def log_mem(label):\n",
    "    if DEBUG_PROGRESS:\n",
    "        gc.collect()\n",
    "        ts = datetime.datetime.now().strftime(\"%H:%M:%S\")\n",
    "        print(f\"[{ts}] {label:<40}: {_rss_gb():6.2f} GB RSS\")\n",
    "\n",
    "def log_df(df: pd.DataFrame, name: str):\n",
    "    if DEBUG_SUMMARY:\n",
    "        print(f\"\\n── {name} ──\"); print(df.info(memory_usage='deep')); print(df.head())\n",
    "\n",
    "def log_json(obj, name: str):\n",
    "    if DEBUG_SUMMARY:\n",
    "        print(f\"\\n── {name} ──\"); print(json.dumps(obj, indent=2))\n",
    "\n",
    "# ───────────────────── Stage 0: environment snapshot ───────────────────────\n",
    "if DEBUG_PROGRESS:\n",
    "    print(f\"bedtools: {subprocess.check_output([BEDTOOLS_BIN,'--version']).decode().strip()}\")\n",
    "    print(f\"CPUs     : {os.cpu_count()}  (workers = {NUM_WORKERS})\")\n",
    "    print(f\"System RAM: {round(psutil.virtual_memory().total/1_073_741_824)} GB\\n\")\n",
    "\n",
    "# ───────────────────── Stage 1: load + prepare FIMO motifs ─────────────────\n",
    "fimo_files = [\n",
    "    \"/Data1/ext_data/motifs/fimo_MEX_0.01.tsv\",\n",
    "    \"/Data1/ext_data/motifs/fimo_MEXII_0.01.tsv\",\n",
    "    \"/Data1/ext_data/motifs/fimo_motifc_0.01.tsv\",\n",
    "]\n",
    "dfs = []\n",
    "for f in fimo_files:\n",
    "    if DEBUG_PROGRESS: print(f\"Loading {f}\")\n",
    "    dfs.append(pd.read_csv(f, sep='\\t'))\n",
    "fimo_df = pd.concat(dfs, ignore_index=True)\n",
    "log_df(fimo_df, \"raw FIMO combined\")\n",
    "\n",
    "# basic harmonisation / filtering\n",
    "fimo_df = (\n",
    "    fimo_df\n",
    "      .assign(\n",
    "          Chromosome     = lambda d: d['sequence_name'].str.replace('chr','CHROMOSOME_'),\n",
    "          Start          = lambda d: d['start'],\n",
    "          End            = lambda d: d['stop'],\n",
    "          motif_priority = lambda d: d['motif_id'].map({'MEXII':1,'MEX':2,'motifC':3}),\n",
    "          ln_p           = lambda d: np.log(d['p-value']),\n",
    "      )\n",
    "      .query(\"(motif_id=='MEX'    and ln_p<=-13) or \"\n",
    "             \"(motif_id=='MEXII'  and ln_p<=-12) or \"\n",
    "             \"(motif_id=='motifC' and ln_p<= -9 )\")\n",
    "      # ▼ keep matched_sequence and rename p‑value → p_value\n",
    "      .loc[:, ['Chromosome','Start','End','strand','score','p-value',\n",
    "               'motif_id','motif_priority','ln_p','matched_sequence']]\n",
    "      .rename(columns={'p-value':'p_value'})\n",
    ")\n",
    "valid_chroms = set(comb_bedmethyl_plot_df['chrom'].unique())\n",
    "fimo_df = fimo_df[fimo_df['Chromosome'].isin(valid_chroms)].copy()\n",
    "log_df(fimo_df, \"after motif filters\")\n",
    "\n",
    "# optional motif‑cluster collapse\n",
    "if COLLAPSE_MOTIF_CLUSTERS:\n",
    "    pr_df = pr.PyRanges(fimo_df.astype({'Start':int,'End':int}))\n",
    "    clusters_df = (pr_df.cluster(strand=False).df\n",
    "                      .sort_values(['Cluster','motif_priority','Start'])\n",
    "                      .drop_duplicates('Cluster', keep='first'))\n",
    "else:\n",
    "    clusters_df = fimo_df.copy(); clusters_df['Cluster'] = np.arange(len(clusters_df))\n",
    "motif_bed = TMPDIR/\"fimo.bed\"\n",
    "# after (optional) cluster‑collapse …\n",
    "bed_cols = ['chrom','motif_start','motif_end','strand','score','p_value',\n",
    "            'motif_id','motif_priority','ln_p','matched_sequence']   # ◀︎ NEW\n",
    "(clusters_df\n",
    "     .rename(columns={'Chromosome':'chrom','Start':'motif_start','End':'motif_end'})\n",
    "     .sort_values(['chrom','motif_start','motif_end'], kind='mergesort')\n",
    "     [bed_cols]\n",
    "     .to_csv(motif_bed, sep='\\t', header=False, index=False))\n",
    "\n",
    "log_mem(\"after writing motif BED\")\n",
    "\n",
    "# ───────────────── Stage 1 b: load + prepare TSS / TES sites ───────────────\n",
    "TSS_TYPES = [f\"TSS_q{i}\" for i in range(1, 5)]\n",
    "TES_TYPES = [f\"TES_q{i}\" for i in range(1, 5)]\n",
    "QTYPES    = TSS_TYPES + TES_TYPES\n",
    "\n",
    "tss_bed = TMPDIR / \"tss_q.bed\"\n",
    "tes_bed = TMPDIR / \"tes_q.bed\"\n",
    "\n",
    "# create empty files upfront\n",
    "for p in (tss_bed, tes_bed):\n",
    "    p.parent.mkdir(parents=True, exist_ok=True)\n",
    "    open(p, \"wb\").close()\n",
    "\n",
    "n_tss = n_tes = 0\n",
    "try:\n",
    "    tt_df = (\n",
    "        pd.read_csv(bed_file, sep=\"\\t\")\n",
    "          .rename(columns={\"chromosome\":\"chrom\", \"start\":\"Site\", \"end\":\"Site_end\"})\n",
    "          .query(\"type in @QTYPES\")\n",
    "          .loc[:, [\"chrom\",\"Site\",\"Site_end\",\"strand\",\"type\"]]\n",
    "    )\n",
    "    # keep only valid chroms\n",
    "    tt_df = tt_df[tt_df[\"chrom\"].isin(valid_chroms)].copy()\n",
    "\n",
    "    # force integer coordinates, drop bad rows\n",
    "    tt_df[\"Site\"]     = pd.to_numeric(tt_df[\"Site\"],     errors=\"coerce\").astype(\"Int64\")\n",
    "    tt_df[\"Site_end\"] = pd.to_numeric(tt_df[\"Site_end\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    tt_df = tt_df.dropna(subset=[\"Site\",\"Site_end\"]).astype({\"Site\":int,\"Site_end\":int})\n",
    "\n",
    "    if not tt_df.empty:\n",
    "        ss = tt_df.query(\"type in @TSS_TYPES\").sort_values([\"chrom\",\"Site\",\"Site_end\"], kind=\"mergesort\")\n",
    "        ts = tt_df.query(\"type in @TES_TYPES\").sort_values([\"chrom\",\"Site\",\"Site_end\"], kind=\"mergesort\")\n",
    "        n_tss, n_tes = len(ss), len(ts)\n",
    "\n",
    "        if n_tss:\n",
    "            ss.to_csv(tss_bed, sep=\"\\t\", header=False, index=False)\n",
    "        if n_tes:\n",
    "            ts.to_csv(tes_bed, sep=\"\\t\", header=False, index=False)\n",
    "\n",
    "    log_df(tt_df, \"TSS/TES filtered\")\n",
    "finally:\n",
    "    RUN_TSS = n_tss > 0\n",
    "    RUN_TES = n_tes > 0\n",
    "    if DEBUG_PROGRESS:\n",
    "        print(f\"RUN_TSS={RUN_TSS} (n={n_tss})  RUN_TES={RUN_TES} (n={n_tes})\")\n",
    "    log_mem(\"after TSS/TES setup\")\n",
    "\n",
    "# ───────────────── Stage 2: prepare per‑read BED chunks ────────────────────\n",
    "comb_bedmethyl_plot_df['bed_start'] = comb_bedmethyl_plot_df['bed_start'].astype(int)\n",
    "comb_bedmethyl_plot_df['bed_end']   = comb_bedmethyl_plot_df['bed_end'].astype(int)\n",
    "\n",
    "comb_bed = comb_bedmethyl_plot_df[['chrom','bed_start','bed_end']].copy()\n",
    "comb_bed['comb_idx'] = comb_bed.index               # preserve row → index mapping\n",
    "\n",
    "chunk_size  = math.ceil(len(comb_bed)/NUM_WORKERS)\n",
    "chunk_paths = []\n",
    "for i in range(NUM_WORKERS):\n",
    "    s,e = i*chunk_size, min((i+1)*chunk_size,len(comb_bed))\n",
    "    if s>=e: break\n",
    "    path = TMPDIR/f\"comb_chunk_{i}.bed\"\n",
    "    (comb_bed.iloc[s:e]\n",
    "        .sort_values(['chrom','bed_start','bed_end'],kind='mergesort')\n",
    "        .to_csv(path, sep='\\t', header=False, index=False))\n",
    "    chunk_paths.append(path)\n",
    "\n",
    "log_json({\"n_chunks\":len(chunk_paths), \"rows_per_chunk\":chunk_size}, \"chunking stats\")\n",
    "log_mem(\"after writing chunk BEDs\")\n",
    "\n",
    "# ───────────────── Stage 3: worker – bedtools intersect ────────────────────\n",
    "def _run_intersect(chunk_path: Path):\n",
    "    import math, json, datetime, psutil, gc, pandas as pd\n",
    "    from collections import defaultdict\n",
    "    out_file = RESULT_DIR / f\"{chunk_path.stem}.feather\"\n",
    "\n",
    "    agg_rel   = defaultdict(list)   # comb_idx → list[int]\n",
    "    agg_attrs = defaultdict(list)   # comb_idx → list[(motif_id, ln_p, strand)]\n",
    "\n",
    "    cmd = [BEDTOOLS_BIN,\"intersect\",\"-a\",str(chunk_path),\"-b\",str(motif_bed.resolve()),\n",
    "       \"-wa\",\"-wb\",\"-sorted\"]\n",
    "    proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, text=True, bufsize=1)\n",
    "\n",
    "    for n,line in enumerate(proc.stdout,1):\n",
    "        c   = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "        idx = int(c[3]); bed_start = int(c[1])\n",
    "        rel = int(c[5]) - bed_start - MOTIF_WINDOW   # centre‑based offset\n",
    "        agg_rel[idx].append(rel)\n",
    "        ln_p = round(math.log(float(c[9])),1)            # natural‑log p, 1 dp\n",
    "        # (c) 4‑tuple with matched_sequence in col 13\n",
    "        agg_attrs[idx].append((c[10], ln_p, c[7], c[13]))   # ◀︎ UPDATED\n",
    "        if DEBUG_PROGRESS and n%STEP_LINES==0:\n",
    "            print(f\"[{datetime.datetime.now():%H:%M:%S}] {chunk_path.name} \"\n",
    "                  f\"{n/1e6:5.1f} M  {psutil.Process().memory_info().rss/1e9:4.1f} GB\")\n",
    "\n",
    "    proc.wait()\n",
    "    if proc.returncode:\n",
    "        raise RuntimeError(f\"{chunk_path.name} bedtools exit {proc.returncode}\")\n",
    "\n",
    "    df_keys = list(agg_rel.keys())\n",
    "    pd.DataFrame({\n",
    "        \"comb_idx\":         df_keys,\n",
    "        \"motif_rel_start\":  [agg_rel[k]   for k in df_keys],\n",
    "        \"motif_attributes\": [json.dumps(agg_attrs[k]) for k in df_keys],\n",
    "    }).to_feather(out_file)\n",
    "    return chunk_path.name\n",
    "\n",
    "# ───────────────── Stage 3 a: worker – TSS intersect ───────────────────────\n",
    "def _run_intersect_tss(chunk_path: Path):\n",
    "    import datetime, psutil, gc, math, json, pandas as pd\n",
    "    from collections import defaultdict\n",
    "    out_file = RESULT_DIR / f\"{chunk_path.stem}_tss.feather\"\n",
    "\n",
    "    # no TSS sites → emit empty shard and exit\n",
    "    if not RUN_TSS:\n",
    "        pd.DataFrame({\"comb_idx\":[], \"tss_rel_start\":[], \"tss_attributes\":[]}).to_feather(out_file)\n",
    "        return chunk_path.name\n",
    "\n",
    "    agg_rel, agg_attrs = defaultdict(list), defaultdict(list)\n",
    "    cmd = [BEDTOOLS_BIN,\"intersect\",\"-a\",str(chunk_path),\"-b\",str(tss_bed.resolve()),\n",
    "       \"-wa\",\"-wb\",\"-sorted\"]\n",
    "    proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, text=True, bufsize=1)\n",
    "\n",
    "    for n,line in enumerate(proc.stdout,1):\n",
    "        c = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "        idx, bed_start = int(c[3]), int(c[1])\n",
    "        rel = int(c[5]) - bed_start -  MOTIF_WINDOW                    # no window shift\n",
    "        if abs(rel) > MOTIF_WINDOW:          # keep only ±MOTIF_WINDOW hits\n",
    "            continue\n",
    "        agg_rel[idx].append(rel)\n",
    "        agg_attrs[idx].append((int(c[5]), c[7], c[8]))  # (start, strand, type)\n",
    "        if DEBUG_PROGRESS and n%STEP_LINES==0:\n",
    "            print(f\"[{datetime.datetime.now():%H:%M:%S}] {chunk_path.name} \"\n",
    "                  f\"TSS {n/1e6:5.1f} M  {psutil.Process().memory_info().rss/1e9:4.1f} GB\")\n",
    "    proc.wait()\n",
    "    if proc.returncode:\n",
    "        raise RuntimeError(f\"{chunk_path.name} TSS bedtools exit {proc.returncode}\")\n",
    "\n",
    "    pd.DataFrame({\n",
    "        \"comb_idx\":        list(agg_rel.keys()),\n",
    "        \"tss_rel_start\":   [agg_rel[k]   for k in agg_rel],\n",
    "        \"tss_attributes\":  [json.dumps(agg_attrs[k]) for k in agg_attrs],\n",
    "    }).to_feather(out_file)\n",
    "    return chunk_path.name\n",
    "\n",
    "# ───────────────── Stage 3 b: worker – TES intersect ───────────────────────\n",
    "def _run_intersect_tes(chunk_path: Path):\n",
    "    import datetime, psutil, gc, math, json, pandas as pd\n",
    "    from collections import defaultdict\n",
    "    out_file = RESULT_DIR / f\"{chunk_path.stem}_tes.feather\"\n",
    "\n",
    "    # no TES sites → emit empty shard and exit\n",
    "    if not RUN_TES:\n",
    "        pd.DataFrame({\"comb_idx\":[], \"tes_rel_start\":[], \"tes_attributes\":[]}).to_feather(out_file)\n",
    "        return chunk_path.name\n",
    "\n",
    "    agg_rel, agg_attrs = defaultdict(list), defaultdict(list)\n",
    "    cmd = [BEDTOOLS_BIN,\"intersect\",\"-a\",str(chunk_path),\"-b\",str(tes_bed.resolve()),\n",
    "       \"-wa\",\"-wb\",\"-sorted\"]\n",
    "    proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, text=True, bufsize=1)\n",
    "\n",
    "    for n,line in enumerate(proc.stdout,1):\n",
    "        c = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "        idx, bed_start = int(c[3]), int(c[1])\n",
    "        rel = int(c[5]) - bed_start - MOTIF_WINDOW                     # no window shift\n",
    "        if abs(rel) > MOTIF_WINDOW:\n",
    "            continue\n",
    "        agg_rel[idx].append(rel)\n",
    "        agg_attrs[idx].append((int(c[5]), c[7], c[8]))  # (start, strand, type)\n",
    "        if DEBUG_PROGRESS and n%STEP_LINES==0:\n",
    "            print(f\"[{datetime.datetime.now():%H:%M:%S}] {chunk_path.name} \"\n",
    "                  f\"TES {n/1e6:5.1f} M  {psutil.Process().memory_info().rss/1e9:4.1f} GB\")\n",
    "    proc.wait()\n",
    "    if proc.returncode:\n",
    "        raise RuntimeError(f\"{chunk_path.name} TES bedtools exit {proc.returncode}\")\n",
    "\n",
    "    pd.DataFrame({\n",
    "        \"comb_idx\":        list(agg_rel.keys()),\n",
    "        \"tes_rel_start\":   [agg_rel[k]   for k in agg_rel],\n",
    "        \"tes_attributes\":  [json.dumps(agg_attrs[k]) for k in agg_attrs],\n",
    "    }).to_feather(out_file)\n",
    "    return chunk_path.name\n",
    "\n",
    "\n",
    "# ───────────────── Stage 4: parallel intersection ──────────────────────────\n",
    "with mp.Pool(len(chunk_paths)) as pool, \\\n",
    "     tqdm(total=len(chunk_paths), desc=\"Chunks\", unit=\"chunk\") as pbar:\n",
    "    for _ in pool.imap_unordered(_run_intersect, chunk_paths):\n",
    "        pbar.update()\n",
    "log_mem(\"all intersects done (worker files on disk)\")\n",
    "\n",
    "# ───────────────── Stage 4 a: parallel TSS intersection ────────────────────\n",
    "if RUN_TSS:\n",
    "    with mp.Pool(len(chunk_paths)) as pool, \\\n",
    "         tqdm(total=len(chunk_paths), desc=\"Chunks-TSS\", unit=\"chunk\") as pbar:\n",
    "        for _ in pool.imap_unordered(_run_intersect_tss, chunk_paths):\n",
    "            pbar.update()\n",
    "    log_mem(\"TSS intersects done\")\n",
    "else:\n",
    "    if DEBUG_PROGRESS: print(\"Skipping TSS intersects (no TSS sites).\")\n",
    "\n",
    "# ───────────────── Stage 4 b: parallel TES intersection ────────────────────\n",
    "if RUN_TES:\n",
    "    with mp.Pool(len(chunk_paths)) as pool, \\\n",
    "         tqdm(total=len(chunk_paths), desc=\"Chunks-TES\", unit=\"chunk\") as pbar:\n",
    "        for _ in pool.imap_unordered(_run_intersect_tes, chunk_paths):\n",
    "            pbar.update()\n",
    "    log_mem(\"TES intersects done\")\n",
    "else:\n",
    "    if DEBUG_PROGRESS: print(\"Skipping TES intersects (no TES sites).\")\n",
    "\n",
    "# ───────────────── Stage 5: consolidate worker shards  ─────────────────────\n",
    "# (motifs only – exclude the “_tss” and “_tes” worker outputs)\n",
    "worker_files = [\n",
    "    p for p in RESULT_DIR.glob(\"*.feather\")\n",
    "    if not p.stem.endswith((\"_tss\", \"_tes\"))\n",
    "]\n",
    "all_rel, all_attrs = defaultdict(list), defaultdict(list)\n",
    "\n",
    "def _merge_one(path: Path):\n",
    "    df = feather.read_table(path).to_pandas()\n",
    "    for idx, rels, attrs_json in zip(\n",
    "            df['comb_idx'], df['motif_rel_start'], df['motif_attributes']):\n",
    "        all_rel[idx].extend(rels)\n",
    "        all_attrs[idx].extend(map(tuple, json.loads(attrs_json)))\n",
    "    return len(df)\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=min(8, len(worker_files))) as pool, \\\n",
    "     tqdm(total=len(worker_files), desc=\"merge‑motif\", unit=\"file\") as pbar:\n",
    "    for _ in pool.map(_merge_one, worker_files):\n",
    "        pbar.update()\n",
    "log_mem(\"motif master dicts built\")\n",
    "\n",
    "# ───────────────── merge TSS files ─────────────────────────────────────────\n",
    "worker_files_tss = list(RESULT_DIR.glob(\"*_tss.feather\"))\n",
    "all_tss_rel, all_tss_attrs = defaultdict(list), defaultdict(list)\n",
    "\n",
    "def _merge_tss(path: Path):\n",
    "    df = feather.read_table(path).to_pandas()\n",
    "    for idx, rels, attrs_json in zip(df['comb_idx'], df['tss_rel_start'], df['tss_attributes']):\n",
    "        all_tss_rel[idx].extend(rels)\n",
    "        all_tss_attrs[idx].extend(map(tuple, json.loads(attrs_json)))\n",
    "    return len(df)\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=min(8,len(worker_files_tss))) as pool, \\\n",
    "     tqdm(total=len(worker_files_tss), desc=\"merge‑TSS\", unit=\"file\") as pbar:\n",
    "    for _ in pool.map(_merge_tss, worker_files_tss):\n",
    "        pbar.update()\n",
    "\n",
    "# ───────────────── merge TES files ─────────────────────────────────────────\n",
    "worker_files_tes = list(RESULT_DIR.glob(\"*_tes.feather\"))\n",
    "all_tes_rel, all_tes_attrs = defaultdict(list), defaultdict(list)\n",
    "\n",
    "def _merge_tes(path: Path):\n",
    "    df = feather.read_table(path).to_pandas()\n",
    "    for idx, rels, attrs_json in zip(df['comb_idx'], df['tes_rel_start'], df['tes_attributes']):\n",
    "        all_tes_rel[idx].extend(rels)\n",
    "        all_tes_attrs[idx].extend(map(tuple, json.loads(attrs_json)))\n",
    "    return len(df)\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=min(8,len(worker_files_tes))) as pool, \\\n",
    "     tqdm(total=len(worker_files_tes), desc=\"merge‑TES\", unit=\"file\") as pbar:\n",
    "    for _ in pool.map(_merge_tes, worker_files_tes):\n",
    "        pbar.update()\n",
    "log_mem(\"TSS/TES master dicts built\")\n",
    "\n",
    "# ───────────────── Stage 6: attach list‑columns to main DataFrame ──────────\n",
    "motif_rel_list, motif_attr_list = [], []\n",
    "for i in range(len(comb_bedmethyl_plot_df)):\n",
    "    motif_rel_list.append(tuple(all_rel.get(i, [])))\n",
    "    motif_attr_list.append(tuple(all_attrs.get(i, [])))\n",
    "\n",
    "comb_bedmethyl_plot_df['motif_rel_start']  = motif_rel_list\n",
    "comb_bedmethyl_plot_df['motif_attributes'] = motif_attr_list\n",
    "\n",
    "# stage 6\n",
    "assert all(len(r) == len(a) for r, a in zip(\n",
    "    comb_bedmethyl_plot_df['motif_rel_start'],\n",
    "    comb_bedmethyl_plot_df['motif_attributes']\n",
    ")), \"row alignment broken – lengths differ after 4‑tuple upgrade\"\n",
    "\n",
    "\n",
    "# ───────────────── Stage 6: attach TSS / TES columns ──────────────────────\n",
    "tss_rel_list  = [tuple(all_tss_rel .get(i, [])) for i in range(len(comb_bedmethyl_plot_df))]\n",
    "tss_attr_list = [tuple(all_tss_attrs.get(i, [])) for i in range(len(comb_bedmethyl_plot_df))]\n",
    "tes_rel_list  = [tuple(all_tes_rel .get(i, [])) for i in range(len(comb_bedmethyl_plot_df))]\n",
    "tes_attr_list = [tuple(all_tes_attrs.get(i, [])) for i in range(len(comb_bedmethyl_plot_df))]\n",
    "\n",
    "comb_bedmethyl_plot_df['tss_rel_start']   = tss_rel_list\n",
    "comb_bedmethyl_plot_df['tss_attributes']  = tss_attr_list\n",
    "comb_bedmethyl_plot_df['tes_rel_start']   = tes_rel_list\n",
    "comb_bedmethyl_plot_df['tes_attributes']  = tes_attr_list\n",
    "\n",
    "\n",
    "# ───────────────── Stage 7: clean‑up scratch dir ───────────────────────────\n",
    "\n",
    "log_mem(\"after attaching motif columns\")\n",
    "\n",
    "log_df(comb_bedmethyl_plot_df.head(), \"final merged preview\")\n",
    "shutil.rmtree(TMPDIR, ignore_errors=True)\n",
    "print(\"\\n✅  motif ↔ read pipeline complete\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cced708aa6c7e4b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check that comb_bedmethyl_plot_df contains rows where type == \"MOTIFS_rex8\"\n",
    "if 'type' in comb_bedmethyl_plot_df.columns:\n",
    "    if not comb_bedmethyl_plot_df[comb_bedmethyl_plot_df['type'] == 'MOTIFS_rex32'].empty:\n",
    "        print(\"Found rows with type 'MOTIFS_rex32':\")\n",
    "        print(comb_bedmethyl_plot_df[comb_bedmethyl_plot_df['type'] == 'MOTIFS_rex32'].head())\n",
    "    else:\n",
    "        print(\"No rows found with type 'MOTIFS_rex32'.\")\n",
    "\n",
    "# print all column names\n",
    "print(\"\\nAll column names in comb_bedmethyl_plot_df:\")\n",
    "print(comb_bedmethyl_plot_df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7209aa97877efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Required\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assume nanotools is a custom module available in your environment\n",
    "# For example:\n",
    "# class Nanotools:\n",
    "#     def display_sample_rows(self, df, n=5):\n",
    "#         if df is not None and not df.empty:\n",
    "#             print(f\"\\nDisplaying first {n} sample rows:\")\n",
    "#             print(df.head(n))\n",
    "#         elif df is not None and df.empty:\n",
    "#             print(\"\\nDataFrame is empty.\")\n",
    "#         else:\n",
    "#             print(\"\\nDataFrame is None.\")\n",
    "# nanotools = Nanotools()\n",
    "# Ensure this (or your actual nanotools) is defined if you run this standalone.\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Configuration (Assumed to be defined elsewhere as per your request)\n",
    "# -----------------------------------------------------------------------------\n",
    "# These would be defined in your actual script:\n",
    "# type_selected = [\"your_type1\", \"your_type2\", \"your_type3\"] # Example\n",
    "# thresh_list = [0.1] # Example\n",
    "# bam_fracs = [0.2] # Example\n",
    "# bed_window = 150 # Example\n",
    "# comb_bedmethyl_plot_df = pd.DataFrame() # Example: This would be your large DataFrame to be chunked\n",
    "\n",
    "FORCE_OVERWRITE = True  # set to True if you want to overwrite existing chunk files\n",
    "\n",
    "base_fn = (\n",
    "    \"temp_files/\"\n",
    "    + \"plot_df_\"\n",
    "    + \"-\".join([str(x)[:3] for x in type_selected[-3:]])\n",
    "    + \"_\"\n",
    "    + str(thresh_list[0])\n",
    "    + \"_\"\n",
    "    + str(bam_fracs[0])\n",
    "    + str(bed_window) # Original had no separator before bed_window, kept as is.\n",
    ")\n",
    "# glob pattern for chunk files\n",
    "chunk_pattern = base_fn + \"_part*.pkl\"\n",
    "\n",
    "\n",
    "# Create temp_files directory if it doesn't exist, for writing.\n",
    "if not os.path.exists(\"temp_files\"):\n",
    "    os.makedirs(\"temp_files\", exist_ok=True)\n",
    "\n",
    "chunks_found = sorted(glob(chunk_pattern))\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Helper function for parallel reading\n",
    "def _read_pickle_part(filepath):\n",
    "    \"\"\"Reads a single pickle chunk into a pandas DataFrame.\"\"\"\n",
    "    return pd.read_pickle(filepath)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Load or write in parallel depending on existing files and FORCE_OVERWRITE\n",
    "# -----------------------------------------------------------------------------\n",
    "plot_df = None # Initialize plot_df\n",
    "\n",
    "if chunks_found and not FORCE_OVERWRITE:\n",
    "    print(f\"Found {len(chunks_found)} chunk file(s), loading in parallel…\")\n",
    "    num_read_workers = max(len(chunks_found), cpu_count(), 64)\n",
    "\n",
    "    if len(chunks_found) == 1:\n",
    "        print(\"Only one chunk found, reading directly.\")\n",
    "        plot_df = pd.read_pickle(chunks_found[0])\n",
    "    else:\n",
    "        try:\n",
    "            with Pool(processes=num_read_workers) as pool:\n",
    "                df_parts = list(tqdm(\n",
    "                    pool.imap(_read_pickle_part, chunks_found),\n",
    "                    total=len(chunks_found),\n",
    "                    desc=\"Reading pickle chunks\"\n",
    "                ))\n",
    "            plot_df = pd.concat(df_parts, ignore_index=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error during parallel read: {e}. Falling back to sequential.\")\n",
    "            plot_df = pd.concat(\n",
    "                (pd.read_pickle(fn) for fn in chunks_found),\n",
    "                ignore_index=True\n",
    "            )\n",
    "\n",
    "elif chunks_found and FORCE_OVERWRITE:\n",
    "    print(f\"Found {len(chunks_found)} chunk file(s), but FORCE_OVERWRITE=True → rewriting…\")\n",
    "    # Fall through to writing below\n",
    "    chunks_found = [] # This will trigger the 'if not chunks_found:' block\n",
    "else:\n",
    "    print(\"No chunk files found — writing in parallel…\")\n",
    "    # This implies chunks_found is empty, so the next block will execute\n",
    "\n",
    "if not chunks_found: # This block executes if chunks weren't found OR if FORCE_OVERWRITE was true\n",
    "    import numpy as np\n",
    "    from multiprocessing import Pool, cpu_count\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    def _write_chunk(args):\n",
    "        i, start, stop = args\n",
    "        # comb_bedmethyl_plot_df and base_fn must be defined in the global scope\n",
    "        part = comb_bedmethyl_plot_df.iloc[start:stop]\n",
    "        fn = f\"{base_fn}_part{i}.pkl\"\n",
    "        part.to_pickle(fn)\n",
    "        return i  # so tqdm knows one task completed\n",
    "\n",
    "    # calculate how many rows and splits\n",
    "    num_splits = 54\n",
    "    n = len(comb_bedmethyl_plot_df)\n",
    "    step = int(np.ceil(n / num_splits))\n",
    "\n",
    "    # prepare (chunk_index, start, stop) tuples, skipping empty slices\n",
    "    tasks = [\n",
    "        (i, i * step, min((i + 1) * step, n))\n",
    "        for i in range(num_splits)\n",
    "        if i * step < n\n",
    "    ]\n",
    "\n",
    "    n_workers = max(cpu_count() - 20, 1)\n",
    "    with Pool(processes=n_workers) as pool:\n",
    "        for _ in tqdm(\n",
    "            pool.imap_unordered(_write_chunk, tasks),\n",
    "            total=len(tasks),\n",
    "            desc=\"Writing pickle chunks\"\n",
    "        ):\n",
    "            pass\n",
    "\n",
    "    print(f\"Finished writing {len(tasks)} chunk(s).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d3d74f51c5606d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Process to add per-read statistics (placeholders for NRL, MAD, NUC) (REQUIRED)\n",
    "import os, glob, math, json\n",
    "# ---------------------------------------------------------------------------\n",
    "# 1.  Stream the pickle shards (per-read rows) in parallel\n",
    "# ---------------------------------------------------------------------------\n",
    "print(\"DEBUG: Parallel streaming of pickle chunks …\")\n",
    "chunk_files = sorted(glob.glob(chunk_pattern))\n",
    "\n",
    "# Use all but one core on a 64‐core machine\n",
    "n_workers = max(cpu_count() - 10, 1)\n",
    "\n",
    "def _process_chunk(pkl_path):\n",
    "    df = pd.read_pickle(pkl_path)\n",
    "    return df if not df.empty else None\n",
    "\n",
    "merged_parts = []\n",
    "with Pool(n_workers) as pool:\n",
    "    for part in tqdm(pool.imap_unordered(_process_chunk, chunk_files),\n",
    "                     total=len(chunk_files), unit=\"file\"):\n",
    "        if part is not None:\n",
    "            merged_parts.append(part)\n",
    "\n",
    "merged_df = pd.concat(merged_parts, ignore_index=True) if merged_parts else pd.DataFrame()\n",
    "print(f\"DEBUG: merged_df assembled → shape={merged_df.shape}\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 2.  Placeholder columns for NUC midpoint calculations\n",
    "# ---------------------------------------------------------------------------\n",
    "merged_df['smallest_positive_nuc_midpoint'] = np.nan\n",
    "merged_df['greatest_negative_nuc_midpoint'] = np.nan\n",
    "merged_df['closest_nuc']                   = np.nan\n",
    "merged_df['inter_nuc_dist']                = np.nan\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3.  Placeholder columns for MAD region summary\n",
    "# ---------------------------------------------------------------------------\n",
    "merged_df['closest_MAD_region']    = None\n",
    "merged_df['MAD_size']              = np.nan\n",
    "merged_df['closest_MAD_midpoint']  = np.nan\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 4.  Placeholder columns for percent_MAD / percent_NUC / percent_OTHER\n",
    "# ---------------------------------------------------------------------------\n",
    "merged_df['percent_MAD'] = np.nan\n",
    "merged_df['percent_NUC'] = np.nan\n",
    "merged_df['percent_OTHER'] = np.nan\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 5.  Placeholder columns for Fiber-NRL lists\n",
    "# ---------------------------------------------------------------------------\n",
    "n = len(merged_df)\n",
    "merged_df['fiber_NRL_list']     = [[] for _ in range(n)]\n",
    "merged_df['fiber_NRL_list_pos'] = [[] for _ in range(n)]\n",
    "merged_df['fiber_NRL_list_neg'] = [[] for _ in range(n)]\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 6.  Final tidy-up & sorting\n",
    "# ---------------------------------------------------------------------------\n",
    "merged_df = merged_df.sort_values(by='percent_MAD', ascending=True).reset_index(drop=True)\n",
    "print(\"DEBUG: merged_df sorted ✓\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 7.  Build grouped (one row per read_id) helper table\n",
    "# ---------------------------------------------------------------------------\n",
    "grouped = (\n",
    "    merged_df\n",
    "    .drop_duplicates('read_id')\n",
    "    .sort_values('read_id')\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Add placeholder lists for nucleosome-related columns\n",
    "grouped['nucs_list']                    = [[] for _ in range(len(grouped))]\n",
    "grouped['inter_nuc_sub']                = [[] for _ in range(len(grouped))]\n",
    "grouped['nuc_list_internuc_aligned']    = [[] for _ in range(len(grouped))]\n",
    "grouped['nucs_list_closest_aligned']    = [[] for _ in range(len(grouped))]\n",
    "\n",
    "# Map exp_id from condition\n",
    "grouped['exp_id'] = grouped['condition'].map(dict(zip(conditions, exp_ids)))\n",
    "\n",
    "nanotools.display_sample_rows(grouped, 5)\n",
    "nanotools.display_sample_rows(merged_df, 5)\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────\n",
    "# 8.  Optional down-sampling for plotting\n",
    "# ─────────────────────────────────────────────────────────────────\n",
    "\n",
    "# Number of reads per condition for plotting (0 = disable)\n",
    "n_read_ids = 0\n",
    "\n",
    "def downsample_group(group):\n",
    "    if n_read_ids == 0:\n",
    "        return group\n",
    "    ids = group['read_id'].unique()\n",
    "    if len(ids) <= n_read_ids:\n",
    "        return group\n",
    "    # … rest of your sampling logic …\n",
    "\n",
    "down_sampled_plot_df = (\n",
    "    merged_df\n",
    "    .groupby(['condition','chr_type','type'], group_keys=False)\n",
    "    .apply(downsample_group)\n",
    "    .reset_index(drop=True)\n",
    "    .sort_values(by=['smallest_positive_nuc_midpoint', 'greatest_negative_nuc_midpoint'])\n",
    ")\n",
    "\n",
    "down_sampled_group_df = grouped[\n",
    "    grouped['read_id'].isin(down_sampled_plot_df['read_id'])\n",
    "]\n",
    "\n",
    "print(\"DEBUG: Down-sampling complete →\",\n",
    "      down_sampled_plot_df.shape, down_sampled_group_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0fdd714d8d019d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "down_sampled_plot_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bdb438c35bab5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Refactored plotting pipeline\n",
    "────────────────────────────\n",
    "All external variables referenced here (e.g. `type_selected`, `analysis_cond`,\n",
    "`down_sampled_plot_df`, …) are *assumed* to be defined elsewhere exactly as in\n",
    "your current workflow.  Variables and constants that were originally declared\n",
    "in this file remain top‑level and unmodified.\n",
    "\n",
    "The code is organized as:\n",
    "\n",
    "1. Imports & global constants\n",
    "2. Generic helpers\n",
    "3. Motif‑table builder\n",
    "4. Main plotting routine (`create_plot`)\n",
    "5. Worker wrapper (`_save_plot`)\n",
    "6. CLI / script entry‑point\n",
    "\"\"\"\n",
    "# ───────────────────────────── Imports ────────────────────────────── #\n",
    "import importlib\n",
    "import io\n",
    "import os\n",
    "from itertools import product\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from typing import List, Sequence, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "from PIL import Image as PILImage           # disambiguate from IPython.display.Image\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "# still imported (for compatibility with earlier notebooks)\n",
    "from scipy.signal import find_peaks\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# reload local dev library for hot‑reload loops in notebooks\n",
    "import nanotools\n",
    "importlib.reload(nanotools)\n",
    "\n",
    "# ──────────────────────────── Constants ───────────────────────────── #\n",
    "# (unchanged ‑ feel free to edit values directly)\n",
    "READ_PIXEL_CLR    = \"#b12537\"\n",
    "M6A_DOT_CLR       = \"#b12537\"\n",
    "NUC_LINE_CLR      = \"#35b779\"\n",
    "NUC_HISTO_OPACITY = 0.60\n",
    "\n",
    "TEMPLATE          = \"plotly_white\"\n",
    "PNG_DPI           = 300\n",
    "TABLE_CELL_HEIGHT = 22\n",
    "TABLE_HEADER_H    = 28\n",
    "TABLE_MARGIN_PX   = 12\n",
    "\n",
    "# Figure size (new top‑level “config” values)\n",
    "FIG_WIDTH  = 700\n",
    "FIG_HEIGHT = 500\n",
    "\n",
    "# Apply global Plotly template immediately\n",
    "pio.templates.default = TEMPLATE\n",
    "\n",
    "# ─────────────────────────── Global BED load ───────────────────────── #\n",
    "# This loads once at module load time:\n",
    "# We will filter for rows where “type” contains \"TSS_q\" or \"TES_q\"\n",
    "tss_tes_df = pd.read_csv(\n",
    "    \"/Data1/reference/tss_tes_rex_combined_v27_WS235.bed\",\n",
    "    sep=r\"\\s+\",\n",
    "    usecols=[\"chromosome\", \"start\", \"end\", \"strand\", \"type\"]\n",
    ")\n",
    "\n",
    "# ───────────────────── Sorting config ───────────────────── #\n",
    "SORT_METHOD    = 0        # 1=binned‐HC, 3=PCA, 4=DTW (else=ACF)\n",
    "BIN_COUNT      = 2000        # number of bins for methods 1, 3, 4\n",
    "HC_LINKAGE     = \"ward\"    # for hierarchical clustering\n",
    "PCA_COMPONENTS = 1         # PCs to extract\n",
    "DTW_DISTANCE   = \"euclidean\"  # placeholder, uses fastdtw under the hood\n",
    "AC_SORT_LAG_MIN = 155\n",
    "AC_SORT_LAG_MAX = 195\n",
    "\n",
    "# ────────────────────────────────────────────────────────── #\n",
    "\n",
    "### SORTING HELPERS\n",
    "\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from sklearn.decomposition import PCA\n",
    "from fastdtw import fastdtw\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "def _order_reads_by_ac(\n",
    "    read_df: pd.DataFrame,\n",
    "    plot_window: int,\n",
    "    lag_lo: int = AC_SORT_LAG_MIN,\n",
    "    lag_hi: int = AC_SORT_LAG_MAX,\n",
    "    debug: bool = False\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute the mean autocorrelation for every read between `lag_lo`\n",
    "    and `lag_hi`, then return a *deduplicated* DataFrame that carries:\n",
    "\n",
    "        read_id | smallest_positive_nuc_midpoint | greatest_negative_nuc_midpoint\n",
    "                | ac_mean | read_count\n",
    "\n",
    "    `read_count` is 1‑based, descending by `ac_mean` (sign kept).\n",
    "    \"\"\"\n",
    "    # ensure sensible limits\n",
    "    lag_lo, lag_hi = sorted((lag_lo, lag_hi))\n",
    "    lag_hi = min(lag_hi, 2 * plot_window)          # never exceed vector length\n",
    "\n",
    "    means = {}\n",
    "    vec_len = 2 * plot_window + 1                  # index 0 == –plot_window\n",
    "\n",
    "    for r in read_df.itertuples():\n",
    "        rel_pos  = np.asarray(r.rel_pos, dtype=int)\n",
    "        mod_bin  = np.asarray(r.mod_qual_bin, dtype=float)   # 0/1; NaNs will stay NaN\n",
    "\n",
    "        # build full vector (NaN‑initialised)\n",
    "        vec = np.full(vec_len, np.nan, dtype=float)\n",
    "        idx = rel_pos + plot_window\n",
    "        keep = (idx >= 0) & (idx < vec_len)\n",
    "        vec[idx[keep]] = mod_bin[keep]\n",
    "\n",
    "        ac = nanotools._autocorr_vec(vec, lag_hi)            # already NaN‑aware\n",
    "        win = ac[lag_lo : lag_hi + 1]\n",
    "        #means[r.read_id] = np.nanmean(win)         # np.nanmean keeps sign\n",
    "        means[r.read_id] = np.nanmax(win)\n",
    "\n",
    "    # build ordering table\n",
    "    nodups = (\n",
    "        read_df[[\"read_id\",\n",
    "                  \"smallest_positive_nuc_midpoint\",\n",
    "                  \"greatest_negative_nuc_midpoint\"]]\n",
    "        .drop_duplicates(\"read_id\")\n",
    "        .assign(ac_mean=lambda d: d[\"read_id\"].map(means).fillna(-np.inf))\n",
    "        .sort_values(\"ac_mean\", ascending=False)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    nodups[\"read_count\"] = np.arange(1, len(nodups) + 1, dtype=int)\n",
    "\n",
    "    if debug:\n",
    "        print(f\"[acf‑sort] lag window {lag_lo}–{lag_hi} bp | \"\n",
    "              f\"{len(nodups)} reads | top‑5 means:\",\n",
    "              nodups[\"ac_mean\"].head().round(3).tolist())\n",
    "\n",
    "    return nodups\n",
    "\n",
    "def _build_binned_matrix(read_df, plot_window, n_bins=BIN_COUNT):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      M      – reads×bins matrix of normalized hit‐densities\n",
    "      read_ids – list of read_id in the same order as rows of M\n",
    "    \"\"\"\n",
    "    edges = np.linspace(-plot_window, plot_window, n_bins+1)\n",
    "    M, read_ids = [], []\n",
    "    for r in read_df.itertuples():\n",
    "        pos = np.asarray(r.rel_pos)[np.asarray(r.mod_qual_bin)==1]\n",
    "        counts, _ = np.histogram(pos, bins=edges)\n",
    "        M.append(counts / np.diff(edges))   # density per bp\n",
    "        read_ids.append(r.read_id)\n",
    "    return np.vstack(M), read_ids\n",
    "\n",
    "def _order_by_binned(read_df, plot_window, debug=False):\n",
    "    M, ids = _build_binned_matrix(read_df, plot_window)\n",
    "    Z      = sch.linkage(M, method=HC_LINKAGE, metric=\"euclidean\")\n",
    "    leaves = sch.dendrogram(Z, no_plot=True)[\"leaves\"]\n",
    "    return _make_nodups(read_df, [ids[i] for i in leaves], debug=debug)\n",
    "\n",
    "def _order_by_pca(read_df, plot_window, debug=False):\n",
    "    M, ids = _build_binned_matrix(read_df, plot_window)\n",
    "    pc1    = PCA(n_components=PCA_COMPONENTS).fit_transform(M)[:,0]\n",
    "    order  = np.argsort(pc1)  # ascending; flip if you want descending\n",
    "    return _make_nodups(read_df, [ids[i] for i in order], debug=debug)\n",
    "\n",
    "def _order_by_dtw(read_df, plot_window, debug=False):\n",
    "    M, ids = _build_binned_matrix(read_df, plot_window)\n",
    "    n      = M.shape[0]\n",
    "    D      = np.zeros((n,n))\n",
    "    for i in range(n):\n",
    "        for j in range(i):\n",
    "            dist, _ = fastdtw(M[i], M[j])\n",
    "            D[i,j] = D[j,i] = dist\n",
    "    # convert to condensed form for linkage\n",
    "    Z      = sch.linkage(squareform(D), method=HC_LINKAGE)\n",
    "    leaves = sch.dendrogram(Z, no_plot=True)[\"leaves\"]\n",
    "    return _make_nodups(read_df, [ids[i] for i in leaves], debug=debug)\n",
    "\n",
    "def _make_nodups(read_df, ordered_ids, debug=False):\n",
    "    \"\"\"\n",
    "    Build the nodups DataFrame (read_id, nuc_midpoints, read_count)\n",
    "    in the exact same format your create_plot expects.\n",
    "    \"\"\"\n",
    "    tmp = (\n",
    "        read_df[[\"read_id\",\n",
    "                 \"smallest_positive_nuc_midpoint\",\n",
    "                 \"greatest_negative_nuc_midpoint\"]]\n",
    "        .drop_duplicates(\"read_id\")\n",
    "    )\n",
    "    # preserve only those in ordered_ids, in that order\n",
    "    tmp = tmp.set_index(\"read_id\").loc[ordered_ids].reset_index()\n",
    "    tmp[\"read_count\"] = np.arange(1, len(tmp)+1)\n",
    "    if debug:\n",
    "        print(f\"[sort] {len(tmp)} reads ordered\")\n",
    "    return tmp\n",
    "\n",
    "\n",
    "\n",
    "# ─────────────────────────── Helpers ──────────────────────────────── #\n",
    "def run_in_pool(\n",
    "    func,\n",
    "    iterable: Sequence,\n",
    "    n_workers: int,\n",
    "    desc: str,\n",
    "    unit: str = \"item\"\n",
    "):\n",
    "    \"\"\"Utility wrapper: tqdm progress bar around `multiprocessing.Pool`.\"\"\"\n",
    "    from tqdm.auto import tqdm\n",
    "\n",
    "    with Pool(processes=n_workers) as pool:\n",
    "        return list(\n",
    "            tqdm(\n",
    "                pool.imap_unordered(func, iterable),\n",
    "                total=len(iterable),\n",
    "                desc=desc,\n",
    "                unit=unit\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "def stitch_vertical(png_top: bytes, png_bottom: bytes) -> PILImage.Image:\n",
    "    \"\"\"Merge two PNG byte‑streams vertically with a small white margin.\"\"\"\n",
    "    top = PILImage.open(io.BytesIO(png_top)).convert(\"RGBA\")\n",
    "    bot = PILImage.open(io.BytesIO(png_bottom)).convert(\"RGBA\")\n",
    "\n",
    "    w = max(top.width, bot.width)\n",
    "    h = top.height + TABLE_MARGIN_PX + bot.height\n",
    "\n",
    "    canvas = PILImage.new(\"RGBA\", (w, h), \"white\")\n",
    "    canvas.paste(top, (0, 0))\n",
    "    canvas.paste(bot, (0, top.height + TABLE_MARGIN_PX))\n",
    "    return canvas\n",
    "\n",
    "# ─────────────────── Chip‑rank lookup ─────────────────── #\n",
    "# loads a small table mapping each motif “type” to its chip‑rank score\n",
    "# (you may need to adjust the path to your actual .bed or .tsv file)\n",
    "chiprank_df = pd.read_csv(\n",
    "    \"/Data1/reference/rex_chiprank.bed\",\n",
    "    sep=r\"\\s+\",\n",
    "    usecols=[\"type\", \"chip_rank\"]\n",
    ")\n",
    "\n",
    "# prepend your “MOTIFS_” prefix so it matches the values of `typ`\n",
    "chiprank_df[\"type\"] = \"MOTIFS_\" + chiprank_df[\"type\"].astype(str)\n",
    "\n",
    "# build the lookup: motif → (chip_rank × 100, rounded)\n",
    "chip_rank_lookup = {\n",
    "    t: round(float(rk) * 100, 0)\n",
    "    for t, rk in zip(chiprank_df[\"type\"], chiprank_df[\"chip_rank\"])\n",
    "}\n",
    "\n",
    "\n",
    "# ────────────────────── 1) Motif‐DF builder ──────────────────────── #\n",
    "def _build_motif_df(read_df: pd.DataFrame, plot_window: int, debug: bool=False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a DataFrame with one row per unique motif (rel_start, motif_id, ln_p, strand, seq, num),\n",
    "    filtered to ±plot_window.  'num' is 1-based row index after sorting by rel_start.\n",
    "    \"\"\"\n",
    "    if debug: print(f\"[motifs] building motif_df from {len(read_df)} reads…\")\n",
    "\n",
    "    # trim out‐of‐window\n",
    "    def _trim(row):\n",
    "        starts = [] if pd.isna(row.motif_rel_start) else list(row.motif_rel_start)\n",
    "        attrs  = [] if pd.isna(row.motif_attributes) else list(row.motif_attributes)\n",
    "        keep   = [i for i,p in enumerate(starts) if -plot_window <= p <= plot_window]\n",
    "        return (tuple(starts[i] for i in keep),\n",
    "                tuple(attrs[i]  for i in keep))\n",
    "\n",
    "    tmp = read_df.apply(_trim, axis=1, result_type=\"expand\")\n",
    "    tmp.columns = [\"rel_start_list\", \"attributes_list\"]\n",
    "\n",
    "    # explode → one pair per row\n",
    "    exploded = (\n",
    "        tmp.assign(pairs=tmp.apply(lambda r: list(zip(r.rel_start_list, r.attributes_list)), axis=1))\n",
    "           .explode(\"pairs\")\n",
    "           .dropna(subset=[\"pairs\"])\n",
    "    )\n",
    "    if exploded.empty:\n",
    "        if debug: print(\"[motifs] no motifs after explode → returning empty df\")\n",
    "        return pd.DataFrame(columns=[\n",
    "            \"rel_start\", \"motif_id\", \"ln_p\", \"strand\", \"seq\", \"num\"\n",
    "        ])\n",
    "\n",
    "    motif_df = pd.DataFrame(exploded[\"pairs\"].tolist(),\n",
    "                            columns=[\"rel_start\", \"attributes\"])\n",
    "    motif_df = motif_df.drop_duplicates().sort_values(\"rel_start\").reset_index(drop=True)\n",
    "    motif_df[\"num\"] = motif_df.index + 1\n",
    "\n",
    "    # unpack attributes tuple into columns\n",
    "    motif_df[[\"motif_id\",\"ln_p\",\"strand\",\"seq\"]] = pd.DataFrame(\n",
    "        motif_df[\"attributes\"].tolist(),\n",
    "        index=motif_df.index\n",
    "    )\n",
    "    motif_df = motif_df[[\"rel_start\",\"motif_id\",\"ln_p\",\"strand\",\"seq\",\"num\"]]\n",
    "\n",
    "    if debug:\n",
    "        print(f\"[motifs] found {len(motif_df)} unique motifs:\")\n",
    "        print(motif_df.head())\n",
    "    return motif_df\n",
    "\n",
    "\n",
    "# ────────────────────── 2) Motif‐table builder ────────────────────── #\n",
    "def create_motif_table(read_df: pd.DataFrame, plot_window: int, debug: bool=False) -> Optional[go.Figure]:\n",
    "    if debug:\n",
    "        print(f\"[motif_table] starting; incoming read_df rows = {len(read_df)}\")\n",
    "        # Show a snippet of the raw motif columns:\n",
    "        print(\"    → motif_rel_start sample:\", read_df[\"motif_rel_start\"].head().tolist())\n",
    "        print(\"    → motif_attributes sample:\", read_df[\"motif_attributes\"].head().tolist())\n",
    "\n",
    "    motif_df = _build_motif_df(read_df, plot_window, debug)\n",
    "    if motif_df.empty:\n",
    "        if debug:\n",
    "            print(f\"[motif_table] _build_motif_df returned empty (no motifs within ±{plot_window}); returning None\")\n",
    "        return None\n",
    "\n",
    "    if debug:\n",
    "        print(f\"[motif_table] actually found {len(motif_df)} motif rows; here are the first few:\\n{motif_df.head()}\")\n",
    "\n",
    "    # compute column widths\n",
    "    px_per_char, pad = 7, 24\n",
    "    def _w(col): return len(str(max(col, key=lambda x: len(str(x))))) * px_per_char + pad\n",
    "    widths = [\n",
    "        _w(motif_df[\"num\"]),\n",
    "        _w(motif_df[\"motif_id\"]),\n",
    "        _w(motif_df[\"ln_p\"]),\n",
    "        _w(motif_df[\"strand\"]),\n",
    "        _w(motif_df[\"seq\"]),\n",
    "    ]\n",
    "\n",
    "    fig = go.Figure(data=[go.Table(\n",
    "        columnwidth=widths,\n",
    "        header=dict(\n",
    "            values=[\"#\", \"Motif\", \"ln(p‑val)\", \"Strand\", \"Sequence\"],\n",
    "            fill_color=\"white\", font=dict(color=\"black\",size=14),\n",
    "            line_color=\"lightgrey\", align=\"center\"\n",
    "        ),\n",
    "        cells=dict(\n",
    "            values=[\n",
    "                motif_df[\"num\"],\n",
    "                motif_df[\"motif_id\"],\n",
    "                motif_df[\"ln_p\"],\n",
    "                motif_df[\"strand\"],\n",
    "                motif_df[\"seq\"]\n",
    "            ],\n",
    "            fill_color=\"white\", font=dict(color=\"black\",size=12),\n",
    "            line_color=\"lightgrey\", align=\"center\",\n",
    "            height=TABLE_CELL_HEIGHT\n",
    "        )\n",
    "    )])\n",
    "    fig.update_layout(\n",
    "        margin=dict(l=0,r=0,t=0,b=0),\n",
    "        paper_bgcolor=\"rgba(0,0,0,0)\", plot_bgcolor=\"rgba(0,0,0,0)\",\n",
    "        height=TABLE_HEADER_H + len(motif_df)*TABLE_CELL_HEIGHT\n",
    "    )\n",
    "    if debug: print(f\"[motif_table] rendered {len(motif_df)} rows\")\n",
    "    return fig\n",
    "\n",
    "# ─────────────────── 2.5) Gene‐table builder (NEW) ─────────────────── #\n",
    "# ─────────────────── 2.5) Gene‐table builder (UPDATED) ─────────────────── #\n",
    "def create_gene_table(read_df: pd.DataFrame, debug: bool=False) -> Optional[go.Figure]:\n",
    "    \"\"\"\n",
    "    Returns a Plotly Table (up to two rows: closest upstream + closest downstream gene)\n",
    "    relative to the 0th position (i.e. bed_start). Now filters for 'gene_q' types only.\n",
    "\n",
    "    Columns:\n",
    "      [\"chromosome\", \"abs_start\", \"abs_end\", \"strand\", \"rel_start\", \"rel_end\", \"exp_quart\"]\n",
    "    \"\"\"\n",
    "    if read_df.empty:\n",
    "        return None\n",
    "\n",
    "    # 1) All reads in read_df should share the same chromosome and bed_start (0 position)\n",
    "    chrom   = read_df[\"chrom\"].iloc[0]       # e.g. \"CHROMOSOME_IV\"\n",
    "    ref_pos = int(read_df[\"bed_start\"].iloc[0])\n",
    "\n",
    "    # 2) Filter tss_tes_df to rows on this chromosome AND type containing \"gene_q\"\n",
    "    subset = tss_tes_df.loc[\n",
    "        (tss_tes_df[\"chromosome\"] == chrom)\n",
    "        & (tss_tes_df[\"type\"].str.contains(\"gene_q\"))\n",
    "    ].copy()\n",
    "    if subset.empty:\n",
    "        if debug:\n",
    "            print(f\"[gene_table] no 'gene_q' entries on {chrom}\")\n",
    "        return None\n",
    "\n",
    "    # 3) Now, for each \"gene_q\" row:\n",
    "    #    - abs_start  = subset[\"start\"]\n",
    "    #    - abs_end    = subset[\"end\"]\n",
    "    subset[\"abs_start\"] = subset[\"start\"]\n",
    "    subset[\"abs_end\"]   = subset[\"end\"]\n",
    "\n",
    "    # 4) Compute rel_start/rel_end relative to ref_pos\n",
    "    subset[\"rel_start\"] = subset[\"abs_start\"] - ref_pos\n",
    "    subset[\"rel_end\"]   = subset[\"abs_end\"]   - ref_pos\n",
    "\n",
    "    # 5) Extract exp_quart (e.g. \"q4\" from \"gene_q4\")\n",
    "    subset[\"exp_quart\"] = subset[\"type\"].str.extract(r\"_(q\\d+)\")[0]\n",
    "\n",
    "    # 6) Split upstream/downstream:\n",
    "    #    - Upstream   = rel_end < 0  → pick the one with max(rel_end)\n",
    "    #    - Downstream = rel_start > 0 → pick the one with min(rel_start)\n",
    "    upstream_df   = subset.loc[subset[\"rel_end\"]   < 0]\n",
    "    downstream_df = subset.loc[subset[\"rel_start\"] > 0]\n",
    "\n",
    "    rows = []\n",
    "    if not upstream_df.empty:\n",
    "        up_row = upstream_df.loc[upstream_df[\"rel_end\"].idxmax()]\n",
    "        rows.append(up_row)\n",
    "    if not downstream_df.empty:\n",
    "        down_row = downstream_df.loc[downstream_df[\"rel_start\"].idxmin()]\n",
    "        rows.append(down_row)\n",
    "\n",
    "    if not rows:\n",
    "        if debug:\n",
    "            print(f\"[gene_table] no genes upstream or downstream on {chrom}\")\n",
    "        return None\n",
    "\n",
    "    gene_df = pd.DataFrame(rows)[\n",
    "        [\"chromosome\", \"abs_start\", \"abs_end\", \"strand\",\n",
    "         \"rel_start\", \"rel_end\", \"exp_quart\"]\n",
    "    ].reset_index(drop=True)\n",
    "\n",
    "    # 7) Build Plotly Table with exactly those two (or fewer) rows\n",
    "    px_per_char, pad = 7, 24\n",
    "    def _w(col):\n",
    "        return len(str(max(col, key=lambda x: len(str(x))))) * px_per_char + pad\n",
    "\n",
    "    widths = [\n",
    "        _w(gene_df[\"chromosome\"]),\n",
    "        _w(gene_df[\"abs_start\"]),\n",
    "        _w(gene_df[\"abs_end\"]),\n",
    "        _w(gene_df[\"strand\"]),\n",
    "        _w(gene_df[\"rel_start\"]),\n",
    "        _w(gene_df[\"rel_end\"]),\n",
    "        _w(gene_df[\"exp_quart\"]),\n",
    "    ]\n",
    "\n",
    "    fig = go.Figure(data=[go.Table(\n",
    "        columnwidth=widths,\n",
    "        header=dict(\n",
    "            values=[\"Chromosome\", \"Abs Start\", \"Abs End\", \"Strand\",\n",
    "                    \"Rel Start\", \"Rel End\", \"Exp Quart\"],\n",
    "            fill_color=\"white\",\n",
    "            font=dict(color=\"black\", size=14),\n",
    "            line_color=\"lightgrey\",\n",
    "            align=\"center\"\n",
    "        ),\n",
    "        cells=dict(\n",
    "            values=[\n",
    "                gene_df[\"chromosome\"],\n",
    "                gene_df[\"abs_start\"],\n",
    "                gene_df[\"abs_end\"],\n",
    "                gene_df[\"strand\"],\n",
    "                gene_df[\"rel_start\"],\n",
    "                gene_df[\"rel_end\"],\n",
    "                gene_df[\"exp_quart\"]\n",
    "            ],\n",
    "            fill_color=\"white\",\n",
    "            font=dict(color=\"black\", size=12),\n",
    "            line_color=\"lightgrey\",\n",
    "            align=\"center\",\n",
    "            height=TABLE_CELL_HEIGHT\n",
    "        )\n",
    "    )])\n",
    "    fig.update_layout(\n",
    "        margin=dict(l=0, r=0, t=0, b=0),\n",
    "        paper_bgcolor=\"rgba(0,0,0,0)\",\n",
    "        plot_bgcolor=\"rgba(0,0,0,0)\",\n",
    "        height=TABLE_HEADER_H + len(gene_df) * TABLE_CELL_HEIGHT\n",
    "    )\n",
    "    if debug:\n",
    "        print(f\"[gene_table] rendered {len(gene_df)} row(s):\\n{gene_df}\")\n",
    "    return fig\n",
    "\n",
    "\n",
    "# ─────────────────────── Core plotting logic ───────────────────────── #\n",
    "def _filter_reads(\n",
    "    df: pd.DataFrame,\n",
    "    condition: str,\n",
    "    chr_type: str,\n",
    "    data_type: str,\n",
    "    plot_window: int,\n",
    "    require_full_span: bool,\n",
    "    debug: bool = False\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Metadata, span, and m6A filters → return copy of filtered reads.\"\"\"\n",
    "    reads = df.loc[\n",
    "        (df[\"condition\"] == condition)\n",
    "        & (df[\"chr_type\"] == chr_type)\n",
    "        & (df[\"type\"] == data_type)\n",
    "    ].copy()\n",
    "\n",
    "    if require_full_span:\n",
    "        span = (reads[\"rel_read_start\"] <= -round(plot_window * 0.3, 0)) & \\\n",
    "               (reads[\"rel_read_end\"]   >=  round(plot_window * 0.3, 0))\n",
    "        reads = reads.loc[span]\n",
    "\n",
    "    # keep only reads with ≥1 high‑confidence m6A call\n",
    "    reads = reads.loc[reads[\"mod_qual_bin\"].apply(lambda lst: 1 in lst)]\n",
    "\n",
    "    if debug:\n",
    "        print(f\"[filter] remaining rows: {len(reads)}\")\n",
    "    return reads\n",
    "\n",
    "\n",
    "def _long_format(read_df: pd.DataFrame, plot_window: int) -> pd.DataFrame:\n",
    "    \"\"\"Explode list columns → long format (one row per base).\"\"\"\n",
    "    records: List[Tuple[str, int, int, int]] = []\n",
    "    for r in read_df.itertuples():\n",
    "        idx = [i for i, p in enumerate(r.rel_pos) if -plot_window <= p <= plot_window]\n",
    "        for i in idx:\n",
    "            records.append((r.read_id, r.rel_pos[i], r.mod_qual_bin[i], r.read_count))\n",
    "    return pd.DataFrame(records,\n",
    "                        columns=[\"read_id\", \"rel_pos\", \"mod_qual_bin\", \"read_count\"])\n",
    "\n",
    "\n",
    "def _add_read_occupancy(fig, read_df, plot_window):\n",
    "    \"\"\"Horizontal bars showing read coverage.\"\"\"\n",
    "    genome_size = 2 * plot_window\n",
    "    read_counts_vec = np.zeros(genome_size, dtype=int)\n",
    "    line_x, line_y = [], []\n",
    "\n",
    "    for row in read_df.itertuples():\n",
    "        mn = max(-plot_window, row.rel_read_start)\n",
    "        mx = min( plot_window, row.rel_read_end)\n",
    "        for p in range(int(mn + plot_window), int(mx + plot_window) + 1):\n",
    "            if 0 <= p < genome_size:\n",
    "                read_counts_vec[p] += 1\n",
    "        line_x += [mn, mx, None]\n",
    "        line_y += [row.read_count] * 3\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=line_x, y=line_y, mode=\"lines\",\n",
    "                             line=dict(color=READ_PIXEL_CLR, width=0.1),\n",
    "                             showlegend=False),\n",
    "                  row=1, col=1)\n",
    "    return read_counts_vec\n",
    "\n",
    "\n",
    "def _add_nuc_segments(fig, down_df, plot_window):\n",
    "    \"\"\"Nucleosome midpoint segments on top read panel.\"\"\"\n",
    "    if down_df.empty:\n",
    "        return [], None\n",
    "\n",
    "    nuc_x, nuc_y, mids = [], [], []\n",
    "    half = 147 / 2\n",
    "    for r in down_df.itertuples():\n",
    "        keep = [n for n in r.nucs_list if -plot_window <= n <= plot_window]\n",
    "        for n in keep:\n",
    "            mids.append(n)\n",
    "            nuc_x += [n - half, n + half, None]\n",
    "            nuc_y += [r.read_count] * 3\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=nuc_x, y=nuc_y, mode=\"lines\",\n",
    "                             line=dict(color=NUC_LINE_CLR, width=2),\n",
    "                             opacity=0.75, showlegend=False),\n",
    "                  row=1, col=1)\n",
    "    return mids\n",
    "\n",
    "\n",
    "def _add_m6a_layers(\n",
    "    fig,\n",
    "    long_df: pd.DataFrame,\n",
    "    show_nuc_subplot: bool,\n",
    "    smoothing_type: str,\n",
    "    smoothing_window: int,\n",
    "    gaussian_sigma: float\n",
    "):\n",
    "    \"\"\"Scatter m6A hits + smoothed % track.\"\"\"\n",
    "    # individual m6A “pixels”\n",
    "    hits = long_df[long_df[\"mod_qual_bin\"] == 1]\n",
    "    fig.add_trace(go.Scatter(x=hits[\"rel_pos\"], y=hits[\"read_count\"],\n",
    "                             mode=\"markers\",\n",
    "                             marker=dict(symbol=\"square\", size=2.5,\n",
    "                                         color=M6A_DOT_CLR),\n",
    "                             showlegend=False),\n",
    "                  row=1, col=1)\n",
    "\n",
    "    # % m6A per position + smoothing\n",
    "    agg = long_df.groupby(\"rel_pos\")[\"mod_qual_bin\"].agg([\"sum\", \"count\"]).reset_index()\n",
    "    agg[\"ratio\"] = agg[\"sum\"] / agg[\"count\"]\n",
    "\n",
    "    if smoothing_type == \"none\":\n",
    "        y = agg[\"ratio\"]\n",
    "    elif smoothing_type == \"moving\":\n",
    "        wt_s = agg[\"sum\"].rolling(smoothing_window, center=True).sum()\n",
    "        wt_c = agg[\"count\"].rolling(smoothing_window, center=True).sum()\n",
    "        y = wt_s / wt_c\n",
    "    elif smoothing_type == \"gaussian\":\n",
    "        y = gaussian_filter1d(agg[\"ratio\"], sigma=gaussian_sigma)\n",
    "    else:\n",
    "        raise ValueError(\"smoothing_type must be 'none'|'moving'|'gaussian'\")\n",
    "\n",
    "    tgt_row = 2 if show_nuc_subplot else 2\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=agg[\"rel_pos\"][~np.isnan(y)], y=y[~np.isnan(y)],\n",
    "        mode=\"lines\", line=dict(color=M6A_DOT_CLR, width=2),\n",
    "        showlegend=False),\n",
    "        row=tgt_row, col=1\n",
    "    )\n",
    "    fig.update_yaxes(tickformat=\".0%\", row=tgt_row, col=1)\n",
    "\n",
    "\n",
    "def _add_nuc_hist(fig, mids, read_counts_vec, plot_window):\n",
    "    \"\"\"Optional nucleosome midpoint histogram + smoothed density.\"\"\"\n",
    "    if not mids:\n",
    "        return\n",
    "    genome_size = 2 * plot_window\n",
    "    bins = int(round(genome_size / 10) + 1)\n",
    "\n",
    "    fig.add_trace(go.Histogram(x=mids, nbinsx=bins,\n",
    "                               marker=dict(color=NUC_LINE_CLR,\n",
    "                                           opacity=NUC_HISTO_OPACITY),\n",
    "                               showlegend=False),\n",
    "                  row=3, col=1, secondary_y=False)\n",
    "\n",
    "    nuc_vec = np.zeros(genome_size)\n",
    "    for m in mids:\n",
    "        idx = int(m + plot_window)\n",
    "        if 0 <= idx < genome_size:\n",
    "            nuc_vec[idx] += 1\n",
    "    density = np.divide(nuc_vec, read_counts_vec, where=read_counts_vec != 0)\n",
    "    smoothed = gaussian_filter1d(density, sigma=10)\n",
    "    xs = np.arange(-plot_window, plot_window)\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=xs, y=smoothed, mode=\"lines\",\n",
    "                             line=dict(color=NUC_LINE_CLR, width=1),\n",
    "                             name=\"Smoothed nuc density\"),\n",
    "                  row=3, col=1, secondary_y=True)\n",
    "\n",
    "\n",
    "# ───────────────── 3) Annotation upgrader ──────────────────── #\n",
    "def _add_motif_annotations(fig, read_df, plot_window, debug: bool=False):\n",
    "    \"\"\"\n",
    "    Draws dashed lines + places the motif‐table row 'num' at a fixed y in paper coords.\n",
    "    \"\"\"\n",
    "    motif_df = _build_motif_df(read_df, plot_window, debug)\n",
    "    if motif_df.empty:\n",
    "        if debug: print(\"[annot] no motifs → skipping annotations\")\n",
    "        return\n",
    "\n",
    "    ANNOT_Y = 1.02\n",
    "    if debug: print(f\"[annot] adding annotations for {len(motif_df)} motifs\")\n",
    "\n",
    "    for row in motif_df.itertuples():\n",
    "        pos, num = row.rel_start, row.num\n",
    "        # dashed line\n",
    "        fig.add_shape(\n",
    "            type=\"line\", x0=pos, x1=pos, y0=0, y1=1,\n",
    "            xref=\"x\", yref=\"paper\",\n",
    "            line=dict(color=\"grey\",width=1,dash=\"dash\")\n",
    "        )\n",
    "        # label = the same 'num' used in table\n",
    "        fig.add_annotation(\n",
    "            x=pos, y=ANNOT_Y, yref=\"paper\", text=str(num),\n",
    "            showarrow=False, font=dict(size=10),\n",
    "            bgcolor=\"rgba(255,255,255,0.5)\"\n",
    "        )\n",
    "        if debug:\n",
    "            print(f\"[annot] motif#{num} at x={pos}\")\n",
    "\n",
    "\n",
    "\n",
    "# ───────────────────────── create_plot (public) ─────────────────────── #\n",
    "def create_plot(\n",
    "    plot_df: pd.DataFrame,\n",
    "    group_df: pd.DataFrame,\n",
    "    condition: str,\n",
    "    chr_type: str,\n",
    "    data_type: str,\n",
    "    plot_window: int,\n",
    "    show_nuc_subplot: bool = True,\n",
    "    plot_motifs: bool = True,\n",
    "    require_full_span: bool = False,\n",
    "    smoothing_type: str = \"moving\",\n",
    "    smoothing_window: int = 50,\n",
    "    gaussian_sigma: float = 10,\n",
    "    debug: bool = False\n",
    "):\n",
    "    \"\"\"Main figure builder – now with flexible read‐ordering.\"\"\"\n",
    "\n",
    "    # 1) Filter reads\n",
    "    read_df = _filter_reads(\n",
    "        plot_df, condition, chr_type, data_type,\n",
    "        plot_window, require_full_span, debug\n",
    "    )\n",
    "    if read_df.empty:\n",
    "        if debug:\n",
    "            print(\"[create_plot] no data – returning empty fig\")\n",
    "        return make_subplots(rows=2, cols=1)\n",
    "\n",
    "    # 2) Order reads by chosen method\n",
    "    if SORT_METHOD == 0:\n",
    "            nodups = (\n",
    "                read_df[[\"read_id\",\n",
    "                         \"smallest_positive_nuc_midpoint\",\n",
    "                         \"greatest_negative_nuc_midpoint\"]]\n",
    "                .drop_duplicates(\"read_id\")\n",
    "            )\n",
    "            nodups[\"read_count\"] = np.arange(1, len(nodups) + 1, dtype=int)\n",
    "    if SORT_METHOD == 1:\n",
    "        nodups = _order_by_binned(read_df, plot_window, debug)\n",
    "    elif SORT_METHOD == 3:\n",
    "        nodups = _order_by_pca(read_df, plot_window, debug)\n",
    "    elif SORT_METHOD == 4:\n",
    "        nodups = _order_by_dtw(read_df, plot_window, debug)\n",
    "    else:\n",
    "        nodups = _order_reads_by_ac(read_df, plot_window, debug=debug)\n",
    "\n",
    "    # graft the ordering back onto every row\n",
    "    read_df = read_df.merge(\n",
    "        nodups[[\"read_id\", \"read_count\"]],\n",
    "        on=\"read_id\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # 3) Prepare nuc‐midpoint df\n",
    "    down_df = (\n",
    "        group_df.loc[group_df[\"read_id\"].isin(nodups[\"read_id\"])]\n",
    "                  .merge(nodups[[\"read_id\", \"read_count\"]], on=\"read_id\")\n",
    "                  .dropna(subset=[\"nucs_list\"])\n",
    "    )\n",
    "    # 3) Explode → long tidy format\n",
    "    long_df = _long_format(read_df, plot_window)\n",
    "    if debug:\n",
    "        print(f\"[create_plot] long_df rows: {len(long_df):,}\")\n",
    "\n",
    "    # 4) Figure scaffold\n",
    "    n_rows, row_heights = ((3, [0.50, 0.25, 0.25])\n",
    "                           if show_nuc_subplot else\n",
    "                           (2, [0.33, 0.67]))\n",
    "    fig = make_subplots(rows=n_rows, cols=1,\n",
    "                        shared_xaxes=True,\n",
    "                        vertical_spacing=0.05,\n",
    "                        specs=[[{\"type\": \"xy\"}]] * n_rows,\n",
    "                        row_heights=row_heights)\n",
    "    fig.update_xaxes(range=[-plot_window, plot_window])\n",
    "\n",
    "    # 5) Layers\n",
    "    read_counts_vec = _add_read_occupancy(fig, read_df, plot_window)\n",
    "    mids = _add_nuc_segments(fig, down_df, plot_window) \\\n",
    "        if show_nuc_subplot else []\n",
    "    _add_m6a_layers(fig, long_df, show_nuc_subplot,\n",
    "                    smoothing_type, smoothing_window, gaussian_sigma)\n",
    "    if show_nuc_subplot:\n",
    "        _add_nuc_hist(fig, mids, read_counts_vec, plot_window)\n",
    "    if plot_motifs:\n",
    "        _add_motif_annotations(fig, read_df, plot_window)\n",
    "\n",
    "    # 6) Layout / labels\n",
    "    fig.update_layout(\n",
    "        template=\"simple_white\",\n",
    "        height=FIG_HEIGHT,\n",
    "        width=FIG_WIDTH,\n",
    "        title=f\"Condition: {condition} | Type: {data_type}\",\n",
    "        font=dict(size=14),\n",
    "        xaxis_title_font=dict(size=16),\n",
    "        yaxis_title_font=dict(size=16)\n",
    "    )\n",
    "\n",
    "    # style Read track\n",
    "    fig.update_yaxes(title_text=\"Read ID\",\n",
    "                     row=1,\n",
    "                     col=1,\n",
    "                     showgrid=False,\n",
    "                    showline=True,\n",
    "                    linecolor=\"darkgrey\",\n",
    "                    linewidth=1,\n",
    "                    ticks=\"outside\"\n",
    "                     )\n",
    "    fig.update_xaxes(\n",
    "                     row=1,\n",
    "                     col=1,\n",
    "                     showgrid=False,\n",
    "                    showline=True,\n",
    "                    linecolor=\"darkgrey\",\n",
    "                    linewidth=1,\n",
    "                    ticks=\"outside\"\n",
    "                     )\n",
    "\n",
    "    # style m6A track (no grid, dark‑grey axes, percent ticks)\n",
    "    tgt_row = 2 if show_nuc_subplot else 2\n",
    "    fig.update_yaxes(\n",
    "        title_text=\"% m6A\",\n",
    "        tickformat=\".0%\",\n",
    "        showgrid=False,\n",
    "        showline=True,\n",
    "        linecolor=\"darkgrey\",\n",
    "        linewidth=1,\n",
    "        ticks=\"outside\",\n",
    "        row=tgt_row, col=1\n",
    "    )\n",
    "    fig.update_xaxes(\n",
    "        title_text=\"Genomic position (bp)\",\n",
    "        showgrid=False,\n",
    "        showline=True,\n",
    "        linecolor=\"darkgrey\",\n",
    "        linewidth=1,\n",
    "        ticks=\"outside\",\n",
    "        row=tgt_row, col=1\n",
    "    )\n",
    "\n",
    "    # style nucleosome subplot (if present)\n",
    "    if show_nuc_subplot:\n",
    "        # you can choose to style these similarly or leave defaults\n",
    "        fig.update_yaxes(title_text=\"Nuc count\",   row=3, col=1)\n",
    "        fig.update_yaxes(title_text=\"Nuc density\", row=3, col=1,\n",
    "                         secondary_y=True)\n",
    "        fig.update_xaxes(title_text=\"Genomic position (bp)\", row=3, col=1)\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "\n",
    "# ────────────────────── Worker wrapper (unchanged API) ──────────────── #\n",
    "# ────────────────────── Worker wrapper (UPDATED) ───────────────────────── #\n",
    "def _save_plot(args):\n",
    "    cond, typ = args\n",
    "    chr_type    = \"X\"\n",
    "    plot_window = temp_bed_w\n",
    "\n",
    "    # ─── skip if no rows ─────────────────────────────────────────\n",
    "    mask = (\n",
    "        (down_sampled_plot_df[\"condition\"] == cond) &\n",
    "        (down_sampled_plot_df[\"chr_type\"]  == chr_type) &\n",
    "        (down_sampled_plot_df[\"type\"]      == typ)\n",
    "    )\n",
    "    if not mask.any():\n",
    "        if debug_usage:\n",
    "            print(f\"[Worker] skipping {cond}/{typ}: no read rows found\")\n",
    "        return None\n",
    "\n",
    "    if debug_usage:\n",
    "        print(f\"[Worker] starting plot for condition={cond}, type={typ}\")\n",
    "\n",
    "    main_fig = create_plot(\n",
    "        plot_df           = down_sampled_plot_df,\n",
    "        group_df          = down_sampled_group_df,\n",
    "        condition         = cond,\n",
    "        chr_type          = chr_type,\n",
    "        data_type         = typ,\n",
    "        plot_window       = plot_window,\n",
    "        show_nuc_subplot  = False,\n",
    "        plot_motifs       = True,\n",
    "        require_full_span = True,\n",
    "        debug             = debug_usage,\n",
    "        smoothing_type    = \"moving\",\n",
    "        smoothing_window  = 50,\n",
    "        gaussian_sigma    = 10\n",
    "    )\n",
    "\n",
    "    # ─── Motif table ───────────────────────────────────────────────\n",
    "    read_subset = down_sampled_plot_df.loc[\n",
    "        (down_sampled_plot_df[\"condition\"] == cond)\n",
    "        & (down_sampled_plot_df[\"chr_type\"] == chr_type)\n",
    "        & (down_sampled_plot_df[\"type\"] == typ)\n",
    "    ].copy()\n",
    "    table_fig = create_motif_table(read_subset, plot_window)\n",
    "\n",
    "    # ─── Gene table (closest upstream/downstream) ───────────────────\n",
    "    gene_table_fig = create_gene_table(read_subset)\n",
    "\n",
    "     # ─── Export images ─────────────────────────────────────────────\n",
    "    main_png = main_fig.to_image(format=\"png\", scale=2)\n",
    "\n",
    "    if table_fig and gene_table_fig:\n",
    "        tbl_png      = table_fig.to_image(format=\"png\", scale=2)\n",
    "        gene_tbl_png = gene_table_fig.to_image(format=\"png\", scale=2)\n",
    "\n",
    "        # 1) Stitch motif table below main plot → produce \"stitched_1\" as a PIL.Image\n",
    "        stitched_1 = stitch_vertical(main_png, tbl_png)\n",
    "\n",
    "        # 2) Re‐encode stitched_1 to PNG bytes before stitching gene table:\n",
    "        buf = io.BytesIO()\n",
    "        stitched_1.save(buf, format=\"PNG\")\n",
    "        stitched_1_png = buf.getvalue()\n",
    "\n",
    "        # 3) Now stitch the gene table PNG below stitched_1:\n",
    "        stitched = stitch_vertical(stitched_1_png, gene_tbl_png)\n",
    "\n",
    "    elif table_fig:\n",
    "        # Only motif table, no gene table\n",
    "        tbl_png  = table_fig.to_image(format=\"png\", scale=2)\n",
    "        stitched = stitch_vertical(main_png, tbl_png)\n",
    "\n",
    "    elif gene_table_fig:\n",
    "        # Only gene table, no motif table\n",
    "        gene_tbl_png = gene_table_fig.to_image(format=\"png\", scale=2)\n",
    "        stitched = stitch_vertical(main_png, gene_tbl_png)\n",
    "\n",
    "    else:\n",
    "        # Neither table\n",
    "        stitched = PILImage.open(io.BytesIO(main_png)).convert(\"RGBA\")\n",
    "\n",
    "    # ─── Build save paths ────────────────────────────────────────────\n",
    "    save_dir = os.path.join(\"images\", cond, str(plot_window))\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    rank_val = chip_rank_lookup.get(typ, None)\n",
    "    if rank_val is None:\n",
    "        rank_pref = \"NA\"\n",
    "    else:\n",
    "        rank_pref = str(int(round(rank_val)))\n",
    "\n",
    "    base = f\"{rank_pref}_{cond}_{typ}_b{plot_window}_chr{chr_type}\"\n",
    "    png_path = os.path.join(save_dir, base + \".png\")\n",
    "    svg_path = os.path.join(save_dir, base + \".svg\")\n",
    "\n",
    "    # ─── Write PNG ───────────────────────────────────────────────────\n",
    "    stitched.save(png_path, dpi=(PNG_DPI, PNG_DPI))\n",
    "\n",
    "    # ─── Write SVG (concatenate motif+gene tables if exist) ──────────\n",
    "    main_svg = main_fig.to_image(format=\"svg\").decode()\n",
    "\n",
    "    if table_fig and gene_table_fig:\n",
    "        tbl_svg       = table_fig.to_image(format=\"svg\").decode()\n",
    "        gene_tbl_svg  = gene_table_fig.to_image(format=\"svg\").decode()\n",
    "\n",
    "        # Build a triple‐stacked <g> structure\n",
    "        svg_head = '<svg xmlns=\"http://www.w3.org/2000/svg\">'\n",
    "        translate_tbl = f'translate(0,{main_fig.layout.height + TABLE_MARGIN_PX})'\n",
    "        translate_gene = f'translate(0,{main_fig.layout.height + TABLE_MARGIN_PX + table_fig.layout.height + TABLE_MARGIN_PX})'\n",
    "        combined_svg = (\n",
    "            svg_head\n",
    "            + f'<g>{main_svg}</g>'\n",
    "            + f'<g transform=\"{translate_tbl}\">{tbl_svg}</g>'\n",
    "            + f'<g transform=\"{translate_gene}\">{gene_tbl_svg}</g>'\n",
    "            + '</svg>'\n",
    "        )\n",
    "\n",
    "    elif table_fig:\n",
    "        tbl_svg = table_fig.to_image(format=\"svg\").decode()\n",
    "        svg_head = '<svg xmlns=\"http://www.w3.org/2000/svg\">'\n",
    "        translate_tbl = f'translate(0,{main_fig.layout.height + TABLE_MARGIN_PX})'\n",
    "        combined_svg = (\n",
    "            svg_head\n",
    "            + f'<g>{main_svg}</g>'\n",
    "            + f'<g transform=\"{translate_tbl}\">{tbl_svg}</g>'\n",
    "            + '</svg>'\n",
    "        )\n",
    "\n",
    "    elif gene_table_fig:\n",
    "        gene_tbl_svg = gene_table_fig.to_image(format=\"svg\").decode()\n",
    "        svg_head = '<svg xmlns=\"http://www.w3.org/2000/svg\">'\n",
    "        translate_gene = f'translate(0,{main_fig.layout.height + TABLE_MARGIN_PX})'\n",
    "        combined_svg = (\n",
    "            svg_head\n",
    "            + f'<g>{main_svg}</g>'\n",
    "            + f'<g transform=\"{translate_gene}\">{gene_tbl_svg}</g>'\n",
    "            + '</svg>'\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        combined_svg = main_svg\n",
    "\n",
    "    with open(svg_path, \"w\") as fh:\n",
    "        fh.write(combined_svg)\n",
    "\n",
    "    if debug_usage:\n",
    "        print(f\"[Worker] saved → {png_path}\")\n",
    "    return png_path\n",
    "\n",
    "\n",
    "# ───────────────────────────── Script main ─────────────────────────── #\n",
    "if __name__ == \"__main__\":\n",
    "    debug_usage = True\n",
    "    num_workers = 50\n",
    "    temp_bed_w  = 1000\n",
    "\n",
    "    motifs_type_selected = [f\"MOTIFS_{t}\" for t in type_selected]\n",
    "\n",
    "    test_combo = [[analysis_cond[0], \"univ_nuc\"]]\n",
    "    all_combos = list(product(analysis_cond, motifs_type_selected))\n",
    "    combos =  test_combo # quick test combo\n",
    "\n",
    "    # Use helper for pool + progress bar\n",
    "    saved_pngs = run_in_pool(\n",
    "        func=_save_plot,\n",
    "        iterable=combos,\n",
    "        n_workers=num_workers,\n",
    "        desc=\"Saving plots\",\n",
    "        unit=\"plot\"\n",
    "    )\n",
    "\n",
    "    if len(combos) == 1:\n",
    "        display(Image(filename=saved_pngs[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b633cf91b3e90796",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ─────────────────────────── Dual‑track plotting ────────────────────────── #\n",
    "from itertools import product, combinations\n",
    "import plotly.graph_objects as go\n",
    "import warnings\n",
    "\n",
    "\n",
    "# Default palettes (override with kwargs)\n",
    "DEFAULT_READ_COLORS     = [\"#b12537\", \"#4974a5\"]\n",
    "DEFAULT_M6A_DOT_COLORS  = [\"#b12537\", \"#4974a5\"]\n",
    "DEFAULT_M6A_LINE_COLORS = [\"#b12537\", \"#4974a5\"]\n",
    "\n",
    "# ───────────────────── Sorting config ───────────────────── #\n",
    "SORT_METHOD    = 0        # 1=binned‐HC, 3=PCA, 4=DTW (else=ACF)\n",
    "BIN_COUNT      = 200        # number of bins for methods 1, 3, 4\n",
    "HC_LINKAGE     = \"ward\"    # for hierarchical clustering\n",
    "PCA_COMPONENTS = 1         # PCs to extract\n",
    "DTW_DISTANCE   = \"euclidean\"  # placeholder, uses fastdtw under the hood\n",
    "AC_SORT_LAG_MIN = 155\n",
    "AC_SORT_LAG_MAX = 195\n",
    "\n",
    "\n",
    "# ──────────────────────── Occupancy helper (colour) ───────────────────── #\n",
    "def _add_read_occupancy_multi(fig, read_df, plot_window, row, color, read_width):\n",
    "    \"\"\"Horizontal bars showing read coverage (for multi‑condition plots).\"\"\"\n",
    "    line_x, line_y = [], []\n",
    "    for r in read_df.itertuples():\n",
    "        mn = max(-plot_window, r.rel_read_start)\n",
    "        mx = min( plot_window, r.rel_read_end)\n",
    "        line_x += [mn, mx, None]\n",
    "        line_y += [r.read_count] * 3\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=line_x, y=line_y, mode=\"lines\",\n",
    "                             line=dict(color=color, width=read_width),\n",
    "                             showlegend=False),\n",
    "                  row=row, col=1)\n",
    "\n",
    "import math # Make sure this import is at the top with other imports if not already there.\n",
    "\n",
    "def create_dual_plot(\n",
    "    plot_df: pd.DataFrame,\n",
    "    group_df: pd.DataFrame,\n",
    "    condition,               # str | list[str]  (≤ 2)\n",
    "    chr_type: str,\n",
    "    data_type: str,\n",
    "    plot_window: int,\n",
    "    plot_motifs: bool = True,\n",
    "    require_full_span: bool = False,\n",
    "    smoothing_type: str = \"moving\",\n",
    "    smoothing_window: int = 25,\n",
    "    gaussian_sigma: float = 10,\n",
    "    debug: bool = False,\n",
    "    max_reads_per_condition: Optional[int] = None, # New parameter\n",
    "    random_state: Optional[int] = 42,              # Added for subsampling reproducibility\n",
    "    # colour overrides\n",
    "    read_colors: list = None,\n",
    "    m6a_dot_colors: list = None,\n",
    "    m6a_line_colors: list = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Two‑track (or single‑track) read plot with a shared %m6A panel.\n",
    "    \"\"\"\n",
    "    # ─── 1. Standardise condition input ──────────────────────────────────\n",
    "    conds = list(condition) if isinstance(condition, (list, tuple)) else [condition]\n",
    "    if not (1 <= len(conds) <= 2):\n",
    "        raise ValueError(\"condition must be a string or a list/tuple of length ≤ 2\")\n",
    "    single = (len(conds) == 1)\n",
    "\n",
    "    # ─── 2. Check for missing reads ─────────────────────────────────────\n",
    "    missing = []\n",
    "    for cond in conds:\n",
    "        df_tmp = _filter_reads(\n",
    "            plot_df, cond, chr_type, data_type,\n",
    "            plot_window, require_full_span, debug\n",
    "        )\n",
    "        if df_tmp.empty:\n",
    "            missing.append(cond)\n",
    "    # warn for each missing condition\n",
    "    for cond in missing:\n",
    "        warnings.warn(\n",
    "            f\"No reads found for condition '{cond}' and type '{data_type}'\",\n",
    "            UserWarning\n",
    "        )\n",
    "    # if *all* are missing, bail out\n",
    "    if len(missing) == len(conds):\n",
    "        warnings.warn(\n",
    "            f\"No reads to plot for conditions {conds} and type '{data_type}'; skipping plot.\",\n",
    "            UserWarning\n",
    "        )\n",
    "        return None\n",
    "\n",
    "    # ─── 3. Colour palette ───────────────────────────────────────────────\n",
    "    read_colors     = read_colors     or DEFAULT_READ_COLORS[:len(conds)]\n",
    "    m6a_dot_colors  = m6a_dot_colors  or DEFAULT_M6A_DOT_COLORS[:len(conds)]\n",
    "    m6a_line_colors = m6a_line_colors or DEFAULT_M6A_LINE_COLORS[:len(conds)]\n",
    "\n",
    "    # ─── 4. Figure scaffold ─────────────────────────────────────────────\n",
    "    if single:\n",
    "        rows, row_heights = 2, [0.4, 0.6]            # read | m6A\n",
    "        read_rows         = [1]\n",
    "        m6a_row           = 2\n",
    "    else:\n",
    "        rows, row_heights = 3, [0.2, 0.4, 0.2]       # readA | m6A | readB\n",
    "        read_rows         = [1, 3]\n",
    "        m6a_row           = 2\n",
    "\n",
    "    fig = make_subplots(\n",
    "        rows=rows, cols=1,\n",
    "        shared_xaxes=True,\n",
    "        vertical_spacing=0.02,\n",
    "        row_heights=row_heights\n",
    "    )\n",
    "    fig.update_xaxes(range=[-plot_window, plot_window])\n",
    "\n",
    "    # ─── 5. Per‑condition layers ─────────────────────────────────────────\n",
    "    motif_collect = []\n",
    "    for idx, cond in enumerate(conds):\n",
    "        df_reads = _filter_reads(\n",
    "            plot_df, cond, chr_type, data_type,\n",
    "            plot_window, require_full_span, debug\n",
    "        )\n",
    "        if df_reads.empty:\n",
    "            continue\n",
    "        motif_collect.append(df_reads[[\"motif_rel_start\", \"motif_attributes\"]])\n",
    "\n",
    "        # Ensure df_reads is not empty before proceeding\n",
    "        if df_reads.empty:\n",
    "            continue\n",
    "\n",
    "        # Subsample reads for this condition before ordering\n",
    "        current_reads_count = df_reads[\"read_id\"].nunique()\n",
    "        # Use a consistent max_reads_to_display here, let's pass it as a parameter to create_dual_plot\n",
    "        # or define it as a local variable for clarity. Let's make it a parameter for dual_plot.\n",
    "\n",
    "        # We need to determine the target number of reads based on `max_reads_per_condition`\n",
    "        # and the actual number of unique reads after filtering.\n",
    "        # We need to determine the target number of reads based on `max_reads_per_condition`\n",
    "        # and the actual number of unique reads after filtering.\n",
    "        if max_reads_per_condition is not None:\n",
    "            num_reads_for_this_cond = min(max_reads_per_condition, current_reads_count)\n",
    "        else:\n",
    "            num_reads_for_this_cond = current_reads_count # No limit if None\n",
    "\n",
    "        if num_reads_for_this_cond < current_reads_count: # Only subsample if needed\n",
    "            read_ids_to_keep = df_reads[\"read_id\"].drop_duplicates().sample(\n",
    "                n=num_reads_for_this_cond, random_state=random_state\n",
    "            )\n",
    "            df_reads = df_reads[df_reads[\"read_id\"].isin(read_ids_to_keep)].copy()\n",
    "            if debug:\n",
    "                print(f\"[dual-plot] Condition '{cond}' subsampled to {len(read_ids_to_keep)} reads.\")\n",
    "\n",
    "        # 4‑B. Sort reads\n",
    "        if SORT_METHOD == 0:\n",
    "            nodups = (\n",
    "                df_reads[[\"read_id\",\n",
    "                         \"smallest_positive_nuc_midpoint\",\n",
    "                         \"greatest_negative_nuc_midpoint\"]]\n",
    "                .drop_duplicates(\"read_id\")\n",
    "            )\n",
    "            nodups[\"read_count\"] = np.arange(1, len(nodups) + 1, dtype=int)\n",
    "        elif SORT_METHOD == 1:\n",
    "            nodups = _order_by_binned(df_reads, plot_window, debug)\n",
    "        elif SORT_METHOD == 3:\n",
    "            nodups = _order_by_pca(df_reads, plot_window, debug)\n",
    "        elif SORT_METHOD == 4:\n",
    "            nodups = _order_by_dtw(df_reads, plot_window, debug)\n",
    "        else:\n",
    "            nodups = _order_reads_by_ac(df_reads, plot_window, debug=debug)\n",
    "\n",
    "        # merge sorted read_count back into the per-base DataFrame\n",
    "        df_reads = df_reads.merge(\n",
    "            nodups[[\"read_id\", \"read_count\"]],\n",
    "            on=\"read_id\",\n",
    "            how=\"left\"\n",
    "        )\n",
    "\n",
    "        # 4‑C. Read occupancy lines\n",
    "        _add_read_occupancy_multi(\n",
    "            fig, df_reads, plot_window,\n",
    "            row=read_rows[idx],\n",
    "            color=read_colors[idx],\n",
    "            read_width = 0.1\n",
    "        )\n",
    "\n",
    "        # 4‑D. m6A dots\n",
    "        long_df = _long_format(df_reads, plot_window)\n",
    "        hits = long_df[long_df[\"mod_qual_bin\"] == 1]\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=hits[\"rel_pos\"], y=hits[\"read_count\"],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(symbol=\"square\", size=2.5, color=m6a_dot_colors[idx]),\n",
    "            showlegend=False),\n",
    "            row=read_rows[idx], col=1\n",
    "        )\n",
    "\n",
    "        # 4‑E. Condition‑specific %m6A line (shared middle panel)\n",
    "        agg = (long_df.groupby(\"rel_pos\")[\"mod_qual_bin\"]\n",
    "                      .agg([\"sum\", \"count\"]).reset_index())\n",
    "        agg[\"ratio\"] = agg[\"sum\"] / agg[\"count\"]\n",
    "        if smoothing_type == \"none\":\n",
    "            y = agg[\"ratio\"]\n",
    "        elif smoothing_type == \"moving\":\n",
    "            y = (agg[\"sum\"].rolling(smoothing_window, center=True).sum() /\n",
    "                 agg[\"count\"].rolling(smoothing_window, center=True).sum())\n",
    "        else:   # gaussian\n",
    "            y = gaussian_filter1d(agg[\"ratio\"], sigma=gaussian_sigma)\n",
    "\n",
    "        mask = ~np.isnan(y)\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=agg[\"rel_pos\"][mask], y=y[mask],\n",
    "            mode=\"lines\",\n",
    "            line=dict(color=m6a_line_colors[idx], width=2),\n",
    "            name=cond),\n",
    "            row=m6a_row, col=1\n",
    "        )\n",
    "\n",
    "    # y‑axis formatting for %m6A\n",
    "    fig.update_yaxes(tickformat=\".0%\", row=m6a_row, col=1)\n",
    "\n",
    "    # ─── 5. Motif dashed lines + labels  (ONE authoritative numbering)        ──\n",
    "    if plot_motifs and motif_collect:\n",
    "        raw_motifs = pd.concat(motif_collect, ignore_index=True)\n",
    "        motif_df   = _build_motif_df(raw_motifs, plot_window, debug)\n",
    "        if not motif_df.empty:\n",
    "            ANNOT_Y = 1.02\n",
    "            for row in motif_df.itertuples():\n",
    "                pos, num = row.rel_start, row.num\n",
    "                fig.add_shape(type=\"line\", x0=pos, x1=pos, y0=0, y1=1,\n",
    "                              xref=\"x\", yref=\"paper\",\n",
    "                              line=dict(color=\"grey\", width=1, dash=\"dash\"))\n",
    "                fig.add_annotation(\n",
    "                    x=pos, y=ANNOT_Y, yref=\"paper\",\n",
    "                    text=str(num),\n",
    "                    showarrow=False,\n",
    "                    font=dict(size=10),\n",
    "                    bgcolor=\"rgba(255,255,255,0.5)\"\n",
    "                )\n",
    "                if debug:\n",
    "                    print(f\"[dual‑plot] motif#{num} at x={pos}\")\n",
    "\n",
    "    # ─── 7. Layout tweaks ────────────────────────────────────────────────\n",
    "    ttl = \" & \".join(conds) if len(conds) == 2 else conds[0]\n",
    "    fig.update_layout(\n",
    "        template=\"simple_white\",\n",
    "        height=700 if single else 700,\n",
    "        width=900,\n",
    "        title=f\"Condition(s): {ttl} | Type: {data_type}\",\n",
    "        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02,\n",
    "                    xanchor=\"right\", x=1)\n",
    "    )\n",
    "\n",
    "    # axis labels\n",
    "    for rid in read_rows:\n",
    "        fig.update_yaxes(title_text=\"Read ID\", row=rid, col=1)\n",
    "    fig.update_yaxes(title_text=\"% m6A\", row=m6a_row, col=1)\n",
    "    fig.update_xaxes(title_text=\"Genomic position (bp)\", row=rows, col=1)\n",
    "\n",
    "    return fig\n",
    "\n",
    "# ───────────────────────── Worker: save / stitch ──────────────────────── #\n",
    "def _save_dual_plot(args):\n",
    "    cond1, cond2, typ = args\n",
    "    chr_sel   = \"X\"\n",
    "    win       = temp_bed_w  # global constant\n",
    "\n",
    "    # 1) Build the dual‐plot figure\n",
    "    fig = create_dual_plot(\n",
    "        plot_df          = down_sampled_plot_df,\n",
    "        group_df         = down_sampled_group_df,\n",
    "        condition        = [cond1, cond2],\n",
    "        chr_type         = chr_sel,\n",
    "        data_type        = typ,\n",
    "        plot_window      = win,\n",
    "        plot_motifs      = True,\n",
    "        require_full_span= True,\n",
    "        debug            = debug_usage,\n",
    "        smoothing_type   = \"moving\",\n",
    "        smoothing_window = 50,\n",
    "        gaussian_sigma   = 10,\n",
    "        max_reads_per_condition = 300,\n",
    "        read_colors      = [\"#b12537\", \"#4974a5\"],\n",
    "        m6a_dot_colors   = [\"#b12537\", \"#4974a5\"],\n",
    "        m6a_line_colors  = [\"#b12537\", \"#4974a5\"],\n",
    "    )\n",
    "\n",
    "    # If fig is None, there were no reads → skip\n",
    "    if fig is None:\n",
    "        return None\n",
    "\n",
    "    # 2) Build read_subset for motif/gene tables\n",
    "    read_subset = down_sampled_plot_df.loc[\n",
    "        (down_sampled_plot_df[\"condition\"].isin([cond1, cond2]))\n",
    "        & (down_sampled_plot_df[\"chr_type\"] == chr_sel)\n",
    "        & (down_sampled_plot_df[\"type\"] == typ)\n",
    "    ].copy()\n",
    "\n",
    "    # 3) Create the motif‐table (if any motifs)\n",
    "    table_fig = create_motif_table(read_subset, win)\n",
    "\n",
    "    # 4) Create the gene‐table (closest upstream/downstream)\n",
    "    gene_table_fig = create_gene_table(read_subset)\n",
    "\n",
    "    # 5) Export main figure to PNG bytes\n",
    "    main_png = fig.to_image(format=\"png\", scale=2)\n",
    "\n",
    "    # 6) Stitch in this order: main_plot → motif_table → gene_table\n",
    "    if table_fig and gene_table_fig:\n",
    "        # (a) get motif PNG bytes\n",
    "        tbl_png      = table_fig.to_image(format=\"png\", scale=2)\n",
    "        # (b) first stitch main + motif\n",
    "        stitched_1   = stitch_vertical(main_png, tbl_png)\n",
    "\n",
    "        # (c) re‐encode stitched_1 as PNG bytes\n",
    "        buf = io.BytesIO()\n",
    "        stitched_1.save(buf, format=\"PNG\")\n",
    "        stitched_1_png = buf.getvalue()\n",
    "\n",
    "        # (d) get gene PNG bytes\n",
    "        gene_tbl_png = gene_table_fig.to_image(format=\"png\", scale=2)\n",
    "        # (e) stitch stitched_1 + gene_table\n",
    "        stitched     = stitch_vertical(stitched_1_png, gene_tbl_png)\n",
    "\n",
    "    elif table_fig:\n",
    "        # Only motif table present\n",
    "        tbl_png  = table_fig.to_image(format=\"png\", scale=2)\n",
    "        stitched = stitch_vertical(main_png, tbl_png)\n",
    "\n",
    "    elif gene_table_fig:\n",
    "        # Only gene table present (no motifs)\n",
    "        gene_tbl_png = gene_table_fig.to_image(format=\"png\", scale=2)\n",
    "        stitched     = stitch_vertical(main_png, gene_tbl_png)\n",
    "\n",
    "    else:\n",
    "        # Neither table present\n",
    "        stitched = PILImage.open(io.BytesIO(main_png)).convert(\"RGBA\")\n",
    "\n",
    "    # 7) Save stitched PNG + assemble SVG similarly\n",
    "    save_dir = os.path.join(\"images_pairwise\", f\"{cond1}_{cond2}_{win}bp\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    rank_val = chip_rank_lookup.get(typ, None)\n",
    "    if rank_val is None:\n",
    "        rank_pref = \"NA\"\n",
    "    else:\n",
    "        rank_pref = str(int(round(rank_val)))\n",
    "\n",
    "    base_fn   = f\"{rank_pref}_{typ}_b{win}_chr{chr_sel}\"\n",
    "    png_path  = os.path.join(save_dir, base_fn + \".png\")\n",
    "    svg_path  = os.path.join(save_dir, base_fn + \".svg\")\n",
    "\n",
    "    # Write PNG\n",
    "    stitched.save(png_path, dpi=(PNG_DPI, PNG_DPI))\n",
    "\n",
    "    # Build combined SVG:\n",
    "    main_svg = fig.to_image(format=\"svg\").decode()\n",
    "\n",
    "    if table_fig and gene_table_fig:\n",
    "        tbl_svg       = table_fig.to_image(format=\"svg\").decode()\n",
    "        gene_tbl_svg  = gene_table_fig.to_image(format=\"svg\").decode()\n",
    "\n",
    "        svg_head = '<svg xmlns=\"http://www.w3.org/2000/svg\">'\n",
    "        # Position motif table just below main\n",
    "        translate_tbl  = f'translate(0,{fig.layout.height + TABLE_MARGIN_PX})'\n",
    "        # Position gene table below motif table\n",
    "        translate_gene = f'translate(0,{fig.layout.height + TABLE_MARGIN_PX + table_fig.layout.height + TABLE_MARGIN_PX})'\n",
    "\n",
    "        combined_svg = (\n",
    "            svg_head\n",
    "            + f'<g>{main_svg}</g>'\n",
    "            + f'<g transform=\"{translate_tbl}\">{tbl_svg}</g>'\n",
    "            + f'<g transform=\"{translate_gene}\">{gene_tbl_svg}</g>'\n",
    "            + '</svg>'\n",
    "        )\n",
    "\n",
    "    elif table_fig:\n",
    "        tbl_svg    = table_fig.to_image(format=\"svg\").decode()\n",
    "        svg_head   = '<svg xmlns=\"http://www.w3.org/2000/svg\">'\n",
    "        translate_tbl = f'translate(0,{fig.layout.height + TABLE_MARGIN_PX})'\n",
    "        combined_svg  = (\n",
    "            svg_head\n",
    "            + f'<g>{main_svg}</g>'\n",
    "            + f'<g transform=\"{translate_tbl}\">{tbl_svg}</g>'\n",
    "            + '</svg>'\n",
    "        )\n",
    "\n",
    "    elif gene_table_fig:\n",
    "        gene_tbl_svg = gene_table_fig.to_image(format=\"svg\").decode()\n",
    "        svg_head     = '<svg xmlns=\"http://www.w3.org/2000/svg\">'\n",
    "        translate_gene = f'translate(0,{fig.layout.height + TABLE_MARGIN_PX})'\n",
    "        combined_svg   = (\n",
    "            svg_head\n",
    "            + f'<g>{main_svg}</g>'\n",
    "            + f'<g transform=\"{translate_gene}\">{gene_tbl_svg}</g>'\n",
    "            + '</svg>'\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        combined_svg = main_svg\n",
    "\n",
    "    with open(svg_path, \"w\") as fh:\n",
    "        fh.write(combined_svg)\n",
    "\n",
    "    if debug_usage:\n",
    "        print(f\"[Worker] saved → {png_path}\")\n",
    "    return png_path\n",
    "\n",
    "\n",
    "# ───────────────────────────── CLI / main ─────────────────────────────── #\n",
    "if __name__ == \"__main__\":\n",
    "    debug_usage = False\n",
    "    num_workers = 50\n",
    "    temp_bed_w  = 1000\n",
    "\n",
    "    # Build motif type list\n",
    "    motifs_type_selected = [f\"MOTIFS_{t}\" for t in type_selected]\n",
    "\n",
    "    # ── FULL run: every unordered pair × every motif type ───────────────\n",
    "    #  Un‑comment to run the whole batch\n",
    "    cond_pairs = list(combinations(analysis_cond, 2))\n",
    "    combos = [(c1, c2, typ) for (c1, c2) in cond_pairs for typ in motifs_type_selected]\n",
    "\n",
    "    # ── QUICK test: one pair + one type ─────────────────────────────────\n",
    "    combos = [[analysis_cond[0], analysis_cond[5], \"MOTIFS_rex48\"]]#\"MOTIFS_rex32\"]]\n",
    "\n",
    "    saved_pngs = run_in_pool(\n",
    "        func=_save_dual_plot,\n",
    "        iterable=combos,\n",
    "        n_workers=num_workers,\n",
    "        desc=\"Saving dual‑track plots\",\n",
    "        unit=\"plot\"\n",
    "    )\n",
    "\n",
    "    # show the single figure inline when only one combo\n",
    "    if len(saved_pngs) == 1:\n",
    "        display(Image(filename=saved_pngs[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afb99c361a631dc",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ─────────────────────────── Three‑track plotting ────────────────────────── #\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# default palettes – you can override via kwargs\n",
    "# red, blue, green, grey\n",
    "DEFAULT_READ_CLRS = [\"#b12537\", \"#4974a5\", \"#47B562\" ]   #\"#47B562\" green | #\"#b12537\" red | #808080 grey | #FF8559 coral\n",
    "\n",
    "# grey color #\n",
    "\n",
    "def create_triple_plot(\n",
    "    plot_df: pd.DataFrame,\n",
    "    condition,                             # str | list[str] (max 3)\n",
    "    chr_type: str,\n",
    "    data_type: str,\n",
    "    plot_window: int,\n",
    "    *,\n",
    "    plot_motifs: bool          = True,\n",
    "    require_full_span: bool    = False,\n",
    "    smoothing_type: str        = \"moving\",\n",
    "    smoothing_window: int      = 50,\n",
    "    gaussian_sigma: float      = 10,\n",
    "    debug: bool                = False,\n",
    "    subsample_reads: bool      = False,\n",
    "    max_reads_to_display: Optional[int] = 300, # New parameter with default\n",
    "    random_state: int           = None,\n",
    "    read_colors: list          = None,\n",
    "    m6a_dot_colors: list       = None,\n",
    "    m6a_line_colors: list      = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Up to three read tracks with interleaved combined %m6A panels.\n",
    "    With three conditions the row order is now (%m6A, %m6A, read, read, read):\n",
    "      1. condA+condB %m6A\n",
    "      2. condB+condC %m6A\n",
    "      3. condA       reads\n",
    "      4. condB       reads\n",
    "      5. condC       reads\n",
    "    \"\"\"\n",
    "    # ─── normalize inputs & palettes ─────────────────────────────────\n",
    "    conds = list(condition) if isinstance(condition, (list, tuple)) else [condition]\n",
    "    if not (1 <= len(conds) <= 3):\n",
    "        raise ValueError(\"condition must be 1–3 strings\")\n",
    "    n_cond = len(conds)\n",
    "\n",
    "    def _fill(col, default_list):\n",
    "        if col is None:\n",
    "            if len(default_list) < n_cond:\n",
    "                raise ValueError(f\"Need at least {n_cond} default colours, got {len(default_list)}\")\n",
    "            return default_list[:n_cond]\n",
    "        if len(col) != n_cond:\n",
    "            raise ValueError(f\"Need {n_cond} colours, got {len(col)}\")\n",
    "        return col\n",
    "\n",
    "    read_colors     = _fill(read_colors,     DEFAULT_READ_CLRS)\n",
    "    m6a_dot_colors  = _fill(m6a_dot_colors,  DEFAULT_READ_CLRS)\n",
    "    m6a_line_colors = _fill(m6a_line_colors, DEFAULT_READ_CLRS)\n",
    "\n",
    "    rng = np.random.default_rng(random_state)\n",
    "\n",
    "    # ─── figure scaffold ────────────────────────────────────────────────\n",
    "    if n_cond == 1:\n",
    "        specs, row_h = [[{}], [{}]], [0.55, 0.45]\n",
    "        read_rows        = [1]\n",
    "        combined_mapping = [(2, [0])]\n",
    "    elif n_cond == 2:\n",
    "        specs, row_h = [[{}], [{}], [{}]], [0.4, 0.4, 0.2]\n",
    "        read_rows        = [1, 2]\n",
    "        combined_mapping = [(3, [0, 1])]\n",
    "    else:\n",
    "        specs  = [[{}], [{}], [{}], [{}], [{}]]\n",
    "        row_h  = [0.35, 0.35, 0.10, 0.10, 0.10]\n",
    "        combined_mapping = [(1, [0, 1]), (2, [1, 2])]\n",
    "        read_rows = [3, 4, 5]\n",
    "\n",
    "    fig = make_subplots(\n",
    "        rows=len(specs), cols=1,\n",
    "        shared_xaxes=True,\n",
    "        vertical_spacing=0.02,\n",
    "        specs=specs, row_heights=row_h\n",
    "    )\n",
    "    # Set x‑axis range and ticks every 300 bp centered on 0\n",
    "    fig.update_xaxes(\n",
    "        range=[-plot_window, plot_window],\n",
    "        tickmode=\"linear\",\n",
    "        tick0=0,\n",
    "        dtick=300\n",
    "    )\n",
    "\n",
    "    # ─── gather / optional subsample ───────────────────────────────────\n",
    "    cond_dfs  = []\n",
    "    read_lens = []\n",
    "    for cond in conds:\n",
    "        df = _filter_reads(plot_df, cond, chr_type, data_type,\n",
    "                           plot_window, require_full_span, debug)\n",
    "        cond_dfs.append(df)\n",
    "        read_lens.append(df[\"read_id\"].nunique())\n",
    "\n",
    "    # ─── gather / optional subsample ───────────────────────────────────\n",
    "    cond_dfs  = []\n",
    "    read_lens = []\n",
    "    for cond in conds:\n",
    "        df = _filter_reads(plot_df, cond, chr_type, data_type,\n",
    "                           plot_window, require_full_span, debug)\n",
    "        cond_dfs.append(df)\n",
    "        read_lens.append(df[\"read_id\"].nunique())\n",
    "\n",
    "    # Determine the number of reads to display for each condition\n",
    "    # It should be the minimum of:\n",
    "    # 1. The maximum number of reads you want to display (e.g., 300)\n",
    "    # 2. The minimum number of reads available across all conditions (if subsample_reads is True)\n",
    "    # ─── gather / optional subsample ───────────────────────────────────\n",
    "    cond_dfs  = []\n",
    "    read_lens = []\n",
    "    for cond in conds:\n",
    "        df = _filter_reads(plot_df, cond, chr_type, data_type,\n",
    "                           plot_window, require_full_span, debug)\n",
    "        cond_dfs.append(df)\n",
    "        read_lens.append(df[\"read_id\"].nunique())\n",
    "\n",
    "    # Determine the number of reads to display for each condition\n",
    "    # It should be the minimum of:\n",
    "    # 1. The maximum number of reads you want to display (e.g., 300)\n",
    "    # 2. The minimum number of reads available across all conditions (if subsample_reads is True)\n",
    "    if subsample_reads and read_lens:\n",
    "        actual_min_reads = min(read_lens)\n",
    "        target_reads_per_condition = min(max_reads_to_display, actual_min_reads) if max_reads_to_display is not None else actual_min_reads\n",
    "        if debug:\n",
    "            print(f\"[triple‑plot] Target reads per condition: {target_reads_per_condition} (min across conditions: {actual_min_reads})\")\n",
    "    else:\n",
    "        # If no subsampling, or no reads, handle accordingly\n",
    "        target_reads_per_condition = None # This means no explicit subsampling based on a max value\n",
    "\n",
    "    motif_collect = []\n",
    "    for idx, (cond, df_reads) in enumerate(zip(conds, cond_dfs)):\n",
    "        if df_reads.empty:\n",
    "            continue\n",
    "\n",
    "        # Apply subsampling for each condition if target_reads_per_condition is set\n",
    "        if target_reads_per_condition is not None and df_reads[\"read_id\"].nunique() > target_reads_per_condition:\n",
    "            unique_read_ids = df_reads[\"read_id\"].drop_duplicates()\n",
    "            # Ensure rng is initialized (it's initialized at the top of create_triple_plot)\n",
    "            ids_to_keep = rng.choice(unique_read_ids, size=target_reads_per_condition, replace=False)\n",
    "            df_reads = df_reads[df_reads[\"read_id\"].isin(ids_to_keep)].copy()\n",
    "            if debug:\n",
    "                print(f\"[triple-plot] Condition '{cond}' subsampled to {len(ids_to_keep)} reads.\")\n",
    "\n",
    "        motif_collect.append(df_reads[[\"motif_rel_start\", \"motif_attributes\"]])\n",
    "\n",
    "        rc_map = (pd.DataFrame({\"read_id\": df_reads[\"read_id\"].unique()})\n",
    "                  .assign(read_count=lambda d: np.arange(1, len(d) + 1)))\n",
    "        df_reads = df_reads.merge(rc_map, on=\"read_id\")\n",
    "\n",
    "        _add_read_occupancy_multi(\n",
    "            fig, df_reads, plot_window,\n",
    "            row=read_rows[idx],\n",
    "            color=read_colors[idx],\n",
    "            read_width=0.025 #0.1 for large format display | 0.025 for compressed figure\n",
    "            # max_reads_to_display is no longer passed here as subsampling is done earlier\n",
    "        )\n",
    "        long_df = _long_format(df_reads, plot_window)\n",
    "        hits    = long_df[long_df[\"mod_qual_bin\"] == 1]\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=hits[\"rel_pos\"], y=hits[\"read_count\"],\n",
    "                mode=\"markers\",\n",
    "                marker=dict(symbol=\"square\", size=1.5, #2.5 for large format display | 1.5 for compressed figure\n",
    "                            color=m6a_dot_colors[idx]),\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=read_rows[idx], col=1\n",
    "        )\n",
    "        fig.update_yaxes(title_text=\"Read ID\",\n",
    "                         row=read_rows[idx], col=1)\n",
    "\n",
    "    # ─── plot combined %m6A panels & track global max ───────────────────\n",
    "    combined_max = 0.0\n",
    "    for comb_row, cond_idxs in combined_mapping:\n",
    "        for ci in cond_idxs:\n",
    "            df_meta = plot_df.loc[\n",
    "                (plot_df[\"condition\"] == conds[ci]) &\n",
    "                (plot_df[\"chr_type\"]   == chr_type) &\n",
    "                (plot_df[\"type\"]       == data_type)\n",
    "            ].copy()\n",
    "\n",
    "            if df_meta.empty:\n",
    "                continue\n",
    "            recs = [\n",
    "                (p, b)\n",
    "                for r in df_meta.itertuples()\n",
    "                for p, b in zip(r.rel_pos, r.mod_qual_bin)\n",
    "                if -plot_window <= p <= plot_window\n",
    "            ]\n",
    "            agg = (pd.DataFrame(recs, columns=[\"pos\", \"bin\"])\n",
    "                   .groupby(\"pos\")[\"bin\"]\n",
    "                   .agg([\"sum\", \"count\"])\n",
    "                   .reset_index())\n",
    "            agg[\"ratio\"] = agg[\"sum\"] / agg[\"count\"]\n",
    "\n",
    "            if smoothing_type == \"none\":\n",
    "                y = agg[\"ratio\"]\n",
    "            elif smoothing_type == \"moving\":\n",
    "                y = (agg[\"sum\"]\n",
    "                     .rolling(smoothing_window, center=True).sum() /\n",
    "                     agg[\"count\"]\n",
    "                     .rolling(smoothing_window, center=True).sum())\n",
    "            else:\n",
    "                y = gaussian_filter1d(agg[\"ratio\"], sigma=gaussian_sigma)\n",
    "\n",
    "            if len(y):\n",
    "                combined_max = max(combined_max, float(y.max()))\n",
    "\n",
    "            mask = ~np.isnan(y)\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=agg[\"pos\"][mask], y=y[mask],\n",
    "                    mode=\"lines\",\n",
    "                    line=dict(color=m6a_line_colors[ci], width=2),\n",
    "                    name=str(conds[ci])\n",
    "                ),\n",
    "                row=comb_row, col=1\n",
    "            )\n",
    "        fig.update_yaxes(title_text=\"% m6A\", row=comb_row, col=1)\n",
    "\n",
    "    # ─── equalize + style the two % m6A axes ─────────────────────────────\n",
    "    if len(combined_mapping) > 1:\n",
    "        # round up to the next 2%\n",
    "        max_pct = math.ceil((combined_max * 100) / 2) * 2\n",
    "        ymax    = max_pct / 100\n",
    "\n",
    "        for comb_row, _ in combined_mapping:\n",
    "            fig.update_xaxes(\n",
    "                range=[-plot_window, plot_window],\n",
    "                tickmode=\"linear\",\n",
    "                tick0=0,\n",
    "                dtick=300,\n",
    "                showgrid=False,\n",
    "                showline=True,\n",
    "                linecolor=\"darkgrey\",\n",
    "                linewidth=1.5,\n",
    "                ticks=\"outside\",\n",
    "                row=comb_row, col=1\n",
    "            )\n",
    "            fig.update_yaxes(\n",
    "                range=[0, ymax],\n",
    "                tickformat=\".0%\",\n",
    "                showgrid=False,\n",
    "                showline=True,\n",
    "                linecolor=\"darkgrey\",\n",
    "                linewidth=1.5,\n",
    "                ticks=\"outside\",\n",
    "                row=comb_row, col=1\n",
    "            )\n",
    "\n",
    "\n",
    "    # ─── 5 . motif dashed lines + labels (table‑synced) ──────────────────\n",
    "    if plot_motifs and motif_collect:\n",
    "        # collapse all conditions’ motif columns → build one authoritative df\n",
    "        raw_motifs = pd.concat(motif_collect, ignore_index=True)\n",
    "        motif_df   = _build_motif_df(raw_motifs, plot_window, debug)\n",
    "\n",
    "        if not motif_df.empty:\n",
    "            ANNOT_Y = 1.02\n",
    "            if debug: print(f\"[triple‑plot] annotating {len(motif_df)} motifs\")\n",
    "\n",
    "            for row in motif_df.itertuples():\n",
    "                pos, num = row.rel_start, row.num\n",
    "\n",
    "                # vertical dashed guide\n",
    "                fig.add_shape(\n",
    "                    type=\"line\", x0=pos, x1=pos, y0=0, y1=1,\n",
    "                    xref=\"x\", yref=\"paper\",\n",
    "                    line=dict(color=\"grey\", width=1, dash=\"dash\")\n",
    "                )\n",
    "\n",
    "                # numeric label (matches the table row number)\n",
    "                fig.add_annotation(\n",
    "                    x=pos, y=ANNOT_Y, yref=\"paper\",\n",
    "                    text=str(num),\n",
    "                    showarrow=False,\n",
    "                    font=dict(size=10),\n",
    "                    bgcolor=\"rgba(255,255,255,0.5)\"\n",
    "                )\n",
    "                if debug:\n",
    "                    print(f\"[triple‑plot] motif#{num} at x={pos}\")\n",
    "    else:\n",
    "        if debug:\n",
    "            print(\"[triple‑plot] no motifs found – skipping annotation\")\n",
    "\n",
    "\n",
    "    # ─── final layout ────────────────────────────────────────────────────\n",
    "    fig.update_layout(\n",
    "        template=\"plotly_white\",\n",
    "        width=FIG_WIDTH,\n",
    "        height=FIG_HEIGHT,\n",
    "        title=f\"Conditions: {', '.join(conds)} | Type: {data_type}\",\n",
    "        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02,\n",
    "                    xanchor=\"right\", x=1)\n",
    "    )\n",
    "    # Ensure bottom x‑axis also has ticks every 300 bp\n",
    "    fig.update_xaxes(\n",
    "        tickmode=\"linear\",\n",
    "        tick0=0,\n",
    "        dtick=300,\n",
    "        row=len(specs), col=1\n",
    "    )\n",
    "    fig.update_xaxes(title_text=\"Genomic position (bp)\",\n",
    "                     row=len(specs), col=1)\n",
    "\n",
    "    for r in read_rows:\n",
    "        fig.update_yaxes(showgrid=False,zeroline=False, row=r, col=1)\n",
    "        fig.update_xaxes(showgrid=False,showline=False, zeroline=False, row=r, col=1)\n",
    "\n",
    "    return fig\n",
    "\n",
    "# ────────────────────────── Worker: save / stitch ───────────────────────── #\n",
    "def _save_triple_plot(args):\n",
    "    conds, motif_type = args\n",
    "    chr_sel   = \"Autosome\"\n",
    "    win       = temp_bed_w  # global constant\n",
    "\n",
    "    # 1) Mask to see if any reads exist for these three conditions + motif_type\n",
    "    mask = (\n",
    "        down_sampled_plot_df[\"condition\"].isin(conds)\n",
    "        & (down_sampled_plot_df[\"chr_type\"] == chr_sel)\n",
    "        & (down_sampled_plot_df[\"type\"] == motif_type)\n",
    "    )\n",
    "    if not mask.any():\n",
    "        if debug_usage:\n",
    "            print(f\"[Worker] skip {motif_type} – no reads for {conds}\")\n",
    "        return None\n",
    "\n",
    "    # 2) Build the triple‐plot figure\n",
    "    fig = create_triple_plot(\n",
    "        plot_df           = down_sampled_plot_df,\n",
    "        condition         = list(conds),\n",
    "        chr_type          = chr_sel,\n",
    "        data_type         = motif_type,\n",
    "        plot_window       = win,\n",
    "        plot_motifs       = False,\n",
    "        require_full_span = True,\n",
    "        smoothing_type    = \"moving\",\n",
    "        smoothing_window  = 50,\n",
    "        gaussian_sigma    = 10,\n",
    "        debug             = debug_usage,\n",
    "        subsample_reads   = True,\n",
    "        max_reads_to_display=300, # Pass your desired max reads here\n",
    "        random_state      = 42,\n",
    "        read_colors       = DEFAULT_READ_CLRS,\n",
    "        m6a_dot_colors    = DEFAULT_READ_CLRS,\n",
    "        m6a_line_colors   = DEFAULT_READ_CLRS,\n",
    "    )\n",
    "\n",
    "    # 3) Build read_subset for tables\n",
    "    read_subset = down_sampled_plot_df.loc[mask].copy()\n",
    "\n",
    "    if debug_usage:\n",
    "        print(f\"[Worker] rows in read_subset: {len(read_subset)}\")\n",
    "        # How many reads even have a non‐null motif_rel_start?\n",
    "        nonnull_motif = read_subset[\"motif_rel_start\"].notna().sum()\n",
    "        print(f\"[Worker] of those, {nonnull_motif} rows have a non‐null motif_rel_start\")\n",
    "        # Show one example of the raw motif lists:\n",
    "        example = read_subset[\"motif_rel_start\"].iloc[0]\n",
    "        print(f\"[Worker] example motif_rel_start (first row): {example}\")\n",
    "\n",
    "    # 4) Create motif‐table (if any)\n",
    "    table_fig       = create_motif_table(read_subset, win)\n",
    "\n",
    "    # 5) Create gene‐table\n",
    "    gene_table_fig  = create_gene_table(read_subset)\n",
    "\n",
    "    # 6) Export main figure to PNG bytes; no forced height/width\n",
    "    main_png = fig.to_image(format=\"png\", scale=1)\n",
    "\n",
    "    # 7) Render motif and gene tables at their “natural” size\n",
    "    if table_fig and gene_table_fig:\n",
    "        tbl_png      = table_fig.to_image(format=\"png\", scale=1)\n",
    "        stitched_1   = stitch_vertical(main_png, tbl_png)\n",
    "\n",
    "        buf = io.BytesIO()\n",
    "        stitched_1.save(buf, format=\"PNG\")\n",
    "        stitched_1_png = buf.getvalue()\n",
    "\n",
    "        gene_tbl_png = gene_table_fig.to_image(format=\"png\", scale=1)\n",
    "        stitched = stitch_vertical(stitched_1_png, gene_tbl_png)\n",
    "\n",
    "    elif table_fig:\n",
    "        tbl_png  = table_fig.to_image(format=\"png\", scale=1)\n",
    "        stitched = stitch_vertical(main_png, tbl_png)\n",
    "\n",
    "    elif gene_table_fig:\n",
    "        gene_tbl_png = gene_table_fig.to_image(format=\"png\", scale=1)\n",
    "        stitched     = stitch_vertical(main_png, gene_tbl_png)\n",
    "\n",
    "    else:\n",
    "        stitched = PILImage.open(io.BytesIO(main_png)).convert(\"RGBA\")\n",
    "\n",
    "    # 8) Save stitched PNG and write out SVG… (same as before)\n",
    "    save_dir = os.path.join(\"images_triple\", f\"{'_'.join(conds)}_{win}bp\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    rank_val = chip_rank_lookup.get(motif_type, None)\n",
    "    if rank_val is None:\n",
    "        rank_pref = \"NA\"\n",
    "    else:\n",
    "        rank_pref = str(int(round(rank_val)))\n",
    "\n",
    "    base      = f\"{rank_pref}_{motif_type}_b{win}_chr{chr_sel}\"\n",
    "    png_path  = os.path.join(save_dir, base + \".png\")\n",
    "    svg_path = os.path.join(save_dir, base + \".svg\")\n",
    "\n",
    "    stitched.save(png_path, dpi=(PNG_DPI, PNG_DPI))\n",
    "\n",
    "    # Build combined SVG (also remove hard‑coded translations for table height)\n",
    "    main_svg = fig.to_image(format=\"svg\", width=FIG_WIDTH, height=FIG_HEIGHT).decode()\n",
    "\n",
    "    if table_fig and gene_table_fig:\n",
    "        tbl_svg      = table_fig.to_image(format=\"svg\").decode()\n",
    "        gene_tbl_svg = gene_table_fig.to_image(format=\"svg\").decode()\n",
    "\n",
    "        svg_head      = '<svg xmlns=\"http://www.w3.org/2000/svg\">'\n",
    "        # place motif table right under the main figure\n",
    "        translate_tbl = f'translate(0,{fig.layout.height + TABLE_MARGIN_PX})'\n",
    "        # place gene table directly under motif table; use motif’s own height\n",
    "        translate_gene = f'translate(0,{fig.layout.height + TABLE_MARGIN_PX + table_fig.layout.height + TABLE_MARGIN_PX})'\n",
    "\n",
    "        combined_svg = (\n",
    "            svg_head\n",
    "            + f'<g>{main_svg}</g>'\n",
    "            + f'<g transform=\"{translate_tbl}\">{tbl_svg}</g>'\n",
    "            + f'<g transform=\"{translate_gene}\">{gene_tbl_svg}</g>'\n",
    "            + '</svg>'\n",
    "        )\n",
    "\n",
    "    elif table_fig:\n",
    "        tbl_svg      = table_fig.to_image(format=\"svg\").decode()\n",
    "        svg_head      = '<svg xmlns=\"http://www.w3.org/2000/svg\">'\n",
    "        translate_tbl = f'translate(0,{fig.layout.height + TABLE_MARGIN_PX})'\n",
    "\n",
    "        combined_svg = (\n",
    "            svg_head\n",
    "            + f'<g>{main_svg}</g>'\n",
    "            + f'<g transform=\"{translate_tbl}\">{tbl_svg}</g>'\n",
    "            + '</svg>'\n",
    "        )\n",
    "\n",
    "    elif gene_table_fig:\n",
    "        gene_tbl_svg  = gene_table_fig.to_image(format=\"svg\").decode()\n",
    "        svg_head      = '<svg xmlns=\"http://www.w3.org/2000/svg\">'\n",
    "        translate_gene = f'translate(0,{fig.layout.height + TABLE_MARGIN_PX})'\n",
    "\n",
    "        combined_svg = (\n",
    "            svg_head\n",
    "            + f'<g>{main_svg}</g>'\n",
    "            + f'<g transform=\"{translate_gene}\">{gene_tbl_svg}</g>'\n",
    "            + '</svg>'\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        combined_svg = main_svg\n",
    "\n",
    "    with open(svg_path, \"w\") as fh:\n",
    "        fh.write(combined_svg)\n",
    "\n",
    "    if debug_usage:\n",
    "        print(f\"[Worker] saved → {png_path}\")\n",
    "    return png_path\n",
    "\n",
    "\n",
    "\n",
    "# ─────────────────────────── CLI / main block ─────────────────────────── #\n",
    "if __name__ == \"__main__\":\n",
    "    # ────────────── CONFIGURATION ──────────────\n",
    "    FIG_WIDTH     = 800    # width in pixels for exported PNG/SVG and layout\n",
    "    FIG_HEIGHT    = 800     # height in pixels for exported PNG/SVG and layout\n",
    "    DISPLAY_WIDTH = 1600     # width in pixels when displaying in‑notebook\n",
    "\n",
    "    debug_usage = False\n",
    "    num_workers = 50\n",
    "    temp_bed_w  = 1000\n",
    "\n",
    "    # three conditions in desired display order\n",
    "    TRIPLE_COND = (analysis_cond[1], analysis_cond[0], analysis_cond[2])\n",
    "\n",
    "    # prepend “MOTIFS_”\n",
    "    motif_types = [f\"MOTIFS_{t}\" for t in type_selected]\n",
    "\n",
    "    # ─── FULL RUN – every motif type for the fixed 3‑condition set ───────\n",
    "    # Uncomment to generate the whole batch\n",
    "    combos = [(TRIPLE_COND, mt) for mt in motif_types]\n",
    "\n",
    "    # ─── QUICK TEST – one type, same 3 conditions ────────────────────────\n",
    "    combos = [(TRIPLE_COND, \"MOTIFS_rex48\")]\n",
    "\n",
    "    saved_pngs = [\n",
    "        p for p in run_in_pool(\n",
    "            func=_save_triple_plot,\n",
    "            iterable=combos,\n",
    "            n_workers=num_workers,\n",
    "            desc=\"Saving three‑track plots\",\n",
    "            unit=\"plot\"\n",
    "        )\n",
    "        if p is not None          # keep only the paths that were actually saved\n",
    "    ]\n",
    "\n",
    "\n",
    "    if len(saved_pngs) == 1:\n",
    "        display(Image(filename=saved_pngs[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93869d8ac590731",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ╔══════════════════════════════════════════════════════════════════════════╗\n",
    "# ║  Unified read-track plotting (1–4 conditions) × motif types              ║\n",
    "# ║                                                                          ║\n",
    "# ║  Usage in notebook (define these in a prior cell):                       ║\n",
    "# ║    CONDITIONS  = (analysis_cond[1], analysis_cond[0], analysis_cond[2])  ║\n",
    "# ║    MOTIF_TYPES = [f\"MOTIFS_{t}\" for t in type_selected]                  ║\n",
    "# ║    run_batch_save(CONDITIONS, MOTIF_TYPES)                               ║\n",
    "# ║                                                                          ║\n",
    "# ║  Assumes your existing variables/dataframes exist:                       ║\n",
    "# ║    down_sampled_plot_df, down_sampled_group_df, temp_bed_w, etc.         ║\n",
    "# ╚══════════════════════════════════════════════════════════════════════════╝\n",
    "\n",
    "# ───────────────────────────── Imports ────────────────────────────── #\n",
    "import os, io, math, warnings\n",
    "from itertools import product\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from typing import List, Sequence, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "from PIL import Image as PILImage\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "# optional (used by some sorters)\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from sklearn.decomposition import PCA\n",
    "from fastdtw import fastdtw\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "# notebook helpers\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# your local lib is assumed to exist already\n",
    "import importlib, nanotools\n",
    "importlib.reload(nanotools)\n",
    "\n",
    "# ──────────────────────────── Plot template ───────────────────────── #\n",
    "pio.templates.default = \"plotly_white\"  # per user preference\n",
    "\n",
    "# ──────────────────────────── Global config ───────────────────────── #\n",
    "# Debug toggles (as requested: progress + data summaries)\n",
    "DEBUG_PROGRESS = False     # terse progress prints from workers\n",
    "DEBUG_SUMMARY  = False     # one-shot data summaries\n",
    "\n",
    "# Sorting config (same semantics as earlier cells)\n",
    "SORT_METHOD       = 0      # 1=binned-HC, 3=PCA, 4=DTW, else=autocorr\n",
    "BIN_COUNT         = 200    # for 1,3,4\n",
    "HC_LINKAGE        = \"ward\"\n",
    "PCA_COMPONENTS    = 1\n",
    "DTW_DISTANCE      = \"euclidean\"\n",
    "AC_SORT_LAG_MIN   = 155\n",
    "AC_SORT_LAG_MAX   = 195\n",
    "\n",
    "# Figure + export\n",
    "FIG_WIDTH   = 900\n",
    "FIG_HEIGHT  = 700\n",
    "PNG_DPI     = 300\n",
    "TABLE_CELL_HEIGHT = 22\n",
    "TABLE_HEADER_H    = 28\n",
    "TABLE_MARGIN_PX   = 12\n",
    "\n",
    "# Colors (up to 4 conditions)\n",
    "NUC_LINE_CLR    = \"#35b779\"\n",
    "\n",
    "# ───────────────────── Reference tables (load once) ────────────────── #\n",
    "# Adjust paths if needed\n",
    "tss_tes_df = pd.read_csv(\n",
    "    \"/Data1/reference/tss_tes_rex_combined_v27_WS235.bed\",\n",
    "    sep=r\"\\s+\",\n",
    "    usecols=[\"chromosome\", \"start\", \"end\", \"strand\", \"type\"]\n",
    ")\n",
    "_chiprank_df = pd.read_csv(\n",
    "    \"/Data1/reference/rex_chiprank.bed\",\n",
    "    sep=r\"\\s+\",\n",
    "    usecols=[\"type\", \"chip_rank\"]\n",
    ")\n",
    "_chiprank_df[\"type\"] = \"MOTIFS_\" + _chiprank_df[\"type\"].astype(str)\n",
    "chip_rank_lookup = {t: round(float(rk) * 100, 0)\n",
    "                    for t, rk in zip(_chiprank_df[\"type\"], _chiprank_df[\"chip_rank\"])}\n",
    "\n",
    "# ─────────────────────────── Utilities ─────────────────────────────── #\n",
    "def run_in_pool(func, iterable: Sequence, n_workers: int, desc: str, unit: str = \"item\"):\n",
    "    \"\"\"tqdm-wrapped Pool.imap_unordered.\"\"\"\n",
    "    from tqdm.auto import tqdm\n",
    "    with Pool(processes=n_workers) as pool:\n",
    "        return list(tqdm(pool.imap_unordered(func, iterable),\n",
    "                         total=len(iterable), desc=desc, unit=unit))\n",
    "\n",
    "def stitch_vertical(png_top: bytes, png_bottom: bytes) -> PILImage.Image:\n",
    "    \"\"\"Vertically stack two PNG byte streams with a small margin.\"\"\"\n",
    "    top = PILImage.open(io.BytesIO(png_top)).convert(\"RGBA\")\n",
    "    bot = PILImage.open(io.BytesIO(png_bottom)).convert(\"RGBA\")\n",
    "    w = max(top.width, bot.width)\n",
    "    h = top.height + TABLE_MARGIN_PX + bot.height\n",
    "    canvas = PILImage.new(\"RGBA\", (w, h), \"white\")\n",
    "    canvas.paste(top, (0, 0))\n",
    "    canvas.paste(bot, (0, top.height + TABLE_MARGIN_PX))\n",
    "    return canvas\n",
    "\n",
    "# ─────────────────────── Filtering / explode ───────────────────────── #\n",
    "def _filter_reads(df: pd.DataFrame, condition: str, chr_type: str, data_type: str,\n",
    "                  plot_window: int, require_full_span: bool,\n",
    "                  strand_filter: Optional[Sequence[str]] = None,\n",
    "                  debug: bool=False) -> pd.DataFrame:\n",
    "    reads = df.loc[(df[\"condition\"] == condition)\n",
    "                   & (df[\"chr_type\"] == chr_type)\n",
    "                   & (df[\"type\"] == data_type)].copy()\n",
    "    if strand_filter is not None and \"bed_strand\" in reads.columns:\n",
    "        reads = reads.loc[reads[\"bed_strand\"].isin(list(strand_filter))]\n",
    "    if require_full_span:\n",
    "        span = ((reads[\"rel_read_start\"] <= -round(plot_window * 0.3, 0)) &\n",
    "                (reads[\"rel_read_end\"]   >=  round(plot_window * 0.3, 0)))\n",
    "        reads = reads.loc[span]\n",
    "    reads = reads.loc[reads[\"mod_qual_bin\"].apply(lambda lst: 1 in lst)]\n",
    "    if debug and DEBUG_SUMMARY:\n",
    "        print(f\"[filter] {condition}/{data_type} → {len(reads)} rows\"\n",
    "              + (f\" | strand∈{list(strand_filter)}\" if strand_filter else \"\"))\n",
    "    return reads\n",
    "\n",
    "def _long_format(read_df: pd.DataFrame, plot_window: int) -> pd.DataFrame:\n",
    "    recs: List[Tuple[str,int,int,int]] = []\n",
    "    for r in read_df.itertuples():\n",
    "        idx = [i for i, p in enumerate(r.rel_pos) if -plot_window <= p <= plot_window]\n",
    "        for i in idx:\n",
    "            recs.append((r.read_id, r.rel_pos[i], r.mod_qual_bin[i], r.read_count))\n",
    "    return pd.DataFrame(recs, columns=[\"read_id\", \"rel_pos\", \"mod_qual_bin\", \"read_count\"])\n",
    "\n",
    "def _add_read_occupancy(fig, read_df, plot_window, row, color, read_width):\n",
    "    line_x, line_y = [], []\n",
    "    for r in read_df.itertuples():\n",
    "        mn = max(-plot_window, r.rel_read_start)\n",
    "        mx = min( plot_window, r.rel_read_end)\n",
    "        line_x += [mn, mx, None]\n",
    "        line_y += [r.read_count] * 3\n",
    "    fig.add_trace(go.Scatter(x=line_x, y=line_y, mode=\"lines\",\n",
    "                             line=dict(color=color, width=read_width),\n",
    "                             showlegend=False), row=row, col=1)\n",
    "\n",
    "# ─────────────────────────── Sorting helpers ───────────────────────── #\n",
    "def _build_binned_matrix(read_df, plot_window, n_bins=BIN_COUNT):\n",
    "    edges = np.linspace(-plot_window, plot_window, n_bins+1)\n",
    "    M, ids = [], []\n",
    "    for r in read_df.itertuples():\n",
    "        pos = np.asarray(r.rel_pos)[np.asarray(r.mod_qual_bin) == 1]\n",
    "        counts, _ = np.histogram(pos, bins=edges)\n",
    "        M.append(counts / np.diff(edges))\n",
    "        ids.append(r.read_id)\n",
    "    return np.vstack(M), ids\n",
    "\n",
    "def _make_nodups(read_df, ordered_ids):\n",
    "    tmp = (read_df[[\"read_id\",\n",
    "                    \"smallest_positive_nuc_midpoint\",\n",
    "                    \"greatest_negative_nuc_midpoint\"]]\n",
    "           .drop_duplicates(\"read_id\")\n",
    "           .set_index(\"read_id\"))\n",
    "    tmp = tmp.loc[[i for i in ordered_ids if i in tmp.index]].reset_index()\n",
    "    tmp[\"read_count\"] = np.arange(1, len(tmp) + 1)\n",
    "    return tmp\n",
    "\n",
    "def _order_by_binned(read_df, plot_window, debug=False):\n",
    "    M, ids = _build_binned_matrix(read_df, plot_window)\n",
    "    Z = sch.linkage(M, method=HC_LINKAGE, metric=\"euclidean\")\n",
    "    leaves = sch.dendrogram(Z, no_plot=True)[\"leaves\"]\n",
    "    return _make_nodups(read_df, [ids[i] for i in leaves])\n",
    "\n",
    "def _order_by_pca(read_df, plot_window, debug=False):\n",
    "    M, ids = _build_binned_matrix(read_df, plot_window)\n",
    "    pc1 = PCA(n_components=PCA_COMPONENTS).fit_transform(M)[:, 0]\n",
    "    order = np.argsort(pc1)\n",
    "    return _make_nodups(read_df, [ids[i] for i in order])\n",
    "\n",
    "def _order_by_dtw(read_df, plot_window, debug=False):\n",
    "    M, ids = _build_binned_matrix(read_df, plot_window)\n",
    "    n = M.shape[0]\n",
    "    D = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(i):\n",
    "            dist, _ = fastdtw(M[i], M[j])\n",
    "            D[i, j] = D[j, i] = dist\n",
    "    Z = sch.linkage(squareform(D), method=HC_LINKAGE)\n",
    "    leaves = sch.dendrogram(Z, no_plot=True)[\"leaves\"]\n",
    "    return _make_nodups(read_df, [ids[i] for i in leaves])\n",
    "\n",
    "def _order_reads_by_ac(read_df: pd.DataFrame, plot_window: int,\n",
    "                       lag_lo: int = AC_SORT_LAG_MIN, lag_hi: int = AC_SORT_LAG_MAX,\n",
    "                       debug: bool=False) -> pd.DataFrame:\n",
    "    lag_lo, lag_hi = sorted((lag_lo, lag_hi))\n",
    "    lag_hi = min(lag_hi, 2 * plot_window)\n",
    "    means = {}\n",
    "    vec_len = 2 * plot_window + 1\n",
    "    for r in read_df.itertuples():\n",
    "        rel_pos = np.asarray(r.rel_pos, dtype=int)\n",
    "        mod_bin = np.asarray(r.mod_qual_bin, dtype=float)\n",
    "        vec = np.full(vec_len, np.nan)\n",
    "        idx = rel_pos + plot_window\n",
    "        keep = (idx >= 0) & (idx < vec_len)\n",
    "        vec[idx[keep]] = mod_bin[keep]\n",
    "        ac = nanotools._autocorr_vec(vec, lag_hi)\n",
    "        win = ac[lag_lo:lag_hi+1]\n",
    "        means[r.read_id] = np.nanmax(win)\n",
    "    nodups = (read_df[[\"read_id\",\n",
    "                       \"smallest_positive_nuc_midpoint\",\n",
    "                       \"greatest_negative_nuc_midpoint\"]]\n",
    "              .drop_duplicates(\"read_id\"))\n",
    "    nodups[\"ac_mean\"] = nodups[\"read_id\"].map(means).fillna(-np.inf)\n",
    "    nodups = nodups.sort_values(\"ac_mean\", ascending=False).drop(columns=\"ac_mean\")\n",
    "    nodups[\"read_count\"] = np.arange(1, len(nodups) + 1)\n",
    "    return nodups\n",
    "\n",
    "def _order_read_df(df_reads, plot_window):\n",
    "    if SORT_METHOD == 1:\n",
    "        nodups = _order_by_binned(df_reads, plot_window)\n",
    "    elif SORT_METHOD == 3:\n",
    "        nodups = _order_by_pca(df_reads, plot_window)\n",
    "    elif SORT_METHOD == 4:\n",
    "        nodups = _order_by_dtw(df_reads, plot_window)\n",
    "    elif SORT_METHOD == 0:\n",
    "        nodups = (df_reads[[\"read_id\",\n",
    "                            \"smallest_positive_nuc_midpoint\",\n",
    "                            \"greatest_negative_nuc_midpoint\"]]\n",
    "                  .drop_duplicates(\"read_id\"))\n",
    "        nodups[\"read_count\"] = np.arange(1, len(nodups) + 1)\n",
    "    else:\n",
    "        nodups = _order_reads_by_ac(df_reads, plot_window)\n",
    "    return df_reads.merge(nodups[[\"read_id\", \"read_count\"]], on=\"read_id\", how=\"left\")\n",
    "\n",
    "# ───────────────────── Motifs: extract + tables + annotations ───────────── #\n",
    "def _build_motif_df(read_df: pd.DataFrame, plot_window: int, debug: bool=False) -> pd.DataFrame:\n",
    "    def _trim(row):\n",
    "        starts = [] if pd.isna(row.motif_rel_start) else list(row.motif_rel_start)\n",
    "        attrs  = [] if pd.isna(row.motif_attributes) else list(row.motif_attributes)\n",
    "        keep   = [i for i,p in enumerate(starts) if -plot_window <= p <= plot_window]\n",
    "        return (tuple(starts[i] for i in keep),\n",
    "                tuple(attrs[i]  for i in keep))\n",
    "    tmp = read_df.apply(_trim, axis=1, result_type=\"expand\")\n",
    "    tmp.columns = [\"rel_start_list\", \"attributes_list\"]\n",
    "    exploded = (tmp.assign(pairs=tmp.apply(lambda r: list(zip(r.rel_start_list, r.attributes_list)), axis=1))\n",
    "                   .explode(\"pairs\").dropna(subset=[\"pairs\"]))\n",
    "    if exploded.empty:\n",
    "        return pd.DataFrame(columns=[\"rel_start\",\"motif_id\",\"ln_p\",\"strand\",\"seq\",\"num\"])\n",
    "    motif_df = pd.DataFrame(exploded[\"pairs\"].tolist(), columns=[\"rel_start\",\"attributes\"])\n",
    "    motif_df = motif_df.drop_duplicates().sort_values(\"rel_start\").reset_index(drop=True)\n",
    "    motif_df[\"num\"] = motif_df.index + 1\n",
    "    motif_df[[\"motif_id\",\"ln_p\",\"strand\",\"seq\"]] = pd.DataFrame(motif_df[\"attributes\"].tolist(), index=motif_df.index)\n",
    "    return motif_df[[\"rel_start\",\"motif_id\",\"ln_p\",\"strand\",\"seq\",\"num\"]]\n",
    "\n",
    "def create_motif_table(read_df: pd.DataFrame, plot_window: int) -> Optional[go.Figure]:\n",
    "    motif_df = _build_motif_df(read_df, plot_window)\n",
    "    if motif_df.empty:\n",
    "        return None\n",
    "    px_per_char, pad = 7, 24\n",
    "    def _w(col): return len(str(max(col, key=lambda x: len(str(x))))) * px_per_char + pad\n",
    "    widths = [_w(motif_df[c]) for c in [\"num\",\"motif_id\",\"ln_p\",\"strand\",\"seq\"]]\n",
    "    fig = go.Figure(data=[go.Table(\n",
    "        columnwidth=widths,\n",
    "        header=dict(values=[\"#\", \"Motif\", \"ln(p-val)\", \"Strand\", \"Sequence\"],\n",
    "                    fill_color=\"white\", font=dict(color=\"black\", size=14),\n",
    "                    line_color=\"lightgrey\", align=\"center\"),\n",
    "        cells=dict(values=[motif_df[\"num\"], motif_df[\"motif_id\"], motif_df[\"ln_p\"],\n",
    "                           motif_df[\"strand\"], motif_df[\"seq\"]],\n",
    "                   fill_color=\"white\", font=dict(color=\"black\", size=12),\n",
    "                   line_color=\"lightgrey\", align=\"center\",\n",
    "                   height=TABLE_CELL_HEIGHT)\n",
    "    )])\n",
    "    fig.update_layout(margin=dict(l=0,r=0,t=0,b=0),\n",
    "                      paper_bgcolor=\"rgba(0,0,0,0)\", plot_bgcolor=\"rgba(0,0,0,0)\",\n",
    "                      height=TABLE_HEADER_H + len(motif_df)*TABLE_CELL_HEIGHT)\n",
    "    return fig\n",
    "\n",
    "def create_gene_table(read_df: pd.DataFrame) -> Optional[go.Figure]:\n",
    "    if read_df.empty:\n",
    "        return None\n",
    "    chrom   = read_df[\"chrom\"].iloc[0]\n",
    "    ref_pos = int(read_df[\"bed_start\"].iloc[0])\n",
    "    subset = tss_tes_df.loc[(tss_tes_df[\"chromosome\"] == chrom) &\n",
    "                            (tss_tes_df[\"type\"].str.contains(\"gene_q\"))].copy()\n",
    "    if subset.empty:\n",
    "        return None\n",
    "    subset[\"abs_start\"] = subset[\"start\"]\n",
    "    subset[\"abs_end\"]   = subset[\"end\"]\n",
    "    subset[\"rel_start\"] = subset[\"abs_start\"] - ref_pos\n",
    "    subset[\"rel_end\"]   = subset[\"abs_end\"]   - ref_pos\n",
    "    subset[\"exp_quart\"] = subset[\"type\"].str.extract(r\"_(q\\d+)\")[0]\n",
    "    upstream   = subset.loc[subset[\"rel_end\"]   < 0]\n",
    "    downstream = subset.loc[subset[\"rel_start\"] > 0]\n",
    "    rows = []\n",
    "    if not upstream.empty:\n",
    "        rows.append(upstream.loc[upstream[\"rel_end\"].idxmax()])\n",
    "    if not downstream.empty:\n",
    "        rows.append(downstream.loc[downstream[\"rel_start\"].idxmin()])\n",
    "    if not rows:\n",
    "        return None\n",
    "    gene_df = pd.DataFrame(rows)[[\"chromosome\",\"abs_start\",\"abs_end\",\"strand\",\"rel_start\",\"rel_end\",\"exp_quart\"]]\n",
    "    px_per_char, pad = 7, 24\n",
    "    def _w(col): return len(str(max(col, key=lambda x: len(str(x))))) * px_per_char + pad\n",
    "    widths = [_w(gene_df[c]) for c in [\"chromosome\",\"abs_start\",\"abs_end\",\"strand\",\"rel_start\",\"rel_end\",\"exp_quart\"]]\n",
    "    fig = go.Figure(data=[go.Table(\n",
    "        columnwidth=widths,\n",
    "        header=dict(values=[\"Chromosome\",\"Abs Start\",\"Abs End\",\"Strand\",\"Rel Start\",\"Rel End\",\"Exp Quart\"],\n",
    "                    fill_color=\"white\", font=dict(color=\"black\", size=14),\n",
    "                    line_color=\"lightgrey\", align=\"center\"),\n",
    "        cells=dict(values=[gene_df[c] for c in [\"chromosome\",\"abs_start\",\"abs_end\",\"strand\",\"rel_start\",\"rel_end\",\"exp_quart\"]],\n",
    "                   fill_color=\"white\", font=dict(color=\"black\", size=12),\n",
    "                   line_color=\"lightgrey\", align=\"center\",\n",
    "                   height=TABLE_CELL_HEIGHT)\n",
    "    )])\n",
    "    fig.update_layout(margin=dict(l=0,r=0,t=0,b=0),\n",
    "                      paper_bgcolor=\"rgba(0,0,0,0)\", plot_bgcolor=\"rgba(0,0,0,0)\",\n",
    "                      height=TABLE_HEADER_H + len(gene_df)*TABLE_CELL_HEIGHT)\n",
    "    return fig\n",
    "\n",
    "def _annotate_motifs(fig, motif_df: pd.DataFrame):\n",
    "    if motif_df.empty:\n",
    "        return\n",
    "    ANNOT_Y = 1.02\n",
    "    for row in motif_df.itertuples():\n",
    "        pos, num = row.rel_start, row.num\n",
    "        fig.add_shape(type=\"line\", x0=pos, x1=pos, y0=0, y1=1,\n",
    "                      xref=\"x\", yref=\"paper\",\n",
    "                      line=dict(color=\"grey\", width=1, dash=\"dash\"))\n",
    "        fig.add_annotation(x=pos, y=ANNOT_Y, yref=\"paper\",\n",
    "                           text=str(num), showarrow=False,\n",
    "                           font=dict(size=10),\n",
    "                           bgcolor=\"rgba(255,255,255,0.5)\")\n",
    "\n",
    "# ───────────────────── %m6A series helper (smoothed) ─────────────────── #\n",
    "def _m6a_series(plot_df: pd.DataFrame, cond: str, chr_type: str, data_type: str,\n",
    "                plot_window: int, smoothing_type: str, smoothing_window: int,\n",
    "                gaussian_sigma: float, strand_filter: Optional[Sequence[str]] = None):\n",
    "    df = plot_df.loc[(plot_df[\"condition\"] == cond)\n",
    "                     & (plot_df[\"chr_type\"] == chr_type)\n",
    "                     & (plot_df[\"type\"] == data_type)]\n",
    "    if strand_filter is not None and \"bed_strand\" in df.columns:\n",
    "        df = df.loc[df[\"bed_strand\"].isin(list(strand_filter))]\n",
    "    if df.empty:\n",
    "        return pd.Series([], dtype=float), pd.Series([], dtype=float)\n",
    "    recs = [(p, b)\n",
    "            for r in df.itertuples()\n",
    "            for p, b in zip(r.rel_pos, r.mod_qual_bin)\n",
    "            if -plot_window <= p <= plot_window]\n",
    "    if not recs:\n",
    "        return pd.Series([], dtype=float), pd.Series([], dtype=float)\n",
    "    agg = (pd.DataFrame(recs, columns=[\"pos\",\"bin\"])\n",
    "             .groupby(\"pos\")[\"bin\"].agg([\"sum\",\"count\"]).reset_index())\n",
    "    agg[\"ratio\"] = agg[\"sum\"] / agg[\"count\"]\n",
    "    if smoothing_type == \"none\":\n",
    "        y = agg[\"ratio\"].to_numpy()\n",
    "    elif smoothing_type == \"moving\":\n",
    "        y = (agg[\"sum\"].rolling(smoothing_window, center=True).sum() /\n",
    "             agg[\"count\"].rolling(smoothing_window, center=True).sum()).to_numpy()\n",
    "    else:\n",
    "        y = gaussian_filter1d(agg[\"ratio\"].to_numpy(), sigma=gaussian_sigma)\n",
    "    x = agg[\"pos\"].to_numpy()\n",
    "    mask = ~np.isnan(y)\n",
    "    return pd.Series(x[mask]), pd.Series(y[mask])\n",
    "\n",
    "# ─────────────────────────── Core multi-plot ─────────────────────────── #\n",
    "def create_multi_plot(\n",
    "    plot_df: pd.DataFrame,\n",
    "    group_df: pd.DataFrame,         # kept for API parity\n",
    "    conditions,                     # str | list[str] (1–4)\n",
    "    chr_type: str,\n",
    "    data_type: str,\n",
    "    plot_window: int,\n",
    "    *,\n",
    "    require_full_span: bool = True,\n",
    "    plot_motifs: bool = True,\n",
    "    smoothing_type: str = \"moving\",\n",
    "    smoothing_window: int = 50,\n",
    "    gaussian_sigma: float = 10,\n",
    "    subsample_reads: bool = True,\n",
    "    max_reads_per_condition: Optional[int] = 300,\n",
    "    random_state: Optional[int] = 42,\n",
    "    strand_filter: Optional[Sequence[str]] = None,\n",
    "    debug: bool = False\n",
    ") -> Optional[go.Figure]:\n",
    "    \"\"\"Single, double, triple, or quadruple read tracks with %m6A panels.\n",
    "       First condition is compared with all others on separate %m6A panels.\"\"\"\n",
    "    conds = list(conditions) if isinstance(conditions, (list, tuple)) else [conditions]\n",
    "    if not (1 <= len(conds) <= 4):\n",
    "        raise ValueError(\"conditions must contain 1–4 entries\")\n",
    "    n = len(conds)\n",
    "\n",
    "    # color map for all conditions (uses nanotools.get_color)\n",
    "    color_for = {c: nanotools.get_color(c) for c in conds}\n",
    "\n",
    "    # collect per-condition filtered reads\n",
    "    cond_dfs = []\n",
    "    for c in conds:\n",
    "        df = _filter_reads(plot_df, c, chr_type, data_type, plot_window,\n",
    "                           require_full_span, strand_filter=strand_filter, debug=debug)\n",
    "        cond_dfs.append(df)\n",
    "\n",
    "    if all(df.empty for df in cond_dfs):\n",
    "        warnings.warn(f\"No reads for {conds} / {data_type}.\")\n",
    "        return None\n",
    "\n",
    "    # optional subsampling per condition\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    if subsample_reads and max_reads_per_condition is not None:\n",
    "        for i, df in enumerate(cond_dfs):\n",
    "            if df.empty:\n",
    "                continue\n",
    "            uniq = df[\"read_id\"].drop_duplicates()\n",
    "            if len(uniq) > max_reads_per_condition:\n",
    "                keep = rng.choice(uniq, size=max_reads_per_condition, replace=False)\n",
    "                cond_dfs[i] = df[df[\"read_id\"].isin(keep)].copy()\n",
    "                if debug and DEBUG_SUMMARY:\n",
    "                    print(f\"[multi] {conds[i]} subsampled to {len(keep)} reads\")\n",
    "\n",
    "    # order reads independently per condition\n",
    "    for i, df in enumerate(cond_dfs):\n",
    "        if df.empty:\n",
    "            continue\n",
    "        cond_dfs[i] = _order_read_df(df, plot_window)\n",
    "\n",
    "    # figure scaffold\n",
    "    if n == 1:\n",
    "        total_rows = 2                         # read, %m6A\n",
    "        row_heights = [0.45, 0.55]\n",
    "        read_rows   = [1]\n",
    "        m6a_rows    = [2]\n",
    "        m6a_pairs   = [(conds[0], None)]       # plot single series\n",
    "    else:\n",
    "        # rows = (n-1) combined panels (cond0 vs cond_i) + n read tracks\n",
    "        total_rows  = (n - 1) + n\n",
    "        # weights: combined panels heavier than read stripes → normalize\n",
    "        weights = [3]*(n-1) + [2]*n\n",
    "        s = float(sum(weights))\n",
    "        row_heights = [w/s for w in weights]\n",
    "        m6a_rows  = list(range(1, n))          # top panels\n",
    "        read_rows = list(range(n, total_rows+1))\n",
    "        m6a_pairs = [(conds[0], conds[i]) for i in range(1, n)]\n",
    "\n",
    "    fig = make_subplots(rows=total_rows, cols=1, shared_xaxes=True, vertical_spacing=0.02,\n",
    "                        row_heights=row_heights)\n",
    "    fig.update_xaxes(range=[-plot_window, plot_window], tickmode=\"linear\", tick0=0, dtick=300)\n",
    "\n",
    "    # per-condition read layers + dots\n",
    "    motif_collect = []\n",
    "    for idx, (c, df_reads) in enumerate(zip(conds, cond_dfs)):\n",
    "        if df_reads.empty:\n",
    "            continue\n",
    "        motif_collect.append(df_reads[[\"motif_rel_start\", \"motif_attributes\"]])\n",
    "\n",
    "        # read occupancy stripes\n",
    "        _add_read_occupancy(\n",
    "            fig, df_reads, plot_window,\n",
    "            row=read_rows[idx], color=color_for[c], read_width=0.025\n",
    "        )\n",
    "\n",
    "        # m6A hit dots\n",
    "        long_df = _long_format(df_reads, plot_window)\n",
    "        hits = long_df[long_df[\"mod_qual_bin\"] == 1]\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=hits[\"rel_pos\"], y=hits[\"read_count\"],\n",
    "                mode=\"markers\",\n",
    "                marker=dict(symbol=\"square\", size=1.5, color=color_for[c]),\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=read_rows[idx], col=1\n",
    "        )\n",
    "        fig.update_yaxes(title_text=\"Read ID\", row=read_rows[idx], col=1)\n",
    "\n",
    "    # %m6A panels: compare cond0 vs each other\n",
    "    global_max = 0.0\n",
    "    if n == 1:\n",
    "        x0, y0 = _m6a_series(plot_df, conds[0], chr_type, data_type,\n",
    "                             plot_window, smoothing_type, smoothing_window, gaussian_sigma,\n",
    "                             strand_filter=strand_filter)\n",
    "        if len(y0):\n",
    "            global_max = max(global_max, float(y0.max()))\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=x0, y=y0, mode=\"lines\",\n",
    "                           line=dict(color=color_for[conds[0]], width=2),\n",
    "                           name=str(conds[0])),\n",
    "                row=m6a_rows[0], col=1\n",
    "            )\n",
    "        fig.update_yaxes(title_text=\"% m6A\", row=m6a_rows[0], col=1)\n",
    "    else:\n",
    "        for r, (c0, ci) in zip(m6a_rows, m6a_pairs):\n",
    "            xA, yA = _m6a_series(plot_df, c0, chr_type, data_type,\n",
    "                                 plot_window, smoothing_type, smoothing_window, gaussian_sigma,\n",
    "                                 strand_filter=strand_filter)\n",
    "            xB, yB = _m6a_series(plot_df, ci, chr_type, data_type,\n",
    "                                 plot_window, smoothing_type, smoothing_window, gaussian_sigma,\n",
    "                                 strand_filter=strand_filter)\n",
    "            if len(yA): global_max = max(global_max, float(yA.max()))\n",
    "            if len(yB): global_max = max(global_max, float(yB.max()))\n",
    "            if len(yA):\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(x=xA, y=yA, mode=\"lines\",\n",
    "                               line=dict(color=color_for[c0], width=2),\n",
    "                               name=str(c0)),\n",
    "                    row=r, col=1\n",
    "                )\n",
    "            if len(yB):\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(x=xB, y=yB, mode=\"lines\",\n",
    "                               line=dict(color=color_for[ci], width=2),\n",
    "                               name=str(ci)),\n",
    "                    row=r, col=1\n",
    "                )\n",
    "            fig.update_yaxes(title_text=\"% m6A\", row=r, col=1)\n",
    "\n",
    "        # equalize y across all %m6A panels\n",
    "        max_pct = math.ceil((max(global_max, 0.01) * 100) / 2) * 2\n",
    "        ymax = max_pct / 100\n",
    "        for r in m6a_rows:\n",
    "            fig.update_yaxes(range=[0, ymax], tickformat=\".0%\", row=r, col=1)\n",
    "\n",
    "    # motif annotations (single authoritative numbering across all conditions)\n",
    "    if plot_motifs and motif_collect:\n",
    "        merged = pd.concat(motif_collect, ignore_index=True)\n",
    "        motif_df = _build_motif_df(merged, plot_window, debug=debug and DEBUG_SUMMARY)\n",
    "        _annotate_motifs(fig, motif_df)\n",
    "\n",
    "    # layout\n",
    "    ttl = \", \".join(conds) if n == 1 else f\"{conds[0]} vs \" + \", \".join(conds[1:])\n",
    "    fig.update_layout(template=\"plotly_white\",\n",
    "                      width=FIG_WIDTH, height=FIG_HEIGHT,\n",
    "                      title=f\"Conditions: {ttl} | Type: {data_type}\",\n",
    "                      legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1))\n",
    "    fig.update_xaxes(title_text=\"Genomic position (bp)\", row=total_rows, col=1)\n",
    "    return fig\n",
    "\n",
    "# ───────────────────────── Saving worker ───────────────────────────── #\n",
    "def _save_multi_plot(args):\n",
    "    \"\"\"Worker for multiprocessing. args = (conditions_tuple, motif_type, chr_sel, win, out_root, strand_filter).\"\"\"\n",
    "    conds, motif_type, chr_sel, win, out_root, strand_filter = args\n",
    "\n",
    "    mask = (down_sampled_plot_df[\"condition\"].isin(conds)\n",
    "            & (down_sampled_plot_df[\"chr_type\"] == chr_sel)\n",
    "            & (down_sampled_plot_df[\"type\"] == motif_type))\n",
    "    if strand_filter is not None and \"bed_strand\" in down_sampled_plot_df.columns:\n",
    "        mask &= down_sampled_plot_df[\"bed_strand\"].isin(list(strand_filter))\n",
    "    if not mask.any():\n",
    "        if DEBUG_PROGRESS:\n",
    "            print(f\"[skip] {conds} / {motif_type}: no rows after strand filter\")\n",
    "        return None\n",
    "\n",
    "    fig = create_multi_plot(\n",
    "        plot_df           = down_sampled_plot_df,\n",
    "        group_df          = down_sampled_group_df,\n",
    "        conditions        = list(conds),\n",
    "        chr_type          = chr_sel,\n",
    "        data_type         = motif_type,\n",
    "        plot_window       = win,\n",
    "        require_full_span = True,\n",
    "        plot_motifs       = True,\n",
    "        smoothing_type    = \"moving\",\n",
    "        smoothing_window  = 50,\n",
    "        gaussian_sigma    = 10,\n",
    "        subsample_reads   = True,\n",
    "        max_reads_per_condition = 300,\n",
    "        random_state      = 42,\n",
    "        strand_filter     = strand_filter,\n",
    "        debug             = DEBUG_PROGRESS\n",
    "    )\n",
    "    if fig is None:\n",
    "        return None\n",
    "\n",
    "    read_subset = down_sampled_plot_df.loc[mask].copy()\n",
    "    motif_tbl   = create_motif_table(read_subset, win)\n",
    "    gene_tbl    = create_gene_table(read_subset)\n",
    "\n",
    "    # directory + filenames\n",
    "    cond_slug = \"_\".join(conds) if len(conds) == 1 else f\"{conds[0]}_vs_\" + \"_\".join(conds[1:])\n",
    "    save_dir  = os.path.join(out_root, f\"{cond_slug}_{win}bp\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    rank_val = chip_rank_lookup.get(motif_type, None)\n",
    "    rank_pref = \"NA\" if rank_val is None else str(int(round(rank_val)))\n",
    "    base      = f\"{rank_pref}_{motif_type}_b{win}_chr{chr_sel}\"\n",
    "    png_path  = os.path.join(save_dir, base + \".png\")\n",
    "    svg_path  = os.path.join(save_dir, base + \".svg\")\n",
    "\n",
    "    # export main\n",
    "    main_png = fig.to_image(format=\"png\", scale=2)\n",
    "\n",
    "    # stitching\n",
    "    if motif_tbl and gene_tbl:\n",
    "        tbl_png      = motif_tbl.to_image(format=\"png\", scale=2)\n",
    "        stitched_1   = stitch_vertical(main_png, tbl_png)\n",
    "        buf = io.BytesIO(); stitched_1.save(buf, format=\"PNG\"); stitched_1_png = buf.getvalue()\n",
    "        gene_tbl_png = gene_tbl.to_image(format=\"png\", scale=2)\n",
    "        stitched     = stitch_vertical(stitched_1_png, gene_tbl_png)\n",
    "    elif motif_tbl:\n",
    "        tbl_png  = motif_tbl.to_image(format=\"png\", scale=2)\n",
    "        stitched = stitch_vertical(main_png, tbl_png)\n",
    "    elif gene_tbl:\n",
    "        gene_tbl_png = gene_tbl.to_image(format=\"png\", scale=2)\n",
    "        stitched     = stitch_vertical(main_png, gene_tbl_png)\n",
    "    else:\n",
    "        stitched = PILImage.open(io.BytesIO(main_png)).convert(\"RGBA\")\n",
    "\n",
    "    # write PNG\n",
    "    stitched.save(png_path, dpi=(PNG_DPI, PNG_DPI))\n",
    "\n",
    "    # assemble SVG stack\n",
    "    main_svg = fig.to_image(format=\"svg\").decode()\n",
    "    if motif_tbl and gene_tbl:\n",
    "        tbl_svg      = motif_tbl.to_image(format=\"svg\").decode()\n",
    "        gene_tbl_svg = gene_tbl.to_image(format=\"svg\").decode()\n",
    "        svg_head = '<svg xmlns=\"http://www.w3.org/2000/svg\">'\n",
    "        translate_tbl  = f'translate(0,{fig.layout.height + TABLE_MARGIN_PX})'\n",
    "        translate_gene = f'translate(0,{fig.layout.height + TABLE_MARGIN_PX + motif_tbl.layout.height + TABLE_MARGIN_PX})'\n",
    "        combined_svg = (svg_head + f'<g>{main_svg}</g>'\n",
    "                        + f'<g transform=\"{translate_tbl}\">{tbl_svg}</g>'\n",
    "                        + f'<g transform=\"{translate_gene}\">{gene_tbl_svg}</g>'\n",
    "                        + '</svg>')\n",
    "    elif motif_tbl:\n",
    "        tbl_svg = motif_tbl.to_image(format=\"svg\").decode()\n",
    "        svg_head = '<svg xmlns=\"http://www.w3.org/2000/svg\">'\n",
    "        translate_tbl = f'translate(0,{fig.layout.height + TABLE_MARGIN_PX})'\n",
    "        combined_svg = svg_head + f'<g>{main_svg}</g>' + f'<g transform=\"{translate_tbl}\">{tbl_svg}</g>' + '</svg>'\n",
    "    elif gene_tbl:\n",
    "        gene_tbl_svg = gene_tbl.to_image(format=\"svg\").decode()\n",
    "        svg_head = '<svg xmlns=\"http://www.w3.org/2000/svg\">'\n",
    "        translate_gene = f'translate(0,{fig.layout.height + TABLE_MARGIN_PX})'\n",
    "        combined_svg = svg_head + f'<g>{main_svg}</g>' + f'<g transform=\"{translate_gene}\">{gene_tbl_svg}</g>' + '</svg>'\n",
    "    else:\n",
    "        combined_svg = main_svg\n",
    "\n",
    "    with open(svg_path, \"w\") as fh:\n",
    "        fh.write(combined_svg)\n",
    "\n",
    "    if DEBUG_PROGRESS:\n",
    "        print(f\"[saved] {png_path}\")\n",
    "    return png_path\n",
    "\n",
    "# ─────────────────────────── Batch runner ───────────────────────────── #\n",
    "def run_batch_save(\n",
    "    CONDITIONS,\n",
    "    MOTIF_TYPES,\n",
    "    *,\n",
    "    chr_sel: str = \"X\",\n",
    "    win: Optional[int] = None,\n",
    "    out_root: str = \"images_multi\",\n",
    "    n_workers: Optional[int] = None,\n",
    "    strand_filter: Optional[Sequence[str]] = None\n",
    "):\n",
    "    if CONDITIONS is None or MOTIF_TYPES is None:\n",
    "        raise ValueError(\"Define CONDITIONS and MOTIF_TYPES before calling run_batch_save().\")\n",
    "    if win is None:\n",
    "        if \"temp_bed_w\" not in globals():\n",
    "            raise ValueError(\"Provide 'win' or define global 'temp_bed_w'.\")\n",
    "        win = int(temp_bed_w)\n",
    "    n_workers = n_workers or max(1, min(cpu_count(), 50))\n",
    "    conds_tuple = tuple(CONDITIONS)\n",
    "    jobs = [(conds_tuple, mt, chr_sel, win, out_root, strand_filter) for mt in list(MOTIF_TYPES)]\n",
    "    results = run_in_pool(_save_multi_plot, jobs, n_workers=n_workers,\n",
    "                          desc=\"Saving plots\", unit=\"plot\")\n",
    "    return [p for p in results if p is not None]\n",
    "\n",
    "# ───────────────────────── End unified cell ─────────────────────────── #\n",
    "# 1) Choose 1–4 conditions in desired order (duplicates allowed)\n",
    "COND_IDXS  = [0,3,4,5]   # e.g., [1], [1,0], [1,0,2], or [1,0,2,2]\n",
    "CONDITIONS = tuple(analysis_cond[i] for i in COND_IDXS)\n",
    "\n",
    "# 2) Choose motif types\n",
    "MOTIF_TYPES = [f\"MOTIFS_{t}\" for t in type_selected]  # or specify a subset, e.g. [\"MOTIFS_rex48\"]\n",
    "MOTIF_TYPES = [\"MEX_motif\"]\n",
    "STRAND_FILTER = [\"+\",\"-\"] # None or [\"+\"]\n",
    "# 3) Parameters \n",
    "CHR       = \"X\"                                         # chromosome group used in your filtered DF\n",
    "WIN       = 1500 #temp_bed_w if \"temp_bed_w\" in globals() else 1000\n",
    "OUT_ROOT  = \"images_multi\"                              # output root folder\n",
    "N_WORKERS = min(50, cpu_count())                        # multiprocessing\n",
    "\n",
    "# 4) Run batch save\n",
    "saved_pngs = run_batch_save(CONDITIONS, MOTIF_TYPES, chr_sel=CHR, win=WIN,\n",
    "                            out_root=OUT_ROOT, n_workers=N_WORKERS,\n",
    "                            strand_filter=STRAND_FILTER)\n",
    "print(f\"Saved {len(saved_pngs)} PNG(s) under '{OUT_ROOT}'\")\n",
    "\n",
    "# 5) Optional inline preview\n",
    "if MOTIF_TYPES:\n",
    "    fig = create_multi_plot(\n",
    "        plot_df=down_sampled_plot_df,\n",
    "        group_df=down_sampled_group_df,\n",
    "        conditions=CONDITIONS,\n",
    "        chr_type=CHR,\n",
    "        data_type=MOTIF_TYPES[0],\n",
    "        plot_window=WIN,\n",
    "        require_full_span=True,\n",
    "        plot_motifs=False,\n",
    "        smoothing_type=\"moving\",\n",
    "        smoothing_window=25,\n",
    "        gaussian_sigma=10,\n",
    "        subsample_reads=True,\n",
    "        max_reads_per_condition=300,\n",
    "        random_state=42,\n",
    "        strand_filter=STRAND_FILTER,\n",
    "        debug=False\n",
    "    )\n",
    "    if fig is not None:\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2434a9da3c44a2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### CELL 1 · PREP FOR PLOTTING  ──────────────────────────────────────────────\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import find_peaks\n",
    "import multiprocessing as mp\n",
    "from tqdm.auto import tqdm\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# ────────────────── CONFIGURATION ──────────────────\n",
    "OUTPUT_DIR = \"images_20250604\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ────────────────── CONFIGURATION ──────────────────\n",
    "CHIP_RANK_CUTOFF        = 80\n",
    "CHR_TYPE_INCLUDE        = [\"X\"]                # [] → include all chr_types\n",
    "REL_POS_RANGE           = 500\n",
    "N_BOOTSTRAP_GROUPS      = 2\n",
    "SMOOTH_WINDOW           = 100              # bp for moving-average smoothing\n",
    "SMOOTH_WINDOW_METRICS   = 10 # for m6A peak and mean\n",
    "RANDOM_STATE            = 36\n",
    "N_WORKERS               = max(2, mp.cpu_count() - 2)\n",
    "DEBUG                   = True\n",
    "PLOT_TEMPLATE           = \"plotly_white\"\n",
    "\n",
    "DOWNSAMPLE_BALANCE      = True                       # ← set True to equalise reads per (type,condition)\n",
    "\n",
    "PEAK_SELECTION_METHOD   = \"closest_to_zero\"  # \"height\" | \"area\" | \"closest_to_zero\"\n",
    "CONDITIONS_TO_INCLUDE   = [analysis_cond[0], analysis_cond[1], analysis_cond[2]]\n",
    "\n",
    "# NEW ▶ when True we keep “intergenic_control” reads, give them their own\n",
    "#       second_type, and rename their type →  intergenic_control_<bed_start>\n",
    "INCLUDE_INTERGENIC      = True             # ← flip to True when needed\n",
    "\n",
    "DEFAULT_READ_CLRS       = [\"#4974a5\", \"#b12537\", \"#47B562\", \"#808080\"]\n",
    "\n",
    "def print_debug(msg: str):\n",
    "    if DEBUG:\n",
    "        print(f\"[DEBUG] {msg}\")\n",
    "\n",
    "# ───── 1) LOAD CHIP RANKS & BUILD LOOKUP ─────\n",
    "print_debug(\"Loading chip rank file and building lookup…\")\n",
    "chiprank_df = (\n",
    "    pd.read_csv(\"/Data1/reference/rex_chiprank.bed\", sep=r\"\\s+\")\n",
    "      .assign(type=lambda d: \"MOTIFS_\" + d[\"type\"].astype(str))\n",
    ")\n",
    "chip_rank_lookup = {\n",
    "    t: round(float(rk) * 100, 3)\n",
    "    for t, rk in zip(chiprank_df[\"type\"], chiprank_df[\"chip_rank\"])\n",
    "}\n",
    "print_debug(f\"Loaded {len(chip_rank_lookup)} motif types from chip rank.\")\n",
    "\n",
    "# ───── 2) FILTER READS & ASSIGN SECOND_TYPE ─────\n",
    "print_debug(\"Filtering merged_df by type, chr_type, and selected conditions…\")\n",
    "\n",
    "# decide which read types are allowed\n",
    "types_to_keep = set(chip_rank_lookup.keys())\n",
    "if INCLUDE_INTERGENIC:\n",
    "    types_to_keep.add(\"intergenic_control\")\n",
    "\n",
    "keep_chr = set(CHR_TYPE_INCLUDE) if CHR_TYPE_INCLUDE else set(merged_df[\"chr_type\"].unique())\n",
    "\n",
    "df = merged_df[\n",
    "    merged_df[\"type\"].isin(types_to_keep)\n",
    "    & merged_df[\"chr_type\"].isin(keep_chr)\n",
    "    & merged_df[\"condition\"].isin(CONDITIONS_TO_INCLUDE)\n",
    "].copy()\n",
    "print_debug(f\"After type/chr_type/condition filter: {len(df)} reads remain.\")\n",
    "\n",
    "# ── NEW second_type logic ────────────────────────────────────────────────\n",
    "# 1) add numeric chip_rank (may be NaN for intergenic)\n",
    "df[\"chip_rank\"] = df[\"type\"].map(chip_rank_lookup)\n",
    "\n",
    "# 2) assign second_type:\n",
    "df[\"second_type\"] = np.where(\n",
    "    df[\"type\"] == \"intergenic_control\",\n",
    "    \"intergenic_control\",\n",
    "    np.where(df[\"chip_rank\"].fillna(-np.inf) >= CHIP_RANK_CUTOFF, \"gt80\", \"lt80\")\n",
    ")\n",
    "\n",
    "# 3) rename type for intergenic reads ▸  intergenic_control_<bed_start>\n",
    "if INCLUDE_INTERGENIC:\n",
    "    mask_intergenic = df[\"type\"] == \"intergenic_control\"\n",
    "    df.loc[mask_intergenic, \"type\"] = (\n",
    "        df.loc[mask_intergenic, \"type\"]\n",
    "        + \"_\"\n",
    "        + df.loc[mask_intergenic, \"bed_start\"].astype(str)\n",
    "    )\n",
    "\n",
    "# chip_rank no longer needed\n",
    "df.drop(columns=\"chip_rank\", inplace=True)\n",
    "print_debug(\"Assigned 'second_type'; renamed intergenic types if applicable.\")\n",
    "\n",
    "# Filter reads by overlap with ±REL_POS_RANGE\n",
    "print_debug(\"Filtering reads by overlap with rel_pos window…\")\n",
    "# Ensure 'rel_pos' column contains NumPy arrays or lists that can be converted\n",
    "# and that they are not empty before applying boolean operations.\n",
    "mask_overlap = df[\"rel_pos\"].apply(lambda arr: isinstance(arr, (list, np.ndarray)) and len(arr) > 0 and ((np.asarray(arr) >= -REL_POS_RANGE) & (np.asarray(arr) <= REL_POS_RANGE)).any())\n",
    "df = df[mask_overlap].reset_index(drop=True)\n",
    "print_debug(f\"After overlap filter: {len(df)} reads remain.\")\n",
    "\n",
    "# Drop reads without any methylation data\n",
    "print_debug(\"Dropping reads with no methylation marks…\")\n",
    "mask_valid = df[\"mod_qual_bin\"].apply(lambda x: isinstance(x, (list, np.ndarray)) and len(x) > 0 and np.nansum(x) > 0)\n",
    "df = df[mask_valid].reset_index(drop=True)\n",
    "print_debug(f\"After methylation filter: {len(df)} reads remain.\")\n",
    "\n",
    "# ───────── OPTIONAL BALANCING OF READ COUNTS PER (type, condition) ─────────\n",
    "if DOWNSAMPLE_BALANCE and not df.empty:\n",
    "    print_debug(\"Balancing read counts per (type, condition)…\")\n",
    "    rng = np.random.RandomState(RANDOM_STATE)\n",
    "\n",
    "    # ── A) snapshot *initial* counts  ────────────────────────────────────────\n",
    "    init_counts_df = (\n",
    "        df.groupby([\"type\", \"condition\"])\n",
    "          .size()\n",
    "          .reset_index(name=\"initial_reads\")\n",
    "    )\n",
    "\n",
    "    # 1) size table\n",
    "    size_tbl = (\n",
    "        df.groupby([\"type\", \"condition\"])\n",
    "          .size()\n",
    "          .unstack(fill_value=0)\n",
    "    )\n",
    "\n",
    "    # 2) drop types missing a condition\n",
    "    types_to_drop = size_tbl[(size_tbl == 0).any(axis=1)].index\n",
    "    if len(types_to_drop):\n",
    "        print_debug(f\"Dropping {len(types_to_drop)} type(s) with zero reads in ≥1 condition: \"\n",
    "                    f\"{', '.join(map(str, types_to_drop))}\")\n",
    "        df = df[~df[\"type\"].isin(types_to_drop)].reset_index(drop=True)\n",
    "        size_tbl = size_tbl.loc[~size_tbl.index.isin(types_to_drop)]\n",
    "\n",
    "    # 3) down-sample each (type,condition) to equal n\n",
    "    balanced_parts = []\n",
    "    for typ, sub_typ in df.groupby(\"type\"):\n",
    "        min_n = sub_typ.groupby(\"condition\").size().min()\n",
    "        for cond, sub_cond in sub_typ.groupby(\"condition\"):\n",
    "            if len(sub_cond) > min_n:\n",
    "                keep_idx = rng.choice(sub_cond.index, size=min_n, replace=False)\n",
    "                balanced_parts.append(sub_cond.loc[keep_idx])\n",
    "            else:\n",
    "                balanced_parts.append(sub_cond)\n",
    "    df = (\n",
    "        pd.concat(balanced_parts, ignore_index=True)\n",
    "          .sample(frac=1.0, random_state=rng)    # optional shuffle\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "    print_debug(f\"After balancing: {len(df)} reads remain.\")\n",
    "\n",
    "    # ── B) snapshot *balanced* counts & merge into summary table ─────────────\n",
    "    bal_counts_df = (\n",
    "        df.groupby([\"type\", \"condition\"])\n",
    "          .size()\n",
    "          .reset_index(name=\"balanced_reads\")\n",
    "    )\n",
    "\n",
    "    downsample_summary_df = (\n",
    "        pd.merge(init_counts_df, bal_counts_df,\n",
    "                 on=[\"type\", \"condition\"], how=\"outer\")\n",
    "          .fillna(0)                             # combos that were dropped → 0\n",
    "          .astype({\"initial_reads\":\"int\", \"balanced_reads\":\"int\"})\n",
    "          .sort_values([\"type\", \"condition\"])\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # (Optional) write to disk so later cells don’t need to recompute\n",
    "    downsample_csv = os.path.join(OUTPUT_DIR, \"downsample_summary.csv\")\n",
    "\n",
    "\n",
    "# ────────────────── 3) ASSIGN BOOTSTRAP GROUPS ──────────────────\n",
    "print_debug(\"Assigning bootstrap groups…\")\n",
    "rng = np.random.RandomState(RANDOM_STATE)\n",
    "group_labels = np.arange(N_BOOTSTRAP_GROUPS)\n",
    "\n",
    "def assign_bootstrap_groups(subdf):\n",
    "    n = len(subdf)\n",
    "    if n == 0: # Handle empty sub-dataframes\n",
    "        # Add the column if it doesn't exist, or ensure it's present\n",
    "        if \"bootstrap_group\" not in subdf.columns:\n",
    "            subdf = subdf.assign(bootstrap_group=pd.Series(dtype=int))\n",
    "        return subdf.copy()\n",
    "    labels = np.tile(group_labels, int(np.ceil(n / N_BOOTSTRAP_GROUPS)))[:n]\n",
    "    rng.shuffle(labels)\n",
    "    subdf = subdf.copy()\n",
    "    subdf[\"bootstrap_group\"] = labels\n",
    "    return subdf\n",
    "\n",
    "if not df.empty:\n",
    "    df = (\n",
    "        df\n",
    "        .groupby([\"second_type\", \"condition\"], group_keys=False)\n",
    "        .apply(assign_bootstrap_groups)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "elif \"bootstrap_group\" not in df.columns : # Ensure column exists if df was empty initially\n",
    "    df[\"bootstrap_group\"] = pd.Series(dtype=int)\n",
    "print_debug(\"Bootstrap groups assigned.\")\n",
    "\n",
    "# ────────────────── 4) FLATTEN READS INTO POSITION-LEVEL ROWS (PARALLEL) ──────────────────\n",
    "print_debug(\"Flattening reads into position-level rows using multiprocessing…\")\n",
    "\n",
    "def flatten_single_read(rec_dict):\n",
    "    \"\"\"\n",
    "    Given a single read (as a dict), return a list of dicts:\n",
    "    each with bootstrap_group, second_type, condition, type, rel_pos, mod\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    # Ensure rel_pos and mod_qual_bin are numpy arrays for processing\n",
    "    rel_pos_arr = np.asarray(rec_dict[\"rel_pos\"], dtype=int)\n",
    "    mods_arr    = np.asarray(rec_dict[\"mod_qual_bin\"], dtype=float)\n",
    "\n",
    "    for pos, m in zip(rel_pos_arr, mods_arr):\n",
    "        if abs(pos) <= REL_POS_RANGE and not np.isnan(m):\n",
    "            out.append({\n",
    "                \"bootstrap_group\": rec_dict[\"bootstrap_group\"],\n",
    "                \"second_type\":     rec_dict[\"second_type\"],\n",
    "                \"condition\":       rec_dict[\"condition\"],\n",
    "                \"type\":            rec_dict[\"type\"],\n",
    "                \"chr_type\":        rec_dict[\"chr_type\"],\n",
    "                \"rel_pos\":         int(pos),\n",
    "                \"mod\":             int(m)  # 1 or 0\n",
    "            })\n",
    "    return out\n",
    "\n",
    "# Convert DataFrame to list of plain dicts for picklable objects\n",
    "records = df.to_dict(orient=\"records\")\n",
    "\n",
    "if records: # Only run multiprocessing if there are records to process\n",
    "    with mp.Pool(N_WORKERS) as pool:\n",
    "        all_lists = list(tqdm(\n",
    "            pool.imap_unordered(flatten_single_read, records),\n",
    "            total=len(records),\n",
    "            desc=\"Flattening reads\"\n",
    "        ))\n",
    "    flat_records = [item for sublist in all_lists for item in sublist]\n",
    "else:\n",
    "    flat_records = []\n",
    "\n",
    "if flat_records:\n",
    "    flat_df = pd.DataFrame(flat_records)\n",
    "else: # Create empty DataFrame with correct columns if no records resulted from flattening\n",
    "    flat_df = pd.DataFrame(columns=[\"bootstrap_group\", \"second_type\", \"condition\", \"type\",\"chr_type\", \"rel_pos\", \"mod\"])\n",
    "print_debug(f\"Flattened DataFrame has {len(flat_df)} rows.\")\n",
    "\n",
    "\n",
    "# ────────────────── 5) COMPUTE METHYLATION FRACTION BY POSITION ──────────────────\n",
    "print_debug(\"Computing methylation fraction by position…\")\n",
    "if not flat_df.empty:\n",
    "    grouped = (\n",
    "        flat_df\n",
    "        .groupby([\"bootstrap_group\", \"second_type\", \"condition\", \"type\",\"chr_type\",\"rel_pos\"])\n",
    "        .agg(total_reads  = (\"mod\", \"size\"),\n",
    "             methylated   = (\"mod\", \"sum\"))\n",
    "        .reset_index()\n",
    "    )\n",
    "    grouped[\"frac\"] = grouped[\"methylated\"] / grouped[\"total_reads\"]\n",
    "else: # Create empty grouped DataFrame with correct columns\n",
    "    grouped = pd.DataFrame(columns=[\"bootstrap_group\", \"second_type\", \"condition\", \"type\",\"chr_type\", \"rel_pos\", \"total_reads\", \"methylated\", \"frac\"])\n",
    "print_debug(f\"Methylation grouping result has {len(grouped)} rows.\")\n",
    "\n",
    "# Prepare the full position index & all combos\n",
    "all_positions = np.arange(-REL_POS_RANGE, REL_POS_RANGE + 1)\n",
    "if not grouped.empty:\n",
    "    combos = (\n",
    "        grouped[[\"bootstrap_group\", \"second_type\", \"condition\", \"type\",\"chr_type\"]]\n",
    "        .drop_duplicates()\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "else: # Create empty combos DataFrame\n",
    "    combos = pd.DataFrame(columns=[\"bootstrap_group\", \"second_type\", \"condition\", \"type\",\"chr_type\"])\n",
    "print_debug(f\"Total (bootstrap_group, second_type, condition, type) combos: {len(combos)}\")\n",
    "\n",
    "\n",
    "# ────────────────── 6) DEFINE PEAK COMPUTATION FUNCTION ──────────────────\n",
    "def compute_peak_for_combo(args):\n",
    "    \"\"\"\n",
    "    Compute peak_loc, peak_height, peak_fwhm for one\n",
    "    (bootstrap_group, second_type, condition, type).\n",
    "    Smoothing now uses a rolling mean that ignores NaNs,\n",
    "    leaving fully-NaN windows as gaps.\n",
    "    \"\"\"\n",
    "    bg, st, cond, typ, chr_typ= args\n",
    "    sub = grouped[\n",
    "        (grouped[\"bootstrap_group\"] == bg)\n",
    "        & (grouped[\"second_type\"]     == st)\n",
    "        & (grouped[\"condition\"]       == cond)\n",
    "        & (grouped[\"type\"]            == typ)\n",
    "    ]\n",
    "\n",
    "    # 1) build raw fraction series with NaN default\n",
    "    methyl_series = pd.Series(index=all_positions, dtype=float)\n",
    "    if not sub.empty:\n",
    "        # assign only positions where we have reads\n",
    "        methyl_series.loc[sub[\"rel_pos\"].values] = sub[\"frac\"].values\n",
    "\n",
    "    # 2a) smoothed signal for PEAK DETECTION / FWHM (uses SMOOTH_WINDOW)\n",
    "    if SMOOTH_WINDOW > 1:\n",
    "        methyl_smooth_detect = (\n",
    "            methyl_series.rolling(window=SMOOTH_WINDOW,\n",
    "                                  center=True, min_periods=1)\n",
    "                         .mean()\n",
    "                         .values\n",
    "        )\n",
    "    else:\n",
    "        methyl_smooth_detect = methyl_series.values\n",
    "\n",
    "    # 2b) second smoothed signal for METRICS (uses SMOOTH_WINDOW_METRICS)\n",
    "    if SMOOTH_WINDOW_METRICS > 1:\n",
    "        methyl_smooth_metrics = (\n",
    "            methyl_series.rolling(window=SMOOTH_WINDOW_METRICS,\n",
    "                                  center=True, min_periods=1)\n",
    "                         .mean()\n",
    "                         .values\n",
    "        )\n",
    "    else:\n",
    "        methyl_smooth_metrics = methyl_series.values\n",
    "\n",
    "    # 3) ── find peaks on the *detection* profile\n",
    "    peaks, props = find_peaks(methyl_smooth_detect, height=0)\n",
    "    if len(peaks) == 0:\n",
    "        return {\n",
    "            \"bootstrap_group\": bg, \"second_type\": st, \"condition\": cond,\n",
    "            \"type\": typ, \"chr_type\": chr_typ,\"peak_loc\": np.nan, \"peak_height\": np.nan,\n",
    "            \"peak_fwhm\": np.nan, \"peak_area\": np.nan,\n",
    "            \"peak_mean_m6A\": np.nan\n",
    "        }\n",
    "\n",
    "    peak_heights = props[\"peak_heights\"]\n",
    "    sorted_idxs  = np.argsort(peak_heights)[-5:]\n",
    "    top_peaks    = peaks[sorted_idxs]\n",
    "\n",
    "    # ── choose best peak (same logic as before) ────────────────────────────\n",
    "    if PEAK_SELECTION_METHOD == \"height\":\n",
    "        chosen_idx_rel = sorted_idxs[np.argmax(peak_heights[sorted_idxs])]\n",
    "    elif PEAK_SELECTION_METHOD == \"area\":\n",
    "        areas = []\n",
    "        for rel_idx in sorted_idxs:\n",
    "            p   = peaks[rel_idx]\n",
    "            h   = methyl_smooth_detect[p]\n",
    "            half = h / 2.0\n",
    "            li, ri = p, p\n",
    "            while li > 0 and methyl_smooth_detect[li] >= half: li -= 1\n",
    "            while ri < len(methyl_smooth_detect)-1 and methyl_smooth_detect[ri] >= half: ri += 1\n",
    "            areas.append(methyl_smooth_detect[li:ri+1].sum())\n",
    "        chosen_idx_rel = sorted_idxs[np.argmax(areas)]\n",
    "    elif PEAK_SELECTION_METHOD == \"closest_to_zero\":\n",
    "        locs = all_positions[top_peaks]\n",
    "        chosen_idx_rel = sorted_idxs[np.argmin(np.abs(locs))]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {PEAK_SELECTION_METHOD}\")\n",
    "\n",
    "    # 4) ── metrics ---------------------------------------------------------\n",
    "    peak_idx    = peaks[chosen_idx_rel]\n",
    "    peak_loc    = all_positions[peak_idx]\n",
    "\n",
    "    # use *metrics* smoothing for peak height\n",
    "    peak_height = methyl_smooth_metrics[peak_idx]\n",
    "\n",
    "    # FWHM boundaries based on detection smoothing\n",
    "    half_val = methyl_smooth_detect[peak_idx] / 2.0\n",
    "    li, ri   = peak_idx, peak_idx\n",
    "    while li > 0 and methyl_smooth_detect[li] >= half_val: li -= 1\n",
    "    while ri < len(methyl_smooth_detect)-1 and methyl_smooth_detect[ri] >= half_val: ri += 1\n",
    "    left_pos, right_pos = all_positions[li], all_positions[ri]\n",
    "\n",
    "    # area & mean m6A use *metrics* smoothing within those boundaries\n",
    "    window_vals   = methyl_smooth_metrics[li:ri+1]\n",
    "    peak_area     = np.nansum(window_vals)\n",
    "    peak_mean_m6A = np.nanmean(window_vals)\n",
    "\n",
    "    return {\n",
    "        \"bootstrap_group\": bg, \"second_type\": st, \"condition\": cond,\n",
    "        \"type\": typ,\n",
    "        \"chr_type\": chr_typ,\n",
    "        \"peak_loc\":      peak_loc,\n",
    "        \"peak_height\":   peak_height,\n",
    "        \"peak_fwhm\":     right_pos - left_pos,\n",
    "        \"peak_area\":     peak_area,\n",
    "        \"peak_mean_m6A\": peak_mean_m6A\n",
    "    }\n",
    "\n",
    "\n",
    "# ────────────────── 7) PARALLEL PEAK METRICS COMPUTATION ──────────────────\n",
    "print_debug(\"Starting parallel peak computation…\")\n",
    "# Ensure combos is a list of tuples for imap_unordered\n",
    "if isinstance(combos, pd.DataFrame) and not combos.empty:\n",
    "    combo_list = list(combos.itertuples(index=False, name=None))\n",
    "elif isinstance(combos, list): # If it's already a list (e.g. from previous empty handling)\n",
    "    combo_list = combos\n",
    "else: # Handle case where combos might be an empty DataFrame or other\n",
    "    combo_list = []\n",
    "\n",
    "\n",
    "if combo_list: # Only proceed if there are combos to compute\n",
    "    with mp.Pool(N_WORKERS) as pool:\n",
    "        results = list(tqdm(\n",
    "            pool.imap_unordered(compute_peak_for_combo, combo_list),\n",
    "            total=len(combo_list),\n",
    "            desc=\"Computing peaks\"\n",
    "        ))\n",
    "    peaks_df = pd.DataFrame(results)\n",
    "else: # Create an empty DataFrame with the expected columns if no results\n",
    "    peaks_df = pd.DataFrame(columns=[\"bootstrap_group\", \"second_type\", \"condition\", \"type\",\n",
    "        \"peak_loc\", \"peak_height\", \"peak_fwhm\",\n",
    "        \"peak_area\", \"peak_mean_m6A\"])\n",
    "\n",
    "print_debug(f\"Computed peak metrics for {len(peaks_df)} combos.\")\n",
    "\n",
    "# Each row corresponds to exactly one (bootstrap_group, second_type, condition, type)\n",
    "if not peaks_df.empty:\n",
    "    assert peaks_df.groupby([\"bootstrap_group\", \"second_type\", \"condition\", \"type\"]).size().max() == 1, \\\n",
    "        \"Peak computation resulted in duplicate entries for a combo.\"\n",
    "\n",
    "\n",
    "# ────────────────── 8) FILTER PEAKS FOR SELECTED CONDITIONS ──────────────────\n",
    "if not peaks_df.empty:\n",
    "    peaks_df = peaks_df[peaks_df[\"condition\"].isin(CONDITIONS_TO_INCLUDE)].reset_index(drop=True)\n",
    "print_debug(f\"Peaks_df now has {len(peaks_df)} rows after filtering\")\n",
    "\n",
    "# ────────────────── MERGE METRICS INTO DOWNSAMPLE SUMMARY ──────────────────\n",
    "# ────────────────── BUILD BOOTSTRAP-LEVEL SUMMARY ──────────────────\n",
    "if DOWNSAMPLE_BALANCE and not peaks_df.empty:\n",
    "    print_debug(\"Creating bootstrap-level down-sampling summary…\")\n",
    "\n",
    "    # A) balanced read counts *per bootstrap group*\n",
    "    bal_counts_bg = (\n",
    "        df.groupby([\"type\", \"condition\", \"bootstrap_group\"])\n",
    "          .size()\n",
    "          .reset_index(name=\"balanced_reads\")\n",
    "    )\n",
    "\n",
    "    # B) peak-level metrics (already one row per bootstrap group)\n",
    "    metrics_bg = peaks_df[[\n",
    "        \"type\", \"condition\", \"bootstrap_group\",\n",
    "        \"peak_fwhm\", \"peak_loc\",\n",
    "        \"peak_height\", \"peak_area\", \"peak_mean_m6A\"\n",
    "    ]]\n",
    "\n",
    "    # C) merge → one row per (type, condition, bootstrap_group)\n",
    "    downsample_summary_bg = (\n",
    "        bal_counts_bg.merge(metrics_bg,\n",
    "                            on=[\"type\", \"condition\", \"bootstrap_group\"],\n",
    "                            how=\"left\")\n",
    "                     .sort_values([\"type\", \"condition\", \"bootstrap_group\"])\n",
    "                     .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # D) write to disk\n",
    "    downsample_csv = os.path.join(OUTPUT_DIR, \"downsample_summary.csv\")\n",
    "    downsample_summary_bg.to_csv(downsample_csv, index=False)\n",
    "    print_debug(f\"Saved bootstrap-level summary to {downsample_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c1191a2e50d097",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### CELL 2 · PLOTTING FWHM & MEAN-m6A  ───────────────────────────────────────\n",
    "import os, itertools, numpy as np, pandas as pd\n",
    "from scipy.stats import ttest_ind\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# peaks_df, CONDITIONS_TO_INCLUDE, DEFAULT_READ_CLRS, PLOT_TEMPLATE,\n",
    "# print_debug are already in memory (created in CELL 1)\n",
    "\n",
    "# ────────────────── REUSABLE BOXPLOT MAKER ──────────────────\n",
    "def make_boxplot(\n",
    "    y_col: str,\n",
    "    title: str,\n",
    "    yaxis_label: str,\n",
    "    file_stub: str,\n",
    "    y_range=None\n",
    "):\n",
    "    \"\"\"Grouped box-plot split by (second_type × chr_type).\"\"\"\n",
    "    print_debug(f\"Generating box plot for '{y_col}' …\")\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # ── 0) create a composite x-label in peaks_df just for plotting\n",
    "    peaks_df[\"cat_x\"] = (\n",
    "        peaks_df[\"second_type\"] + \" | \" + peaks_df[\"chr_type\"]\n",
    "    )\n",
    "\n",
    "    # ── 1) traces\n",
    "    for idx, cond_val in enumerate(CONDITIONS_TO_INCLUDE):\n",
    "        color = DEFAULT_READ_CLRS[idx % len(DEFAULT_READ_CLRS)]\n",
    "        sub   = peaks_df[peaks_df[\"condition\"] == cond_val]\n",
    "        if sub.empty:\n",
    "            continue\n",
    "        fig.add_trace(\n",
    "            go.Box(\n",
    "                x=sub[\"cat_x\"], y=sub[y_col],\n",
    "                name=cond_val,\n",
    "                marker=dict(color=color, line=dict(width=1)),\n",
    "                fillcolor=\"rgba(0,0,0,0)\",\n",
    "                boxmean=False, boxpoints=\"all\",\n",
    "                jitter=0.5, pointpos=0,\n",
    "                marker_symbol=\"circle\", marker_size=2,\n",
    "                marker_color=color, marker_opacity=0.5,\n",
    "                marker_line_width=0,\n",
    "                legendgroup=cond_val,\n",
    "                alignmentgroup=\"cat_x\",\n",
    "                offsetgroup=str(idx)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # ── 2) layout\n",
    "    fig.update_layout(\n",
    "        template=PLOT_TEMPLATE,\n",
    "        title=title,\n",
    "        xaxis_title=\"Second_Type | Chr_Type\",\n",
    "        yaxis_title=yaxis_label,\n",
    "        boxmode=\"group\",\n",
    "        width=700, height=650\n",
    "    )\n",
    "    if y_range is not None:\n",
    "        fig.update_yaxes(range=y_range)\n",
    "\n",
    "    # ── 3) ordering & labels ────────────────────────────────────────\n",
    "    # order second_type first, then chr_type alphabetically\n",
    "    sec_order = [\"gt80\", \"lt80\", \"intergenic_control\"]\n",
    "    ordered_cats = []\n",
    "    for sec in sec_order:\n",
    "        chrs_here = sorted(\n",
    "            peaks_df.loc[peaks_df[\"second_type\"] == sec, \"chr_type\"].unique()\n",
    "        )\n",
    "        ordered_cats.extend([f\"{sec} | {c}\" for c in chrs_here])\n",
    "\n",
    "    # compute counts per composite category\n",
    "    comp_counts = (\n",
    "        peaks_df\n",
    "        .groupby([\"second_type\",\"chr_type\"])[\"type\"]\n",
    "        .nunique()\n",
    "        .to_dict()\n",
    "    )\n",
    "\n",
    "    # base labels for second_type\n",
    "    base_labels = {\n",
    "        \"gt80\":               \"80th ChIP Percentile\",\n",
    "        \"lt80\":               \"<80th ChIP Percentile\",\n",
    "        \"intergenic_control\": \"Intergenic\"\n",
    "    }\n",
    "\n",
    "    # build tickvals/ticktext\n",
    "    tickvals = ordered_cats\n",
    "    ticktext = []\n",
    "    for cat in ordered_cats:\n",
    "        sec, chr_t = cat.split(\" | \")\n",
    "        n = comp_counts.get((sec, chr_t), 0)\n",
    "        ticktext.append(f\"{base_labels[sec]} | {chr_t} (n={n})\")\n",
    "\n",
    "    fig.update_xaxes(\n",
    "        categoryorder=\"array\",\n",
    "        categoryarray=ordered_cats,\n",
    "        tickmode=\"array\",\n",
    "        tickvals=tickvals,\n",
    "        ticktext=ticktext\n",
    "    )\n",
    "\n",
    "    # mapping cat_x ⇢ position\n",
    "    cat_to_num = {c:i for i,c in enumerate(ordered_cats)}\n",
    "    total_conds = len(CONDITIONS_TO_INCLUDE)\n",
    "    OFFSET_STEP = 0.2\n",
    "    cond_offsets = {c:(i-(total_conds-1)/2)*OFFSET_STEP\n",
    "                    for i,c in enumerate(CONDITIONS_TO_INCLUDE)}\n",
    "\n",
    "    # ── 4) significance tests per combined category\n",
    "    shapes, stats_rows = [], []\n",
    "    for cat in ordered_cats:\n",
    "        sub_cat = peaks_df[peaks_df[\"cat_x\"] == cat]\n",
    "        if sub_cat.empty: continue\n",
    "\n",
    "        y_min, y_max = sub_cat[y_col].min(), sub_cat[y_col].max()\n",
    "        delta = max((y_max - y_min) * 0.05, 1e-3)\n",
    "        comp_idx = 0\n",
    "\n",
    "        for cond1, cond2 in itertools.combinations(CONDITIONS_TO_INCLUDE, 2):\n",
    "            v1 = sub_cat[sub_cat[\"condition\"]==cond1][y_col].dropna()\n",
    "            v2 = sub_cat[sub_cat[\"condition\"]==cond2][y_col].dropna()\n",
    "            stat, pval = (ttest_ind(v1, v2, equal_var=False)\n",
    "                          if len(v1)>1 and len(v2)>1 else (np.nan, np.nan))\n",
    "\n",
    "            lbl = (\"***\" if pval < 0.001 else\n",
    "                   \"**\"  if pval < 0.01  else\n",
    "                   \"*\"   if pval < 0.05  else \"ns\") if pd.notna(pval) else \"ns\"\n",
    "\n",
    "            if lbl in (\"*\",\"**\",\"***\"):\n",
    "                y_pos = y_max + delta + comp_idx*delta\n",
    "                bar_y = y_max + (delta/2) + comp_idx*delta\n",
    "                cat_x_num = cat_to_num[cat]\n",
    "                x1 = cat_x_num + cond_offsets[cond1]\n",
    "                x2 = cat_x_num + cond_offsets[cond2]\n",
    "                shapes.append(dict(type=\"line\", x0=x1, x1=x2,\n",
    "                                   y0=bar_y, y1=bar_y,\n",
    "                                   xref=\"x\", yref=\"y\",\n",
    "                                   line=dict(color=\"black\", width=1)))\n",
    "                fig.add_annotation(x=(x1+x2)/2, y=y_pos, text=lbl,\n",
    "                                   showarrow=False, font=dict(size=14),\n",
    "                                   xref=\"x\", yref=\"y\")\n",
    "                comp_idx += 1\n",
    "\n",
    "            stats_rows.append({\n",
    "                \"cat_x\": cat, \"condition_1\": cond1, \"condition_2\": cond2,\n",
    "                \"n1\": len(v1), \"n2\": len(v2),\n",
    "                \"mean1\": v1.mean() if len(v1) else np.nan,\n",
    "                \"mean2\": v2.mean() if len(v2) else np.nan,\n",
    "                \"t_statistic\": stat, \"p_value\": pval, \"label\": lbl\n",
    "            })\n",
    "\n",
    "    if shapes:\n",
    "        fig.update_layout(shapes=shapes)\n",
    "\n",
    "    # ── 5) save + show\n",
    "    fig.write_image(os.path.join(OUTPUT_DIR, f\"{file_stub}.png\"))\n",
    "    fig.write_image(os.path.join(OUTPUT_DIR, f\"{file_stub}.svg\"))\n",
    "    pd.DataFrame(stats_rows).to_csv(\n",
    "        os.path.join(OUTPUT_DIR, f\"{file_stub}_stats.csv\"), index=False\n",
    "    )\n",
    "    print_debug(f\"Saved {file_stub} plot & stats.\")\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "# ────────────────── PLOT 1: ORIGINAL FWHM ──────────────────\n",
    "if not peaks_df.empty:\n",
    "    make_boxplot(\n",
    "        y_col=\"peak_fwhm\",\n",
    "        title=\"FWHM Distribution by Second_Type and Condition\",\n",
    "        yaxis_label=\"Full Width at Half-Max (bp)\",\n",
    "        file_stub=\"peaks_fwhm_boxplot\",\n",
    "        y_range=[50, 650]          # keep original explicit range\n",
    "    )\n",
    "\n",
    "    # ────────────────── PLOT 2: MEAN m6A WITHIN FWHM ──────────────────\n",
    "    # (assumes 'peak_mean_m6A' added in CELL 1)\n",
    "    make_boxplot(\n",
    "        y_col=\"peak_mean_m6A\",\n",
    "        title=\"Mean m6A (within FWHM) by Second_Type and Condition\",\n",
    "        yaxis_label=\"Average m6A Fraction\",\n",
    "        file_stub=\"peaks_meanm6A_boxplot\",\n",
    "        y_range=None               # let Plotly auto-scale\n",
    "    )\n",
    "    \n",
    "        # (assumes 'peak_mean_m6A' added in CELL 1)\n",
    "    make_boxplot(\n",
    "        y_col=\"peak_height\",\n",
    "        title=\"m6A peak (within FWHM)\",\n",
    "        yaxis_label=\"m6A Peak\",\n",
    "        file_stub=\"peak_m6A_boxplot\",\n",
    "        y_range=[0,0.35],               # let Plotly auto-scale\n",
    "        \n",
    "    )\n",
    "    \n",
    "        # (assumes 'peak_mean_m6A' added in CELL 1)\n",
    "    make_boxplot(\n",
    "        y_col=\"peak_area\",\n",
    "        title=\"Peak integrated area\",\n",
    "        yaxis_label=\"Peak integrated area\",\n",
    "        file_stub=\"integrated_area_boxplot\",\n",
    "        y_range=None               # let Plotly auto-scale\n",
    "    )\n",
    "else:\n",
    "    print_debug(\"peaks_df is empty — skipping plot generation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe67d1fa5f0fe2f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import find_peaks\n",
    "import plotly.graph_objects as go\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Assumes `grouped` and `peaks_df` already exist in the notebook’s namespace,\n",
    "# computed by the previous cell.\n",
    "\n",
    "# ────────────────── DEBUG PLOT: MULTI‐TRACE PER CONDITION/TYPE ──────────────────\n",
    "\n",
    "# 1) Gather all unique (condition, type) combinations present in peaks_df\n",
    "combos = peaks_df[[\"condition\", \"type\"]].drop_duplicates().reset_index(drop=True)\n",
    "combo_tuples = [tuple(x) for x in combos.values]\n",
    "\n",
    "all_positions = np.arange(-REL_POS_RANGE, REL_POS_RANGE + 1)\n",
    "\n",
    "# We will build a single Figure with:\n",
    "# - For each (cond, typ, bootstrap_group), one line for methyl_smooth\n",
    "# - For each such combo, a vertical dashed line at peak_loc\n",
    "# - For each such combo, a horizontal dotted line at half-max (approx. using FWHM)\n",
    "\n",
    "# Keep track of which trace indices belong to each (cond, typ) pair\n",
    "trace_indices_by_combo = {}\n",
    "\n",
    "fig = go.Figure()\n",
    "trace_counter = 0\n",
    "\n",
    "for cond, typ in combo_tuples:\n",
    "    # Filter peaks_df to get only rows matching this (cond, typ)\n",
    "    sub_peaks = peaks_df[\n",
    "        (peaks_df[\"condition\"] == cond) &\n",
    "        (peaks_df[\"type\"] == typ)\n",
    "    ]\n",
    "    # If none, skip\n",
    "    if sub_peaks.empty:\n",
    "        continue\n",
    "\n",
    "    indices = []  # will collect trace indices for this combo\n",
    "\n",
    "    # For each bootstrap_group in this subset\n",
    "    for bg in sorted(sub_peaks[\"bootstrap_group\"].unique()):\n",
    "        row = sub_peaks[sub_peaks[\"bootstrap_group\"] == bg].iloc[0]\n",
    "        peak_loc = row[\"peak_loc\"]\n",
    "        peak_height = row[\"peak_height\"]\n",
    "        peak_fwhm = row[\"peak_fwhm\"]\n",
    "\n",
    "        # Reconstruct methyl_smooth array for this (bg, typ, cond)\n",
    "        sub_group = grouped[\n",
    "            (grouped[\"bootstrap_group\"] == bg)\n",
    "            & (grouped[\"condition\"] == cond)\n",
    "            & (grouped[\"type\"] == typ)\n",
    "        ]\n",
    "        methyl_series = pd.Series(0.0, index=all_positions)\n",
    "        if not sub_group.empty:\n",
    "            methyl_series.update(sub_group.set_index(\"rel_pos\")[\"frac\"])\n",
    "        methyl_array = methyl_series.values\n",
    "\n",
    "        # Apply moving‐average smoothing over SMOOTH_WINDOW bp\n",
    "        if SMOOTH_WINDOW > 1:\n",
    "            kernel = np.ones(SMOOTH_WINDOW) / SMOOTH_WINDOW\n",
    "            methyl_smooth = np.convolve(methyl_array, kernel, mode=\"same\")\n",
    "        else:\n",
    "            methyl_smooth = methyl_array\n",
    "\n",
    "        # 1a) Plot methyl_smooth vs all_positions\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=all_positions,\n",
    "                y=methyl_smooth,\n",
    "                mode=\"lines\",\n",
    "                name=f\"BG {bg}\",\n",
    "                line=dict(width=1),\n",
    "                visible=False\n",
    "            )\n",
    "        )\n",
    "        indices.append(trace_counter)\n",
    "        trace_counter += 1\n",
    "\n",
    "        # 1b) Vertical dashed line at peak_loc (from y=0 to y=peak_height)\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=[peak_loc, peak_loc],\n",
    "                y=[0, peak_height],\n",
    "                mode=\"lines\",\n",
    "                line=dict(color=\"black\", dash=\"dash\"),\n",
    "                showlegend=False,\n",
    "                visible=False\n",
    "            )\n",
    "        )\n",
    "        indices.append(trace_counter)\n",
    "        trace_counter += 1\n",
    "\n",
    "        # 1c) Horizontal dotted line at half‐max over [left_pos, right_pos]\n",
    "        # Approximate left/right as peak_loc ± peak_fwhm/2\n",
    "        half_val = peak_height / 2.0\n",
    "        left_pos = peak_loc - peak_fwhm / 2.0\n",
    "        right_pos = peak_loc + peak_fwhm / 2.0\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=[left_pos, right_pos],\n",
    "                y=[half_val, half_val],\n",
    "                mode=\"lines\",\n",
    "                line=dict(color=\"black\", dash=\"dot\"),\n",
    "                showlegend=False,\n",
    "                visible=False\n",
    "            )\n",
    "        )\n",
    "        indices.append(trace_counter)\n",
    "        trace_counter += 1\n",
    "\n",
    "    trace_indices_by_combo[(cond, typ)] = indices\n",
    "\n",
    "# 2) Build dropdown buttons so user can pick (condition, type)\n",
    "buttons = []\n",
    "for (cond, typ), idx_list in trace_indices_by_combo.items():\n",
    "    visibility = [False] * trace_counter\n",
    "    for i in idx_list:\n",
    "        visibility[i] = True\n",
    "    buttons.append(\n",
    "        dict(\n",
    "            label=f\"{cond}   |   {typ}\",\n",
    "            method=\"update\",\n",
    "            args=[\n",
    "                {\"visible\": visibility},\n",
    "                {\"title\": f\"Condition: {cond} | Type: {typ}\"}\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "# 3) Initialize the figure with the first combo visible by default\n",
    "if buttons:\n",
    "    first_combo = list(trace_indices_by_combo.keys())[0]\n",
    "    # set that combo’s traces to visible=True\n",
    "    first_vis = [False] * trace_counter\n",
    "    for i in trace_indices_by_combo[first_combo]:\n",
    "        first_vis[i] = True\n",
    "    fig.data = [trace.update(visible=first_vis[i]) for i, trace in enumerate(fig.data)]\n",
    "    fig.update_layout(title=f\"Condition: {first_combo[0]} | Type: {first_combo[1]}\")\n",
    "\n",
    "# 4) Add dropdown menu to layout\n",
    "fig.update_layout(\n",
    "    updatemenus=[\n",
    "        dict(\n",
    "            active=0,\n",
    "            buttons=buttons,\n",
    "            x=0.0,\n",
    "            y=1.1,\n",
    "            xanchor=\"left\",\n",
    "            yanchor=\"top\",\n",
    "            direction=\"down\"\n",
    "        )\n",
    "    ],\n",
    "    template=\"plotly_white\",\n",
    "    xaxis_title=\"Relative Position (bp)\",\n",
    "    yaxis_title=\"Methylation Fraction (smoothed)\",\n",
    "    width = 800\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b7a3baf29a08c0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set### Cross Correlation code\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from tqdm.auto import tqdm\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from itertools import combinations\n",
    "from scipy.stats import pearsonr\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# ────────────────── 1) CONFIGURATION ──────────────────\n",
    "CHIP_RANK_CUTOFF       = 90\n",
    "ABOVE_FLAG             = True\n",
    "TYPES_TO_INCLUDE       = [\"MOTIFS_rex48\"]   # e.g. [\"MOTIFS_rex14\", ...]\n",
    "CHR_TYPE_INCLUDE       = []\n",
    "\n",
    "# Pre‐existing conditions list\n",
    "CONDITIONS_TO_INCLUDE  = [\n",
    "    analysis_cond[0],\n",
    "    analysis_cond[1],\n",
    "    analysis_cond[2],\n",
    "    analysis_cond[3]\n",
    "]\n",
    "\n",
    "MIN_READ_LENGTH        = 500\n",
    "REQUIRE_CENTRAL        = False\n",
    "\n",
    "ZSCORE_BEFORE_AC       = False\n",
    "\n",
    "PERFORM_FILLING        = True\n",
    "MET_DOMAIN_WIDTH       = 9\n",
    "\n",
    "PERFORM_INTERP         = True\n",
    "INTERP_WINDOW          = 20\n",
    "\n",
    "RAW_SMOOTH_KIND        = \"gaussian\" # gaussian, moving\n",
    "RAW_MOVING_WIN         = 50\n",
    "RAW_GAUSS_SIGMA        = 5\n",
    "RAW_CLAMP_01           = False\n",
    "\n",
    "REL_POS_RANGE          = 2000\n",
    "\n",
    "# ── REL_POS BIN WIDTH ──\n",
    "REL_POS_BIN_WIDTH      = 200\n",
    "CENTER_BINS            = list(range(\n",
    "    -REL_POS_RANGE,\n",
    "     REL_POS_RANGE + 1,\n",
    "     REL_POS_BIN_WIDTH\n",
    "))\n",
    "\n",
    "WINDOW_SIZE            = 100\n",
    "\n",
    "MAX_READS_PER_COND     = 0\n",
    "RANDOM_STATE           = 42\n",
    "\n",
    "N_WORKERS              = max(2, mp.cpu_count() - 2)\n",
    "CHUNK_SIZE             = 10\n",
    "DEBUG                  = True  # only used to gate the “EXCESS CORR” print\n",
    "\n",
    "PLOT_TEMPLATE          = \"plotly_white\"\n",
    "BOX_WIDTH              = 0.4    # width for violin traces\n",
    "\n",
    "\n",
    "def print_debug(msg: str):\n",
    "    if DEBUG:\n",
    "        print(f\"[DEBUG] {msg}\")\n",
    "\n",
    "\n",
    "# ────────────────── 2) FILTERING ──────────────────\n",
    "chiprank_df = (\n",
    "    pd.read_csv(\"/Data1/reference/rex_chiprank.bed\", sep=r\"\\s+\")\n",
    "      .assign(type=lambda d: \"MOTIFS_\" + d[\"type\"].astype(str))\n",
    ")\n",
    "chip_rank_lookup = {\n",
    "    t: round(float(rk) * 100, 3)\n",
    "    for t, rk in zip(chiprank_df[\"type\"], chiprank_df[\"chip_rank\"])\n",
    "}\n",
    "\n",
    "keep_conds = set(CONDITIONS_TO_INCLUDE)\n",
    "keep_types = (\n",
    "    set(TYPES_TO_INCLUDE)\n",
    "    if TYPES_TO_INCLUDE\n",
    "    else {t for t, r in chip_rank_lookup.items()\n",
    "          if (r >= CHIP_RANK_CUTOFF) == ABOVE_FLAG}\n",
    ")\n",
    "keep_chr = (\n",
    "    set(CHR_TYPE_INCLUDE)\n",
    "    if CHR_TYPE_INCLUDE\n",
    "    else set(merged_df[\"chr_type\"].unique())\n",
    ")\n",
    "\n",
    "df0 = merged_df.query(\n",
    "    \"condition in @keep_conds and type in @keep_types and chr_type in @keep_chr\"\n",
    ").copy()\n",
    "print(f\"[INFO] after metadata filter: {len(df0)} reads\")\n",
    "\n",
    "mask_overlap = df0[\"rel_pos\"].apply(\n",
    "    lambda arr: ((arr >= -REL_POS_RANGE) & (arr <= REL_POS_RANGE)).any()\n",
    ")\n",
    "df0 = df0[mask_overlap].reset_index(drop=True)\n",
    "print(f\"[INFO] after overlap filter:  {len(df0)} reads\")\n",
    "\n",
    "if REQUIRE_CENTRAL:\n",
    "    half = MIN_READ_LENGTH // 2\n",
    "    mask_central = df0[\"rel_pos\"].apply(\n",
    "        lambda arr: (arr.min() <= -half) and (arr.max() >= half)\n",
    "    )\n",
    "    filtered_reads_df = df0[mask_central].reset_index(drop=True)\n",
    "else:\n",
    "    filtered_reads_df = df0[df0[\"read_length\"] >= MIN_READ_LENGTH].reset_index(drop=True)\n",
    "print(f\"[INFO] after length/central filter: {len(filtered_reads_df)} reads\")\n",
    "\n",
    "mask_valid = filtered_reads_df[\"mod_qual_bin\"].apply(\n",
    "    lambda x: isinstance(x, (list, np.ndarray)) and np.nansum(x) > 0\n",
    ")\n",
    "filtered_reads_df = filtered_reads_df[mask_valid].reset_index(drop=True)\n",
    "print(f\"[INFO] after methylation filter: {len(filtered_reads_df)} reads\")\n",
    "\n",
    "\n",
    "# ────────────────── 3) OPTIONAL DOWNSAMPLING ──────────────────\n",
    "if MAX_READS_PER_COND > 0:\n",
    "    reads_to_process = (\n",
    "        filtered_reads_df\n",
    "          .groupby(\"condition\", group_keys=False)\n",
    "          .apply(lambda df: df.sample(\n",
    "              n=min(len(df), MAX_READS_PER_COND),\n",
    "              random_state=RANDOM_STATE\n",
    "          ))\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "    print_debug(f\"Down‑sampled: {len(reads_to_process)} / {len(filtered_reads_df)} reads\")\n",
    "else:\n",
    "    reads_to_process = filtered_reads_df.copy()\n",
    "    print_debug(f\"Using all {len(reads_to_process)} reads\")\n",
    "\n",
    "records = reads_to_process.to_dict(orient=\"records\")\n",
    "\n",
    "\n",
    "# ────────────────── 4) SMOOTHING HELPERS ──────────────────\n",
    "def _fill_met_domains(arr, width):\n",
    "    idx = np.where(arr == 1)[0]\n",
    "    if len(idx) < 2:\n",
    "        return arr\n",
    "    out = arr.copy()\n",
    "    for a, b in zip(idx, idx[1:]):\n",
    "        if b - a <= width:\n",
    "            out[a : b + 1] = 1\n",
    "    return out\n",
    "\n",
    "def _mode_interpolate(arr, radius):\n",
    "    isnan = np.isnan(arr)\n",
    "    if not isnan.any():\n",
    "        return arr\n",
    "    valid = (~isnan).astype(int)\n",
    "    ones  = ((arr == 1) & ~isnan).astype(int)\n",
    "    c_val = np.concatenate(([0], np.cumsum(valid)))\n",
    "    c_one = np.concatenate(([0], np.cumsum(ones)))\n",
    "    def count(c_vec, i):\n",
    "        lo, hi = max(0, i - radius), min(len(arr) - 1, i + radius)\n",
    "        return c_vec[hi + 1] - c_vec[lo]\n",
    "    out = arr.copy()\n",
    "    for i in np.where(isnan)[0]:\n",
    "        tot = count(c_val, i)\n",
    "        out[i] = 0.0 if tot == 0 else (1.0 if count(c_one, i) > tot / 2 else 0.0)\n",
    "    return out\n",
    "\n",
    "def _apply_smoothing(y, kind, moving_win, gauss_sigma, clamp):\n",
    "    if kind == \"moving\" and moving_win > 1:\n",
    "        y = np.convolve(y, np.ones(moving_win)/moving_win, mode=\"same\")\n",
    "    elif kind == \"gaussian\" and gauss_sigma > 0:\n",
    "        y = gaussian_filter1d(y, sigma=gauss_sigma, mode=\"nearest\")\n",
    "    return np.clip(y, 0, 1) if clamp else y\n",
    "\n",
    "raw_smooth = partial(\n",
    "    _apply_smoothing,\n",
    "    kind=RAW_SMOOTH_KIND,\n",
    "    moving_win=RAW_MOVING_WIN,\n",
    "    gauss_sigma=RAW_GAUSS_SIGMA,\n",
    "    clamp=RAW_CLAMP_01\n",
    ")\n",
    "\n",
    "\n",
    "# ────────────────── 5) EXTRACT & SMOOTH WINDOWS ──────────────────\n",
    "def extract_and_smooth(rec):\n",
    "    read_id = rec[\"read_id\"]\n",
    "    cond    = rec[\"condition\"]\n",
    "    rtype   = rec[\"type\"]\n",
    "    rel_pos = np.asarray(rec[\"rel_pos\"], dtype=int)\n",
    "    signal  = np.asarray(rec[\"mod_qual_bin\"], dtype=float)\n",
    "    pos_to_idx = {p: i for i, p in enumerate(rel_pos)}\n",
    "\n",
    "    rows = []\n",
    "    min_center = rel_pos.min() + WINDOW_SIZE\n",
    "    max_center = rel_pos.max() - WINDOW_SIZE\n",
    "\n",
    "    for center in CENTER_BINS:\n",
    "        vec = np.full(2 * WINDOW_SIZE + 1, np.nan)\n",
    "\n",
    "        if center < min_center or center > max_center:\n",
    "            rows.append({\n",
    "                \"read_id\":     read_id,\n",
    "                \"type\":        rtype,\n",
    "                \"condition\":   cond,\n",
    "                \"center\":      center,\n",
    "                \"vec_smoothed\": None,\n",
    "                \"has_marks\":   False\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        for p, idx in pos_to_idx.items():\n",
    "            offset = p - center\n",
    "            if -WINDOW_SIZE <= offset <= WINDOW_SIZE:\n",
    "                vec[offset + WINDOW_SIZE] = signal[idx]\n",
    "\n",
    "        valid_pts = ~np.isnan(vec)\n",
    "        if valid_pts.sum() == 0:\n",
    "            rows.append({\n",
    "                \"read_id\":     read_id,\n",
    "                \"type\":        rtype,\n",
    "                \"condition\":   cond,\n",
    "                \"center\":      center,\n",
    "                \"vec_smoothed\": None,\n",
    "                \"has_marks\":   False\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        if PERFORM_FILLING:\n",
    "            vec = _fill_met_domains(vec, MET_DOMAIN_WIDTH)\n",
    "        if PERFORM_INTERP:\n",
    "            vec = _mode_interpolate(vec, INTERP_WINDOW)\n",
    "        vec = raw_smooth(vec)\n",
    "\n",
    "        valid_pts = ~np.isnan(vec)\n",
    "        if ZSCORE_BEFORE_AC and valid_pts.sum() > 1:\n",
    "            m, s = np.nanmean(vec[valid_pts]), np.nanstd(vec[valid_pts])\n",
    "            vec[valid_pts] = (vec[valid_pts] - m)/s if s > 0 else 0.0\n",
    "\n",
    "        rows.append({\n",
    "            \"read_id\":     read_id,\n",
    "            \"type\":        rtype,\n",
    "            \"condition\":   cond,\n",
    "            \"center\":      center,\n",
    "            \"vec_smoothed\": vec,\n",
    "            \"has_marks\":   True\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "print_debug(f\"Launching pool with {N_WORKERS} workers for smoothing\")\n",
    "with mp.Pool(N_WORKERS) as pool:\n",
    "    it = pool.imap_unordered(extract_and_smooth, records, chunksize=CHUNK_SIZE)\n",
    "    dfs = [df for df in tqdm(it, total=len(records), desc=\"smoothing\")]\n",
    "all_windows = pd.concat(dfs, ignore_index=True)\n",
    "print(f\"[INFO] all_windows shape: {all_windows.shape}\")\n",
    "\n",
    "\n",
    "# ────────────────── 6) PERCENT WITH ZERO‑METHYLATION PER BIN ──────────────────\n",
    "total_by_bin = all_windows.groupby([\"condition\", \"center\"]).size().reset_index(name=\"total\")\n",
    "no_mark_by_bin = all_windows[all_windows[\"has_marks\"] == False] \\\n",
    "                  .groupby([\"condition\", \"center\"]).size().reset_index(name=\"no_marks\")\n",
    "pct_zero_df = pd.merge(total_by_bin, no_mark_by_bin, on=[\"condition\", \"center\"], how=\"left\")\n",
    "pct_zero_df[\"no_marks\"] = pct_zero_df[\"no_marks\"].fillna(0).astype(int)\n",
    "pct_zero_df[\"pct_zero\"] = pct_zero_df[\"no_marks\"] / pct_zero_df[\"total\"]\n",
    "\n",
    "\n",
    "# ────────────────── 7) FILTER OUT NO‑MARK WINDOWS FOR CORRELATION ──────────────────\n",
    "windows_with_marks = all_windows[all_windows[\"has_marks\"]].dropna(subset=[\"vec_smoothed\"]).reset_index(drop=True)\n",
    "print_debug(f\"Windows with marks: {len(windows_with_marks)} (out of {len(all_windows)})\")\n",
    "\n",
    "\n",
    "# ────────────────── 8) PAIRWISE CROSS‑CORRELATION WITHIN (type, condition, center) ──────────────────\n",
    "def compute_pairwise_corr(group_df):\n",
    "    \"\"\"\n",
    "    Compute pairwise Pearson correlation among vec_smoothed arrays within one group.\n",
    "    Returns list of dicts: {'type', 'condition', 'center', 'corr'}.\n",
    "    \"\"\"\n",
    "    rtype  = group_df[\"type\"].iloc[0]\n",
    "    cond   = group_df[\"condition\"].iloc[0]\n",
    "    center = group_df[\"center\"].iloc[0]\n",
    "    vecs   = group_df[\"vec_smoothed\"].tolist()\n",
    "    read_ids = group_df[\"read_id\"].tolist()\n",
    "    results = []\n",
    "\n",
    "    if len(vecs) < 2:\n",
    "        return results\n",
    "\n",
    "    for i, j in combinations(range(len(vecs)), 2):\n",
    "        v1 = vecs[i]\n",
    "        v2 = vecs[j]\n",
    "        rid1 = read_ids[i]\n",
    "        rid2 = read_ids[j]\n",
    "\n",
    "        mask = (~np.isnan(v1)) & (~np.isnan(v2))\n",
    "        valid_n = mask.sum()\n",
    "        if valid_n < 2:\n",
    "            continue\n",
    "\n",
    "        raw_r = pearsonr(v1[mask], v2[mask])[0]\n",
    "\n",
    "        if raw_r > 1.0 or raw_r < -1.0:\n",
    "            # Only now do we print a debug line\n",
    "            summary_v1 = v1[mask][:5]\n",
    "            summary_v2 = v2[mask][:5]\n",
    "            print_debug(\n",
    "                f\"EXCESS CORR r={raw_r:.5f} for reads ({rid1}, {rid2}), \"\n",
    "                f\"center={center}, overlap={valid_n} \"\n",
    "                f\"first5(v1)={summary_v1}, first5(v2)={summary_v2}\"\n",
    "            )\n",
    "            r = np.clip(raw_r, -1.0, 1.0)\n",
    "            print_debug(f\"  Clamped to r={r:.5f}\")\n",
    "        else:\n",
    "            r = raw_r\n",
    "\n",
    "        results.append({\n",
    "            \"type\":      rtype,\n",
    "            \"condition\": cond,\n",
    "            \"center\":    center,\n",
    "            \"corr\":      float(r)\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "grouped = windows_with_marks.groupby([\"type\", \"condition\", \"center\"])\n",
    "pairwise_results = []\n",
    "for (_, _, _), group_df in tqdm(grouped, desc=\"pairwise groups\"):\n",
    "    pairwise_results.extend(compute_pairwise_corr(group_df))\n",
    "\n",
    "pair_corrs_df = pd.DataFrame(pairwise_results)\n",
    "print(f\"[INFO] Total pairwise correlations computed: {len(pair_corrs_df)}\")\n",
    "\n",
    "\n",
    "# ────────────────── 9) PREPARE FOR PLOTTING ──────────────────\n",
    "bins = np.arange(-REL_POS_RANGE, REL_POS_RANGE + REL_POS_BIN_WIDTH, REL_POS_BIN_WIDTH)\n",
    "bin_labels_str = [str(int(b)) for b in bins[:-1]]\n",
    "\n",
    "pair_corrs_df[\"bin_str\"] = pair_corrs_df[\"center\"].astype(int).astype(str)\n",
    "pct_zero_df[\"bin_str\"]  = pct_zero_df[\"center\"].astype(int).astype(str)\n",
    "\n",
    "\n",
    "# ────────────────── 10) COMPUTE MEDIAN & IQR FOR EACH (condition, bin) ──────────────────\n",
    "summary_stats = (\n",
    "    pair_corrs_df\n",
    "      .groupby([\"condition\", \"bin_str\"])[\"corr\"]\n",
    "      .agg([\n",
    "          (\"median\", lambda arr: float(np.nanmedian(arr))),\n",
    "          (\"q1\",     lambda arr: float(np.nanpercentile(arr, 25))),\n",
    "          (\"q3\",     lambda arr: float(np.nanpercentile(arr, 75)))\n",
    "      ])\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "stats_by_cond = {}\n",
    "for cond in summary_stats[\"condition\"].unique():\n",
    "    df_sub = summary_stats[summary_stats[\"condition\"] == cond].set_index(\"bin_str\")\n",
    "    stats_by_cond[cond] = {\n",
    "        \"median\": df_sub[\"median\"].reindex(bin_labels_str, fill_value=np.nan).tolist(),\n",
    "        \"q1\":     df_sub[\"q1\"].reindex(bin_labels_str, fill_value=np.nan).tolist(),\n",
    "        \"q3\":     df_sub[\"q3\"].reindex(bin_labels_str, fill_value=np.nan).tolist()\n",
    "    }\n",
    "\n",
    "# ────────────────── After computing pair_corrs_df ──────────────────\n",
    "\n",
    "print(\"\\n>> CORR SUMMARY BEFORE PLOTTING:\")\n",
    "print(\"   min corr =\", pair_corrs_df[\"corr\"].min())\n",
    "print(\"   max corr =\", pair_corrs_df[\"corr\"].max(), \"\\n\")\n",
    "\n",
    "# If you still see values outside [-1, 1], force‑clamp:\n",
    "pair_corrs_df[\"corr\"] = pair_corrs_df[\"corr\"].clip(-1.0, 1.0)\n",
    "\n",
    "print(\">> CORR SUMMARY AFTER CLAMP:\")\n",
    "print(\"   min corr =\", pair_corrs_df[\"corr\"].min())\n",
    "print(\"   max corr =\", pair_corrs_df[\"corr\"].max(), \"\\n\")\n",
    "\n",
    "# ────────────────── 11) VIOLIN + MEDIAN/IQR RIBBON PLOTS (STACKED) ──────────────────\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=1,\n",
    "    shared_xaxes=True,\n",
    "    vertical_spacing=0.10,\n",
    "    row_heights=[0.7, 0.3],\n",
    "    subplot_titles=[\n",
    "        \"Pairwise Cross‑Correlation Distributions by Bin & Condition (Violin)\",\n",
    "        \"Median & IQR of Pairwise r by Bin & Condition\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "for cond in sorted(pair_corrs_df[\"condition\"].unique()):\n",
    "    sub = pair_corrs_df[pair_corrs_df[\"condition\"] == cond]\n",
    "    fig.add_trace(\n",
    "        go.Violin(\n",
    "            x=sub[\"bin_str\"],\n",
    "            y=sub[\"corr\"],\n",
    "            name=cond,\n",
    "            legendgroup=cond,\n",
    "            scalegroup=cond,\n",
    "            offsetgroup=cond,\n",
    "            width=BOX_WIDTH,\n",
    "            points=False,\n",
    "            showlegend=True\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "color_map = px.colors.qualitative.Plotly\n",
    "cond_list = sorted(summary_stats[\"condition\"].unique())\n",
    "for idx, cond in enumerate(cond_list):\n",
    "    color = color_map[idx % len(color_map)]\n",
    "    stats = stats_by_cond[cond]\n",
    "    med = stats[\"median\"]\n",
    "    q1  = stats[\"q1\"]\n",
    "    q3  = stats[\"q3\"]\n",
    "\n",
    "    band_x = bin_labels_str + bin_labels_str[::-1]\n",
    "    band_y = q3 + q1[::-1]\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=band_x,\n",
    "            y=band_y,\n",
    "            fill=\"toself\",\n",
    "            fillcolor=f\"rgba({int(color[1:3],16)},{int(color[3:5],16)},{int(color[5:7],16)},0.2)\",\n",
    "            line=dict(color=\"rgba(0,0,0,0)\"),\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=bin_labels_str,\n",
    "            y=med,\n",
    "            mode=\"lines\",\n",
    "            line=dict(color=color, width=2),\n",
    "            name=cond,\n",
    "            legendgroup=cond,\n",
    "            showlegend=True\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    template=PLOT_TEMPLATE,\n",
    "    height=900,\n",
    "    width=1200\n",
    ")\n",
    "fig.update_xaxes(\n",
    "    title_text=\"Relative Position Bin (left edge, bp)\",\n",
    "    type=\"category\",\n",
    "    categoryorder=\"array\",\n",
    "    categoryarray=bin_labels_str,\n",
    "    row=2, col=1\n",
    ")\n",
    "fig.update_xaxes(visible=False, row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Pairwise Pearson r\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Median r (with IQR)\", row=2, col=1)\n",
    "fig.show()\n",
    "\n",
    "\n",
    "# ────────────────── 12) LINE PLOT: % ZERO‑MARK WINDOWS PER BIN ──────────────────\n",
    "fig2 = go.Figure()\n",
    "for idx, cond in enumerate(sorted(pct_zero_df[\"condition\"].unique())):\n",
    "    sub = pct_zero_df[pct_zero_df[\"condition\"] == cond].copy()\n",
    "    sub = sub.set_index(\"bin_str\").reindex(bin_labels_str, fill_value=0).reset_index()\n",
    "    fig2.add_trace(\n",
    "        go.Scatter(\n",
    "            x=sub[\"bin_str\"],\n",
    "            y=sub[\"pct_zero\"] * 100,\n",
    "            mode=\"lines+markers\",\n",
    "            name=cond,\n",
    "            line=dict(width=2),\n",
    "            marker=dict(size=6)\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig2.update_layout(\n",
    "    template=PLOT_TEMPLATE,\n",
    "    title=\"% Windows with Zero Methylation by Bin & Condition\",\n",
    "    xaxis=dict(\n",
    "        title=\"Relative Position Bin (left edge, bp)\",\n",
    "        type=\"category\",\n",
    "        categoryorder=\"array\",\n",
    "        categoryarray=bin_labels_str\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title=\"Fraction with Zero Marks (%)\",\n",
    "        ticksuffix=\"%\"\n",
    "    ),\n",
    "    width=1200,\n",
    "    height=500\n",
    ")\n",
    "fig2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9556967552c18412",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SINGLE FIBER CROSS CORRELATION\n",
    "#  PIPELINE  v3‑with‑mean/ideal PLOT (per‑type ideal patterns)\n",
    "#  • NaN‑aware smoothing for *all* signals (reads + per‑type mean)\n",
    "#  • automatic PAD_LEN check so every CENTER_BIN has a pattern segment\n",
    "#  • extra debug prints to verify coverage per bin & condition\n",
    "#  • UPDATED: compute & shift one “ideal pattern” per `type`, then correlate each read only to its own type’s pattern\n",
    "#\n",
    "# Assumptions: `merged_df`, `analysis_cond` and other upstream objects exist.\n",
    "# Drop‑in replacement for the previous cell.  No mock data is generated here.\n",
    "\n",
    "import numpy as np, pandas as pd, multiprocessing as mp\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.stats import pearsonr\n",
    "from functools import partial\n",
    "from tqdm.auto import tqdm\n",
    "import plotly.graph_objects as go, plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import collections, itertools, warnings\n",
    "\n",
    "# ─────────────────── CONFIG ─────────────────── #\n",
    "CHIP_RANK_CUTOFF = 80\n",
    "ABOVE_FLAG       = True\n",
    "TYPES_TO_INCLUDE = [\"MOTIFS_rex48\"]\n",
    "CHR_TYPE_INCLUDE = []\n",
    "\n",
    "CONDITIONS_TO_INCLUDE = [\n",
    "    analysis_cond[0], analysis_cond[1],\n",
    "    analysis_cond[2], analysis_cond[3]\n",
    "]\n",
    "\n",
    "MIN_READ_LENGTH = 500\n",
    "REQUIRE_CENTRAL = False\n",
    "\n",
    "RAW_SMOOTH_KIND  = \"gaussian\"      # gaussian | moving\n",
    "RAW_MOVING_WIN   = 50              # bp\n",
    "RAW_GAUSS_SIGMA  = 5               # bp\n",
    "RAW_CLAMP_01     = False           # clip result to [0,1]?\n",
    "\n",
    "REL_POS_RANGE          = 1000\n",
    "LAG_MIN, LAG_MAX       = 140, 200\n",
    "PATTERN_TYPE           = \"cosine\"     # boxcar | cosine | binary_mean\n",
    "CORE_LEN               = 147\n",
    "PAD_LEN                = 1500         # auto‑expanded if too small\n",
    "SHIFT_RANGE            = 100          # how much to shift ideal signal from center\n",
    "WINDOW_SIZE            = 100\n",
    "REL_POS_BIN_WIDTH      = 100\n",
    "\n",
    "PERFORM_FILLING        = True\n",
    "MET_DOMAIN_WIDTH       = 9\n",
    "PERFORM_INTERP         = True\n",
    "INTERP_WINDOW          = 15\n",
    "ZSCORE_BEFORE_AC       = False        # per‑read vector z‑score?\n",
    "\n",
    "MAX_READS_PER_COND     = 0\n",
    "RANDOM_STATE           = 42\n",
    "N_WORKERS              = max(2, mp.cpu_count() - 2)\n",
    "CHUNK_SIZE             = 10\n",
    "DEBUG                  = True\n",
    "\n",
    "PLOT_TEMPLATE          = \"plotly_white\"\n",
    "BOX_WIDTH              = 0.4\n",
    "\n",
    "def dbg(msg):\n",
    "    if DEBUG:\n",
    "        print(f\"[DEBUG] {msg}\")\n",
    "\n",
    "# ────────────────── 1. READ FILTERS ────────────────── #\n",
    "chiprank_df = (\n",
    "    pd.read_csv(\"/Data1/reference/rex_chiprank.bed\", sep=r\"\\s+\")\n",
    "      .assign(type=lambda d: \"MOTIFS_\" + d[\"type\"].astype(str))\n",
    ")\n",
    "chip_rank_lookup = {\n",
    "    t: round(float(rk) * 100, 3)\n",
    "    for t, rk in zip(chiprank_df[\"type\"], chiprank_df[\"chip_rank\"])\n",
    "}\n",
    "\n",
    "keep_conds = set(CONDITIONS_TO_INCLUDE)\n",
    "keep_types = (set(TYPES_TO_INCLUDE)\n",
    "              if TYPES_TO_INCLUDE else\n",
    "              {t for t, r in chip_rank_lookup.items()\n",
    "               if (r >= CHIP_RANK_CUTOFF) == ABOVE_FLAG})\n",
    "keep_chr   = (set(CHR_TYPE_INCLUDE)\n",
    "              if CHR_TYPE_INCLUDE else\n",
    "              set(merged_df[\"chr_type\"].unique()))\n",
    "\n",
    "df0 = merged_df.query(\n",
    "    \"condition in @keep_conds and type in @keep_types and chr_type in @keep_chr\"\n",
    ").copy()\n",
    "dbg(f\"after metadata filter: {len(df0)} reads\")\n",
    "\n",
    "df0 = df0[df0[\"rel_pos\"].apply(\n",
    "    lambda arr: ((arr >= -REL_POS_RANGE) & (arr <= REL_POS_RANGE)).any()\n",
    ")].reset_index(drop=True)\n",
    "dbg(f\"after overlap filter:  {len(df0)} reads\")\n",
    "\n",
    "if REQUIRE_CENTRAL:\n",
    "    half = MIN_READ_LENGTH // 2\n",
    "    df0 = df0[df0[\"rel_pos\"].apply(\n",
    "        lambda a: (a.min() <= -half) and (a.max() >= half)\n",
    "    )].reset_index(drop=True)\n",
    "else:\n",
    "    df0 = df0[df0[\"read_length\"] >= MIN_READ_LENGTH].reset_index(drop=True)\n",
    "dbg(f\"after length/central filter: {len(df0)} reads\")\n",
    "\n",
    "df0 = df0[df0[\"mod_qual_bin\"].apply(\n",
    "    lambda x: isinstance(x, (list, np.ndarray)) and np.nansum(x) > 0\n",
    ")].reset_index(drop=True)\n",
    "dbg(f\"after methylation filter: {len(df0)} reads\")\n",
    "\n",
    "reads_df = (\n",
    "    df0.groupby(\"condition\", group_keys=False)\n",
    "       .apply(lambda d: d.sample(min(len(d), MAX_READS_PER_COND),\n",
    "                                 random_state=RANDOM_STATE))\n",
    "       .reset_index(drop=True)\n",
    "    if MAX_READS_PER_COND > 0 else df0.copy()\n",
    ")\n",
    "dbg(f\"reads to process: {len(reads_df)}\")\n",
    "\n",
    "# ────────────────── 2. NAN‑AWARE SMOOTHING HELPERS ────────────────── #\n",
    "def _nan_gauss(a, sigma):\n",
    "    if sigma <= 0:\n",
    "        return a\n",
    "    mask   = ~np.isnan(a)\n",
    "    if not mask.any():\n",
    "        return a\n",
    "    filled = np.where(mask, a, 0.0)\n",
    "    sm_val = gaussian_filter1d(filled, sigma=sigma, mode=\"nearest\")\n",
    "    sm_wt  = gaussian_filter1d(mask.astype(float), sigma=sigma,\n",
    "                               mode=\"nearest\")\n",
    "    out = sm_val / sm_wt\n",
    "    out[sm_wt < 1e-6] = np.nan\n",
    "    return out\n",
    "\n",
    "def _nan_moving(a, win):\n",
    "    if win <= 1:\n",
    "        return a\n",
    "    mask   = ~np.isnan(a)\n",
    "    if not mask.any():\n",
    "        return a\n",
    "    filled = np.where(mask, a, 0.0)\n",
    "    kernel = np.ones(win, dtype=float)\n",
    "    sm_val = np.convolve(filled, kernel, \"same\")\n",
    "    sm_wt  = np.convolve(mask.astype(float), kernel, \"same\")\n",
    "    out = sm_val / sm_wt\n",
    "    out[sm_wt == 0] = np.nan\n",
    "    return out\n",
    "\n",
    "def _apply_smoothing(arr, kind=RAW_SMOOTH_KIND, moving_win=RAW_MOVING_WIN,\n",
    "                     gauss_sigma=RAW_GAUSS_SIGMA, clamp=RAW_CLAMP_01):\n",
    "    if kind == \"moving\":\n",
    "        arr = _nan_moving(arr, moving_win)\n",
    "    elif kind == \"gaussian\":\n",
    "        arr = _nan_gauss(arr, gauss_sigma)\n",
    "    if clamp:\n",
    "        arr = np.clip(arr, 0, 1)\n",
    "    return arr\n",
    "\n",
    "raw_smooth = partial(\n",
    "    _apply_smoothing,\n",
    "    kind=RAW_SMOOTH_KIND,\n",
    "    moving_win=RAW_MOVING_WIN,\n",
    "    gauss_sigma=RAW_GAUSS_SIGMA,\n",
    "    clamp=RAW_CLAMP_01\n",
    ")\n",
    "\n",
    "# ────────────────── 3. PER‑TYPE MEAN SIGNAL & lag★ ────────────────── #\n",
    "dbg(\"building per‑type mean %m6A …\")\n",
    "axis = np.arange(-REL_POS_RANGE, REL_POS_RANGE + 1)\n",
    "types_list = sorted(reads_df[\"type\"].unique())\n",
    "\n",
    "# Containers for per‑type data\n",
    "mean_sig_by_type    = {}\n",
    "lag_star_by_type    = {}\n",
    "axis_list           = axis  # same axis for all types\n",
    "\n",
    "for rtype in types_list:\n",
    "    sub_df = reads_df[reads_df[\"type\"] == rtype]\n",
    "    # accumulate methylation values at each position for this type\n",
    "    acc = {p: [] for p in axis_list}\n",
    "    for pos, met in zip(sub_df[\"rel_pos\"], sub_df[\"mod_qual_bin\"]):\n",
    "        for p, m in zip(pos, met):\n",
    "            if p in acc:\n",
    "                acc[p].append(float(m))\n",
    "    mean_raw = np.array([np.mean(acc[p]) if acc[p] else np.nan\n",
    "                         for p in axis_list])\n",
    "    mean_sig = raw_smooth(mean_raw)\n",
    "    mean_sig_by_type[rtype] = mean_sig\n",
    "\n",
    "    # compute lag★ (peak autocorrelation) for this type\n",
    "    def pearson_acf(x, L):\n",
    "        a, b = x[:-L], x[L:]\n",
    "        m    = ~np.isnan(a) & ~np.isnan(b)\n",
    "        return np.nan if m.sum() < 2 else pearsonr(a[m], b[m])[0]\n",
    "\n",
    "    lags     = np.arange(LAG_MIN, LAG_MAX + 1)\n",
    "    acf_vals = [pearson_acf(mean_sig, L) for L in lags]\n",
    "    lag_star = int(lags[np.nanargmax(acf_vals)])\n",
    "    lag_star_by_type[rtype] = lag_star\n",
    "    dbg(f\"[{rtype}] lag★ = {lag_star} bp  (r_max = {np.nanmax(acf_vals):.4f})\")\n",
    "\n",
    "# ────────────────── 4. PER‑TYPE IDEALIZED PATTERN (shifted) ────────────────── #\n",
    "# Ensure padding is sufficient\n",
    "min_half_pad = WINDOW_SIZE + SHIFT_RANGE\n",
    "if PAD_LEN // 2 < min_half_pad:\n",
    "    dbg(f\"PAD_LEN too small, increasing from {PAD_LEN} → {2 * min_half_pad}\")\n",
    "    PAD_LEN = 2 * min_half_pad\n",
    "half_pad = PAD_LEN // 2\n",
    "\n",
    "pattern_idx_full = np.arange(\n",
    "    -REL_POS_RANGE - half_pad,\n",
    "     REL_POS_RANGE + half_pad + 1\n",
    ")\n",
    "\n",
    "# Function to build a \"raw\" pattern array given lag★ and pattern type\n",
    "def build_raw_pattern(idx_full, lag_star):\n",
    "    if PATTERN_TYPE == \"boxcar\":\n",
    "        pat = np.ones_like(idx_full, float)\n",
    "        pat[np.abs(idx_full) <= CORE_LEN // 2] = 0.0\n",
    "    elif PATTERN_TYPE == \"cosine\":\n",
    "        pat = 0.5 * (1 + np.cos(2 * np.pi * idx_full / lag_star))\n",
    "    elif PATTERN_TYPE == \"binary_mean\":\n",
    "        # use median of the per‑type mean signal as threshold\n",
    "        thr  = np.nanmedian(mean_sig)  # `mean_sig` replaced below per type\n",
    "        # this branch will be replaced per type, so we won't call this directly\n",
    "        raise ValueError(\"binary_mean should be handled per type\")\n",
    "    else:\n",
    "        raise ValueError(\"bad PATTERN_TYPE\")\n",
    "    return pat\n",
    "\n",
    "# Per‑type shifted patterns & segments\n",
    "pattern_segments = {}  # nested: pattern_segments[rtype][center] -> array\n",
    "\n",
    "for rtype in types_list:\n",
    "    mean_sig = mean_sig_by_type[rtype]\n",
    "    lag_star = lag_star_by_type[rtype]\n",
    "\n",
    "    # (re)build raw pattern for this type\n",
    "    if PATTERN_TYPE == \"binary_mean\":\n",
    "        thr   = np.nanmedian(mean_sig)\n",
    "        base  = (mean_sig >= thr).astype(float)\n",
    "        rep   = int(np.ceil(len(pattern_idx_full) / len(base)))\n",
    "        pat   = np.tile(base, rep)[:len(pattern_idx_full)]\n",
    "    else:\n",
    "        pat = build_raw_pattern(pattern_idx_full, lag_star)\n",
    "\n",
    "    pat_series = pd.Series(pat, index=pattern_idx_full)\n",
    "\n",
    "    # split axis into negative / positive for separate shifting\n",
    "    neg_ax   = axis_list[axis_list < 0]\n",
    "    pos_ax   = axis_list[axis_list > 0]\n",
    "    mean_neg = mean_sig[axis_list < 0]\n",
    "    mean_pos = mean_sig[axis_list > 0]\n",
    "\n",
    "    def best_shift(p_series, ref_idx, ref_sig, rng):\n",
    "        best_r, best_s = -np.inf, 0\n",
    "        for s in range(-rng, rng + 1):\n",
    "            shifted = p_series.reindex(ref_idx + s).to_numpy()\n",
    "            m       = ~np.isnan(ref_sig) & ~np.isnan(shifted)\n",
    "            if m.sum() < 2:\n",
    "                continue\n",
    "            r = pearsonr(ref_sig[m], shifted[m])[0]\n",
    "            if r > best_r:\n",
    "                best_r, best_s = r, s\n",
    "        return best_s\n",
    "\n",
    "    shift_neg = best_shift(pat_series, neg_ax, mean_neg, SHIFT_RANGE)\n",
    "    shift_pos = best_shift(pat_series, pos_ax, mean_pos, SHIFT_RANGE)\n",
    "    dbg(f\"[{rtype}] best shifts  upstream={shift_neg} bp  downstream={shift_pos} bp\")\n",
    "\n",
    "    # apply shifts to build the final per‑type “ideal pattern”\n",
    "    pat_shift = pat_series.copy()\n",
    "    pat_shift.loc[pat_shift.index < 0] = pat_series.reindex(\n",
    "        pat_shift.index[pat_shift.index < 0] + shift_neg\n",
    "    ).values\n",
    "    pat_shift.loc[pat_shift.index > 0] = pat_series.reindex(\n",
    "        pat_shift.index[pat_shift.index > 0] + shift_pos\n",
    "    ).values\n",
    "\n",
    "    # store segments for each center bin\n",
    "    CENTER_BINS = list(range(-REL_POS_RANGE, REL_POS_RANGE + 1,\n",
    "                             REL_POS_BIN_WIDTH))\n",
    "    pattern_segments[rtype] = {}\n",
    "    for c in CENTER_BINS:\n",
    "        idx = np.arange(c - WINDOW_SIZE, c + WINDOW_SIZE + 1)\n",
    "        pattern_segments[rtype][c] = pat_shift.reindex(idx).to_numpy()\n",
    "\n",
    "    # (Optional) compute and plot per‑type mean vs. its ideal pattern\n",
    "    pattern_on_axis = pat_shift.reindex(axis_list).to_numpy()\n",
    "    mask_mp = (~np.isnan(mean_sig)) & (~np.isnan(pattern_on_axis))\n",
    "    r_mean_pattern = np.nan\n",
    "    if mask_mp.sum() >= 2:\n",
    "        r_mean_pattern = pearsonr(mean_sig[mask_mp], pattern_on_axis[mask_mp])[0]\n",
    "    dbg(f\"[{rtype}] Pearson r between mean_sig and ideal pattern = {r_mean_pattern:.4f}\")\n",
    "\n",
    "    # Example per‑type plot (comment out if not needed)\n",
    "    fig_mp = go.Figure()\n",
    "    fig_mp.add_trace(go.Scatter(\n",
    "        x=axis_list, y=mean_sig,\n",
    "        mode=\"lines\", name=f\"{rtype} mean_sig\",\n",
    "        line=dict(width=2, color=\"blue\")\n",
    "    ))\n",
    "    fig_mp.add_trace(go.Scatter(\n",
    "        x=axis_list, y=pattern_on_axis,\n",
    "        mode=\"lines\", name=f\"{rtype} ideal_pattern_shifted\",\n",
    "        line=dict(width=2, color=\"red\")\n",
    "    ))\n",
    "    fig_mp.update_layout(\n",
    "        template=PLOT_TEMPLATE,\n",
    "        title=f\"{rtype}: Mean methylation (blue) vs. Idealized pattern (red)<br>\"\n",
    "              f\"Pearson r = {r_mean_pattern:.4f}\",\n",
    "        xaxis_title=\"Relative Position (bp)\",\n",
    "        yaxis_title=\"Methylation signal (0–1)\",\n",
    "        width=900, height=400\n",
    "    )\n",
    "    fig_mp.show()\n",
    "\n",
    "# ────────────────── 5. READ WINDOW EXTRACTION ────────────────── #\n",
    "def _fill_met_domains(arr, width):\n",
    "    idx = np.where(arr == 1)[0]\n",
    "    if len(idx) < 2:\n",
    "        return arr\n",
    "    out = arr.copy()\n",
    "    for a, b in zip(idx, idx[1:]):\n",
    "        if b - a <= width:\n",
    "            out[a:b+1] = 1\n",
    "    return out\n",
    "\n",
    "def _mode_interpolate(arr, radius):\n",
    "    isnan = np.isnan(arr)\n",
    "    if not isnan.any():\n",
    "        return arr\n",
    "    valid = (~isnan).astype(int)\n",
    "    ones  = ((arr == 1) & ~isnan).astype(int)\n",
    "    c_val = np.concatenate(([0], np.cumsum(valid)))\n",
    "    c_one = np.concatenate(([0], np.cumsum(ones)))\n",
    "    def cnt(c, i):\n",
    "        lo, hi = max(0, i-radius), min(len(arr)-1, i+radius)\n",
    "        return c[hi+1] - c[lo]\n",
    "    out = arr.copy()\n",
    "    for i in np.where(isnan)[0]:\n",
    "        tot = cnt(c_val, i)\n",
    "        out[i] = 0.0 if tot == 0 else (1.0 if cnt(c_one, i) > tot/2 else 0.0)\n",
    "    return out\n",
    "\n",
    "def get_windows(rec):\n",
    "    rid, cond, rtype = rec[\"read_id\"], rec[\"condition\"], rec[\"type\"]\n",
    "    pos, met = np.asarray(rec[\"rel_pos\"], int), np.asarray(rec[\"mod_qual_bin\"], float)\n",
    "    lut = {p: i for i, p in enumerate(pos)}\n",
    "    rows = []\n",
    "    min_c, max_c = pos.min() + WINDOW_SIZE, pos.max() - WINDOW_SIZE\n",
    "    for c in CENTER_BINS:\n",
    "        vec = np.full(2 * WINDOW_SIZE + 1, np.nan)\n",
    "        if c < min_c or c > max_c:\n",
    "            rows.append((rid, cond, rtype, c, None, False))\n",
    "            continue\n",
    "        for p, i in lut.items():\n",
    "            off = p - c\n",
    "            if -WINDOW_SIZE <= off <= WINDOW_SIZE:\n",
    "                vec[off + WINDOW_SIZE] = met[i]\n",
    "        if np.isnan(vec).all():\n",
    "            rows.append((rid, cond, rtype, c, None, False))\n",
    "            continue\n",
    "        if PERFORM_FILLING:\n",
    "            vec = _fill_met_domains(vec, MET_DOMAIN_WIDTH)\n",
    "        if PERFORM_INTERP:\n",
    "            vec = _mode_interpolate(vec, INTERP_WINDOW)\n",
    "        vec = raw_smooth(vec)\n",
    "        if np.isnan(vec).all():\n",
    "            rows.append((rid, cond, rtype, c, None, False))\n",
    "            continue\n",
    "        if ZSCORE_BEFORE_AC:\n",
    "            msk = ~np.isnan(vec)\n",
    "            if msk.sum() > 1:\n",
    "                m, s = np.nanmean(vec[msk]), np.nanstd(vec[msk])\n",
    "                if s > 0:\n",
    "                    vec[msk] = (vec[msk] - m) / s\n",
    "        rows.append((rid, cond, rtype, c, vec, True))\n",
    "    return rows\n",
    "\n",
    "dbg(f\"smoothing per‑read windows ({N_WORKERS} workers)…\")\n",
    "with mp.Pool(N_WORKERS) as pool:\n",
    "    flat = list(itertools.chain.from_iterable(\n",
    "        tqdm(pool.imap_unordered(\n",
    "            get_windows, reads_df.to_dict(\"records\"),\n",
    "            chunksize=CHUNK_SIZE),\n",
    "            total=len(reads_df), desc=\"windows\")))\n",
    "all_win = pd.DataFrame(flat, columns=[\n",
    "    \"read_id\", \"condition\", \"type\", \"center\", \"vec_smoothed\", \"has_marks\"\n",
    "])\n",
    "dbg(f\"all_windows shape: {all_win.shape}\")\n",
    "\n",
    "# ────────────────── 6. ZERO‑MARK STATS ────────────────── #\n",
    "tot = all_win.groupby([\"condition\", \"center\"]).size().reset_index(name=\"total\")\n",
    "nom = (all_win[~all_win[\"has_marks\"]]\n",
    "       .groupby([\"condition\", \"center\"])\n",
    "       .size().reset_index(name=\"no_marks\"))\n",
    "pct = pd.merge(tot, nom, on=[\"condition\", \"center\"], how=\"left\")\n",
    "pct[\"no_marks\"] = pct[\"no_marks\"].fillna(0).astype(int)\n",
    "pct[\"pct_zero\"] = pct[\"no_marks\"] / pct[\"total\"]\n",
    "\n",
    "# ────────────────── 7. READ‑vs‑PATTERN CORRELATIONS (per‑type) ────────────────── #\n",
    "def corr_job(row):\n",
    "    if not row[\"has_marks\"]:\n",
    "        return None\n",
    "    vec = row[\"vec_smoothed\"]\n",
    "    rtype = row[\"type\"]\n",
    "    # look up this read's type‑specific pattern for its bin\n",
    "    pat = pattern_segments[rtype].get(row[\"center\"], None)\n",
    "    if pat is None:\n",
    "        return None\n",
    "    m   = (~np.isnan(vec)) & (~np.isnan(pat))\n",
    "    if m.sum() < 2:\n",
    "        return None\n",
    "    r = pearsonr(vec[m], pat[m])[0]\n",
    "    return {\n",
    "        \"condition\": row[\"condition\"],\n",
    "        \"type\":      rtype,\n",
    "        \"center\":    row[\"center\"],\n",
    "        \"corr\":      float(np.clip(r, -1, 1))\n",
    "    }\n",
    "\n",
    "dbg(\"computing correlations …\")\n",
    "with mp.Pool(N_WORKERS) as pool:\n",
    "    corr_dicts = list(tqdm(pool.imap_unordered(\n",
    "        corr_job, all_win.to_dict(\"records\"), chunksize=CHUNK_SIZE),\n",
    "        total=len(all_win), desc=\"corr\"))\n",
    "corr_df = pd.DataFrame([d for d in corr_dicts if d])\n",
    "dbg(f\"corr rows: {len(corr_df)}\")\n",
    "\n",
    "if DEBUG:\n",
    "    flat = (corr_df.groupby([\"type\", \"center\"])\n",
    "                   .agg(n=(\"corr\", \"size\"),\n",
    "                        std=(\"corr\", \"std\"),\n",
    "                        unique_vals=(\"corr\", lambda x: len(set(np.round(x,5)))))\n",
    "                   .reset_index())\n",
    "    print(\"\\n[DEBUG] variance of r by type × bin\")\n",
    "    print(flat)\n",
    "\n",
    "    empty = flat[flat[\"std\"].fillna(0) == 0]\n",
    "    if not empty.empty:\n",
    "        print(\"\\n[DEBUG] bins with zero variance (per type):\")\n",
    "        print(empty)\n",
    "\n",
    "# ────────────────── 8½.  BIN LABELS & axis sanity  ──────────────────\n",
    "bin_lbl = [str(c) for c in CENTER_BINS]\n",
    "corr_df[\"bin_str\"] = corr_df[\"center\"].astype(str)\n",
    "pct[\"bin_str\"]     = pct[\"center\"].astype(str)\n",
    "\n",
    "if DEBUG:\n",
    "    print(\"\\n[DEBUG] unique bin_str in corr_df:\", sorted(corr_df[\"bin_str\"].unique()))\n",
    "    missing = set(bin_lbl) - set(corr_df[\"bin_str\"])\n",
    "    if missing:\n",
    "        print(\"[DEBUG]  !!  These bins have no corr rows:\", sorted(missing))\n",
    "    extra = set(corr_df[\"bin_str\"]) - set(bin_lbl)\n",
    "    if extra:\n",
    "        print(\"[DEBUG]  !!  These bins are in corr_df but not bin_lbl:\", sorted(extra))\n",
    "\n",
    "if corr_df.empty:\n",
    "    raise RuntimeError(\"No valid correlations — adjust WINDOW_SIZE, SHIFT_RANGE, or smoothing parameters.\")\n",
    "\n",
    "# ────────────────── 9. PLOT PREPARATION  (patched) ────────────────── #\n",
    "summary = (corr_df.groupby([\"condition\", \"bin_str\"])[\"corr\"]\n",
    "           .agg(median=lambda x: float(np.nanmedian(x)),\n",
    "                q1=lambda x: float(np.nanpercentile(x, 25)),\n",
    "                q3=lambda x: float(np.nanpercentile(x, 75)))\n",
    "           .reset_index())\n",
    "\n",
    "stats = {}\n",
    "for cond in summary[\"condition\"].unique():\n",
    "    sub = (summary[summary[\"condition\"] == cond]\n",
    "           .set_index(\"bin_str\")[[\"median\", \"q1\", \"q3\"]]\n",
    "           .reindex(bin_lbl))\n",
    "    stats[cond] = {\n",
    "        \"median\": sub[\"median\"].tolist(),\n",
    "        \"q1\":     sub[\"q1\"].tolist(),\n",
    "        \"q3\":     sub[\"q3\"].tolist()\n",
    "    }\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=1, shared_xaxes=True,\n",
    "    vertical_spacing=0.10, row_heights=[0.7, 0.3],\n",
    "    subplot_titles=[\n",
    "        \"Read vs Ideal‑Pattern Correlations\",\n",
    "        \"Median ± IQR by Bin & Condition\"\n",
    "    ]\n",
    ")\n",
    "palette = px.colors.qualitative.Plotly\n",
    "for i, cond in enumerate(sorted(corr_df[\"condition\"].unique())):\n",
    "    col = palette[i % len(palette)]\n",
    "    sub = corr_df[corr_df[\"condition\"] == cond]\n",
    "    fig.add_trace(\n",
    "        go.Violin(\n",
    "            x=sub[\"bin_str\"], y=sub[\"corr\"], name=cond,\n",
    "            legendgroup=cond, offsetgroup=cond, width=BOX_WIDTH,\n",
    "            points=False, line_color=col, fillcolor=\"rgba(0,0,0,0)\",\n",
    "            box_visible=True, meanline_visible=True\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    s = stats[cond]\n",
    "    band_x = bin_lbl + bin_lbl[::-1]\n",
    "    band_y = s[\"q3\"] + s[\"q1\"][::-1]\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=band_x, y=band_y, fill=\"toself\",\n",
    "            fillcolor=\"rgba(0,0,0,0)\", line=dict(color=\"rgba(0,0,0,0)\"),\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=bin_lbl, y=s[\"median\"], mode=\"lines\",\n",
    "            line=dict(color=col, width=2),\n",
    "            name=cond, legendgroup=cond\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "\n",
    "fig.update_layout(template=PLOT_TEMPLATE, height=900, width=1200)\n",
    "fig.update_xaxes(\n",
    "    type=\"category\", categoryorder=\"array\",\n",
    "    categoryarray=bin_lbl, row=2, col=1\n",
    ")\n",
    "fig.update_xaxes(visible=False, row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Pearson r\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Median r (IQR)\", row=2, col=1)\n",
    "fig.update_xaxes(\n",
    "    type=\"category\", categoryorder=\"array\",\n",
    "    categoryarray=bin_lbl, row=1, col=1\n",
    ")\n",
    "fig.update_xaxes(\n",
    "    type=\"category\", categoryorder=\"array\",\n",
    "    categoryarray=bin_lbl, row=2, col=1\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# ────────────────── 10. FIGURE 2: % ZERO‑MARK ────────────────── #\n",
    "fig2 = go.Figure()\n",
    "for i, cond in enumerate(sorted(pct[\"condition\"].unique())):\n",
    "    sub = (\n",
    "        pct[pct[\"condition\"] == cond]\n",
    "        .set_index(\"bin_str\")\n",
    "        .reindex(bin_lbl, fill_value=0)\n",
    "        .reset_index()\n",
    "    )\n",
    "    fig2.add_trace(go.Scatter(\n",
    "        x=sub[\"bin_str\"], y=sub[\"pct_zero\"] * 100,\n",
    "        mode=\"lines+markers\", name=cond,\n",
    "        line=dict(width=2), marker=dict(size=6)\n",
    "    ))\n",
    "fig2.update_layout(\n",
    "    template=PLOT_TEMPLATE,\n",
    "    width=1200, height=500,\n",
    "    title=\"% Windows with Zero Methylation\",\n",
    "    xaxis=dict(\n",
    "        type=\"category\", categoryorder=\"array\",\n",
    "        categoryarray=bin_lbl,\n",
    "        title=\"Relative Position Bin (left edge, bp)\"\n",
    "    ),\n",
    "    yaxis=dict(title=\"Zero‑mark windows (%)\", ticksuffix=\"%\")\n",
    ")\n",
    "fig2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f49fb1235e531f2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### single fiber autocorrelation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from tqdm.auto import tqdm\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# ────────────────── 1) CONFIGURATION ──────────────────\n",
    "CHIP_RANK_CUTOFF       = 80\n",
    "ABOVE_FLAG             = True\n",
    "TYPES_TO_INCLUDE       = []\n",
    "CHR_TYPE_INCLUDE       = []\n",
    "\n",
    "# Existing list of conditions (defined earlier in the notebook)\n",
    "CONDITIONS_TO_INCLUDE  = [\n",
    "    analysis_cond[0],\n",
    "    analysis_cond[1],\n",
    "    analysis_cond[2],\n",
    "    analysis_cond[3]\n",
    "]\n",
    "\n",
    "MIN_READ_LENGTH        = 500\n",
    "REQUIRE_CENTRAL        = False\n",
    "\n",
    "ZSCORE_BEFORE_AC       = False\n",
    "\n",
    "PERFORM_FILLING        = True\n",
    "MET_DOMAIN_WIDTH       = 9\n",
    "\n",
    "PERFORM_INTERP         = True\n",
    "INTERP_WINDOW          = 5\n",
    "\n",
    "RAW_SMOOTH_KIND        = \"gaussian\"\n",
    "RAW_MOVING_WIN         = 21\n",
    "RAW_GAUSS_SIGMA        = 10\n",
    "RAW_CLAMP_01           = False\n",
    "\n",
    "REL_POS_RANGE          = 1000\n",
    "\n",
    "# ── REL_POS BIN WIDTH (compute centers every bin_width) ──\n",
    "REL_POS_BIN_WIDTH      = 100      # configurable\n",
    "CENTER_BINS            = list(range(-REL_POS_RANGE,\n",
    "                                     REL_POS_RANGE + 1,\n",
    "                                     REL_POS_BIN_WIDTH))\n",
    "\n",
    "WINDOW_SIZE            = 150       # ensure WINDOW_SIZE >= LAG_MAX * 2\n",
    "\n",
    "# ── LAG SEARCH RANGE ──\n",
    "LAG_MIN                = 140       # inclusive\n",
    "LAG_MAX                = 200       # inclusive\n",
    "\n",
    "# ── AC PEAK THRESHOLD ──\n",
    "AC_THRESHOLD           = 0.1       # configurable\n",
    "\n",
    "MAX_READS_PER_COND     = 0         # 0 → no down‑sampling\n",
    "RANDOM_STATE           = 42\n",
    "\n",
    "N_WORKERS              = max(2, mp.cpu_count() - 2)\n",
    "CHUNK_SIZE             = 10\n",
    "DEBUG                  = True\n",
    "\n",
    "# Plot styling\n",
    "PLOT_TEMPLATE          = \"plotly_white\"\n",
    "BOX_WIDTH              = 0.4       # width of each box trace\n",
    "\n",
    "\n",
    "def print_debug(msg: str):\n",
    "    if DEBUG:\n",
    "        print(f\"[DEBUG] {msg}\")\n",
    "\n",
    "\n",
    "# ────────────────── 2) FILTERING ──────────────────\n",
    "chiprank_df = (\n",
    "    pd.read_csv(\"/Data1/reference/rex_chiprank.bed\", sep=r\"\\s+\")\n",
    "      .assign(type=lambda d: \"MOTIFS_\" + d[\"type\"].astype(str))\n",
    ")\n",
    "chip_rank_lookup = {\n",
    "    t: round(float(rk) * 100, 3)\n",
    "    for t, rk in zip(chiprank_df[\"type\"], chiprank_df[\"chip_rank\"])\n",
    "}\n",
    "\n",
    "keep_conds = set(CONDITIONS_TO_INCLUDE)\n",
    "keep_types = (\n",
    "    set(TYPES_TO_INCLUDE)\n",
    "    if TYPES_TO_INCLUDE\n",
    "    else {t for t, r in chip_rank_lookup.items()\n",
    "          if (r >= CHIP_RANK_CUTOFF) == ABOVE_FLAG}\n",
    ")\n",
    "keep_chr = (\n",
    "    set(CHR_TYPE_INCLUDE)\n",
    "    if CHR_TYPE_INCLUDE\n",
    "    else set(merged_df[\"chr_type\"].unique())\n",
    ")\n",
    "\n",
    "df0 = merged_df.query(\n",
    "    \"condition in @keep_conds and type in @keep_types and chr_type in @keep_chr\"\n",
    ").copy()\n",
    "print(f\"[INFO] after metadata filter: {len(df0)} reads\")\n",
    "\n",
    "mask_overlap = df0[\"rel_pos\"].apply(\n",
    "    lambda arr: ((arr >= -REL_POS_RANGE) & (arr <= REL_POS_RANGE)).any()\n",
    ")\n",
    "df0 = df0[mask_overlap].reset_index(drop=True)\n",
    "print(f\"[INFO] after overlap filter:  {len(df0)} reads\")\n",
    "\n",
    "if REQUIRE_CENTRAL:\n",
    "    half = MIN_READ_LENGTH // 2\n",
    "    mask_central = df0[\"rel_pos\"].apply(\n",
    "        lambda arr: (arr.min() <= -half) and (arr.max() >= half)\n",
    "    )\n",
    "    filtered_reads_df = df0[mask_central].reset_index(drop=True)\n",
    "else:\n",
    "    filtered_reads_df = df0[df0[\"read_length\"] >= MIN_READ_LENGTH].reset_index(drop=True)\n",
    "print(f\"[INFO] after length/central filter: {len(filtered_reads_df)} reads\")\n",
    "\n",
    "# drop reads with no methylation data\n",
    "mask_valid = filtered_reads_df[\"mod_qual_bin\"].apply(\n",
    "    lambda x: isinstance(x, (list, np.ndarray)) and np.nansum(x) > 0\n",
    ")\n",
    "filtered_reads_df = filtered_reads_df[mask_valid].reset_index(drop=True)\n",
    "print(f\"[INFO] after methylation filter: {len(filtered_reads_df)} reads\")\n",
    "\n",
    "\n",
    "# ────────────────── 3) OPTIONAL DOWNSAMPLING ──────────────────\n",
    "if MAX_READS_PER_COND > 0:\n",
    "    reads_to_process = (\n",
    "        filtered_reads_df\n",
    "          .groupby(\"condition\", group_keys=False)\n",
    "          .apply(lambda df: df.sample(\n",
    "              n=min(len(df), MAX_READS_PER_COND),\n",
    "              random_state=RANDOM_STATE\n",
    "          ))\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "    print_debug(f\"Down‑sampled: {len(reads_to_process)} / {len(filtered_reads_df)} reads\")\n",
    "else:\n",
    "    reads_to_process = filtered_reads_df.copy()\n",
    "    print_debug(f\"Using all {len(reads_to_process)} reads\")\n",
    "\n",
    "records = reads_to_process.to_dict(orient=\"records\")\n",
    "\n",
    "\n",
    "# ────────────────── 4) SMOOTHING HELPERS ──────────────────\n",
    "def _fill_met_domains(arr, width):\n",
    "    idx = np.where(arr == 1)[0]\n",
    "    if len(idx) < 2:\n",
    "        return arr\n",
    "    out = arr.copy()\n",
    "    for a, b in zip(idx, idx[1:]):\n",
    "        if b - a <= width:\n",
    "            out[a : b + 1] = 1\n",
    "    return out\n",
    "\n",
    "def _mode_interpolate(arr, radius):\n",
    "    isnan = np.isnan(arr)\n",
    "    if not isnan.any():\n",
    "        return arr\n",
    "    valid = (~isnan).astype(int)\n",
    "    ones  = ((arr == 1) & ~isnan).astype(int)\n",
    "    c_val = np.concatenate(([0], np.cumsum(valid)))\n",
    "    c_one = np.concatenate(([0], np.cumsum(ones)))\n",
    "    def count(c_vec, i):\n",
    "        lo, hi = max(0, i - radius), min(len(arr) - 1, i + radius)\n",
    "        return c_vec[hi + 1] - c_vec[lo]\n",
    "    out = arr.copy()\n",
    "    for i in np.where(isnan)[0]:\n",
    "        tot = count(c_val, i)\n",
    "        out[i] = 0.0 if tot == 0 else (1.0 if count(c_one, i) > tot / 2 else 0.0)\n",
    "    return out\n",
    "\n",
    "def _apply_smoothing(y, kind, moving_win, gauss_sigma, clamp):\n",
    "    if kind == \"moving\" and moving_win > 1:\n",
    "        y = np.convolve(y, np.ones(moving_win)/moving_win, mode=\"same\")\n",
    "    elif kind == \"gaussian\" and gauss_sigma > 0:\n",
    "        y = gaussian_filter1d(y, sigma=gauss_sigma, mode=\"nearest\")\n",
    "    return np.clip(y, 0, 1) if clamp else y\n",
    "\n",
    "raw_smooth = partial(\n",
    "    _apply_smoothing,\n",
    "    kind=RAW_SMOOTH_KIND,\n",
    "    moving_win=RAW_MOVING_WIN,\n",
    "    gauss_sigma=RAW_GAUSS_SIGMA,\n",
    "    clamp=RAW_CLAMP_01\n",
    ")\n",
    "\n",
    "\n",
    "# ────────────────── 5) COMPUTE PEAK NRL PER WINDOW ──────────────────\n",
    "def compute_peak_nrl(rec):\n",
    "    \"\"\"\n",
    "    For each center in CENTER_BINS, extract a window around that center,\n",
    "    smooth/fill/interpolate, compute full AC[k] for k in [LAG_MIN, LAG_MAX],\n",
    "    smooth the AC curve, pick lag_star = argmax AC_smooth, and record:\n",
    "      - lag_star (NaN if below threshold)\n",
    "      - peak_val (always computed if marks exist)\n",
    "      - included flag (True if peak_val >= AC_THRESHOLD)\n",
    "      - has_marks flag (True if any methylation present in window)\n",
    "    \"\"\"\n",
    "    read_id = rec[\"read_id\"]\n",
    "    cond    = rec[\"condition\"]\n",
    "    rel_pos = np.asarray(rec[\"rel_pos\"], dtype=int)\n",
    "    signal  = np.asarray(rec[\"mod_qual_bin\"], dtype=float)\n",
    "    pos_to_idx = {p: i for i, p in enumerate(rel_pos)}\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    min_center = rel_pos.min() + WINDOW_SIZE\n",
    "    max_center = rel_pos.max() - WINDOW_SIZE\n",
    "\n",
    "    for center in CENTER_BINS:\n",
    "        lag_star = np.nan\n",
    "        peak_val = np.nan\n",
    "        included_flag = False\n",
    "        has_marks = False\n",
    "\n",
    "        # Skip windows entirely outside the read\n",
    "        if center < min_center or center > max_center:\n",
    "            rows.append({\n",
    "                \"read_id\":    read_id,\n",
    "                \"condition\":  cond,\n",
    "                \"center\":     center,\n",
    "                \"lag_star\":   lag_star,\n",
    "                \"peak_val\":   peak_val,\n",
    "                \"included\":   included_flag,\n",
    "                \"has_marks\":  False\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # Build the raw window around 'center'\n",
    "        win_len = 2 * WINDOW_SIZE + 1\n",
    "        vec = np.full(win_len, np.nan)\n",
    "        for p, idx in pos_to_idx.items():\n",
    "            offset = p - center\n",
    "            if -WINDOW_SIZE <= offset <= WINDOW_SIZE:\n",
    "                vec[offset + WINDOW_SIZE] = signal[idx]\n",
    "\n",
    "        valid_pts = ~np.isnan(vec)\n",
    "        if valid_pts.sum() == 0:\n",
    "            # No methylation marks in this window\n",
    "            rows.append({\n",
    "                \"read_id\":    read_id,\n",
    "                \"condition\":  cond,\n",
    "                \"center\":     center,\n",
    "                \"lag_star\":   lag_star,\n",
    "                \"peak_val\":   peak_val,\n",
    "                \"included\":   included_flag,\n",
    "                \"has_marks\":  False\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        has_marks = True\n",
    "\n",
    "        # Smoothing / filling / interpolation\n",
    "        if PERFORM_FILLING:\n",
    "            vec = _fill_met_domains(vec, MET_DOMAIN_WIDTH)\n",
    "        if PERFORM_INTERP:\n",
    "            vec = _mode_interpolate(vec, INTERP_WINDOW)\n",
    "        vec = raw_smooth(vec)\n",
    "\n",
    "        valid_pts = ~np.isnan(vec)\n",
    "        if ZSCORE_BEFORE_AC and valid_pts.sum() > 1:\n",
    "            m, s = np.nanmean(vec[valid_pts]), np.nanstd(vec[valid_pts])\n",
    "            vec[valid_pts] = (vec[valid_pts] - m)/s if s > 0 else 0.0\n",
    "\n",
    "        # Compute raw AC[k] for k=0..LAG_MAX\n",
    "        ac_full = np.full(LAG_MAX + 1, np.nan)\n",
    "        n = len(vec)\n",
    "        for k in range(min(LAG_MAX, n - 1) + 1):\n",
    "            x1, x2 = vec[:n - k], vec[k:]\n",
    "            vp = (~np.isnan(x1)) & (~np.isnan(x2))\n",
    "            if vp.sum() < 2:\n",
    "                continue\n",
    "            x1v, x2v = x1[vp], x2[vp]\n",
    "            num = np.sum((x1v - x1v.mean()) * (x2v - x2v.mean()))\n",
    "            den = np.sqrt(np.sum((x1v - x1v.mean())**2) *\n",
    "                          np.sum((x2v - x2v.mean())**2))\n",
    "            ac_full[k] = num/den if den > 0 else np.nan\n",
    "\n",
    "        # Extract and smooth AC segment [LAG_MIN:LAG_MAX]\n",
    "        ac_segment = ac_full[LAG_MIN : LAG_MAX + 1]\n",
    "        if not np.all(np.isnan(ac_segment)):\n",
    "            ac_smoothed = gaussian_filter1d(\n",
    "                np.nan_to_num(ac_segment, nan=0.0),\n",
    "                sigma=2,\n",
    "                mode=\"nearest\"\n",
    "            )\n",
    "            idx_peak = np.nanargmax(ac_smoothed)\n",
    "            lag_star = LAG_MIN + int(idx_peak)\n",
    "            peak_val = ac_smoothed[idx_peak]\n",
    "            included_flag = (peak_val >= AC_THRESHOLD)\n",
    "\n",
    "        rows.append({\n",
    "            \"read_id\":    read_id,\n",
    "            \"condition\":  cond,\n",
    "            \"center\":     center,\n",
    "            \"lag_star\":   lag_star,\n",
    "            \"peak_val\":   peak_val,\n",
    "            \"included\":   included_flag,\n",
    "            \"has_marks\":  has_marks\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# ────────────────── 6) MULTIPROCESSING & COLLECTION ──────────────────\n",
    "print_debug(f\"Launching pool with {N_WORKERS} workers\")\n",
    "with mp.Pool(N_WORKERS) as pool:\n",
    "    it  = pool.imap_unordered(compute_peak_nrl, records, chunksize=CHUNK_SIZE)\n",
    "    dfs = [df for df in tqdm(it, total=len(records), desc=\"reads\") if not df.empty]\n",
    "per_read_nrl = pd.concat(dfs, ignore_index=True)\n",
    "print(f\"[INFO] per_read_nrl shape: {per_read_nrl.shape}\")\n",
    "\n",
    "# ────────────────── 7) EXCLUDE WINDOWS WITH NO MARKS ──────────────────\n",
    "total_windows = len(per_read_nrl)\n",
    "no_mark_windows = (per_read_nrl[\"has_marks\"] == False).sum()\n",
    "print_debug(f\"Total windows: {total_windows}; windows with NO methylation marks: {no_mark_windows}\")\n",
    "\n",
    "# Keep only windows that had at least one methylation mark\n",
    "per_read_nrl = per_read_nrl[per_read_nrl[\"has_marks\"]].copy()\n",
    "print_debug(f\"After excluding no-mark windows: {len(per_read_nrl)} windows remain\")\n",
    "\n",
    "\n",
    "# ────────────────── 8) BINNING ──────────────────\n",
    "\n",
    "# Define bins from -REL_POS_RANGE to +REL_POS_RANGE in steps of REL_POS_BIN_WIDTH\n",
    "bins = np.arange(-REL_POS_RANGE,\n",
    "                 REL_POS_RANGE + REL_POS_BIN_WIDTH,\n",
    "                 REL_POS_BIN_WIDTH)\n",
    "\n",
    "# Create stringified labels for ordering\n",
    "bin_labels_str = [str(int(b)) for b in bins[:-1]]  # e.g. [\"-2000\",\"-1900\",...,\"1900\"]\n",
    "\n",
    "# Assign each window’s center to a numeric bin\n",
    "per_read_nrl[\"bin\"] = pd.cut(\n",
    "    per_read_nrl[\"center\"],\n",
    "    bins=bins,\n",
    "    labels=bins[:-1]\n",
    ").astype(float)\n",
    "\n",
    "# Drop any rows that did not fall into a bin (NaN)\n",
    "nan_bins = per_read_nrl[\"bin\"].isna().sum()\n",
    "print_debug(f\"Windows with center outside bin range (dropped): {nan_bins}\")\n",
    "per_read_nrl = per_read_nrl.dropna(subset=[\"bin\"]).copy()\n",
    "\n",
    "# Convert bin numeric to string for categorical plotting\n",
    "per_read_nrl[\"bin_str\"] = per_read_nrl[\"bin\"].astype(int).astype(str)\n",
    "\n",
    "print_debug(\"After bin assignment, sample data:\")\n",
    "print_debug(per_read_nrl.head())\n",
    "\n",
    "\n",
    "# ────────────────── 9) BOX PLOT (Grouped by Condition & Bin) ──────────────────\n",
    "\n",
    "# Filter to only windows that passed the AC threshold\n",
    "df_included = per_read_nrl[per_read_nrl[\"included\"]].copy()\n",
    "df_included = df_included.dropna(subset=[\"bin_str\", \"lag_star\"])\n",
    "print_debug(f\"Number of included windows (for box plot): {len(df_included)}\")\n",
    "\n",
    "# Use Plotly Express to create a grouped box plot:\n",
    "fig1 = px.box(\n",
    "    df_included,\n",
    "    x=\"bin_str\",\n",
    "    y=\"lag_star\",\n",
    "    color=\"condition\",\n",
    "    category_orders={\"bin_str\": bin_labels_str},\n",
    "    points=False\n",
    ")\n",
    "\n",
    "fig1.update_layout(\n",
    "    template=PLOT_TEMPLATE,\n",
    "    title=\"Called NRLs by Rel_Pos Bin and Condition (Box Plot)\",\n",
    "    xaxis_title=\"Relative Position Bin (left edge, bp)\",\n",
    "    yaxis_title=\"Called NRL (bp)\",\n",
    "    boxmode=\"group\",       # place boxes side-by-side\n",
    "    width=1200,\n",
    "    height=600\n",
    ")\n",
    "\n",
    "# Remove interior fill by setting each trace’s fillcolor to transparent\n",
    "for trace in fig1.data:\n",
    "    trace.update(fillcolor=\"rgba(0,0,0,0)\", line=dict(width=1))\n",
    "\n",
    "fig1.show()\n",
    "\n",
    "# ────────────────── 9b) LINE PLOT: MEDIAN NRL BY CONDITION ──────────────────\n",
    "\n",
    "# Compute the median NRL (lag_star) for each condition\n",
    "median_df = df_included.groupby(\"condition\")[\"lag_star\"].median().reset_index()\n",
    "\n",
    "fig_median = go.Figure()\n",
    "fig_median.add_trace(\n",
    "    go.Scatter(\n",
    "        x=median_df[\"condition\"],\n",
    "        y=median_df[\"lag_star\"],\n",
    "        mode=\"lines+markers\",\n",
    "        line=dict(width=2),\n",
    "        marker=dict(size=6),\n",
    "        name=\"Median NRL\"\n",
    "    )\n",
    ")\n",
    "fig_median.update_layout(\n",
    "    template=PLOT_TEMPLATE,\n",
    "    title=\"Median Called NRL by Condition\",\n",
    "    xaxis_title=\"Condition\",\n",
    "    yaxis_title=\"Median NRL (bp)\",\n",
    "    width=800,\n",
    "    height=500\n",
    ")\n",
    "fig_median.show()\n",
    "# ────────────────── NEW: LINE PLOT OF MEDIAN NRL BY BIN & CONDITION ──────────────────\n",
    "\n",
    "# Compute median lag_star per (condition, bin_str)\n",
    "median_per_bin = (\n",
    "    df_included\n",
    "      .groupby([\"condition\", \"bin_str\"])[\"lag_star\"]\n",
    "      .median()\n",
    "      .reset_index(name=\"median_lag_star\")\n",
    ")\n",
    "\n",
    "# For plotting, ensure bins are sorted in numeric order:\n",
    "median_per_bin[\"bin_int\"] = median_per_bin[\"bin_str\"].astype(int)\n",
    "median_per_bin = median_per_bin.sort_values([\"condition\", \"bin_int\"])\n",
    "\n",
    "# Build a Plotly Go figure with one line per condition\n",
    "fig_line = go.Figure()\n",
    "for cond in CONDITIONS_TO_INCLUDE:\n",
    "    subset = median_per_bin[median_per_bin[\"condition\"] == cond]\n",
    "    if subset.empty:\n",
    "        continue\n",
    "    fig_line.add_trace(\n",
    "        go.Scatter(\n",
    "            x=subset[\"bin_str\"],\n",
    "            y=subset[\"median_lag_star\"],\n",
    "            mode=\"lines+markers\",\n",
    "            name=cond,\n",
    "            line=dict(width=2),\n",
    "            marker=dict(size=6)\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig_line.update_layout(\n",
    "    template=PLOT_TEMPLATE,\n",
    "    title=\"Median Called NRL by Bin and Condition (Line Plot)\",\n",
    "    xaxis_title=\"Relative Position Bin (left edge, bp)\",\n",
    "    yaxis_title=\"Median Called NRL (bp)\",\n",
    "    width=1200,\n",
    "    height=600\n",
    ")\n",
    "\n",
    "# Enforce the same bin ordering on the x-axis\n",
    "fig_line.update_xaxes(\n",
    "    type=\"category\",\n",
    "    categoryorder=\"array\",\n",
    "    categoryarray=bin_labels_str\n",
    ")\n",
    "\n",
    "fig_line.show()\n",
    "\n",
    "\n",
    "\n",
    "# ────────────────── 10) INCLUDED vs. EXCLUDED COUNTS & PERCENTAGES ──────────────────\n",
    "\n",
    "# Aggregate counts per condition and bin\n",
    "summary = (\n",
    "    per_read_nrl\n",
    "      .groupby([\"condition\", \"bin\"])\n",
    "      .agg(\n",
    "          included_count=pd.NamedAgg(column=\"included\", aggfunc=lambda x: np.sum(x)),\n",
    "          total_count=pd.NamedAgg(column=\"included\", aggfunc=\"size\")\n",
    "      )\n",
    "      .reset_index()\n",
    ")\n",
    "summary[\"excluded_count\"] = summary[\"total_count\"] - summary[\"included_count\"]\n",
    "summary[\"pct_included\"] = summary[\"included_count\"] / summary[\"total_count\"]\n",
    "\n",
    "print_debug(\"Summary counts (first 10 rows):\")\n",
    "print_debug(summary.head(10))\n",
    "\n",
    "# Create subplots: one row per condition\n",
    "conds = sorted(summary[\"condition\"].unique())\n",
    "num_conds = len(conds)\n",
    "fig2 = make_subplots(\n",
    "    rows=num_conds,\n",
    "    cols=1,\n",
    "    shared_xaxes=True,\n",
    "    vertical_spacing=0.05,\n",
    "    specs=[[{\"secondary_y\": True}] for _ in conds],\n",
    "    subplot_titles=[f\"Condition: {c}\" for c in conds]\n",
    ")\n",
    "\n",
    "for idx, cond in enumerate(conds, start=1):\n",
    "    cond_df = summary[summary[\"condition\"] == cond].copy()\n",
    "    # Reindex so every bin is present\n",
    "    cond_df = cond_df.set_index(\"bin\").reindex(bins[:-1], fill_value=0).reset_index()\n",
    "    cond_df[\"bin_str\"] = cond_df[\"bin\"].astype(int).astype(str)\n",
    "\n",
    "    # Bar traces: Included vs. Excluded\n",
    "    fig2.add_trace(\n",
    "        go.Bar(\n",
    "            x=cond_df[\"bin_str\"],\n",
    "            y=cond_df[\"included_count\"],\n",
    "            name=\"Included\",\n",
    "            marker_color=\"green\",\n",
    "            offsetgroup=str(idx) + \"_incl\",\n",
    "            showlegend=(idx == 1),\n",
    "        ),\n",
    "        row=idx, col=1, secondary_y=False\n",
    "    )\n",
    "    fig2.add_trace(\n",
    "        go.Bar(\n",
    "            x=cond_df[\"bin_str\"],\n",
    "            y=cond_df[\"excluded_count\"],\n",
    "            name=\"Excluded\",\n",
    "            marker_color=\"lightgrey\",\n",
    "            offsetgroup=str(idx) + \"_excl\",\n",
    "            showlegend=(idx == 1),\n",
    "        ),\n",
    "        row=idx, col=1, secondary_y=False\n",
    "    )\n",
    "\n",
    "    # Line trace: % Included\n",
    "    fig2.add_trace(\n",
    "        go.Scatter(\n",
    "            x=cond_df[\"bin_str\"],\n",
    "            y=cond_df[\"pct_included\"] * 100,\n",
    "            name=\"% Included\",\n",
    "            mode=\"lines+markers\",\n",
    "            line=dict(color=\"black\", width=1),\n",
    "            marker=dict(size=4),\n",
    "            showlegend=(idx == 1),\n",
    "        ),\n",
    "        row=idx, col=1, secondary_y=True\n",
    "    )\n",
    "\n",
    "    # Y-axis labels\n",
    "    fig2.update_yaxes(title_text=\"Count\", row=idx, col=1, secondary_y=False)\n",
    "    fig2.update_yaxes(title_text=\"% Included\", row=idx, col=1, secondary_y=True)\n",
    "\n",
    "# Layout adjustments\n",
    "fig2.update_layout(\n",
    "    template=PLOT_TEMPLATE,\n",
    "    title=\"Included vs. Excluded Windows & % Included per Bin and Condition\",\n",
    "    xaxis_title=\"Relative Position Bin (left edge, bp)\",\n",
    "    barmode=\"group\",  # group bars side-by-side\n",
    "    width=1200,\n",
    "    height=300 * num_conds\n",
    ")\n",
    "\n",
    "# Enforce x-axis ordering across subplots\n",
    "fig2.update_xaxes(\n",
    "    type=\"category\",\n",
    "    categoryorder=\"array\",\n",
    "    categoryarray=bin_labels_str\n",
    ")\n",
    "\n",
    "fig2.show()\n",
    "\n",
    "\n",
    "# ────────────────── 11) HISTOGRAM OF ALL SLIDING-WINDOW MAX-AC VALUES ──────────────────\n",
    "\n",
    "# Extract all peak_val (drop NaN)\n",
    "all_peak_vals = per_read_nrl[\"peak_val\"].dropna()\n",
    "print_debug(f\"Windows with a valid peak_val: {len(all_peak_vals)} \"\n",
    "            f\"out of {len(per_read_nrl)}\")\n",
    "\n",
    "fig3 = go.Figure(\n",
    "    go.Histogram(\n",
    "        x=all_peak_vals,\n",
    "        nbinsx=50,\n",
    "        marker_color=\"blue\",\n",
    "        opacity=0.75\n",
    "    )\n",
    ")\n",
    "fig3.update_layout(\n",
    "    template=PLOT_TEMPLATE,\n",
    "    title=\"Histogram of Sliding-Window Peak AC Values (All Bins)\",\n",
    "    xaxis_title=\"Peak AC Value\",\n",
    "    yaxis_title=\"Number of Windows\",\n",
    "    width=800,\n",
    "    height=500\n",
    ")\n",
    "fig3.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b728252fd1b154",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ╔══════════════════════════════════════════════════════════════════╗\n",
    "# ║  Stratified empirical‑null simulator for template‑specific FDR   ║\n",
    "# ╚══════════════════════════════════════════════════════════════════╝\n",
    "#\n",
    "#  • Builds a *full* null distribution of correlation scores r for every\n",
    "#    (template, k‑bin [,condition]) combination.\n",
    "#  • Provides helpers:\n",
    "#        p_value_for_tpl_k(tpl, k, r [,cond])   → tail p‑value\n",
    "#        bh_threshold_for_tpl(tpl, m_tests, q)  → r‑cut‑off for BH FDR\n",
    "#\n",
    "#  Assumes `merged_df` exists and supplies `mod_qual_bin`\n",
    "#  vector‑of‑ints/NaNs for p(m6A) estimation.\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "import math, os, multiprocessing as mp\n",
    "from collections import defaultdict\n",
    "import numpy as np, pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ─────────────────────────  USER CONFIG  ─────────────────────────── #\n",
    "# ───────── BOOSTING TOGGLE ───────── #\n",
    "USE_M6A_BOOST = True          # False ⇒ every position weight = 1\n",
    "# ─────────────────────────────────── #\n",
    "\n",
    "PERFORM_FILLING       = False\n",
    "MET_DOMAIN_WIDTH      = 9\n",
    "TEMPLATES      = [\n",
    "    #(14, 298, 14),                 # T1  (175 bp total)   ← legacy default\n",
    "    (14, 147, 14),                 # T1  (175 bp total)   ← legacy default\n",
    "    (14, 166, 14),                 # T1  (175 bp total)   ← legacy default\n",
    "    # (14, 170, 14),                 # T1  (175 bp total)   ← legacy default\n",
    "    # (14, 160, 14),                 # T1  (175 bp total)   ← legacy default\n",
    "    # (14, 150, 14),                 # T1  (175 bp total)   ← legacy default\n",
    "    # (14, 140, 14),                 # T1  (175 bp total)   ← legacy default\n",
    "    # (14, 130, 14),                 # T2\n",
    "    # (14, 120, 14),                 # T2\n",
    "    (14, 110, 14),                 # T2\n",
    "    # (14,  100, 14),                 # T3\n",
    "    # (14,  90, 14),                 # T3\n",
    "    (14,  80, 14),                 # T3\n",
    "    # (14, 175, 14),                 # T1  (175 bp total)   ← legacy default\n",
    "    # (14, 165, 14),                 # T1  (175 bp total)   ← legacy default\n",
    "    # (14, 155, 14),                 # T1  (175 bp total)   ← legacy default\n",
    "    # (14, 145, 14),                 # T1  (175 bp total)   ← legacy default\n",
    "    # (14, 135, 14),                 # T2\n",
    "    # (14, 125, 14),                 # T2\n",
    "    # (14, 115, 14),                 # T2\n",
    "    # (14,  105, 14),                 # T3\n",
    "    # (14,  95, 14),                 # T3\n",
    "    # (14,  85, 14),                 # T3\n",
    "    # (10,  5, 10),                 # T3\n",
    "    # (10,  10, 10),                 # T3\n",
    "    # (10,  15, 10),                 # T3\n",
    "    ]\n",
    "# #(14,166,14), (14,147,14),(14,110,14), (14, 80,14)]               # (14,298,14), (u,core,d)\n",
    "N_SIM_PER_BIN  = 50_000                                   # Monte‑Carlo reps\n",
    "K_BIN_EDGES    = [10,12,15,17,20,22,24,26,28,30,31,32,33,34,35,36,37,38,39,40,42,44,46,48,50,53,55,60,70,80,100,150]         # inclusive edges\n",
    "PER_CONDITION  = False                                    # True ⇒ per‑cond null\n",
    "N_WORKERS      = max(2, mp.cpu_count() - 2)\n",
    "RND_SEED       = 43\n",
    "rng_global     = np.random.default_rng(RND_SEED)\n",
    "# ──────────────────────────────────────────────────────────────────── #\n",
    "# ------------------------------------------------------------------\n",
    "# >>>  INSERT THIS IN CELL 1 *before* the line that defines TPL_ONES\n",
    "# ------------------------------------------------------------------\n",
    "_tpl_cache = {}\n",
    "_tpl_stats = {}          # (u,core,d) → (tpl‑array, mean, std, half_len)\n",
    "\n",
    "for (u, core, d) in TEMPLATES:\n",
    "    tpl        = np.r_[np.ones(u), np.zeros(core), np.ones(d)]\n",
    "    tpl_m      = tpl.mean()\n",
    "    tpl_s      = tpl.std()\n",
    "    half_len   = tpl.size // 2\n",
    "    _tpl_stats[(u, core, d)] = (tpl, tpl_m, tpl_s, half_len)\n",
    "\n",
    "# handy aliases used by the caller and the plotting code\n",
    "(u1, c1, d1)  = TEMPLATES[0]\n",
    "TPL_1, TPL_M_1, TPL_S_1, TPL_HALF_1 = _tpl_stats[(u1, c1, d1)]\n",
    "CORE_SIZE_1    = c1\n",
    "LINKER_LEN_1   = u1\n",
    "\n",
    "# full‑length (linker+core+linker) for every core size\n",
    "CORE_TO_FULL_LEN = {core: 1 + core + 1 for (_, core, _) in TEMPLATES}\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "def _fill_met_domains(arr, width):\n",
    "    \"\"\"\n",
    "    Fill NaNs/0s between consecutive 1’s that are ≤ width bp apart.\n",
    "    (Same logic as the caller.)\n",
    "    \"\"\"\n",
    "    idx = np.where(arr == 1)[0]\n",
    "    if len(idx) < 2:\n",
    "        return arr\n",
    "    out = arr.copy()\n",
    "    for a, b in zip(idx, idx[1:]):\n",
    "        if b - a <= width:\n",
    "            out[a : b + 1] = 1\n",
    "    return out\n",
    "\n",
    "# ─── helper: global or per‑condition P(m6A|A) ────────────────────── #\n",
    "def _estimate_pm6A(seq_like):\n",
    "    \"\"\"\n",
    "    Concatenate the (optionally filled) vectors and return P(vec==1).\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    for x in seq_like:\n",
    "        if not isinstance(x, (list, np.ndarray)):\n",
    "            continue\n",
    "        arr = np.asarray(x, dtype=float)\n",
    "        if PERFORM_FILLING:\n",
    "            arr = _fill_met_domains(arr, MET_DOMAIN_WIDTH)\n",
    "        chunks.append(arr)\n",
    "\n",
    "    if not chunks:\n",
    "        return 0.0\n",
    "    flat = np.concatenate(chunks)\n",
    "    flat = flat[~np.isnan(flat)]\n",
    "    return np.count_nonzero(flat == 1) / len(flat)\n",
    "\n",
    "if PER_CONDITION:\n",
    "    PM6A = {c: _estimate_pm6A(g[\"mod_qual_bin\"])\n",
    "            for c, g in merged_df.groupby(\"condition\")}\n",
    "else:\n",
    "    PM6A = _estimate_pm6A(merged_df[\"mod_qual_bin\"])\n",
    "\n",
    "TPL_ONES = {tpl: tpl_arr.mean()      # f_tpl\n",
    "            for tpl, (tpl_arr, *_ ) in _tpl_stats.items()}\n",
    "# ───────── WEIGHT FOR vec==1  (derived once, reused everywhere) ───────── #\n",
    "if USE_M6A_BOOST:\n",
    "    if isinstance(PM6A, dict):\n",
    "        W1_BOOST = {c: (1.0 - p) / p for c, p in PM6A.items()}\n",
    "    else:\n",
    "        W1_BOOST = (1.0 - PM6A) / PM6A\n",
    "else:\n",
    "    W1_BOOST = 1.0 if not isinstance(PM6A, dict) else {c: 1.0 for c in PM6A}\n",
    "\n",
    "\n",
    "# ─── simulation worker (fork‑safe) ───────────────────────────────── #\n",
    "def _sim_worker(arg):\n",
    "    tpl_key, k, pm6A, bin_key, tpl_arr, tpl_m, tpl_s = arg\n",
    "    rng  = np.random.default_rng()\n",
    "\n",
    "    # ------------ simulate vector & weights ------------\n",
    "    idx  = rng.choice(tpl_arr.size, k, replace=False)\n",
    "    vec  = np.full(tpl_arr.size, np.nan, float)\n",
    "    vec[idx] = (rng.random(k) < pm6A).astype(float)\n",
    "    if PERFORM_FILLING:\n",
    "        vec = _fill_met_domains(vec, MET_DOMAIN_WIDTH)\n",
    "\n",
    "    f_tpl = TPL_ONES[tpl_key]\n",
    "    boost = 1.0 if not USE_M6A_BOOST else ((1.0 - pm6A) / pm6A) / f_tpl\n",
    "    w_all = np.where(vec == 1, boost, 1.0)\n",
    "\n",
    "    mask = ~np.isnan(vec)\n",
    "    if mask.sum() < 2:                 # <‑‑ still too little information\n",
    "        # treat as r = 0 so it counts in the null but never in the tail\n",
    "        return tpl_key, bin_key, 0.0\n",
    "\n",
    "    v = vec[mask]\n",
    "    t = tpl_arr[mask]\n",
    "    w = w_all[mask]\n",
    "\n",
    "    w_sum  = w.sum()\n",
    "    v_mean = np.dot(w, v) / w_sum\n",
    "    t_mean = np.dot(w, t) / w_sum\n",
    "    v_var  = np.dot(w, (v - v_mean) ** 2) / w_sum\n",
    "    t_var  = np.dot(w, (t - t_mean) ** 2) / w_sum\n",
    "    if v_var == 0 or t_var == 0:                     # <‑‑ constant vector\n",
    "        return tpl_key, bin_key, np.nan # 0.0   # <<< changed: r = 0.0\n",
    "    cov    = np.dot(w, (v - v_mean) * (t - t_mean)) / w_sum\n",
    "    r      = cov / math.sqrt(v_var * t_var)\n",
    "    return tpl_key, bin_key, r\n",
    "\n",
    "\n",
    "\n",
    "# ─── build empirical null for one p(m6A) value ───────────────────── #\n",
    "def _build_null(pm6A):\n",
    "    bin_ranges = list(zip(K_BIN_EDGES[:-1], K_BIN_EDGES[1:]))\n",
    "    args = []\n",
    "    for tpl_key in TEMPLATES:\n",
    "        tpl_arr = np.r_[np.ones(tpl_key[0]), np.zeros(tpl_key[1]), np.ones(tpl_key[2])]\n",
    "        tpl_m, tpl_s = tpl_arr.mean(), tpl_arr.std()\n",
    "        for bin_key in bin_ranges:\n",
    "            lo, hi = bin_key\n",
    "            eff_lo = max(1, min(lo, tpl_arr.size))\n",
    "            eff_hi = min(hi, tpl_arr.size)\n",
    "            if eff_lo > eff_hi:\n",
    "                continue\n",
    "            ks = rng_global.integers(eff_lo, eff_hi + 1, size=N_SIM_PER_BIN)\n",
    "            args.extend(\n",
    "                (tpl_key, int(k), pm6A, bin_key, tpl_arr, tpl_m, tpl_s)\n",
    "                for k in ks\n",
    "            )\n",
    "\n",
    "    accum = defaultdict(lambda: defaultdict(list))          # tpl → k‑bin → [r]\n",
    "    with mp.Pool(N_WORKERS) as pool:\n",
    "        for tpl_key, bin_key, r in tqdm(\n",
    "                pool.imap_unordered(_sim_worker, args, chunksize=2048),\n",
    "                total=len(args), desc=\"null sims\"):\n",
    "            accum[tpl_key][bin_key].append(r)\n",
    "\n",
    "    # ---------- convert lists to sorted numpy arrays ------------------------\n",
    "    return {\n",
    "        tpl_key: {\n",
    "            bin_key: np.sort(np.asarray(r_list, float))     # no NaNs anymore\n",
    "            for bin_key, r_list in bin_dict.items()\n",
    "        }\n",
    "        for tpl_key, bin_dict in accum.items()\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════ #\n",
    "#  Runtime helpers                                                   #\n",
    "# ═══════════════════════════════════════════════════════════════════ #\n",
    "def _table(cond=None):\n",
    "    if not PER_CONDITION:\n",
    "        return NULL_TABLE\n",
    "    if cond is None:\n",
    "        raise ValueError(\"cond must be specified when PER_CONDITION=True\")\n",
    "    return NULL_TABLE[cond]\n",
    "\n",
    "def _bin_for_k(k):\n",
    "    for lo, hi in zip(K_BIN_EDGES[:-1], K_BIN_EDGES[1:]):\n",
    "        if lo <= k < hi:\n",
    "            return (lo, hi)\n",
    "    return max(zip(K_BIN_EDGES[:-1], K_BIN_EDGES[1:]))\n",
    "\n",
    "def p_value_for_tpl_k(tpl_key, k, r_val, *, cond=None):\n",
    "    \"\"\"Upper‑tail p‑value under template‑specific null.\"\"\"\n",
    "    arr = _table(cond)[tpl_key][_bin_for_k(k)]\n",
    "    pos = np.searchsorted(arr, r_val, side=\"right\")\n",
    "    return 1.0 - pos / arr.size\n",
    "\n",
    "def alpha_threshold_for_tpl(tpl_key, alpha=0.001, *, cond=None):\n",
    "    \"\"\"\n",
    "    For each k‑bin, return the r value above which the upper‑tail\n",
    "    probability under the null is ≤ alpha (default 0.05).\n",
    "    \"\"\"\n",
    "    tbl = _table(cond)[tpl_key]\n",
    "    out = {}\n",
    "    for bin_key, arr in tbl.items():\n",
    "        arr = arr[~np.isnan(arr)]          # drop NaNs from zero‑var sims\n",
    "        if arr.size == 0:                  # safety for tiny k‑bins\n",
    "            out[bin_key] = np.nan\n",
    "            continue\n",
    "        idx = max(0, int(np.ceil((1.0 - alpha) * arr.size)) - 1)\n",
    "        out[bin_key] = arr[idx]\n",
    "    return out\n",
    "\n",
    "ALPHA = 0.05\n",
    "# ─── run sims (global or per condition) ──────────────────────────── #\n",
    "if PER_CONDITION:\n",
    "    NULL_TABLE = {cond: _build_null(p) for cond, p in tqdm(PM6A.items(), desc=\"conds\")}\n",
    "    ALPHA_THRESHOLDS = {\n",
    "        cond: {\n",
    "            tpl: alpha_threshold_for_tpl(tpl, ALPHA, cond=cond)\n",
    "            for tpl in TEMPLATES\n",
    "        }\n",
    "        for cond in PM6A        # PM6A.keys()\n",
    "    }\n",
    "else:\n",
    "    NULL_TABLE = _build_null(PM6A)\n",
    "    ALPHA_THRESHOLDS = {\n",
    "        tpl: alpha_threshold_for_tpl(tpl, ALPHA)\n",
    "        for tpl in TEMPLATES\n",
    "    }\n",
    "\n",
    "print(\"\\n[INFO] Template‑specific α=0.05 thresholds:\")\n",
    "if PER_CONDITION:\n",
    "    for cond, tpl_map in ALPHA_THRESHOLDS.items():\n",
    "        print(f\"\\nCondition: {cond}\")\n",
    "        for tpl_key, bin_map in tpl_map.items():\n",
    "            print(f\"  Template {tpl_key}:\")\n",
    "            for (lo, hi), r_thr in bin_map.items():\n",
    "                print(f\"    {lo:3d}–{hi:3d}: r ≥ {r_thr:.3f}\")\n",
    "else:\n",
    "    for tpl_key, bin_map in ALPHA_THRESHOLDS.items():\n",
    "        print(f\"\\nTemplate {tpl_key}:\")\n",
    "        for (lo, hi), r_thr in bin_map.items():\n",
    "            print(f\"    {lo:3d}–{hi:3d}: r ≥ {r_thr:.3f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ea82656c028721",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "#  NUC-CALLING PIPELINE v5 + 3rd-pass ACCESSIBLE FILTER (debug only on first read)\n",
    "#\n",
    "#  • 1st-pass fixed-template corr-calls   → candidate centres (no pruning)\n",
    "#  • 2nd-pass grid-search (±REFINE_WIN)  → best core/linker sizes per centre\n",
    "#  • 3rd-pass accessible-filter → remove or trim any core overlapping ANY\n",
    "#      high-confidence “open” (continuously methylated) footprint\n",
    "#  • final overlap-prune keeps highest-r → nuc_coords / nuc_centers\n",
    "#  • debug (detailed) only for the first read\n",
    "###############################################################################\n",
    "\n",
    "import os, math, itertools, warnings, multiprocessing as mp\n",
    "import numpy as np, pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.ndimage import uniform_filter1d, gaussian_filter1d\n",
    "from scipy.stats  import gaussian_kde, pearsonr\n",
    "from scipy.signal import find_peaks\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from IPython.display import display, Image   # Image used in read-scatter helper\n",
    "\n",
    "# ────────────────── 0) CONFIGURATION ──────────────────\n",
    "METHOD                = \"corr-calls\"         # \"fiber-tools\" | \"corr-calls\"\n",
    "\n",
    "# ---- 1st-pass template (narrow, fast) --------------------------\n",
    "import os, math, itertools, warnings, multiprocessing as mp\n",
    "import numpy as np, pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.ndimage import uniform_filter1d, gaussian_filter1d\n",
    "\n",
    "CORR_THRESHOLD    = thr                # min smoothed-r to accept a peak\n",
    "# ───────────────────── USER-TUNABLE CONFIG ───────────────────── #\n",
    "# ---- 1st-pass templates (can be 1 or many) ---------------------\n",
    "TEMPLATES = [                       # (u, core, d) 80, 110, 147, 166\n",
    "    # (14, 298, 14),                 # T1  (175 bp total)   ← legacy default\n",
    "    (14, 80, 14),                 # T1  (175 bp total)   ← legacy default\n",
    "    (14, 110, 14),                 # T1  (175 bp total)   ← legacy default\n",
    "    (14, 147, 14),                 # T1  (175 bp total)   ← legacy default\n",
    "    (14, 166, 14),                 # T1  (175 bp total)   ← legacy default\n",
    "    # (14, 170, 14),                 # T1  (175 bp total)   ← legacy default\n",
    "    # (14, 160, 14),                 # T1  (175 bp total)   ← legacy default\n",
    "    # (14, 150, 14),                 # T1  (175 bp total)   ← legacy default\n",
    "    # (14, 140, 14),                 # T1  (175 bp total)   ← legacy default\n",
    "    # (14, 130, 14),                 # T2\n",
    "    # (14, 120, 14),                 # T2\n",
    "    # (14, 110, 14),                 # T2\n",
    "    # (14,  100, 14),                 # T3\n",
    "    # (14,  90, 14),                 # T3\n",
    "    # (14,  80, 14),                 # T3\n",
    "    # (14, 175, 14),                 # T1  (175 bp total)   ← legacy default\n",
    "    # (14, 165, 14),                 # T1  (175 bp total)   ← legacy default\n",
    "    # (14, 155, 14),                 # T1  (175 bp total)   ← legacy default\n",
    "    # (14, 145, 14),                 # T1  (175 bp total)   ← legacy default\n",
    "    # (14, 135, 14),                 # T2\n",
    "    # (14, 125, 14),                 # T2\n",
    "    # (14, 115, 14),                 # T2\n",
    "    # (14,  105, 14),                 # T3\n",
    "    # (14,  95, 14),                 # T3\n",
    "    # (14,  85, 14),                 # T3\n",
    "    # (10,  5, 10),                 # T3\n",
    "    # (10,  10, 10),                 # T3\n",
    "    # (10,  15, 10),                 # T3\n",
    "]\n",
    "# Optional per-template correlation thresholds (falls back to CORR_THRESHOLD)\n",
    "CORR_THRESHOLDS = {\n",
    "    #(14, 298, 14): 0.2,                 # T1  (175 bp total)   ← legacy default\n",
    "    (14, 166, 14): 0.2,                 # T1  (175 bp total)   ← legacy default\n",
    "    (14, 147, 14): 0.2,   # thr1\n",
    "    (14, 110, 14): 0.2,   # thr2  – adjust as needed\n",
    "    (14,  80, 14): 0.2,   # thr3\n",
    "}\n",
    "USE_SIM_THRESHOLDS = True\n",
    "print(f\"Using {len(TEMPLATES)} templates: {TEMPLATES}\")\n",
    "print(f\"Using correlation thresholds: {CORR_THRESHOLDS}\")\n",
    "\n",
    "CORE_RANGE        = range(147, 148, 1)  # 2nd-pass grid – core sizes to test\n",
    "LINK_RANGE        = range(14, 15, 1)     # 2nd-pass grid – each linker arm\n",
    "REFINE_WIN        = 1                  # ±bp shifts tested around a centre\n",
    "MOV_AVG_WINDOW    = 10                  # smoothing 1st-pass r for peak pick\n",
    "\n",
    "ACCESS_FOOTPRINTS = [250]#25, 50, 75, 100, 150, 250]\n",
    "ACCESS_THRESHOLDS = [0.8]#,0.8, 0.8, 0.8, 0.8, 0.8]\n",
    "MIN_CORE_LEN      = min(CORE_RANGE)\n",
    "N_WORKERS         = max(2, mp.cpu_count() - 2)\n",
    "CHUNK_SIZE        = 10\n",
    "\n",
    "\n",
    "CORE_LENGTH_PENALTY = 0        # 0 → no penalty\n",
    "DEBUG             = True                # ⇢ extra logging on read 0 only\n",
    "\n",
    "MIN_OVERLAP           = 100                  # bp minimum overlap to accept an r\n",
    "MAX_NUC_SIZE          = 320                  # for 1st-pass clipping only\n",
    "# keep only cores whose final r ≥ this value\n",
    "MIN_FINAL_R = 0       # 0 → disable\n",
    "USE_SECOND_PASS   = False    # ← set to False to skip grid-search & shift\n",
    "\n",
    "# ---- misc heuristics (unchanged) ---------------------------------\n",
    "NUC_MIN               = 75\n",
    "NUC_COMBINED          = 100\n",
    "NUC_EXTEND            = 25\n",
    "ALLOWED_M6A_SKIPS     = 2\n",
    "# ---- fill / interp --------------------------------------------------------\n",
    "# ───────── BOOSTING TOGGLE ───────── #\n",
    "USE_M6A_BOOST = True          # False ⇒ every position weight = 1\n",
    "# ─────────────────────────────────── #\n",
    "\n",
    "PERFORM_FILLING       = False\n",
    "MET_DOMAIN_WIDTH      = 9\n",
    "PERFORM_INTERP        = False\n",
    "INTERP_WINDOW         = 1\n",
    "GAUSS_SMOOTH_RAW      = False\n",
    "GAUSS_SIGMA_RAW       = 1\n",
    "# ---- plotting / globals -----------------------------------------------------\n",
    "REL_POS_RANGE         = 3000\n",
    "FIG_TEMPLATE          = \"plotly_white\"\n",
    "DEFAULT_READ_CLRS     = [\"#b12537\", \"#4974a5\", \"#47B562\", \"#808080\"]\n",
    "SUBSAMPLE_READS_PLOT  = 100\n",
    "SMOOTH_WINDOW_PCT     = 50\n",
    "CENTER_HIST_BIN       = 10\n",
    "KDE_POINTS            = 10000\n",
    "DEBUG                 = True\n",
    "N_WORKERS             = max(2, mp.cpu_count() - 2)\n",
    "CHUNK_SIZE            = 10\n",
    "\n",
    "CHIP_RANK_CUTOFF      = 80\n",
    "ABOVE_FLAG            = True\n",
    "KEEP_FIRST_BED_START_ONLY = False   # True ⇒ drop all but first bed_start\n",
    "TYPES_TO_INCLUDE      = [\"MEX_motif\"]  #\"univ_nuc\",,\"MEXII_motif\" \"ALL\" for all types, or list of specific types\n",
    "CHR_TYPE_INCLUDE      = [\"X\"]#,\"Autosome\"]\n",
    "BED_STRANDS_TO_INCLUDE = [] # Filter reads by strand. Examples: [\"+\"], [\"+\",\"-\"]. [] ⇒ no filter.\n",
    "CONDITIONS_TO_INCLUDE = [\n",
    "    analysis_cond[0],\n",
    "    # analysis_cond[3],\n",
    "    # analysis_cond[4],\n",
    "    analysis_cond[5]\n",
    "]\n",
    "MIN_READ_LENGTH        = 300\n",
    "REQUIRE_CENTRAL        = False\n",
    "USE_TRIM_READS         = False    # set False to skip the ±175 bp trimming step\n",
    "\n",
    "\n",
    "def dbg(msg, always=False):\n",
    "    if DEBUG or always:\n",
    "        print(f\"[DEBUG] {msg}\")\n",
    "\n",
    "\n",
    "# ────────────────── 0a) CACHING CONFIG ──────────────────\n",
    "import os\n",
    "\n",
    "# where to stash your final DataFrame\n",
    "TEMP_DF_PATH   = \"/tmp/filtered_reads_df.pkl\"\n",
    "\n",
    "# control flags\n",
    "USE_CACHE      = False   # if True, and file exists & not forcing replace, we’ll load\n",
    "FORCE_REPLACE  = True   # if True, always rerun pipeline and overwrite existing file\n",
    "\n",
    "# ────────────────── MAIN PIPELINE WRAPPER ──────────────────\n",
    "if USE_CACHE and os.path.exists(TEMP_DF_PATH) and not FORCE_REPLACE:\n",
    "    filesize = os.path.getsize(TEMP_DF_PATH)\n",
    "    dbg(f\"Cache file found at {TEMP_DF_PATH} ({filesize} bytes), loading…\")\n",
    "    filtered_reads_df = pd.read_pickle(TEMP_DF_PATH)\n",
    "    dbg(f\"✔ Loaded DataFrame: shape={filtered_reads_df.shape}, columns={list(filtered_reads_df.columns)}\")\n",
    "else:\n",
    "    dbg(\"⚙️ Running full NUC-CALLING pipeline (cache bypassed or not present)\")\n",
    "\n",
    "    # ────────────────── 1) FILTERING (unchanged) ──────────────────\n",
    "    chiprank_df = (\n",
    "        pd.read_csv(\"/Data1/reference/rex_chiprank.bed\", sep=r\"\\s+\")\n",
    "          .assign(type=lambda d: \"MOTIFS_\" + d[\"type\"].astype(str))\n",
    "    )\n",
    "    chip_rank_lookup = {\n",
    "        t: round(float(rk) * 100, 3)\n",
    "        for t, rk in zip(chiprank_df[\"type\"], chiprank_df[\"chip_rank\"])\n",
    "    }\n",
    "\n",
    "    keep_conds = set(CONDITIONS_TO_INCLUDE)\n",
    "\n",
    "    if TYPES_TO_INCLUDE == [\"ALL\"]:\n",
    "        # pass every type that actually appears in merged_df\n",
    "        keep_types = set(merged_df[\"type\"].unique())\n",
    "        print(f\"Including all types: {keep_types}\")\n",
    "\n",
    "    elif TYPES_TO_INCLUDE:  # any non‑empty iterable (manual inclusion)\n",
    "        keep_types = set(TYPES_TO_INCLUDE)\n",
    "        print(f\"Including specified types: {keep_types}\")\n",
    "\n",
    "    else:  # fall back to chip‑rank threshold logic\n",
    "        keep_types = {\n",
    "            t for t, r in chip_rank_lookup.items()\n",
    "            if (r >= CHIP_RANK_CUTOFF) == ABOVE_FLAG\n",
    "        }\n",
    "\n",
    "    keep_chr = (\n",
    "        set(CHR_TYPE_INCLUDE)\n",
    "        if CHR_TYPE_INCLUDE\n",
    "        else set(merged_df[\"chr_type\"].unique())\n",
    "    )\n",
    "\n",
    "    # Derive allowed strands\n",
    "    if \"bed_strand\" not in merged_df.columns:\n",
    "        raise KeyError(\"Expected column 'bed_strand' not found in merged_df\")\n",
    "    if BED_STRANDS_TO_INCLUDE:\n",
    "        keep_strands = set(BED_STRANDS_TO_INCLUDE)\n",
    "        dbg(f\"Including specified bed_strands: {keep_strands}\")\n",
    "    else:\n",
    "        keep_strands = set(merged_df[\"bed_strand\"].unique())\n",
    "        dbg(f\"No bed_strand filter set; using all: {keep_strands}\")\n",
    "    \n",
    "    # Apply metadata filters, now including bed_strand\n",
    "    df0 = merged_df.query(\n",
    "        \"condition in @keep_conds and type in @keep_types and chr_type in @keep_chr and bed_strand in @keep_strands\"\n",
    "    ).copy()\n",
    "\n",
    "    # ───── NEW optional single-bed_start filter ─────\n",
    "    if KEEP_FIRST_BED_START_ONLY and not df0.empty:\n",
    "        # grab the first 50 unique bed_start values in appearance order\n",
    "        unique_beds = pd.Index(df0[\"bed_start\"]).unique()[:5]\n",
    "        pre_n = len(df0)\n",
    "        df0 = df0[df0[\"bed_start\"].isin(unique_beds)].reset_index(drop=True)\n",
    "        dbg(f\"kept only first 50 bed_starts ({len(unique_beds)} IDs): {pre_n} → {len(df0)} reads\")\n",
    "    # ───\n",
    "    dbg(f\"after metadata filter: {len(df0)} reads\")\n",
    "\n",
    "    mask_overlap = df0[\"rel_pos\"].apply(\n",
    "        lambda arr: ((arr >= -REL_POS_RANGE) & (arr <= REL_POS_RANGE)).any()\n",
    "    )\n",
    "    df0 = df0[mask_overlap].reset_index(drop=True)\n",
    "    dbg(f\"after overlap filter:  {len(df0)} reads\")\n",
    "\n",
    "    # NEW: debug surviving types after metadata filter\n",
    "    surviving_types = set(df0[\"type\"].unique())\n",
    "    dbg(f\"Types surviving metadata filter: {surviving_types}\")\n",
    "\n",
    "    dbg(f\"after metadata filter: {len(df0)} reads\")\n",
    "\n",
    "    if REQUIRE_CENTRAL:\n",
    "        half = MIN_READ_LENGTH // 2\n",
    "        # must span ±half around origin\n",
    "        mask_central = df0[\"rel_pos\"].apply(\n",
    "            lambda arr: (arr.min() <= -half) and (arr.max() >= half)\n",
    "        )\n",
    "        # must also have ≥ MIN_READ_LENGTH bases inside [-REL_POS_RANGE, REL_POS_RANGE]\n",
    "        overlap_mask = df0[\"rel_pos\"].apply(\n",
    "            lambda arr: (\n",
    "                min(arr.max(), REL_POS_RANGE)\n",
    "                - max(arr.min(), -REL_POS_RANGE)\n",
    "                + 1\n",
    "            ) >= MIN_READ_LENGTH\n",
    "        )\n",
    "        combined_mask = mask_central & overlap_mask\n",
    "        filtered_reads_df = df0[combined_mask].reset_index(drop=True)\n",
    "    else:\n",
    "        # only enforce overlap-length\n",
    "        overlap_mask = df0[\"rel_pos\"].apply(\n",
    "            lambda arr: (\n",
    "                min(arr.max(), REL_POS_RANGE)\n",
    "                - max(arr.min(), -REL_POS_RANGE)\n",
    "                + 1\n",
    "            ) >= MIN_READ_LENGTH\n",
    "        )\n",
    "        filtered_reads_df = df0[overlap_mask].reset_index(drop=True)\n",
    "    dbg(f\"after length/central filter: {len(filtered_reads_df)} reads\")\n",
    "\n",
    "    mask_valid = filtered_reads_df[\"mod_qual_bin\"].apply(\n",
    "        lambda x: isinstance(x, (list, np.ndarray)) and np.nansum(x) > 0\n",
    "    )\n",
    "    filtered_reads_df = filtered_reads_df[mask_valid].reset_index(drop=True)\n",
    "    dbg(f\"after methylation filter: {len(filtered_reads_df)} reads\")\n",
    "\n",
    "    # ────────────────── 2) HELPER FUNCTIONS ──────────────────\n",
    "    def _fill_met_domains(arr, width):\n",
    "        idx = np.where(arr == 1)[0]\n",
    "        if len(idx) < 2:\n",
    "            return arr\n",
    "        out = arr.copy()\n",
    "        for a, b in zip(idx, idx[1:]):\n",
    "            if b - a <= width:\n",
    "                out[a : b + 1] = 1\n",
    "        return out\n",
    "\n",
    "    def _mode_interpolate(arr, radius):\n",
    "        isnan = np.isnan(arr)\n",
    "        if not isnan.any():\n",
    "            return arr\n",
    "\n",
    "        valid = (~isnan).astype(int)\n",
    "        pos1  = (arr ==  1).astype(int) & ~isnan\n",
    "        neg1  = (arr == -1).astype(int) & ~isnan\n",
    "\n",
    "        c_val = np.concatenate(([0], np.cumsum(valid)))\n",
    "        c_pos = np.concatenate(([0], np.cumsum(pos1)))\n",
    "        c_neg = np.concatenate(([0], np.cumsum(neg1)))\n",
    "\n",
    "        out = arr.copy()\n",
    "        for i in np.where(isnan)[0]:\n",
    "            lo, hi = max(0, i - radius), min(len(arr) - 1, i + radius)\n",
    "            tot  = c_val[hi + 1] - c_val[lo]\n",
    "            if tot == 0:\n",
    "                out[i] = 0.0                # no data in window\n",
    "                continue\n",
    "            n_pos = c_pos[hi + 1] - c_pos[lo]\n",
    "            n_neg = c_neg[hi + 1] - c_neg[lo]\n",
    "            if n_pos > n_neg:\n",
    "                out[i] =  1.0\n",
    "            elif n_neg > n_pos:\n",
    "                out[i] = -1.0\n",
    "            else:\n",
    "                out[i] =  0.0               # tie → neutral\n",
    "        return out\n",
    "\n",
    "    def _maybe_gauss(y):\n",
    "        return gaussian_filter1d(y, GAUSS_SIGMA_RAW, mode=\"nearest\") if GAUSS_SMOOTH_RAW else y\n",
    "\n",
    "    # ────────────────── 3) FIBER-TOOLS HEURISTIC ──────────────────\n",
    "    def _fiber_find(m6a_sites):\n",
    "        nucs=[]\n",
    "        pre_idx, pre_gap, pre_added = -1, 0, False\n",
    "        idx = 0\n",
    "        while idx < len(m6a_sites):\n",
    "            cur = m6a_sites[idx]\n",
    "            gap = cur - pre_idx - 1\n",
    "            nxt = 0 if idx == len(m6a_sites) - 1 else m6a_sites[idx+1] - cur - 1\n",
    "\n",
    "            can_combine = (pre_idx >= 0 and pre_gap + gap + 1 >= NUC_COMBINED)\n",
    "            cstart = cur - pre_gap - gap - 1\n",
    "\n",
    "            if (ALLOWED_M6A_SKIPS >= 1 and can_combine and\n",
    "                (gap < NUC_MIN or pre_gap < NUC_MIN) and\n",
    "                max(gap, pre_gap) <= NUC_COMBINED + NUC_EXTEND and\n",
    "                min(gap, pre_gap) >= NUC_EXTEND):\n",
    "                if pre_added:\n",
    "                    nucs.pop()\n",
    "                nucs.append((cstart, cur - cstart))\n",
    "                pre_added = True\n",
    "\n",
    "            elif gap >= NUC_MIN:\n",
    "                nucs.append((pre_idx + 1, gap))\n",
    "                pre_added = True\n",
    "\n",
    "            elif (ALLOWED_M6A_SKIPS >= 1 and can_combine and pre_gap < NUC_MIN and\n",
    "                  not (nxt > pre_gap and nxt < NUC_MIN)):\n",
    "                nucs.append((cstart, cur - cstart))\n",
    "                pre_added = True\n",
    "\n",
    "            elif (ALLOWED_M6A_SKIPS >= 2 and pre_idx >= 0 and nxt > 0 and pre_gap < NUC_MIN and\n",
    "                  pre_gap + gap + nxt + 2 >= NUC_COMBINED and nxt < NUC_MIN and\n",
    "                  gap + nxt + 1 < NUC_COMBINED):\n",
    "                cur = m6a_sites[idx + 1]\n",
    "                nucs.append((cstart, cur - cstart))\n",
    "                idx += 1\n",
    "                pre_added = True\n",
    "            else:\n",
    "                pre_added = False\n",
    "\n",
    "            pre_gap, pre_idx = gap, cur\n",
    "            idx += 1\n",
    "        return nucs\n",
    "\n",
    "    # ────────────────── TEMPLATE PRE-COMPUTE ────────────────── #\n",
    "    _tpl_cache = {}\n",
    "    _tpl_stats = {}          # (u,core,d) → (tpl, mean, std, half_len)\n",
    "\n",
    "    for (u, core, d) in TEMPLATES:\n",
    "        tpl = np.r_[ np.ones(u), np.zeros(core), np.ones(d) ]\n",
    "        tpl_m, tpl_s = tpl.mean(), tpl.std()\n",
    "        half_len     = tpl.size // 2\n",
    "        _tpl_stats[(u, core, d)] = (tpl, tpl_m, tpl_s, half_len)\n",
    "\n",
    "    (u1, c1, d1)      = TEMPLATES[0]\n",
    "    TPL_1, TPL_M_1, TPL_S_1, TPL_HALF_1 = _tpl_stats[(u1, c1, d1)]\n",
    "    CORE_SIZE_1       = c1     # alias for legacy constant\n",
    "    LINKER_LEN_1      = u1\n",
    "\n",
    "    # CORE_TO_FULL_LEN = {core: u + core + d for (u, core, d) in TEMPLATES}\n",
    "    CORE_TO_FULL_LEN = {core: 1 + core + 1 for (u, core, d) in TEMPLATES}\n",
    "\n",
    "    def _greedy_non_overlapping(peaks):\n",
    "        \"\"\"\n",
    "        peaks : list[(centre_bp, core_len_bp, r_val, *extra)]\n",
    "                *extra is ignored (allows passing tpl_key etc.)\n",
    "\n",
    "        Returns a list of (centre, core_len, r_val) that:\n",
    "          1.  are sorted by genomic position,\n",
    "          2.  are ≥ ½(sum(full_lengths)) apart,\n",
    "          3.  keep the highest‑r within each collision window\n",
    "              (ties → longer core wins, but that can only happen\n",
    "               inside the per‑centre replacement clause below).\n",
    "\n",
    "        Logic:\n",
    "          – candidates are pre‑sorted by (centre, r) ascending;\n",
    "          – we scan left→right, keeping a `current_best`;\n",
    "          – if the next peak collides, we replace current_best\n",
    "            only when it has higher r (or equal r but longer core).\n",
    "        \"\"\"\n",
    "        if not peaks:\n",
    "            return []\n",
    "\n",
    "        # 1) sort as requested: centre asc, then r asc\n",
    "        peaks_sorted = sorted(peaks, key=lambda t: (t[0], t[2]))\n",
    "\n",
    "        winners = []\n",
    "        cur_cen, cur_core, cur_r = peaks_sorted[0][:3]\n",
    "\n",
    "        for cen, core, r, *_ in peaks_sorted[1:]:\n",
    "            full_cur  = CORE_TO_FULL_LEN[cur_core]\n",
    "            full_next = CORE_TO_FULL_LEN[core]\n",
    "            clash     = abs(cen - cur_cen) < 0.5 * (full_cur + full_next)\n",
    "\n",
    "            if clash:\n",
    "                # keep the better of the two\n",
    "                if (r > cur_r) or (r == cur_r and core > cur_core):\n",
    "                    cur_cen, cur_core, cur_r = cen, core, r\n",
    "            else:\n",
    "                winners.append((cur_cen, cur_core, cur_r))\n",
    "                cur_cen, cur_core, cur_r = cen, core, r\n",
    "\n",
    "        winners.append((cur_cen, cur_core, cur_r))\n",
    "        return winners\n",
    "\n",
    "\n",
    "    # ───────────── shared sliding-r helper ───────────── #\n",
    "    def _corr_window_nan(vec, tpl, *, debug=False, cond=None):\n",
    "        \"\"\"\n",
    "        Weighted Pearson r — weights = W1_BOOST if vec==1 else 1.\n",
    "        vec : 1 / 0 / NaN\n",
    "        tpl : 1 or 0\n",
    "        cond: optional condition name (for per‑condition weighting)\n",
    "        \"\"\"\n",
    "        win = tpl.size\n",
    "        n   = vec.size\n",
    "        if n < win:\n",
    "            return np.empty(0, float)\n",
    "\n",
    "        # pre‑compute weight vector once for the whole read\n",
    "        if USE_M6A_BOOST:\n",
    "            base  = W1_BOOST[cond] if isinstance(W1_BOOST, dict) else W1_BOOST\n",
    "            boost = base / tpl.mean()\n",
    "            w_vec = np.where(vec == 1, boost, 1.0)\n",
    "        else:\n",
    "            w_vec = np.ones_like(vec, dtype=float)\n",
    "\n",
    "        out = np.full(n - win + 1, np.nan, dtype=float)\n",
    "        for i in range(len(out)):\n",
    "            mask   = ~np.isnan(vec[i : i + win])\n",
    "            k      = mask.sum()\n",
    "            if k < 2:\n",
    "                continue\n",
    "\n",
    "            v      = vec[i : i + win][mask]\n",
    "            t      = tpl[mask]\n",
    "            w      = w_vec[i : i + win][mask]\n",
    "\n",
    "            w_sum  = w.sum()\n",
    "            v_mean = np.dot(w, v) / w_sum\n",
    "            t_mean = np.dot(w, t) / w_sum\n",
    "\n",
    "            v_var  = np.dot(w, (v - v_mean) ** 2) / w_sum\n",
    "            t_var  = np.dot(w, (t - t_mean) ** 2) / w_sum\n",
    "            if v_var == 0 or t_var == 0:\n",
    "                out[i] = 0.0\n",
    "                continue\n",
    "\n",
    "            cov    = np.dot(w, (v - v_mean) * (t - t_mean)) / w_sum\n",
    "            out[i] = cov / math.sqrt(v_var * t_var)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    # overwrite the old version\n",
    "    def _corr_window(vec, tpl, tpl_m=None, tpl_s=None, *, cond=None):\n",
    "        # pass debug=True to get a printout when you hit a too-short case\n",
    "        return _corr_window_nan(vec, tpl, cond=cond)\n",
    "\n",
    "\n",
    "    # 1st pass  (k-aware threshold)\n",
    "    def _corr_call_first_pass(vec, pos_min, cond=None):\n",
    "        \"\"\"\n",
    "        Multi-template first pass.\n",
    "        Returns list of (centre_abs, r_val, core_len).\n",
    "        Tie-break rule: if two centres are within ½(sum of core lenses) bp,\n",
    "        keep the one with higher r; on exact-r ties, keep the longer core.\n",
    "        \"\"\"\n",
    "        candidates = []                    # (centre, r, core, tpl_key)\n",
    "\n",
    "        for tpl_key, (tpl, tpl_m, tpl_s, half_len) in _tpl_stats.items():\n",
    "            r_raw = _corr_window(vec, tpl, tpl_m, tpl_s, cond=cond)\n",
    "            if r_raw.size == 0:\n",
    "                continue\n",
    "            r_smooth = uniform_filter1d(r_raw, MOV_AVG_WINDOW, mode=\"nearest\")\n",
    "            x_corr   = np.arange(pos_min, pos_min + r_raw.size) + half_len\n",
    "\n",
    "            peak_idx, _ = find_peaks(r_smooth)\n",
    "\n",
    "            for p in peak_idx:\n",
    "                k = int(np.count_nonzero(~np.isnan(vec[p : p + tpl.size])))\n",
    "\n",
    "                # ─── choose the threshold source ───────────────────────────\n",
    "                if USE_SIM_THRESHOLDS:\n",
    "                    thr_src = ALPHA_THRESHOLDS[cond] if PER_CONDITION else ALPHA_THRESHOLDS\n",
    "                    thr     = thr_src[tpl_key][_bin_for_k(k)]\n",
    "                else:\n",
    "                    thr = CORR_THRESHOLDS.get(tpl_key, CORR_THRESHOLD)\n",
    "                # ───────────────────────────────────────────────────────────\n",
    "\n",
    "                if r_raw[p] >= thr:\n",
    "                    # ⬇︎ extra gate for the 298‑bp dinucleosome template\n",
    "                    if tpl_key == (14, 298, 14):\n",
    "                        win_start = p                      # index into vec\n",
    "                        win_end   = p + tpl.size\n",
    "                        if win_end > vec.size:             # safety (truncated read)\n",
    "                            continue\n",
    "\n",
    "                        win_vec   = vec[win_start:win_end]       # NaNs = un‑observed\n",
    "                        left_ok   = np.nansum(win_vec[:14]  == 1) > 0\n",
    "                        right_ok  = np.nansum(win_vec[-14:] == 1) > 0\n",
    "                        if not (left_ok and right_ok):\n",
    "                            # at least one linker lacks a methylated position – discard peak\n",
    "                            continue\n",
    "                        # (optional) dbg:\n",
    "                        # dbg(f\"    kept 298‑bp peak at {int(x_corr[p])} – linkers OK\")\n",
    "                    centre = int(x_corr[p])\n",
    "                    candidates.append(\n",
    "                        (centre, tpl_key[1], float(r_raw[p]), tpl_key)\n",
    "                    )\n",
    "\n",
    "        # ---------- resolve overlaps / tie-breaks ---------------------\n",
    "        # sort: highest r first; on equal r, longer core wins\n",
    "                # Bail out early if no peaks survived the threshold\n",
    "        if not candidates:\n",
    "            return []\n",
    "\n",
    "        # If exactly one candidate, no clustering needed\n",
    "        if len(candidates) == 1:\n",
    "            cen, core_len, r_val, _ = candidates[0]\n",
    "            return [(cen, core_len, r_val)]\n",
    "\n",
    "        winners = _greedy_non_overlapping(candidates)\n",
    "        return winners\n",
    "\n",
    "\n",
    "\n",
    "    # 2nd pass (vectorised)\n",
    "    # ───────── 2nd-pass: arg-max with core-length penalty ───────── #\n",
    "    def _best_match_for_centre_vectorised(vec, cen_rel, pre_r_dict):\n",
    "        \"\"\"\n",
    "        Return template/shift with max( r – λ·ΔL ),\n",
    "        where ΔL = (core_len – CORE_SIZE_1)  and λ = CORE_LENGTH_PENALTY.\n",
    "        \"\"\"\n",
    "        best_score, best_tpl = -np.inf, None\n",
    "\n",
    "        for (u, core, d), (r_arr, off) in pre_r_dict.items():\n",
    "            lo = max(0, cen_rel - off - REFINE_WIN)\n",
    "            hi = min(r_arr.size - 1, cen_rel - off + REFINE_WIN)\n",
    "            if lo > hi:\n",
    "                continue\n",
    "\n",
    "            k     = lo + r_arr[lo:hi + 1].argmax()\n",
    "            r_val = r_arr[k]\n",
    "\n",
    "            # ----- one-liner penalty ---------------------------------\n",
    "            adj_score = r_val - CORE_LENGTH_PENALTY * (core - CORE_SIZE_1)\n",
    "            # ---------------------------------------------------------\n",
    "\n",
    "            if adj_score > best_score:\n",
    "                best_score = adj_score\n",
    "                best_tpl   = (u, core, d, off, k, r_val)\n",
    "\n",
    "        if best_tpl is None:               # fallback (shouldn’t happen)\n",
    "            return cen_rel, LINKER_LEN_1, CORE_SIZE_1, LINKER_LEN_1, -1.0, 0\n",
    "\n",
    "        u, core, d, off, k, r_val = best_tpl\n",
    "        shift = (k + off) - cen_rel\n",
    "        return k + off, u, core, d, r_val, shift\n",
    "\n",
    "    # 3rd pass (open-region filter)\n",
    "    def _compute_open_spans(vec, pos_min, dbg_flag=False):\n",
    "        vec_len = vec.size\n",
    "        csum    = np.r_[0, np.cumsum(vec)]\n",
    "        spans   = []\n",
    "        for L, thr in zip(ACCESS_FOOTPRINTS, ACCESS_THRESHOLDS):\n",
    "            if L > vec_len:\n",
    "                continue\n",
    "            mean = (csum[L:] - csum[:-L]) / L\n",
    "            for i in np.where(mean >= thr)[0]:\n",
    "                spans.append((pos_min + i, pos_min + i + L - 1))\n",
    "        if not spans:\n",
    "            if dbg_flag:\n",
    "                dbg(\"    [accessible] no open intervals detected\")\n",
    "            return []\n",
    "        spans.sort(key=lambda x: x[0])\n",
    "        merged = [spans[0]]\n",
    "        for s, e in spans[1:]:\n",
    "            if s <= merged[-1][1] + 1:\n",
    "                merged[-1] = (merged[-1][0], max(merged[-1][1], e))\n",
    "            else:\n",
    "                merged.append((s, e))\n",
    "        if dbg_flag:\n",
    "            dbg(f\"    [accessible] merged open spans = {merged}\")\n",
    "        return merged\n",
    "\n",
    "    def _remove_under_accessible_and_shift(vec, pos_min, refined, dbg_flag=False):\n",
    "        open_sp = _compute_open_spans(vec, pos_min, dbg_flag)\n",
    "        if not open_sp:\n",
    "            return refined, 0\n",
    "\n",
    "        out, removed = [], 0\n",
    "        for cen, core, r in refined:\n",
    "            s, e   = cen - core//2, cen + core//2\n",
    "            discard = False\n",
    "            for os, oe in open_sp:\n",
    "                if e < os or s > oe:\n",
    "                    continue\n",
    "                if s < os and e > oe:         # fully covered → drop\n",
    "                    discard = True; removed += 1; break\n",
    "                if e >= os and e <= oe:       # overlap on right → trim only\n",
    "                    core -= (e - os + 1)\n",
    "                elif s >= os and s <= oe:     # overlap on left → trim only\n",
    "                    core -= (oe - s + 1)\n",
    "                if core < MIN_CORE_LEN:\n",
    "                    discard = True; removed += 1; break\n",
    "                s, e = cen - core//2, cen + core//2   # re-calc for next open span\n",
    "            if not discard:\n",
    "                out.append((cen, core, r))            # cen unchanged\n",
    "        if dbg_flag:\n",
    "            dbg(f\"    [accessible] removed={removed}, kept={len(out)}\")\n",
    "        return out, removed\n",
    "\n",
    "    # prune overlaps (unchanged)\n",
    "    def _prune_overlaps(cands):\n",
    "        cands.sort(key=lambda x: x[0])     # left→right\n",
    "        out = []\n",
    "        for cen, core, r in cands:\n",
    "            s, e = cen - core//2 - 10, cen + core//2 + 10\n",
    "            updated = []\n",
    "            replace = False\n",
    "\n",
    "            for oc, oc_core, oc_r in out:\n",
    "                os, oe = oc - oc_core//2, oc + oc_core//2\n",
    "                if e < os or s > oe:                   # no overlap\n",
    "                    updated.append((oc, oc_core, oc_r))\n",
    "                else:                                  # overlap\n",
    "                    if r > oc_r or (r == oc_r and core > oc_core):\n",
    "                        # incoming peak is better → drop the older one\n",
    "                        replace = True\n",
    "                    else:\n",
    "                        # older one is better → discard incoming\n",
    "                        replace = False\n",
    "                        break\n",
    "\n",
    "            if replace or not any(\n",
    "                    abs(cen - oc) < 0.5 * (core + oc_core) for oc, oc_core, _ in updated):\n",
    "                updated.append((cen, core, r))\n",
    "\n",
    "            out = updated\n",
    "        return out\n",
    "\n",
    "\n",
    "    # ───────────── per-read worker (returns stats) ───────────── #\n",
    "    def _call_nucs_for_read(idx_rec):\n",
    "        try:\n",
    "            idx, rec = idx_rec\n",
    "            rel_pos  = np.asarray(rec[\"rel_pos\"], int)\n",
    "            signal   = np.asarray(rec[\"mod_qual_bin\"], float)\n",
    "\n",
    "            #signal[signal == 0] = -1          # vectorised in-place\n",
    "\n",
    "            pos_min  = rel_pos.min()\n",
    "            span_len = rel_pos.ptp() + 1\n",
    "            vec      = np.full(span_len, np.nan, float)\n",
    "            vec[rel_pos - pos_min] = signal\n",
    "            if PERFORM_FILLING:\n",
    "                vec = _fill_met_domains(vec, MET_DOMAIN_WIDTH)\n",
    "            if PERFORM_INTERP:\n",
    "                vec = _mode_interpolate(vec, INTERP_WINDOW)\n",
    "            # after optional fill / interp\n",
    "            # if not PERFORM_INTERP:          # i.e. you skipped the majority vote\n",
    "            #     vec[np.isnan(vec)] = 0.0    # simple NaN → 0 mapping\n",
    "            # if np.isnan(vec).any():\n",
    "            #     raise ValueError(f\"NaNs remain in read {rec['read_id']}\")\n",
    "\n",
    "            cand_peaks = _corr_call_first_pass(vec, pos_min, rec['condition'])\n",
    "            if DEBUG and idx == 0:\n",
    "                dbg(f\"[{rec['read_id']}] 1st-pass peaks = {cand_peaks[:5]}\")\n",
    "\n",
    "            if not cand_peaks:\n",
    "                empty_stats = dict(n_cands=0, n_refined=0, n_after_access=0,\n",
    "                                   n_adjusted=0, n_removed_access=0,n_removed_low_r=0,\n",
    "                                   template_counts={})\n",
    "                return (idx, [], [], empty_stats)\n",
    "\n",
    "             # ───────── second-pass handling ─────────\n",
    "            if USE_SECOND_PASS:\n",
    "                # -------- existing code (unchanged) --------\n",
    "                pre_r = {k: (_corr_window(vec, *tpl[:3]), tpl[3])\n",
    "                         for k, tpl in _tpl_cache.items()}\n",
    "\n",
    "                refined, template_cnt = [], Counter()\n",
    "                adjusted = 0\n",
    "                for cen_abs, _ in cand_peaks:\n",
    "                    cen_rel = cen_abs - pos_min\n",
    "                    cen_best_rel, u, c_len, d, r_val, shift = (\n",
    "                        _best_match_for_centre_vectorised(vec, cen_rel, pre_r)\n",
    "                    )\n",
    "                    if (shift != 0) or (u != LINKER_LEN_1) or (c_len != CORE_SIZE_1) or (d != LINKER_LEN_1):\n",
    "                        adjusted += 1\n",
    "                    template_cnt[(u, c_len, d)] += 1\n",
    "                    refined.append((cen_best_rel + pos_min, c_len, r_val))\n",
    "            else:\n",
    "                # -------- bypass: keep 1st-pass peaks “as is”, but carry core_len --------\n",
    "                refined = [\n",
    "                    (cen_abs, core_len, r_val)\n",
    "                    for cen_abs, core_len, r_val in cand_peaks\n",
    "                ]\n",
    "\n",
    "                adjusted     = 0\n",
    "                template_cnt = Counter((14, core, 14) for _, core, _ in refined)\n",
    "\n",
    "            if DEBUG and idx == 0:\n",
    "                dbg(f\"[{rec['read_id']}] 2nd-pass refined = {[(c,cl) for c,cl,_ in refined]}\")\n",
    "\n",
    "            refined, n_removed = _remove_under_accessible_and_shift(\n",
    "                vec, pos_min, refined, dbg_flag=(idx == 0))\n",
    "\n",
    "            final = _prune_overlaps(refined)\n",
    "            # drop weak cores (unchanged)\n",
    "\n",
    "\n",
    "            # ── NEW helper: expand a dinucleosome core (298 bp ≡ 147+4+147) ──\n",
    "            # ── helper: expand a dinucleosome core (298 bp ≡ 147+4+147) ──\n",
    "            def _expand_dinuc(cen, core_len):\n",
    "                \"\"\"\n",
    "                Return nucleosome tuples for downstream storage.\n",
    "\n",
    "                Regular nucleosome  → (centre, core_len)                (2‑tuple)\n",
    "                Dinucleosome 298 bp → (centre, 147, 298)  for each half (3‑tuple)\n",
    "\n",
    "                The third element “parent_core_len” lets the plotting layer colour both\n",
    "                halves together without affecting any size‑based analytics.\n",
    "                \"\"\"\n",
    "                if core_len == 298:\n",
    "                    offset = int(round((147 + 4) / 2))      # ≈76 bp; 4 bp linker\n",
    "                    return [\n",
    "                        (cen - offset, 147, 298),           # left half\n",
    "                        (cen + offset, 147, 298),           # right half\n",
    "                    ]\n",
    "                return [(cen, core_len)]                    # unchanged for 80‑,110‑,147‑bp\n",
    "\n",
    "            # ── build expanded coord / centre lists ───────────────────────────\n",
    "            kept = [t for t in final if t[2] >= MIN_FINAL_R]\n",
    "            nuc_coords_exp = []\n",
    "            for cen, core, _ in kept:\n",
    "                nuc_coords_exp.extend(_expand_dinuc(cen, core))\n",
    "\n",
    "            # tuples may now be length‑2 or length‑3 → grab only index 0\n",
    "            nuc_centres_exp = [t[0] for t in nuc_coords_exp]\n",
    "\n",
    "            # ---------- stats bookkeeping -----------------------------------\n",
    "            stats = dict(\n",
    "                n_cands          = len(cand_peaks),\n",
    "                n_refined        = len(refined) + n_removed,\n",
    "                n_after_access   = len(nuc_coords_exp),       # ← updated count\n",
    "                n_adjusted       = adjusted,\n",
    "                n_removed_access = n_removed,\n",
    "                n_removed_low_r  = len(final) - len(kept),\n",
    "                template_counts  = dict(template_cnt),\n",
    "            )\n",
    "\n",
    "            return (idx, nuc_centres_exp, nuc_coords_exp, stats)\n",
    "        except Exception as e:\n",
    "            import traceback, sys\n",
    "            tb = traceback.format_exc()\n",
    "            print(f\"\\n\\n[WORKER EXCEPTION] read-idx {idx_rec[0]}\\n{tb}\\n\",\n",
    "                  file=sys.stderr, flush=True)\n",
    "            raise  # re-raise so the main loop knows this task failed\n",
    "\n",
    "    # ╔════════════════════════════════════════════════════════════════════════╗\n",
    "    # ║  MULTI-PROCESS DRIVER + GLOBAL STATS  (clean, single instance)         ║\n",
    "    # ╚════════════════════════════════════════════════════════════════════════╝\n",
    "    from collections import Counter\n",
    "    import gc, sys\n",
    "\n",
    "    dbg(\"Running nucleosome caller v7.2 – starting pool\", always=True)\n",
    "\n",
    "    records = filtered_reads_df.to_dict(\"records\")\n",
    "    n_tasks = len(records)\n",
    "\n",
    "    results_slots = [None] * n_tasks       # pre-allocate to keep order\n",
    "    received = 0\n",
    "    with mp.Pool(N_WORKERS) as pool, tqdm(total=n_tasks, desc=\"nuc-calls\") as bar:\n",
    "        for idx, *payload in pool.imap_unordered(\n",
    "                _call_nucs_for_read, enumerate(records), chunksize=CHUNK_SIZE):\n",
    "            if results_slots[idx] is None:\n",
    "                results_slots[idx] = (idx, *payload)\n",
    "                received += 1\n",
    "                bar.update(1)\n",
    "\n",
    "    if received != n_tasks:\n",
    "        missing = n_tasks - received\n",
    "        raise RuntimeError(\n",
    "            f\"Pool finished early: expected {n_tasks} results, got {received}. \"\n",
    "            f\"Check worker stderr for exceptions (↑).\")\n",
    "\n",
    "    gc.collect()  # explicitly free the per-read r-arrays\n",
    "\n",
    "    # ────────── aggregate statistics ──────────\n",
    "    tot_cand = tot_ref = tot_final = tot_adj = tot_removed = 0\n",
    "    tpl_hist = Counter()\n",
    "    tot_removed_low_r = 0\n",
    "\n",
    "    for _, _, _, st in results_slots:\n",
    "        tot_cand    += st[\"n_cands\"]\n",
    "        tot_ref     += st[\"n_refined\"]\n",
    "        tot_final   += st[\"n_after_access\"]\n",
    "        tot_adj     += st[\"n_adjusted\"]\n",
    "        tot_removed += st[\"n_removed_access\"]\n",
    "        tot_removed_low_r += st[\"n_removed_low_r\"]\n",
    "        tpl_hist.update(st[\"template_counts\"])\n",
    "\n",
    "    dbg(\"\\n================  GLOBAL NUC-CALL STATS  ================ \", always=True)\n",
    "    dbg(f\"• 1st-pass peaks (candidates):       {tot_cand:,}\", always=True)\n",
    "    dbg(f\"• After 2nd-pass refinement:         {tot_ref:,}\", always=True)\n",
    "    dbg(f\"      ↳ adjusted template/centre:    {tot_adj:,}\", always=True)\n",
    "    dbg(f\"• Removed by accessible filter:      {tot_removed:,}\", always=True)\n",
    "    dbg(f\"• Final non-overlapping cores:       {tot_final:,}\\n\", always=True)\n",
    "    dbg(f\"• Removed for r < {MIN_FINAL_R}:      {tot_removed_low_r:,}\", always=True)\n",
    "\n",
    "    dbg(\"Top templates chosen (u,core,d)  count\", always=True)\n",
    "    for tpl, ct in tpl_hist.most_common(15):\n",
    "        dbg(f\"   {tpl}   {ct:,}\", always=True)\n",
    "\n",
    "    sys.stdout.flush()  # guarantee all DEBUG lines are shown before cell prompt\n",
    "\n",
    "    # ────────── attach results to DataFrame & cache ──────────\n",
    "    results_slots.sort(key=lambda x: x[0])\n",
    "    filtered_reads_df[\"nuc_centers\"] = [cent for _, cent, _, _ in results_slots]\n",
    "    filtered_reads_df[\"nuc_coords\"]  = [coord for _, _, coord, _ in results_slots]\n",
    "\n",
    "    # ╔════════════════════════════════════════════════════════════════════════╗\n",
    "    # ║  4) TRIM READS TO ±175 bp AROUND FIRST/LAST NUCLEOSOME               ║\n",
    "    # ╚════════════════════════════════════════════════════════════════════════╝\n",
    "    from statistics import mean, median\n",
    "    from collections  import Counter\n",
    "    import numpy as np\n",
    "    from tqdm.auto    import tqdm\n",
    "\n",
    "    TRIM_PAD_BP = 175          # ⇠ single configurable constant\n",
    "\n",
    "    # columns whose *array-like* entries must be sliced\n",
    "    ARRAY_COLS  = [\n",
    "        \"mod_qual\", \"base_qual\",\n",
    "        \"forward_read_position\", \"ref_position\", \"rel_pos\", \"mod_qual_bin\"\n",
    "    ]\n",
    "\n",
    "    # helper ------------------------------------------------------------------\n",
    "    def _trim_read(row):\n",
    "        \"\"\"Return (row, n_drop_start, n_drop_end) or None if the read is dropped.\"\"\"\n",
    "        centres = row[\"nuc_centers\"]\n",
    "        if not centres:                 # ➊ drop reads with no nucleosomes\n",
    "            return None\n",
    "\n",
    "        # window in *rel_pos* space (integers; round OK)\n",
    "        keep_lo = int(min(centres) - TRIM_PAD_BP)\n",
    "        keep_hi = int(max(centres) + TRIM_PAD_BP)\n",
    "\n",
    "        # vectorised mask on rel_pos\n",
    "        rel = np.asarray(row[\"rel_pos\"])\n",
    "        mask = (rel >= keep_lo) & (rel <= keep_hi)\n",
    "\n",
    "        n_start = int((rel < keep_lo).sum())\n",
    "        n_end   = int((rel > keep_hi).sum())\n",
    "\n",
    "        # ➋ slice array-type columns\n",
    "        for col in ARRAY_COLS:\n",
    "            arr = row[col]\n",
    "            row[col] = [v for v, m in zip(arr, mask) if m]\n",
    "\n",
    "        # ➌ scalar updates\n",
    "        row[\"rel_read_start\"] = max(row[\"rel_read_start\"], keep_lo)\n",
    "        row[\"rel_read_end\"]   = min(row[\"rel_read_end\"],   keep_hi)\n",
    "        row[\"read_length\"]    = row[\"rel_read_end\"] - row[\"rel_read_start\"] + 1\n",
    "\n",
    "        # ➍ motif filtering (keep indexing stable)\n",
    "        if row[\"motif_rel_start\"] is not None:\n",
    "            keep_idx = [\n",
    "                i for i, s in enumerate(row[\"motif_rel_start\"])\n",
    "                if keep_lo <= s <= keep_hi\n",
    "            ]\n",
    "            row[\"motif_rel_start\"]  = tuple(row[\"motif_rel_start\"][i]  for i in keep_idx)\n",
    "            row[\"motif_attributes\"] = tuple(row[\"motif_attributes\"][i] for i in keep_idx)\n",
    "\n",
    "        return row, n_start, n_end\n",
    "\n",
    "\n",
    "    # ---------------------------- main loop -----------------------------------\n",
    "    trimmed_records   = []\n",
    "    drop_no_nuc       = 0\n",
    "    drop_stats_start  = []   # bp discarded on 5' side\n",
    "    drop_stats_end    = []   # bp discarded on 3' side\n",
    "    example_before    = None\n",
    "    example_after     = None\n",
    "    \n",
    "    if USE_TRIM_READS:\n",
    "        \n",
    "        dbg(\"⚙️  Trimming reads to ±175 bp around nucleosome block…\", always=True)\n",
    "        for i, row in tqdm(filtered_reads_df.iterrows(), total=len(filtered_reads_df), desc=\"trim\"):\n",
    "            if example_before is None:          # stash an untouched copy of the 1st row\n",
    "                example_before = row.copy(deep=True)\n",
    "    \n",
    "            out = _trim_read(row)\n",
    "            if out is None:                     # dropped for empty nuc list\n",
    "                drop_no_nuc += 1\n",
    "                continue\n",
    "    \n",
    "            row, n_lo, n_hi = out\n",
    "            drop_stats_start.append(n_lo)\n",
    "            drop_stats_end.append(n_hi)\n",
    "    \n",
    "            if example_after is None:           # first surviving row after trim\n",
    "                example_after = row.copy(deep=True)\n",
    "    \n",
    "            trimmed_records.append(row)\n",
    "\n",
    "        # rebuild DataFrame ---------------------------------------------------------\n",
    "        trimmed_df = pd.DataFrame(trimmed_records).reset_index(drop=True)\n",
    "    \n",
    "        # ----------------------------- DEBUG LINES ---------------------------------\n",
    "        dbg(\"\\n================  TRIM STATS  ================ \", always=True)\n",
    "        dbg(f\"• Reads dropped (no nucleosomes):   {drop_no_nuc:,}\", always=True)\n",
    "    \n",
    "        if drop_stats_start:\n",
    "            disc_start = np.array(drop_stats_start)\n",
    "            disc_end   = np.array(drop_stats_end)\n",
    "            disc_tot   = disc_start + disc_end\n",
    "    \n",
    "            dbg(\"• bp removed at start  (5'): \"\n",
    "                f\"min={disc_start.min():,}  \"\n",
    "                f\"median={int(median(disc_start)):,}  \"\n",
    "                f\"mean={mean(disc_start):.1f}  \"\n",
    "                f\"max={disc_start.max():,}\", always=True)\n",
    "    \n",
    "            dbg(\"• bp removed at end    (3'): \"\n",
    "                f\"min={disc_end.min():,}  \"\n",
    "                f\"median={int(median(disc_end)):,}  \"\n",
    "                f\"mean={mean(disc_end):.1f}  \"\n",
    "                f\"max={disc_end.max():,}\", always=True)\n",
    "    \n",
    "            dbg(\"• bp removed combined: \"\n",
    "                f\"min={disc_tot.min():,}  \"\n",
    "                f\"median={int(median(disc_tot)):,}  \"\n",
    "                f\"mean={mean(disc_tot):.1f}  \"\n",
    "                f\"max={disc_tot.max():,}\\n\", always=True)\n",
    "        else:\n",
    "            dbg(\"• No reads needed trimming.\\n\", always=True)\n",
    "    \n",
    "        # pretty-print example ------------------------------------------------------\n",
    "        if example_before is not None and example_after is not None:\n",
    "            dbg(\"===== EXAMPLE ROW BEFORE TRIM =====\", always=True)\n",
    "            dbg(example_before, always=True)\n",
    "            dbg(\"===== EXAMPLE ROW AFTER  TRIM =====\", always=True)\n",
    "            dbg(example_after,  always=True)\n",
    "    \n",
    "        # finally, swap the DataFrame so everything downstream sees the trimmed set\n",
    "        filtered_reads_df = trimmed_df\n",
    "        dbg(f\"✔ Trimmed DataFrame shape = {filtered_reads_df.shape}\", always=True)\n",
    "    else:\n",
    "        dbg(\"⚙️  Skipping trimming step (USE_TRIM_READS=False)\", always=True)\n",
    "    # filtered_reads_df.to_pickle(TEMP_DF_PATH)\n",
    "    # dbg(f\"💾 Saved to {TEMP_DF_PATH} ({os.path.getsize(TEMP_DF_PATH)} bytes)\", always=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85de363bfaa607d5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print unique combinations of condition and exp_id in filtered_reads_df, with count of each\n",
    "print(\"Unique combinations of condition and exp_id in filtered_reads_df, with count of each:\")\n",
    "print(filtered_reads_df.groupby(['condition', 'exp_id']).size().reset_index(name='count'))\n",
    "print(filtered_reads_df.groupby(['type', 'bed_start']).size().reset_index(name='count'))\n",
    "\n",
    "# print 1 row\n",
    "print(\"\\nOne row from filtered_reads_df:\")\n",
    "print(filtered_reads_df.iloc[0])\n",
    "# ─────────────────── CONFIG ───────────────────\n",
    "TEMPLATE     = \"plotly_white\"\n",
    "WIDTH, HEIGHT = 950, 600\n",
    "OUTDIR       = \"plots\"\n",
    "SAVE_HTML    = True\n",
    "WITHIN_BIN   = \"longest\"   # 'longest' or 'random'\n",
    "STEP_PCT     = 2.5         # 0.5% steps of the 99th percentile\n",
    "UPPER_Q      = 99.0\n",
    "RANDOM_STATE = 42\n",
    "DEBUG_SUMMARY= True\n",
    "\n",
    "# ─────────────────── IMPORTS ──────────────────\n",
    "import os, numpy as np, pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "rng = np.random.default_rng(RANDOM_STATE)\n",
    "\n",
    "# ──────────────── HELPERS ─────────────────────\n",
    "def _ecdf_xy(v):\n",
    "    v = np.asarray(v, float)\n",
    "    v = v[~np.isnan(v)]\n",
    "    if v.size == 0:\n",
    "        return np.array([]), np.array([])\n",
    "    x = np.sort(v)\n",
    "    y = np.arange(1, x.size + 1) / x.size\n",
    "    return x, y\n",
    "\n",
    "def _bin_edges_from_p99(lengths, step_pct=0.5, upper_q=99.0):\n",
    "    v = np.asarray(lengths, float)\n",
    "    v = v[~np.isnan(v)]\n",
    "    if v.size == 0:\n",
    "        raise ValueError(\"No read lengths available.\")\n",
    "    p99 = np.percentile(v, upper_q)\n",
    "    vmax = float(np.max(v))\n",
    "    step = max(1.0, (step_pct/100.0) * p99)  # at least 1 bp\n",
    "    edges = np.arange(0.0, p99 + step, step)\n",
    "    edges = np.unique(edges)\n",
    "    # include overflow bin up to max length if needed\n",
    "    if vmax > edges[-1]:\n",
    "        edges = np.concatenate([edges, [vmax]])\n",
    "    # ensure at least two edges\n",
    "    if edges.size < 2:\n",
    "        edges = np.array([0.0, vmax])\n",
    "    return edges\n",
    "\n",
    "def downsample_bin_matched(\n",
    "    df, group_col=\"condition\", len_col=\"read_length\",\n",
    "    within_bin=\"longest\", edges=None, random_state=42\n",
    "):\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    d = df[[group_col, len_col]].copy()\n",
    "    d = d.dropna(subset=[len_col])\n",
    "    d[\"_bin\"] = pd.cut(d[len_col].astype(float), bins=edges, include_lowest=True, right=True)\n",
    "\n",
    "    # per-bin min across conditions; keep only feasible bins\n",
    "    ct = d.groupby([group_col, \"_bin\"]).size().rename(\"n\").reset_index()\n",
    "    per_bin_min = ct.groupby(\"_bin\")[\"n\"].min()\n",
    "    feasible_bins = per_bin_min[per_bin_min > 0].index\n",
    "    per_bin_min = per_bin_min.loc[feasible_bins]\n",
    "\n",
    "    selected_idx = []\n",
    "    for b in feasible_bins:\n",
    "        target = int(per_bin_min.loc[b])\n",
    "        sub_b = d[d[\"_bin\"] == b]\n",
    "        for cond, sub_bc in sub_b.groupby(group_col, sort=False):\n",
    "            pool_idx = sub_bc.index\n",
    "            if within_bin == \"longest\":\n",
    "                chosen = sub_bc.sort_values(len_col, ascending=False).index[:target]\n",
    "            else:\n",
    "                chosen = rng.choice(pool_idx.to_numpy(), size=target, replace=False)\n",
    "            selected_idx.append(np.asarray(chosen))\n",
    "\n",
    "    if selected_idx:\n",
    "        selected_idx = np.concatenate(selected_idx)\n",
    "        out = df.loc[selected_idx].copy()\n",
    "    else:\n",
    "        out = df.iloc[0:0].copy()\n",
    "\n",
    "    return out, feasible_bins\n",
    "\n",
    "def plot_ecdf(df, title, group_col=\"condition\", len_col=\"read_length\", width=950, height=600, template=\"plotly_white\", save_path=None):\n",
    "    fig = go.Figure()\n",
    "    for cond, sub in df.groupby(group_col, sort=False):\n",
    "        x, y = _ecdf_xy(sub[len_col].values)\n",
    "        fig.add_trace(go.Scatter(x=x, y=y, mode=\"lines\", name=str(cond)))\n",
    "    fig.update_layout(template=template, width=width, height=height,\n",
    "                      title=title, xaxis_title=\"Read length (bp)\", yaxis_title=\"Cumulative fraction\")\n",
    "    fig.show()\n",
    "    if save_path:\n",
    "        fig.write_html(save_path)\n",
    "\n",
    "def plot_box(df, title, group_col=\"condition\", len_col=\"read_length\", width=950, height=600, template=\"plotly_white\", save_path=None):\n",
    "    fig = go.Figure()\n",
    "    for cond, sub in df.groupby(group_col, sort=False):\n",
    "        fig.add_trace(go.Box(y=sub[len_col], name=str(cond), boxmean=True))\n",
    "    fig.update_layout(template=template, width=width, height=height,\n",
    "                      title=title, yaxis_title=\"Read length (bp)\")\n",
    "    fig.show()\n",
    "    if save_path:\n",
    "        fig.write_html(save_path)\n",
    "\n",
    "# ───────────── PREP + BINS ────────────────────\n",
    "assert \"condition\" in filtered_reads_df.columns and \"read_length\" in filtered_reads_df.columns\n",
    "pre_df = filtered_reads_df.copy()\n",
    "pre_df = pre_df.dropna(subset=[\"read_length\"])\n",
    "\n",
    "edges = _bin_edges_from_p99(pre_df[\"read_length\"].values, step_pct=STEP_PCT, upper_q=UPPER_Q)\n",
    "\n",
    "# ───────────── DOWNSAMPLE ─────────────────────\n",
    "filtered_reads_df, feasible_bins = downsample_bin_matched(\n",
    "    pre_df, group_col=\"condition\", len_col=\"read_length\",\n",
    "    within_bin=WITHIN_BIN, edges=edges, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# ───────────── STATS ──────────────────────────\n",
    "before = pre_df.groupby(\"condition\").size().rename(\"n_before\")\n",
    "after  = filtered_reads_df.groupby(\"condition\").size().rename(\"n_after\")\n",
    "stats  = pd.concat([before, after], axis=1).fillna(0).astype(int)\n",
    "stats[\"retained_frac\"] = np.where(stats[\"n_before\"]>0, stats[\"n_after\"]/stats[\"n_before\"], np.nan)\n",
    "stats = stats.sort_index()\n",
    "\n",
    "total_before = int(stats[\"n_before\"].sum())\n",
    "total_after  = int(stats[\"n_after\"].sum())\n",
    "total_frac   = (total_after / total_before) if total_before > 0 else np.nan\n",
    "\n",
    "if DEBUG_SUMMARY:\n",
    "    print(\"Feasible bins retained:\", len(feasible_bins), \"of\", len(edges)-1, \"total bins\")\n",
    "    print(\"Reads before (total):\", total_before)\n",
    "    print(\"Reads after  (total):\",  total_after)\n",
    "    print(f\"Overall retained fraction: {total_frac:.4f}\")\n",
    "    display(stats)\n",
    "\n",
    "# ───────────── PLOTS: BEFORE/AFTER ────────────\n",
    "plot_ecdf(\n",
    "    pre_df,\n",
    "    title=\"ECDF of read lengths by condition — BEFORE bin-matched downsampling\",\n",
    "    save_path=os.path.join(OUTDIR, \"ecdf_before.html\") if SAVE_HTML else None,\n",
    "    template=TEMPLATE, width=WIDTH, height=HEIGHT\n",
    ")\n",
    "\n",
    "plot_ecdf(\n",
    "    filtered_reads_df,\n",
    "    title=\"ECDF of read lengths by condition — AFTER bin-matched downsampling\",\n",
    "    save_path=os.path.join(OUTDIR, \"ecdf_after.html\") if SAVE_HTML else None,\n",
    "    template=TEMPLATE, width=WIDTH, height=HEIGHT\n",
    ")\n",
    "\n",
    "plot_box(\n",
    "    pre_df,\n",
    "    title=\"Read length by condition (box) — BEFORE downsampling\",\n",
    "    save_path=os.path.join(OUTDIR, \"box_before.html\") if SAVE_HTML else None,\n",
    "    template=TEMPLATE, width=WIDTH, height=HEIGHT\n",
    ")\n",
    "\n",
    "plot_box(\n",
    "    filtered_reads_df,\n",
    "    title=\"Read length by condition (box) — AFTER downsampling\",\n",
    "    save_path=os.path.join(OUTDIR, \"box_after.html\") if SAVE_HTML else None,\n",
    "    template=TEMPLATE, width=WIDTH, height=HEIGHT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dd4b6648626ef6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ╔══════════════════════════════════════════════════════════════════╗\n",
    "# ║  CELL X: edge‑growing / merging of nucleosomes (parallel)        ║\n",
    "# ╚══════════════════════════════════════════════════════════════════╝\n",
    "#\n",
    "#  New behaviour (opt‑in):\n",
    "#    ⋄ set SPLIT_LONG_NUCS = True\n",
    "#    ⋄ any grown nucleosome of 200‑450 bp is split into two equal halves\n",
    "#    ⋄ any grown nucleosome  >450 bp is dropped\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "import multiprocessing as mp\n",
    "from copy import deepcopy\n",
    "from itertools import repeat\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "DEBUG_FIRST      = True                 # original flag\n",
    "\n",
    "# ─── NEW CONFIG ────────────────────────────────────────────────────\n",
    "SPLIT_LONG_NUCS  = True                 # ← toggle the feature\n",
    "SPLIT_MIN_LEN    = 200                  # lower bound (inclusive)\n",
    "SPLIT_MAX_LEN    = 400                  # upper bound (inclusive)\n",
    "# ───────────────────────────────────────────────────────────────────\n",
    "\n",
    "# ─── helpers (unchanged) ──────────────────────────────────────────\n",
    "def _interval_from_coord(cen, core):\n",
    "    half = core // 2\n",
    "    return [cen - half, cen + half]     # mutable [start,end]\n",
    "\n",
    "def _coords_from_interval(start, end):\n",
    "    core = end - start                  # inclusive → core_len=end‑start\n",
    "    cen  = start + core // 2\n",
    "    return (int(cen), int(core))\n",
    "\n",
    "# ─── worker ───────────────────────────────────────────────────────\n",
    "def _grow_single(row_idx):\n",
    "    row   = filtered_reads_df.loc[row_idx]\n",
    "    rel_p = np.asarray(row[\"rel_pos\"],      dtype=int)\n",
    "    m_bin = np.asarray(row[\"mod_qual_bin\"], dtype=int)\n",
    "    obs1  = {p for p,b in zip(rel_p, m_bin) if b == 1}\n",
    "\n",
    "    # your original intervals\n",
    "    intervals = sorted(_interval_from_coord(c,cl)\n",
    "                       for (c,cl,*_) in row[\"nuc_coords\"])\n",
    "    grown = deepcopy(intervals)\n",
    "\n",
    "    # ← INSERT THESE TWO LINES\n",
    "    left_edge, right_edge = rel_p.min(), rel_p.max()\n",
    "\n",
    "    max_it = max(1, len(rel_p))\n",
    "    for _ in range(max_it):\n",
    "        changed = False\n",
    "\n",
    "        # ── REPLACE the old “grow starts” loop with this ───────────\n",
    "        # ── REPLACE the old “grow starts” loop with this ───────────\n",
    "        for i, (s, e) in enumerate(grown):\n",
    "            if s in obs1:                       # edge already on a mod site → skip\n",
    "                continue\n",
    "            new_s = s - 1\n",
    "            if new_s in obs1:                   # proposed base on a mod site → skip\n",
    "                continue\n",
    "            if ((i > 0 and new_s > grown[i-1][1])         # interior\n",
    "                or (i == 0 and new_s >= left_edge)):       # left‑most\n",
    "                grown[i][0] = new_s\n",
    "                changed = True\n",
    "    \n",
    "        # ── REPLACE the old “grow ends” loop with this ─────────────\n",
    "        for i in reversed(range(len(grown))):\n",
    "            s, e = grown[i]\n",
    "            if e in obs1:                       # edge already on a mod site → skip\n",
    "                continue\n",
    "            new_e = e + 1\n",
    "            if new_e in obs1:                   # proposed base on a mod site → skip\n",
    "                continue\n",
    "            if ((i < len(grown)-1 and new_e < grown[i+1][0])    # interior\n",
    "                or (i == len(grown)-1 and new_e <= right_edge)): # right‑most\n",
    "                grown[i][1] = new_e\n",
    "                changed = True\n",
    "\n",
    "        if not changed:\n",
    "            break\n",
    "\n",
    "    # merge touching / overlapping\n",
    "    merged=[]\n",
    "    for s,e in sorted(grown):\n",
    "        if not merged or s > merged[-1][1]+1:\n",
    "            merged.append([s,e])\n",
    "        else:\n",
    "            merged[-1][1]=max(merged[-1][1],e)\n",
    "\n",
    "    # ─── NEW post‑processing: split or drop long cores ────────────\n",
    "    processed=[]\n",
    "    for s,e in merged:\n",
    "        core_len = e-s\n",
    "        if core_len > SPLIT_MAX_LEN and SPLIT_LONG_NUCS:\n",
    "            continue                              # drop\n",
    "        if SPLIT_LONG_NUCS and SPLIT_MIN_LEN <= core_len <= SPLIT_MAX_LEN:\n",
    "            half = core_len // 2\n",
    "            left_end  = s + half - 1              # inclusive\n",
    "            right_sta = left_end + 1\n",
    "            processed.extend([[s, left_end],\n",
    "                              [right_sta, e]])\n",
    "        else:\n",
    "            processed.append([s,e])\n",
    "\n",
    "    new_coords  = [_coords_from_interval(s,e) for s,e in processed]\n",
    "    new_centers = [c for c,_ in new_coords]\n",
    "\n",
    "    dbg=None\n",
    "    if DEBUG_FIRST and row_idx==0:\n",
    "        dbg=dict(\n",
    "            read_id=row[\"read_id\"],\n",
    "            original_intervals=intervals,\n",
    "            grown_intervals=[tuple(iv) for iv in grown],\n",
    "            merged_intervals=[tuple(iv) for iv in merged],\n",
    "            final_intervals=[tuple(iv) for iv in processed],\n",
    "            new_coords=new_coords\n",
    "        )\n",
    "    return row_idx,new_centers,new_coords,dbg\n",
    "\n",
    "# ─── multiprocess driver (unchanged) ──────────────────────────────\n",
    "idx_all   = filtered_reads_df.index.to_numpy()\n",
    "n_workers = max(2, mp.cpu_count()-2)\n",
    "\n",
    "print(f\"⚙️  Growing nucleosome edges using {n_workers} workers…\")\n",
    "with mp.Pool(n_workers) as pool, tqdm(total=len(idx_all), desc=\"expand\") as bar:\n",
    "    for row_idx, centers, coords, dbg in pool.imap_unordered(_grow_single,\n",
    "                                                             idx_all,\n",
    "                                                             chunksize=128):\n",
    "        filtered_reads_df.at[row_idx,\"nuc_centers\"]=centers\n",
    "        filtered_reads_df.at[row_idx,\"nuc_coords\" ]=coords\n",
    "        if dbg is not None:\n",
    "            print(\"\\n[DEBUG — first read]\")\n",
    "            for k,v in dbg.items(): print(f\"  {k:20s}: {v}\")\n",
    "        bar.update(1)\n",
    "\n",
    "print(\"✅ edge‑growing complete – DataFrame updated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73858e7a74004f6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Add ChIP signal columns from 50-bp bedgraphs using N nearest bins to center\n",
    "# Fix: use float distances when masking the containing bin (prevents OverflowError)\n",
    "# Adds: DEBUG prints and validation summary\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ─── Config ───────────────────────────────────────────────────────────────────\n",
    "N_NEAREST_BINS = 4           # strictly this many selected when available\n",
    "PROGRESS       = True        # tqdm progress\n",
    "SUMMARIZE      = True        # final summary lines\n",
    "DEBUG_VALIDATE = True        # extra validation checks and sample prints\n",
    "DEBUG_SAMPLES  = 3           # number of rows to sample for detailed checks\n",
    "N_WORKERS      = max(2, mp.cpu_count() - 2)\n",
    "\n",
    "BEDGRAPH_PATHS = {\n",
    "    \"dpy27_chip\": \"/Data1/ext_data/qiming_2024/50bp_bins/BMQY009_2_DPY27_rep1_antimNeon.sort.bedgraph\",\n",
    "    \"sdc2_chip\":  \"/Data1/ext_data/qiming_2024/50bp_bins/BMQY009_5_SDC2_rep1_antimNeon.sort.bedgraph\",\n",
    "    \"sdc3_chip\":  \"/Data1/ext_data/qiming_2024/50bp_bins/BMQY010_2_SDC3_rep1_antimNeon.sort.bedgraph\",\n",
    "}\n",
    "\n",
    "CHR_MAP = {\n",
    "    \"chrI\":\"CHROMOSOME_I\", \"chrII\":\"CHROMOSOME_II\", \"chrIII\":\"CHROMOSOME_III\",\n",
    "    \"chrIV\":\"CHROMOSOME_IV\", \"chrV\":\"CHROMOSOME_V\", \"chrX\":\"CHROMOSOME_X\",\n",
    "}\n",
    "\n",
    "# Ensure chromosome values match the bedgraph mapping\n",
    "if filtered_reads_df[\"chrom\"].dtype.name == \"category\":\n",
    "    filtered_reads_df[\"chrom\"] = filtered_reads_df[\"chrom\"].astype(str)\n",
    "filtered_reads_df[\"chrom\"] = filtered_reads_df[\"chrom\"].astype(str)\n",
    "\n",
    "# ─── Helpers ──────────────────────────────────────────────────────────────────\n",
    "def _load_bedgraph(path):\n",
    "    df = pd.read_csv(\n",
    "        path, sep=r\"\\s+\", header=None, names=[\"chrom\",\"start\",\"end\",\"value\"],\n",
    "        dtype={\"chrom\":\"category\",\"start\":np.int64,\"end\":np.int64,\"value\":np.float32}\n",
    "    )\n",
    "    df[\"chrom\"] = df[\"chrom\"].map(CHR_MAP).astype(\"category\")\n",
    "    df = df.dropna(subset=[\"chrom\"]).sort_values([\"chrom\",\"start\"]).reset_index(drop=True)\n",
    "    df[\"center\"] = (df[\"start\"].values + df[\"end\"].values) // 2\n",
    "    out = {}\n",
    "    for c, sub in df.groupby(\"chrom\", sort=False):\n",
    "        out[str(c)] = {\n",
    "            \"start\":  sub[\"start\"].to_numpy(np.int64, copy=False),\n",
    "            \"end\":    sub[\"end\"].to_numpy(np.int64, copy=False),\n",
    "            \"center\": sub[\"center\"].to_numpy(np.int64, copy=False),\n",
    "            \"value\":  sub[\"value\"].to_numpy(np.float32, copy=False),\n",
    "        }\n",
    "    return out\n",
    "\n",
    "def _containing_index(starts, ends, pos):\n",
    "    \"\"\"Index i with starts[i] ≤ pos < ends[i], or None if not contained.\"\"\"\n",
    "    i = int(np.searchsorted(ends, pos, side=\"right\"))\n",
    "    if i == len(ends):\n",
    "        return None\n",
    "    return i if starts[i] <= pos < ends[i] else None\n",
    "\n",
    "def _mean_nearest_bins(bdg_chrom, pos, n_bins):\n",
    "    \"\"\"Mean of up to n_bins nearest bins to pos. Always include containing bin if present.\"\"\"\n",
    "    if bdg_chrom is None:\n",
    "        return np.nan\n",
    "    starts  = bdg_chrom[\"start\"]\n",
    "    ends    = bdg_chrom[\"end\"]\n",
    "    centers = bdg_chrom[\"center\"]\n",
    "    values  = bdg_chrom[\"value\"]\n",
    "    if len(values) == 0:\n",
    "        return np.nan\n",
    "\n",
    "    # Locate containing bin if present\n",
    "    i_cont = _containing_index(starts, ends, pos)\n",
    "\n",
    "    # Float distances so we can use np.inf safely\n",
    "    dist = np.abs(centers - int(pos)).astype(np.float64)\n",
    "\n",
    "    chosen = []\n",
    "    if i_cont is not None:\n",
    "        chosen.append(i_cont)\n",
    "\n",
    "    # Select remaining neighbors by smallest distance, excluding i_cont\n",
    "    k_rem = max(0, n_bins - len(chosen))\n",
    "    if k_rem > 0:\n",
    "        if i_cont is not None:\n",
    "            dist[i_cont] = np.inf  # safe now because dist is float64\n",
    "        finite_ct = int(np.isfinite(dist).sum())\n",
    "        k_take = min(k_rem, finite_ct)\n",
    "        if k_take > 0:\n",
    "            idxs = np.argpartition(dist, kth=k_take-1)[:k_take]\n",
    "            chosen.extend(list(idxs))\n",
    "\n",
    "    if not chosen:\n",
    "        return np.nan\n",
    "    return float(np.nanmean(values[np.array(chosen, dtype=int)]))\n",
    "\n",
    "# Globals for workers\n",
    "_BDG_IDX = {}\n",
    "_NBINS   = N_NEAREST_BINS\n",
    "\n",
    "def _init_pool(bdg_idx, n_bins):\n",
    "    global _BDG_IDX, _NBINS\n",
    "    _BDG_IDX = bdg_idx\n",
    "    _NBINS   = n_bins\n",
    "\n",
    "def _worker(task):\n",
    "    chrom, bstart, bend = task\n",
    "    center = (int(bstart) + int(bend)) // 2\n",
    "    out = [chrom, bstart, bend]\n",
    "    for key in (\"dpy27_chip\",\"sdc2_chip\",\"sdc3_chip\"):\n",
    "        bdg_chrom = _BDG_IDX[key].get(chrom)\n",
    "        out.append(_mean_nearest_bins(bdg_chrom, center, _NBINS))\n",
    "    return tuple(out)\n",
    "\n",
    "# ─── Load bedgraphs once ──────────────────────────────────────────────────────\n",
    "_bdg_idx = {name: _load_bedgraph(path) for name, path in BEDGRAPH_PATHS.items()}\n",
    "\n",
    "# ─── Unique queries to minimize work ──────────────────────────────────────────\n",
    "keys_df = filtered_reads_df.loc[:, [\"chrom\",\"bed_start\",\"bed_end\"]].drop_duplicates().reset_index(drop=True)\n",
    "tasks   = [tuple(x) for x in keys_df.to_records(index=False)]\n",
    "\n",
    "# ─── Parallel compute ─────────────────────────────────────────────────────────\n",
    "results = []\n",
    "if PROGRESS:\n",
    "    with mp.Pool(N_WORKERS, initializer=_init_pool, initargs=(_bdg_idx, N_NEAREST_BINS)) as pool:\n",
    "        for rec in tqdm(pool.imap_unordered(_worker, tasks, chunksize=1024),\n",
    "                        total=len(tasks), desc=f\"chip-annot (N={N_NEAREST_BINS})\"):\n",
    "            results.append(rec)\n",
    "else:\n",
    "    with mp.Pool(N_WORKERS, initializer=_init_pool, initargs=(_bdg_idx, N_NEAREST_BINS)) as pool:\n",
    "        results = pool.map(_worker, tasks, chunksize=1024)\n",
    "\n",
    "res_df = pd.DataFrame(results, columns=[\"chrom\",\"bed_start\",\"bed_end\",\"dpy27_chip\",\"sdc2_chip\",\"sdc3_chip\"])\n",
    "\n",
    "# ─── Merge back ───────────────────────────────────────────────────────────────\n",
    "pre_cols = set(filtered_reads_df.columns)\n",
    "filtered_reads_df = filtered_reads_df.merge(res_df, on=[\"chrom\",\"bed_start\",\"bed_end\"], how=\"left\")\n",
    "post_cols = set(filtered_reads_df.columns)\n",
    "\n",
    "# ─── Validation / Debug ───────────────────────────────────────────────────────\n",
    "added = [\"dpy27_chip\",\"sdc2_chip\",\"sdc3_chip\"]\n",
    "missing_cols = [c for c in added if c not in post_cols]\n",
    "if missing_cols:\n",
    "    raise RuntimeError(f\"Expected columns missing after merge: {missing_cols}\")\n",
    "\n",
    "if SUMMARIZE:\n",
    "    n_keys = len(keys_df)\n",
    "    n_rows = len(filtered_reads_df)\n",
    "    na_counts = filtered_reads_df[added].isna().sum().to_dict()\n",
    "    print(f\"[chip-annot] unique keys: {n_keys:,}  rows updated: {n_rows:,}  N_NEAREST_BINS={N_NEAREST_BINS}\")\n",
    "    print(f\"[chip-annot] NaN counts: {na_counts}\")\n",
    "\n",
    "if DEBUG_VALIDATE:\n",
    "    # 1) Show distinct chromosomes found in bedgraphs vs dataframe\n",
    "    bg_chroms = sorted({c for d in _bdg_idx.values() for c in d.keys()})\n",
    "    df_chroms = sorted(filtered_reads_df[\"chrom\"].unique().tolist())\n",
    "    print(f\"[debug] bedgraph chroms: {bg_chroms}\")\n",
    "    print(f\"[debug] df chroms:       {df_chroms}\")\n",
    "\n",
    "    # 2) Sample a few rows and recompute single-row values to verify containment logic\n",
    "    samp = filtered_reads_df.sample(min(DEBUG_SAMPLES, len(filtered_reads_df)), random_state=1)\n",
    "    for i, r in enumerate(samp.itertuples(index=False), 1):\n",
    "        chrom  = r.chrom\n",
    "        center = (int(r.bed_start) + int(r.bed_end)) // 2\n",
    "        for key in (\"dpy27_chip\",\"sdc2_chip\",\"sdc3_chip\"):\n",
    "            bdg_chrom = _bdg_idx[key].get(chrom)\n",
    "            recomputed = _mean_nearest_bins(bdg_chrom, center, N_NEAREST_BINS)\n",
    "            orig = getattr(r, key)\n",
    "            ok = (np.isnan(orig) and np.isnan(recomputed)) or (not np.isnan(orig) and np.isclose(orig, recomputed, rtol=0, atol=1e-6))\n",
    "            print(f\"[debug] row#{i} {key}: center={center}  orig={orig}  recomputed={recomputed}  match={ok}\")\n",
    "\n",
    "    # 3) Basic stats\n",
    "    desc = filtered_reads_df[added].describe(percentiles=[0.1,0.5,0.9]).T\n",
    "    with pd.option_context(\"display.max_columns\", 8, \"display.precision\", 3):\n",
    "        print(\"[debug] column summary:\")\n",
    "        print(desc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7dc03800be835d2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Add/overwrite two columns on filtered_reads_df:\n",
    "#   • ind           → distance between nearest left/right nucleosome around rel_pos=0\n",
    "#                      configurable: \"centers\" or \"edges\", using the SAME center-bracketed pair\n",
    "#                      optional gating by # of m6A (mod_qual_bin==1) between the pair\n",
    "#   • percent_m6a   → % of mod_qual_bin==1 within rel_pos ∈ [M6A_WIN]\n",
    "# Notes:\n",
    "#   • Pair selection ALWAYS brackets x=0 by centers from nuc_coords for both modes.\n",
    "#   • In \"edges\" mode: IND = (cR - kR//2) - (cL + kL//2). Negative gaps → NaN.\n",
    "#   • If nuc_coords missing cores and IND_MODE==\"centers\", falls back to nuc_centers at x=0.\n",
    "#   • Safe to rerun: columns are overwritten.\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ========================== CONFIG ========================== #\n",
    "IND_MODE                 = \"edges\"   # \"edges\" or \"centers\"\n",
    "MIN_M6A_IN_IND           = 2         # require ≥ this many mod_qual_bin==1 between the chosen pair; 0 disables\n",
    "INCLUDE_EDGES_IN_IND     = True      # gating interval closed vs open at boundaries\n",
    "MAX_IND_BP               = None      # e.g., 600; None disables exclusion by max gap\n",
    "USE_CANONICAL_CORE       = None      # e.g., 147 to force standard core; None uses per-nuc core lengths\n",
    "M6A_WIN                  = (-150, 150) # rel_pos window for percent_m6a only\n",
    "SHOW_PROGRESS            = True\n",
    "SHOW_SUMMARY             = True\n",
    "DEBUG_VALIDATE           = True\n",
    "DEBUG_SAMPLES            = 3\n",
    "_CHUNKSIZE               = 1024\n",
    "\n",
    "# ======================= WORKERS SETUP ====================== #\n",
    "try:\n",
    "    _NWORKERS = max(2, mp.cpu_count() - 2) if 'N_WORKERS' not in globals() else max(2, int(N_WORKERS))\n",
    "except Exception:\n",
    "    _NWORKERS = max(2, mp.cpu_count() - 2)\n",
    "\n",
    "# ======================= HELPERS (IND) ====================== #\n",
    "def _centers_and_cores(nuc_coords):\n",
    "    \"\"\"Return sorted centers and matching cores from [(center, core_len, ...), ...].\"\"\"\n",
    "    if not nuc_coords:\n",
    "        return np.array([], dtype=np.int64), np.array([], dtype=np.int64)\n",
    "    cc, kk = [], []\n",
    "    for t in nuc_coords:\n",
    "        try:\n",
    "            c = int(t[0])\n",
    "            k = int(t[1])\n",
    "        except Exception:\n",
    "            continue\n",
    "        if USE_CANONICAL_CORE is not None:\n",
    "            k = int(USE_CANONICAL_CORE)\n",
    "        cc.append(c); kk.append(k)\n",
    "    if not cc:\n",
    "        return np.array([], dtype=np.int64), np.array([], dtype=np.int64)\n",
    "    cc = np.asarray(cc, dtype=np.int64)\n",
    "    kk = np.asarray(kk, dtype=np.int64)\n",
    "    order = np.argsort(cc, kind=\"mergesort\")\n",
    "    return cc[order], kk[order]\n",
    "\n",
    "def _pair_indices_by_center_at_x(C, x=0):\n",
    "    \"\"\"Return indices (l, r) bracketing x by centers, or (None, None).\"\"\"\n",
    "    n = C.size\n",
    "    if n < 2:\n",
    "        return None, None\n",
    "    r = np.searchsorted(C, x, side=\"left\")  # first center ≥ x\n",
    "    l = r - 1                                # last center <  x\n",
    "    if l < 0 or r >= n:\n",
    "        return None, None\n",
    "    return l, r\n",
    "\n",
    "def _count_m6a_between(rel_pos, mod_bin, lo, hi, include_edges=True):\n",
    "    \"\"\"Count mod_qual_bin==1 within (lo,hi) or [lo,hi] depending on include_edges.\"\"\"\n",
    "    if rel_pos is None or mod_bin is None:\n",
    "        return 0\n",
    "    rp = np.asarray(rel_pos); mb = np.asarray(mod_bin)\n",
    "    if rp.size == 0 or mb.size == 0:\n",
    "        return 0\n",
    "    n = min(rp.size, mb.size)\n",
    "    rp = rp[:n]; mb = mb[:n]\n",
    "    if include_edges:\n",
    "        mask = (rp >= lo) & (rp <= hi)\n",
    "    else:\n",
    "        mask = (rp >  lo) & (rp <  hi)\n",
    "    if not np.any(mask):\n",
    "        return 0\n",
    "    sel = mb[mask]\n",
    "    return int((sel == 1).sum())\n",
    "\n",
    "def _ind_at_rel0_from_nuc_coords(nuc_coords, rel_pos, mod_bin, mode=\"edges\",\n",
    "                                 min_m6a=0, include_edges=True, max_ind_bp=None):\n",
    "    \"\"\"\n",
    "    Compute IND at x=0 using center-bracketed pair. Optionally gate on #m6A between the pair.\n",
    "      centers: IND = cR - cL                (≥0 by construction)\n",
    "      edges:   IND = (cR - kR//2) - (cL + kL//2); negative → NaN\n",
    "    \"\"\"\n",
    "    C, K = _centers_and_cores(nuc_coords)\n",
    "    l, r = _pair_indices_by_center_at_x(C, x=0)\n",
    "    if l is None or r is None:\n",
    "        return np.nan\n",
    "\n",
    "    cL, cR = C[l], C[r]\n",
    "    if mode == \"centers\":\n",
    "        ind = float(cR - cL)\n",
    "        lo, hi = (cL, cR)\n",
    "    else:\n",
    "        kL, kR = K[l], K[r]\n",
    "        end_left    = cL + (kL // 2)\n",
    "        start_right = cR - (kR // 2)\n",
    "        ind = float(start_right - end_left)\n",
    "        lo, hi = (end_left, start_right)\n",
    "\n",
    "    if max_ind_bp is not None and ind > max_ind_bp:\n",
    "        return np.nan\n",
    "    if mode == \"edges\" and ind < 0:\n",
    "        return np.nan\n",
    "\n",
    "    if min_m6a > 0 and ind >= 0:\n",
    "        cnt = _count_m6a_between(rel_pos, mod_bin, lo, hi, include_edges=include_edges)\n",
    "        if cnt < min_m6a:\n",
    "            return np.nan\n",
    "\n",
    "    return ind\n",
    "\n",
    "def _ind_at_rel0_centers_fallback(nuc_centers):\n",
    "    \"\"\"Fallback if cores are unavailable and IND_MODE == 'centers'.\"\"\"\n",
    "    if not nuc_centers or len(nuc_centers) < 2:\n",
    "        return np.nan\n",
    "    arr = np.asarray(nuc_centers, dtype=np.int64)\n",
    "    left_mask  = arr <= 0\n",
    "    right_mask = arr >  0\n",
    "    if not left_mask.any() or not right_mask.any():\n",
    "        return np.nan\n",
    "    left  = arr[left_mask].max()\n",
    "    right = arr[right_mask].min()\n",
    "    d = int(right - left)\n",
    "    return float(d) if d >= 0 else np.nan\n",
    "\n",
    "# ======================= HELPERS (%m6A) ====================== #\n",
    "def _pct_m6a_in_window(rel_pos, mod_bin, lo=-50, hi=50):\n",
    "    \"\"\"% of 1's among {0,1} in window [lo,hi]. NaN if none.\"\"\"\n",
    "    if rel_pos is None or mod_bin is None:\n",
    "        return np.nan\n",
    "    rp = np.asarray(rel_pos); mb = np.asarray(mod_bin)\n",
    "    if rp.size == 0 or mb.size == 0:\n",
    "        return np.nan\n",
    "    n = min(rp.size, mb.size)\n",
    "    rp = rp[:n]; mb = mb[:n]\n",
    "    mask = (rp >= lo) & (rp <= hi)\n",
    "    if not np.any(mask):\n",
    "        return np.nan\n",
    "    mbw = mb[mask]\n",
    "    valid = (mbw == 0) | (mbw == 1)\n",
    "    if not np.any(valid):\n",
    "        return np.nan\n",
    "    mbv = mbw[valid]\n",
    "    return 100.0 * (float((mbv == 1).sum()) / float(mbv.size))\n",
    "\n",
    "# ========================== WORKER ========================== #\n",
    "def _compute_two_metrics(row_idx):\n",
    "    \"\"\"Worker: returns (row_idx, ind, pct_m6a).\"\"\"\n",
    "    row = filtered_reads_df.loc[row_idx]\n",
    "\n",
    "    # IND at x=0 using nuc_coords; fallback for centers if needed\n",
    "    ind_val = _ind_at_rel0_from_nuc_coords(\n",
    "        row.get(\"nuc_coords\", []),\n",
    "        row.get(\"rel_pos\", []),\n",
    "        row.get(\"mod_qual_bin\", []),\n",
    "        mode=IND_MODE,\n",
    "        min_m6a=MIN_M6A_IN_IND,\n",
    "        include_edges=INCLUDE_EDGES_IN_IND,\n",
    "        max_ind_bp=MAX_IND_BP\n",
    "    )\n",
    "    if np.isnan(ind_val) and IND_MODE == \"centers\":\n",
    "        # optional fallback if cores missing\n",
    "        ind_val = _ind_at_rel0_centers_fallback(row.get(\"nuc_centers\", []))\n",
    "\n",
    "    # %m6A in fixed window\n",
    "    pct_val = _pct_m6a_in_window(row.get(\"rel_pos\", []), row.get(\"mod_qual_bin\", []),\n",
    "                                 lo=M6A_WIN[0], hi=M6A_WIN[1])\n",
    "    return (row_idx, ind_val, pct_val)\n",
    "\n",
    "# ======================= PARALLEL MAP ======================== #\n",
    "idx_all = filtered_reads_df.index.to_numpy()\n",
    "results = []\n",
    "if SHOW_PROGRESS:\n",
    "    with mp.Pool(_NWORKERS) as pool:\n",
    "        for rec in tqdm(pool.imap_unordered(_compute_two_metrics, idx_all, chunksize=_CHUNKSIZE),\n",
    "                        total=len(idx_all), desc=f\"ind({IND_MODE}, m6A≥{MIN_M6A_IN_IND}) + %m6A\"):\n",
    "            results.append(rec)\n",
    "else:\n",
    "    with mp.Pool(_NWORKERS) as pool:\n",
    "        results = pool.map(_compute_two_metrics, idx_all, chunksize=_CHUNKSIZE)\n",
    "\n",
    "# =================== WRITE BACK TO DATAFRAME ================= #\n",
    "inds  = pd.Series({i: ind for (i, ind, pct) in results}, dtype=float)\n",
    "pcts  = pd.Series({i: pct for (i, ind, pct) in results}, dtype=float)\n",
    "\n",
    "filtered_reads_df[\"ind\"]         = inds.reindex(filtered_reads_df.index).astype(float)\n",
    "filtered_reads_df[\"percent_m6a\"] = pcts.reindex(filtered_reads_df.index).astype(float)\n",
    "\n",
    "# ========================= VALIDATION ======================== #\n",
    "if SHOW_SUMMARY:\n",
    "    n_rows = len(filtered_reads_df)\n",
    "    na_ind = int(filtered_reads_df[\"ind\"].isna().sum())\n",
    "    na_pct = int(filtered_reads_df[\"percent_m6a\"].isna().sum())\n",
    "    print(f\"[add-cols] mode={IND_MODE}  m6A_gate={MIN_M6A_IN_IND}  include_edges={INCLUDE_EDGES_IN_IND}  \"\n",
    "          f\"max_ind={MAX_IND_BP}  canon_core={USE_CANONICAL_CORE}\")\n",
    "    print(f\"[add-cols] rows={n_rows:,}  workers={_NWORKERS}\")\n",
    "    print(f\"[add-cols] NaN counts → ind: {na_ind:,}  percent_m6a: {na_pct:,}\")\n",
    "\n",
    "if DEBUG_VALIDATE:\n",
    "    with pd.option_context(\"display.precision\", 3):\n",
    "        print(\"[debug] ind summary:\")\n",
    "        print(filtered_reads_df[\"ind\"].describe(percentiles=[0.1,0.5,0.9]))\n",
    "        print(\"[debug] percent_m6a summary:\")\n",
    "        print(filtered_reads_df[\"percent_m6a\"].describe(percentiles=[0.1,0.5,0.9]))\n",
    "    sample_n = min(DEBUG_SAMPLES, len(filtered_reads_df))\n",
    "    if sample_n > 0:\n",
    "        samp = filtered_reads_df.sample(sample_n, random_state=11)\n",
    "        for i, r in enumerate(samp.itertuples(index=False), 1):\n",
    "            ind_re = _ind_at_rel0_from_nuc_coords(\n",
    "                r.nuc_coords, r.rel_pos, r.mod_qual_bin,\n",
    "                mode=IND_MODE, min_m6a=MIN_M6A_IN_IND,\n",
    "                include_edges=INCLUDE_EDGES_IN_IND, max_ind_bp=MAX_IND_BP\n",
    "            )\n",
    "            if np.isnan(ind_re) and IND_MODE == \"centers\":\n",
    "                ind_re = _ind_at_rel0_centers_fallback(getattr(r, \"nuc_centers\", []))\n",
    "            pct_re = _pct_m6a_in_window(r.rel_pos, r.mod_qual_bin, lo=M6A_WIN[0], hi=M6A_WIN[1])\n",
    "            ok_ind = (np.isnan(r.ind) and np.isnan(ind_re)) or (np.isclose(r.ind, ind_re, rtol=0, atol=1e-9))\n",
    "            ok_pct = (np.isnan(r.percent_m6a) and np.isnan(pct_re)) or (np.isclose(r.percent_m6a, pct_re, rtol=0, atol=1e-9))\n",
    "            print(f\"[debug] row#{i} ind({r.ind} vs {ind_re}) match={ok_ind} | %m6A({r.percent_m6a} vs {pct_re}) match={ok_pct}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b69bb25156f46b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Motif-0 lnP ceiling filter (applies ONLY to selected types) + IND and m6A plots\n",
    "#\n",
    "# • Apply lnP@rel_start==0 filter ONLY for types in TARGET_TYPES.\n",
    "#     – For TARGET_TYPES: keep iff motif at 0 exists AND lnP <= LNP_CEILING.\n",
    "#     – For all other types: keep unconditionally (no lnP requirement).\n",
    "# • Debug prints show what was filtered among TARGET_TYPES, then plots proceed.\n",
    "# • MODE:\n",
    "#     - \"PAIR\": show both conditions and connect with green (Δ>0) / orange (Δ<0).\n",
    "#     - \"DELTA\": plot Δ vs ChIP.  (No regression overlay in this cell.)\n",
    "#\n",
    "# Requirements in `filtered_reads_df`:\n",
    "#   ['condition','type','bed_start','ind','percent_m6a',\n",
    "#    'motif_rel_start','motif_attributes',\n",
    "#    'dpy27_chip','sdc2_chip','sdc3_chip']\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from nanotools import get_color\n",
    "\n",
    "# ─── CONFIG ───────────────────────────────────────────────────────────────────\n",
    "try:\n",
    "    COND1, COND2 = analysis_cond[5], analysis_cond[0]\n",
    "except Exception:\n",
    "    COND1, COND2 = \"COND_BASE\", \"COND_TREAT\"\n",
    "\n",
    "MODE         = \"DELTA\"      # \"PAIR\" or \"DELTA\"\n",
    "LNP_CEILING  = -15.5      # lnP ceiling for motif at rel_start == 0 (keep <= this)\n",
    "IND_CEILING  = 1000         # None to disable (applies to IND only)\n",
    "TEMPLATE     = \"plotly_white\"\n",
    "EPS          = 1e-6\n",
    "CLR_POS      = \"#2CA02C\"    # green for positive Δ\n",
    "CLR_NEG      = \"#FF7F0E\"    # orange for negative Δ\n",
    "X_AXIS_MODE = \"rank\"   # or \"rank\"\n",
    "TARGET_TYPES = {\"MEX_motif\",\"MEXII_motif\",\"motifC\"}  # lnP filter applies only to these\n",
    "CHR_ONLY = \"X\" # \"X\" \"Autosome\" or None\n",
    "TYPES_ONLY = [\"MEX_motif\",\"MEXII_motif\",\"univ_nuc\"]#, \"MEXII_motif\",\"univ_nuc\"]#, \"motifC\"]  # example\n",
    "CHIP_MAP = {\n",
    "    \"DPY-27 ChIP\": \"dpy27_chip\",\n",
    "    #\"SDC-2 ChIP\":  \"sdc2_chip\",\n",
    "    #\"SDC-3 ChIP\":  \"sdc3_chip\",\n",
    "}\n",
    "\n",
    "need_cols = {\"condition\",\"type\",\"bed_start\",\"ind\",\"percent_m6a\",\n",
    "             \"motif_rel_start\",\"motif_attributes\",\"chr_type\"} | set(CHIP_MAP.values())\n",
    "missing = need_cols - set(filtered_reads_df.columns)\n",
    "if missing:\n",
    "    raise KeyError(f\"Missing columns: {sorted(missing)}\")\n",
    "\n",
    "# ─── Extract lnP at rel_start == 0 ────────────────────────────────────────────\n",
    "def _get_lnp_at_zero(row):\n",
    "    rs = row[\"motif_rel_start\"]; attrs = row[\"motif_attributes\"]\n",
    "    if rs is None or attrs is None:\n",
    "        return np.nan\n",
    "    try:\n",
    "        rs_list = list(rs)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "    if 0 not in rs_list:\n",
    "        return np.nan\n",
    "    i = rs_list.index(0)\n",
    "    try:\n",
    "        return float(attrs[i][1])  # (motif_name, lnP, strand, seq)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "df0 = filtered_reads_df[list(need_cols)].copy()\n",
    "df0[\"condition\"] = df0[\"condition\"].astype(str)\n",
    "df0[\"type\"]      = df0[\"type\"].astype(str)\n",
    "\n",
    "if TYPES_ONLY:\n",
    "    TYPES_ONLY = [str(t) for t in TYPES_ONLY]\n",
    "    df0 = df0[df0[\"type\"].astype(str).isin(TYPES_ONLY)].copy()\n",
    "    if df0.empty:\n",
    "        raise RuntimeError(f\"No rows left after type filter: {TYPES_ONLY}\")\n",
    "\n",
    "# ── chr filter (normalize then subset) ─────────────────────────────────\n",
    "df0[\"chr_type\"] = np.where(df0[\"chr_type\"].astype(str).str.upper()==\"X\", \"X\", \"Autosome\")\n",
    "if CHR_ONLY in {\"X\",\"Autosome\"}:\n",
    "    df0 = df0[df0[\"chr_type\"] == CHR_ONLY].copy()\n",
    "    if df0.empty:\n",
    "        raise RuntimeError(f\"No rows after chr_type filter '{CHR_ONLY}'.\")\n",
    "\n",
    "lnp0 = df0.apply(_get_lnp_at_zero, axis=1)\n",
    "is_target   = df0[\"type\"].isin(TARGET_TYPES)\n",
    "\n",
    "# Apply filter ONLY on TARGET_TYPES\n",
    "has_zero_t  = is_target & lnp0.notna()\n",
    "fail_t      = is_target & (lnp0 > float(LNP_CEILING)) & lnp0.notna()\n",
    "keep_target = is_target & lnp0.notna() & (lnp0 <= float(LNP_CEILING))\n",
    "keep_other  = ~is_target  # all non-target rows pass\n",
    "mask_keep   = keep_other | keep_target\n",
    "\n",
    "# ─── Debug summary ────────────────────────────────────────────────────────────\n",
    "n_total       = len(df0)\n",
    "n_target      = int(is_target.sum())\n",
    "n_other       = n_total - n_target\n",
    "n_t_no_zero   = int((is_target & lnp0.isna()).sum())\n",
    "n_t_fail      = int(fail_t.sum())\n",
    "n_t_keep      = int(keep_target.sum())\n",
    "n_other_keep  = int((~is_target).sum())\n",
    "n_keep_total  = int(mask_keep.sum())\n",
    "\n",
    "print(f\"[filter] total={n_total:,}  target_types={n_target:,}  other_types={n_other:,}\")\n",
    "print(f\"[filter] TARGET drop (no motif@0): {n_t_no_zero:,}\")\n",
    "print(f\"[filter] TARGET drop (lnP@0 > {LNP_CEILING}): {n_t_fail:,}\")\n",
    "print(f\"[filter] kept TARGET: {n_t_keep:,}  kept OTHER: {n_other_keep:,}  kept TOTAL: {n_keep_total:,}\")\n",
    "\n",
    "if n_t_no_zero > 0:\n",
    "    ex = df0.loc[is_target & lnp0.isna(), [\"type\",\"bed_start\",\"motif_rel_start\"]].head(5)\n",
    "    print(\"[filter] examples (TARGET missing 0-pos motif):\")\n",
    "    print(ex.to_string(index=False))\n",
    "if n_t_fail > 0:\n",
    "    cols = [\"type\",\"bed_start\"];\n",
    "    if \"read_id\" in df0.columns: cols = [\"read_id\"] + cols\n",
    "    ex = df0.loc[fail_t, cols].head(5).copy()\n",
    "    ex[\"lnP_0\"] = lnp0[fail_t].head(5).values\n",
    "    print(f\"[filter] examples (TARGET failing lnP ceiling > {LNP_CEILING}):\")\n",
    "    print(ex.to_string(index=False))\n",
    "\n",
    "# Apply\n",
    "df = df0[mask_keep].copy()\n",
    "df[\"lnp0\"] = lnp0[mask_keep].values\n",
    "if df.empty:\n",
    "    raise RuntimeError(\"All rows filtered; nothing to plot.\")\n",
    "\n",
    "# ─── Helpers for plots ────────────────────────────────────────────────────────\n",
    "# ─── Shape helpers (place above the plotting loops) ───────────────────────────\n",
    "MarkerTrace = go.Scatter  # SVG scatter → supports marker.symbol\n",
    "def _rank_int(s):\n",
    "    # unique ranks 1..N in ascending order; stable for ties\n",
    "    return pd.Series(s).rank(method=\"first\").astype(int)\n",
    "\n",
    "def _xaxis_layout():\n",
    "    if str(X_AXIS_MODE).upper() == \"RANK\":\n",
    "        return dict(title=\"Rank by ChIP (weak → strong)\")\n",
    "    else:\n",
    "        return dict(title=\"ChIP signal (log10)\", type=\"log\")\n",
    "\n",
    "def _attach_chr(dfin, df_source,\n",
    "                chr_col_candidates=(\"chr_type\",\"bed_chr_type\",\"bed_chr\",\"chr\",\"chrom\")):\n",
    "    \"\"\"Ensure a 'chr_type' column with values in {'X','Autosome'}.\"\"\"\n",
    "    out = dfin.copy()\n",
    "    for c in chr_col_candidates:\n",
    "        if c in df_source.columns:\n",
    "            m = df_source[[\"bed_start\", c]].drop_duplicates(\"bed_start\")\n",
    "            if c in (\"bed_chr\",\"chr\",\"chrom\"):\n",
    "                v = m[c].astype(str).str.upper()\n",
    "                is_x = v.eq(\"X\") | v.eq(\"CHRX\") | v.str.endswith(\"X\")\n",
    "                m = pd.DataFrame({\n",
    "                    \"bed_start\": m[\"bed_start\"],\n",
    "                    \"chr_type\": np.where(is_x, \"X\", \"Autosome\")\n",
    "                })\n",
    "            else:\n",
    "                m = m.rename(columns={c: \"chr_type\"})\n",
    "            out = out.merge(m, on=\"bed_start\", how=\"left\")\n",
    "            break\n",
    "    if \"chr_type\" not in out.columns:\n",
    "        out[\"chr_type\"] = \"X\"\n",
    "    else:\n",
    "        out[\"chr_type\"] = out[\"chr_type\"].fillna(\"X\")\n",
    "        mask = ~out[\"chr_type\"].astype(str).str.upper().isin([\"X\",\"AUTOSOME\"])\n",
    "        out.loc[mask, \"chr_type\"] = \"Autosome\"\n",
    "    # quick sanity\n",
    "    try:\n",
    "        print(f\"[chr] counts:\", out[\"chr_type\"].value_counts(dropna=False).to_dict())\n",
    "    except Exception:\n",
    "        pass\n",
    "    return out\n",
    "\n",
    "def _by_chr_groups(dfin):\n",
    "    if \"chr_type\" not in dfin.columns:\n",
    "        yield (\"all\", \"circle\", dfin)\n",
    "        return\n",
    "    su = dfin[\"chr_type\"].astype(str).str.upper()\n",
    "    m_auto = su != \"X\"\n",
    "    yield (\"X\",        \"circle\", dfin[~m_auto])\n",
    "    yield (\"Autosome\", \"square\", dfin[m_auto])\n",
    "\n",
    "def _add_shape_legend(fig):\n",
    "    # legend-only guides for shapes\n",
    "    fig.add_trace(MarkerTrace(x=[None], y=[None], mode=\"markers\",\n",
    "        name=\"X (circle)\", showlegend=True,\n",
    "        marker=dict(symbol=\"circle\", color=\"#666\", size=8)))\n",
    "    fig.add_trace(MarkerTrace(x=[None], y=[None], mode=\"markers\",\n",
    "        name=\"Autosome (square)\", showlegend=True,\n",
    "        marker=dict(symbol=\"square\", color=\"#666\", size=8)))\n",
    "\n",
    "def _agg_ind(dfin, chip_col, cond):\n",
    "    sub = dfin[dfin[\"condition\"] == cond]\n",
    "    if sub.empty: return pd.DataFrame(columns=[\"type\",\"bed_start\",\"ind_med\",\"chip_med\"])\n",
    "    return (sub.groupby([\"type\",\"bed_start\"], as_index=False)\n",
    "              .agg(ind_med=(\"ind\",\"median\"), chip_med=(chip_col,\"median\")))\n",
    "\n",
    "def _agg_m6a(dfin, chip_col, cond):\n",
    "    sub = dfin[dfin[\"condition\"] == cond]\n",
    "    if sub.empty: return pd.DataFrame(columns=[\"type\",\"bed_start\",\"m6a_mean\",\"chip_med\"])\n",
    "    return (sub.groupby([\"type\",\"bed_start\"], as_index=False)\n",
    "              .agg(m6a_mean=(\"percent_m6a\",\"mean\"), chip_med=(chip_col,\"median\")))\n",
    "\n",
    "def _pair_filter(dfin, x1, y1, x2, y2, apply_ind_ceiling=False):\n",
    "    m = (np.isfinite(dfin[x1]) & np.isfinite(dfin[y1]) &\n",
    "         np.isfinite(dfin[x2]) & np.isfinite(dfin[y2]) &\n",
    "         (dfin[x1] > 0) & (dfin[x2] > 0))\n",
    "    if apply_ind_ceiling and IND_CEILING is not None:\n",
    "        m &= (dfin[y1] <= IND_CEILING) & (dfin[y2] <= IND_CEILING)\n",
    "    return dfin[m].copy()\n",
    "\n",
    "def _build_seg_arrays(dfin, x1, y1, x2, y2):\n",
    "    pos_x, pos_y, neg_x, neg_y = [], [], [], []\n",
    "    for r in dfin.itertuples(index=False):\n",
    "        dy = getattr(r, y2) - getattr(r, y1)\n",
    "        xs = [getattr(r, x1) + EPS, getattr(r, x2) + EPS, None]\n",
    "        ys = [getattr(r, y1),       getattr(r, y2),       None]\n",
    "        if dy >= 0: pos_x += xs; pos_y += ys\n",
    "        else:       neg_x += xs; neg_y += ys\n",
    "    return pos_x, pos_y, neg_x, neg_y\n",
    "\n",
    "# --- Trend helpers (rank x) ---------------------------------------------------\n",
    "TREND_WINDOW = None  # set to an odd int (e.g. 101) to override; None = auto (~5% of N)\n",
    "\n",
    "def _spearman_no_scipy(x, y):\n",
    "    xr = pd.Series(x).rank(method=\"average\").to_numpy(float)\n",
    "    yr = pd.Series(y).rank(method=\"average\").to_numpy(float)\n",
    "    xc = xr - xr.mean(); yc = yr - yr.mean()\n",
    "    den = np.sqrt((xc**2).sum() * (yc**2).sum())\n",
    "    return np.nan if den == 0 else float((xc*yc).sum() / den)\n",
    "\n",
    "def _overlay_rank_trends(fig, dfin, xcol, ycol, window=None, legend_group=\"trend\"):\n",
    "    d = dfin[[xcol, ycol]].dropna().sort_values(xcol)\n",
    "    if d.empty: return np.nan, 0\n",
    "    n = len(d)\n",
    "    if window is None:\n",
    "        window = max(31, int(round(n * 0.05)) | 1)  # odd, ~5% of N\n",
    "    minp = max(10, window // 3)\n",
    "\n",
    "    x = d[xcol].to_numpy(float)\n",
    "    s = d[ycol].reset_index(drop=True)\n",
    "\n",
    "    med = s.rolling(window, center=True, min_periods=minp).median()\n",
    "    try:\n",
    "        q25 = s.rolling(window, center=True, min_periods=minp).quantile(0.25)\n",
    "        q75 = s.rolling(window, center=True, min_periods=minp).quantile(0.75)\n",
    "    except Exception:\n",
    "        q25 = s.rolling(window, center=True, min_periods=minp).apply(lambda v: np.nanpercentile(v, 25))\n",
    "        q75 = s.rolling(window, center=True, min_periods=minp).apply(lambda v: np.nanpercentile(v, 75))\n",
    "\n",
    "    mask = med.notna() & q25.notna() & q75.notna()\n",
    "    if not mask.any(): return np.nan, 0\n",
    "    xv, lo, hi, mv = x[mask.to_numpy()], q25[mask].to_numpy(float), q75[mask].to_numpy(float), med[mask].to_numpy(float)\n",
    "\n",
    "    # IQR ribbon (regular Scatter so fill works)\n",
    "    fig.add_trace(go.Scatter(x=xv, y=lo, mode=\"lines\",\n",
    "        line=dict(width=0), showlegend=False, hoverinfo=\"skip\", name=\"_iqr_lo\", legendgroup=legend_group))\n",
    "    fig.add_trace(go.Scatter(x=xv, y=hi, mode=\"lines\", fill=\"tonexty\",\n",
    "        line=dict(width=0), fillcolor=\"rgba(0,0,0,0.15)\",\n",
    "        name=\"Rolling IQR\", showlegend=True, hoverinfo=\"skip\", legendgroup=legend_group))\n",
    "    # Median\n",
    "    fig.add_trace(go.Scatter(x=xv, y=mv, mode=\"lines\",\n",
    "        line=dict(width=2, color=\"#222\"),\n",
    "        name=\"Rolling median\", showlegend=True, hoverinfo=\"skip\", legendgroup=legend_group))\n",
    "\n",
    "    # Spearman (rank-based)\n",
    "    rho = _spearman_no_scipy(d[xcol], d[ycol])\n",
    "    fig.add_annotation(xref=\"paper\", yref=\"paper\", x=0.98, y=0.02, xanchor=\"right\", yanchor=\"bottom\",\n",
    "                       text=f\"Spearman ρ={rho:.2f} (n={n})\", showarrow=False, font=dict(size=12, color=\"#333\"))\n",
    "    return rho, n\n",
    "# ─────────────────────────── IND plots ────────────────────────────────────────\n",
    "for title, chip_col in CHIP_MAP.items():\n",
    "    a1 = _agg_ind(df, chip_col, COND1)\n",
    "    a2 = _agg_ind(df, chip_col, COND2)\n",
    "    if a1.empty or a2.empty:\n",
    "        print(f\"[IND] {title}: missing data for one condition after filter.\"); continue\n",
    "    pair = a1.merge(a2, on=[\"type\",\"bed_start\"], suffixes=(\"_c1\",\"_c2\"))\n",
    "    if pair.empty:\n",
    "        print(f\"[IND] {title}: no overlapping (type, bed_start).\"); continue\n",
    "\n",
    "    if MODE.upper() == \"PAIR\":\n",
    "        pair = _pair_filter(pair, \"chip_med_c1\",\"ind_med_c1\", \"chip_med_c2\",\"ind_med_c2\", apply_ind_ceiling=True)\n",
    "        if pair.empty:\n",
    "            print(f\"[IND-PAIR] {title}: nothing after numeric/ceiling filter.\"); continue\n",
    "\n",
    "        # choose x\n",
    "        if str(X_AXIS_MODE).upper() == \"RANK\":\n",
    "            pair[\"x_c1\"] = _rank_int(pair[\"chip_med_c1\"])\n",
    "            pair[\"x_c2\"] = _rank_int(pair[\"chip_med_c2\"])\n",
    "            x1, x2 = \"x_c1\", \"x_c2\"\n",
    "            x_hover = \"rank\"\n",
    "        else:\n",
    "            pair[\"x_c1\"] = pair[\"chip_med_c1\"].to_numpy(float) + EPS\n",
    "            pair[\"x_c2\"] = pair[\"chip_med_c2\"].to_numpy(float) + EPS\n",
    "            x1, x2 = \"x_c1\", \"x_c2\"\n",
    "            x_hover = \"ChIP\"\n",
    "\n",
    "        # segments by sign of ΔIND\n",
    "        pos_x, pos_y, neg_x, neg_y = _build_seg_arrays(pair, x1,\"ind_med_c1\", x2,\"ind_med_c2\")\n",
    "        fig = go.Figure()\n",
    "        if pos_x:\n",
    "            fig.add_trace(go.Scattergl(x=pos_x, y=pos_y, mode=\"lines\",\n",
    "                line=dict(color=CLR_POS, width=1), name=\"ΔIND>0\", showlegend=True, hoverinfo=\"skip\"))\n",
    "        if neg_x:\n",
    "            fig.add_trace(go.Scattergl(x=neg_x, y=neg_y, mode=\"lines\",\n",
    "                line=dict(color=CLR_NEG, width=1), name=\"ΔIND<0\", showlegend=True, hoverinfo=\"skip\"))\n",
    "\n",
    "        pair_chr = _attach_chr(pair, df)\n",
    "        for cond, xcol, ycol in [(COND1,\"x_c1\",\"ind_med_c1\"),\n",
    "                                 (COND2,\"x_c2\",\"ind_med_c2\")]:\n",
    "            for grp, sym, sub in _by_chr_groups(pair_chr):\n",
    "                if sub.empty: continue\n",
    "                fig.add_trace(MarkerTrace(\n",
    "                    x=sub[xcol].to_numpy(float),\n",
    "                    y=sub[ycol].to_numpy(float),\n",
    "                    mode=\"markers\",\n",
    "                    name=str(cond) if grp == \"X\" else f\"{cond} (auto)\",\n",
    "                    showlegend=True,\n",
    "                    legendgroup=str(cond),\n",
    "                    marker=dict(color=get_color(str(cond)), size=7, opacity=0.85, symbol=sym),\n",
    "                    customdata=np.stack([sub[\"type\"].to_numpy(str), sub[\"bed_start\"].to_numpy()], axis=1),\n",
    "                    hovertemplate=(f\"cond=%{{fullData.name}}<br>type=%{{customdata[0]}}\"\n",
    "                                   f\"<br>bed_start=%{{customdata[1]}}<br>IND=%{{y:.1f}} bp\"\n",
    "                                   f\"<br>{x_hover}=%{{x}}<extra></extra>\")\n",
    "                ))\n",
    "\n",
    "        _add_shape_legend(fig)\n",
    "        fig.update_layout(\n",
    "            title=f\"{title}: IND paired ({COND1} ↔ {COND2}) [lnP0≤{LNP_CEILING} for {sorted(TARGET_TYPES)}]\",\n",
    "            template=TEMPLATE,\n",
    "            xaxis=_xaxis_layout(),\n",
    "            yaxis=dict(title=\"IND (bp)\"),\n",
    "            legend=dict(title=\"Condition / Shapes\", itemsizing=\"constant\", groupclick=\"toggleitem\"),\n",
    "            width=800\n",
    "        )\n",
    "        print(f\"[IND-PAIR] {title}: pairs={len(pair):,}  pos={bool(pos_x)}  neg={bool(neg_x)}\")\n",
    "        fig.show()\n",
    "\n",
    "    elif MODE.upper() == \"DELTA\":\n",
    "        joined = pair.copy()\n",
    "        if IND_CEILING is not None:\n",
    "            joined = joined[(joined[\"ind_med_c1\"] <= IND_CEILING) & (joined[\"ind_med_c2\"] <= IND_CEILING)]\n",
    "        if joined.empty:\n",
    "            print(f\"[IND-DELTA] {title}: nothing after ceiling.\"); continue\n",
    "\n",
    "        joined[\"delta_ind\"] = joined[\"ind_med_c2\"] - joined[\"ind_med_c1\"]\n",
    "        joined[\"chip_x\"] = np.nanmedian(\n",
    "            np.stack([joined[\"chip_med_c1\"].to_numpy(float),\n",
    "                      joined[\"chip_med_c2\"].to_numpy(float)], axis=1), axis=1)\n",
    "        m = np.isfinite(joined[\"delta_ind\"]) & np.isfinite(joined[\"chip_x\"]) & (joined[\"chip_x\"] > 0)\n",
    "        data = joined.loc[m, [\"type\",\"bed_start\",\"delta_ind\",\"chip_x\"]]\n",
    "        if data.empty:\n",
    "            print(f\"[IND-DELTA] {title}: nothing after finite/log filter.\"); continue\n",
    "\n",
    "        # choose x (rank or chip)\n",
    "        if str(X_AXIS_MODE).upper() == \"RANK\":\n",
    "            data[\"x_plot\"] = pd.Series(data[\"chip_x\"]).rank(method=\"first\").astype(int)\n",
    "            x_hover = \"rank\"\n",
    "        else:\n",
    "            data[\"x_plot\"] = data[\"chip_x\"].to_numpy(float) + EPS\n",
    "            x_hover = \"ChIP\"\n",
    "\n",
    "        fig = go.Figure()\n",
    "        if str(X_AXIS_MODE).upper() == \"RANK\":\n",
    "            _overlay_rank_trends(fig, data, \"x_plot\", \"delta_ind\", window=TREND_WINDOW)\n",
    "        data_chr = _attach_chr(data, df)\n",
    "        for typ, sub_t in data_chr.groupby(\"type\", sort=False):\n",
    "            for grp, sym, sub in _by_chr_groups(sub_t):\n",
    "                if sub.empty: continue\n",
    "                fig.add_trace(MarkerTrace(\n",
    "                    x=sub[\"x_plot\"].to_numpy(float),\n",
    "                    y=sub[\"delta_ind\"].to_numpy(float),\n",
    "                    mode=\"markers\",\n",
    "                    name=str(typ) if grp == \"X\" else f\"{typ} (auto)\",\n",
    "                    showlegend=True,\n",
    "                    legendgroup=str(typ),\n",
    "                    marker=dict(color=get_color(str(typ)), size=7, opacity=0.85, symbol=sym),\n",
    "                    customdata=np.stack([sub[\"type\"].to_numpy(str), sub[\"bed_start\"].to_numpy()], axis=1),\n",
    "                    hovertemplate=(f\"ΔIND=%{{y:.1f}} bp<br>type=%{{customdata[0]}}\"\n",
    "                                   f\"<br>bed_start=%{{customdata[1]}}<br>{x_hover}=%{{x}}\"\n",
    "                                   \"<extra></extra>\")\n",
    "                ))\n",
    "\n",
    "        _add_shape_legend(fig)\n",
    "        fig.update_layout(\n",
    "            title=f\"{title}: ΔIND ({COND2} − {COND1}) [lnP0≤{LNP_CEILING} for {sorted(TARGET_TYPES)}]\",\n",
    "            template=TEMPLATE,\n",
    "            xaxis=_xaxis_layout(),\n",
    "            yaxis=dict(title=\"ΔIND (bp)\"),\n",
    "            legend=dict(title=\"Type / Shapes\", itemsizing=\"constant\", groupclick=\"toggleitem\"),\n",
    "            width=800\n",
    "        )\n",
    "        print(f\"[IND-DELTA] {title}: points={len(data):,}\")\n",
    "        fig.show()\n",
    "\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"MODE must be 'PAIR' or 'DELTA'\")\n",
    "\n",
    "# ───────────────────── m6A accessibility plots ────────────────────────────────\n",
    "for title, chip_col in CHIP_MAP.items():\n",
    "    b1 = _agg_m6a(df, chip_col, COND1)\n",
    "    b2 = _agg_m6a(df, chip_col, COND2)\n",
    "    if b1.empty and b2.empty:\n",
    "        print(f\"[m6A] {title}: no data after filter.\"); continue\n",
    "    pair = b1.merge(b2, on=[\"type\",\"bed_start\"], suffixes=(\"_c1\",\"_c2\"))\n",
    "    if pair.empty:\n",
    "        print(f\"[m6A] {title}: no overlapping (type, bed_start).\"); continue\n",
    "\n",
    "    if MODE.upper() == \"PAIR\":\n",
    "        pair = _pair_filter(pair, \"chip_med_c1\",\"m6a_mean_c1\", \"chip_med_c2\",\"m6a_mean_c2\", apply_ind_ceiling=False)\n",
    "        if pair.empty:\n",
    "            print(f\"[m6A-PAIR] {title}: nothing after numeric filter.\"); continue\n",
    "\n",
    "        if str(X_AXIS_MODE).upper() == \"RANK\":\n",
    "            pair[\"x_c1\"] = _rank_int(pair[\"chip_med_c1\"])\n",
    "            pair[\"x_c2\"] = _rank_int(pair[\"chip_med_c2\"])\n",
    "            x1, x2 = \"x_c1\",\"x_c2\"\n",
    "            x_hover = \"rank\"\n",
    "        else:\n",
    "            pair[\"x_c1\"] = pair[\"chip_med_c1\"].to_numpy(float) + EPS\n",
    "            pair[\"x_c2\"] = pair[\"chip_med_c2\"].to_numpy(float) + EPS\n",
    "            x1, x2 = \"x_c1\",\"x_c2\"\n",
    "            x_hover = \"ChIP\"\n",
    "\n",
    "        pos_x, pos_y, neg_x, neg_y = _build_seg_arrays(pair, x1,\"m6a_mean_c1\", x2,\"m6a_mean_c2\")\n",
    "        fig = go.Figure()\n",
    "        if pos_x:\n",
    "            fig.add_trace(go.Scattergl(x=pos_x, y=pos_y, mode=\"lines\",\n",
    "                line=dict(color=CLR_POS, width=1), name=\"Δm6A>0\", showlegend=True, hoverinfo=\"skip\"))\n",
    "        if neg_x:\n",
    "            fig.add_trace(go.Scattergl(x=neg_x, y=neg_y, mode=\"lines\",\n",
    "                line=dict(color=CLR_NEG, width=1), name=\"Δm6A<0\", showlegend=True, hoverinfo=\"skip\"))\n",
    "\n",
    "        pair_chr = _attach_chr(pair, df)\n",
    "        for cond, xcol, ycol in [(COND1,\"x_c1\",\"m6a_mean_c1\"),\n",
    "                                 (COND2,\"x_c2\",\"m6a_mean_c2\")]:\n",
    "            for grp, sym, sub in _by_chr_groups(pair_chr):\n",
    "                if sub.empty: continue\n",
    "                fig.add_trace(MarkerTrace(\n",
    "                    x=sub[xcol].to_numpy(float),\n",
    "                    y=sub[ycol].to_numpy(float),\n",
    "                    mode=\"markers\",\n",
    "                    name=str(cond) if grp == \"X\" else f\"{cond} (auto)\",\n",
    "                    showlegend=True,\n",
    "                    legendgroup=str(cond),\n",
    "                    marker=dict(color=get_color(str(cond)), size=7, opacity=0.85, symbol=sym),\n",
    "                    customdata=np.stack([sub[\"type\"].to_numpy(str), sub[\"bed_start\"].to_numpy()], axis=1),\n",
    "                    hovertemplate=(f\"cond=%{{fullData.name}}<br>type=%{{customdata[0]}}\"\n",
    "                                   f\"<br>bed_start=%{{customdata[1]}}<br>mean m6A=%{{y:.2f}}%%\"\n",
    "                                   f\"<br>{x_hover}=%{{x}}<extra></extra>\")\n",
    "                ))\n",
    "\n",
    "        _add_shape_legend(fig)\n",
    "        fig.update_layout(\n",
    "            title=f\"{title}: Accessibility (%m6A) paired ({COND1} ↔ {COND2}) [lnP0 filter only for {sorted(TARGET_TYPES)}]\",\n",
    "            template=TEMPLATE,\n",
    "            xaxis=_xaxis_layout(),\n",
    "            yaxis=dict(title=\"Accessibility (% m6A in ±50 bp)\"),\n",
    "            legend=dict(title=\"Condition / Shapes\", itemsizing=\"constant\", groupclick=\"toggleitem\"),\n",
    "            width=800\n",
    "        )\n",
    "        print(f\"[m6A-PAIR] {title}: pairs={len(pair):,}  pos={bool(pos_x)}  neg={bool(neg_x)}\")\n",
    "        fig.show()\n",
    "\n",
    "\n",
    "    elif MODE.upper() == \"DELTA\":\n",
    "        pair = pair[\n",
    "            np.isfinite(pair[\"m6a_mean_c1\"]) & np.isfinite(pair[\"m6a_mean_c2\"]) &\n",
    "            np.isfinite(pair[\"chip_med_c1\"]) & np.isfinite(pair[\"chip_med_c2\"]) &\n",
    "            (pair[\"chip_med_c1\"] > 0) & (pair[\"chip_med_c2\"] > 0)\n",
    "        ].copy()\n",
    "        if pair.empty:\n",
    "            print(f\"[m6A-DELTA] {title}: nothing after numeric/log filter.\"); continue\n",
    "\n",
    "        # % change relative to base (COND1)\n",
    "        base  = pair[\"m6a_mean_c1\"].to_numpy(float)\n",
    "        treat = pair[\"m6a_mean_c2\"].to_numpy(float)\n",
    "        REL_EPS = 1e-6\n",
    "        pair[\"delta_m6a_rel\"] = np.divide(\n",
    "            100.0 * (treat - base), base,\n",
    "            out=np.full_like(base, np.nan, dtype=float),\n",
    "            where=np.abs(base) >= REL_EPS\n",
    "        )\n",
    "\n",
    "        # central x (chip) then choose plotting x per X_AXIS_MODE\n",
    "        pair[\"chip_x\"] = np.nanmedian(\n",
    "            np.stack([pair[\"chip_med_c1\"].to_numpy(float),\n",
    "                      pair[\"chip_med_c2\"].to_numpy(float)], axis=1), axis=1)\n",
    "\n",
    "        ok = np.isfinite(pair[\"delta_m6a_rel\"]) & np.isfinite(pair[\"chip_x\"]) & (pair[\"chip_x\"] > 0)\n",
    "        data = pair.loc[ok, [\"type\",\"bed_start\",\"delta_m6a_rel\",\"chip_x\"]]\n",
    "        if data.empty:\n",
    "            print(f\"[m6A-DELTA] {title}: nothing after finite/log filter.\"); continue\n",
    "\n",
    "        # choose x: rank or chip\n",
    "        if str(X_AXIS_MODE).upper() == \"RANK\":\n",
    "            data[\"x_plot\"] = pd.Series(data[\"chip_x\"]).rank(method=\"first\").astype(int)\n",
    "            x_hover = \"rank\"\n",
    "        else:\n",
    "            data[\"x_plot\"] = data[\"chip_x\"].to_numpy(float) + EPS\n",
    "            x_hover = \"ChIP\"\n",
    "\n",
    "        fig = go.Figure()\n",
    "        if str(X_AXIS_MODE).upper() == \"RANK\":\n",
    "            _overlay_rank_trends(fig, data, \"x_plot\", \"delta_m6a_rel\", window=TREND_WINDOW)\n",
    "        data_chr = _attach_chr(data, df)\n",
    "        for typ, sub_t in data_chr.groupby(\"type\", sort=False):\n",
    "            for grp, sym, sub in _by_chr_groups(sub_t):\n",
    "                if sub.empty: continue\n",
    "                fig.add_trace(MarkerTrace(\n",
    "                    x=sub[\"x_plot\"].to_numpy(float),\n",
    "                    y=sub[\"delta_m6a_rel\"].to_numpy(float),\n",
    "                    mode=\"markers\",\n",
    "                    name=str(typ) if grp == \"X\" else f\"{typ} (auto)\",\n",
    "                    showlegend=True,\n",
    "                    legendgroup=str(typ),\n",
    "                    marker=dict(color=get_color(str(typ)), size=7, opacity=0.85, symbol=sym),\n",
    "                    customdata=np.stack([sub[\"type\"].to_numpy(str), sub[\"bed_start\"].to_numpy()], axis=1),\n",
    "                    hovertemplate=(f\"Δ% m6A (rel)=%{{y:.1f}}%<br>type=%{{customdata[0]}}\"\n",
    "                                   f\"<br>bed_start=%{{customdata[1]}}<br>{x_hover}=%{{x}}\"\n",
    "                                   \"<extra></extra>\")\n",
    "                ))\n",
    "\n",
    "        _add_shape_legend(fig)\n",
    "        fig.update_layout(\n",
    "            title=f\"{title}: ΔAccessibility (% change vs base) [lnP0 filter only for {sorted(TARGET_TYPES)}]\",\n",
    "            template=TEMPLATE,\n",
    "            xaxis=_xaxis_layout(),\n",
    "            yaxis=dict(title=\"ΔAccessibility (% change vs base)\"),\n",
    "            legend=dict(title=\"Type / Shapes\", itemsizing=\"constant\", groupclick=\"toggleitem\"),\n",
    "            width=800\n",
    "        )\n",
    "        print(f\"[m6A-DELTA] {title}: points={len(data):,}\")\n",
    "        fig.show()\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"MODE must be 'PAIR' or 'DELTA'\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c513ac2cc1cd65c6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ╔══════════════════════════════════════════════════════════════════╗\n",
    "# ║  CELL A: Build per-read MNase dyad occupancy (parallel)          ║\n",
    "# ╚══════════════════════════════════════════════════════════════════╝\n",
    "# Loads two MNase dyad bedGraph WIG files, filters to chromosomes\n",
    "# and windows present in `filtered_reads_df`, and computes for each\n",
    "# read a binned dyad-occupancy vector in rel_pos space (pos - bed_start).\n",
    "# Output: global `mnase_seq_nuc_occupancy_df`\n",
    "# ────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# ========================== CONFIG ========================== #\n",
    "MNASE_WIG_FILES = [\n",
    "    \"/Data1/ext_data/lieb_mnase_2017/GSM2098437_RT_rep1_MNaseTC_30m_smoothDyads_ce11.wig\",\n",
    "    \"/Data1/ext_data/lieb_mnase_2017/GSM2098437_RT_rep2_MNaseTC_30m_smoothDyads_ce11.wig\",\n",
    "]\n",
    "MNASE_BIN_BP       = 10        # bin step for MNase sampling\n",
    "MNASE_CACHE_PATH   = None      # e.g. \"/Data1/cache/mnase_seq_nuc_occupancy.parquet\"\n",
    "MNASE_NUM_WORKERS  = max(2, mp.cpu_count() - 2)\n",
    "MNASE_CHUNKSIZE    = 256\n",
    "MNASE_DEBUG        = True\n",
    "# relative-zero for MNase rel_pos\n",
    "MNASE_RELATIVE_ZERO = \"midpoint\"   # 'midpoint' or 'start'\n",
    "MNASE_ROUND_REL     = True         # round to nearest bp so rel_pos is int-like\n",
    "\n",
    "# ========================== IMPORTS ========================= #\n",
    "import os, sys, math\n",
    "import numpy as np, pandas as pd\n",
    "import multiprocessing as mp\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# =================== CHROM NAME MAPPING ===================== #\n",
    "_ROMAN = {\"I\":\"I\",\"II\":\"II\",\"III\":\"III\",\"IV\":\"IV\",\"V\":\"V\",\"X\":\"X\",\"M\":\"M\"}\n",
    "def _chrom_ce11_from_df(name: str) -> str:\n",
    "    # \"CHROMOSOME_II\" → \"chrII\"; \"CHROMOSOME_X\"→\"chrX\"\n",
    "    if not isinstance(name, str): return None\n",
    "    if name.startswith(\"CHROMOSOME_\"):\n",
    "        suf = name.split(\"_\", 1)[1]\n",
    "        return f\"chr{_ROMAN.get(suf, suf)}\"\n",
    "    return None\n",
    "\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "print(\"dtypes:\", filtered_reads_df[[\"chrom\",\"bed_start\",\"bed_end\"]].dtypes.to_dict())\n",
    "\n",
    "probe = filtered_reads_df[[\"chrom\",\"bed_start\",\"bed_end\"]].copy()\n",
    "probe[\"wig_chrom\"] = probe[\"chrom\"].map(lambda s: (\"chr\"+s.split(\"_\",1)[1]) if isinstance(s,str) and s.startswith(\"CHROMOSOME_\") else None)\n",
    "\n",
    "# counts\n",
    "print(\"NaN bed_start:\", probe[\"bed_start\"].isna().sum(),\n",
    "      \"NaN bed_end:\", probe[\"bed_end\"].isna().sum())\n",
    "\n",
    "print(\"±inf bed_start:\", np.isinf(probe[\"bed_start\"]).sum(),\n",
    "      \"±inf bed_end:\", np.isinf(probe[\"bed_end\"]).sum())\n",
    "\n",
    "print(\"wig_chrom value_counts:\\n\", probe[\"wig_chrom\"].value_counts(dropna=False).head(20))\n",
    "\n",
    "# rows that will poison the groupby\n",
    "bad = (\n",
    "    probe[\"wig_chrom\"].notna()\n",
    "    & (~np.isfinite(probe[\"bed_start\"]) | ~np.isfinite(probe[\"bed_end\"]))\n",
    ")\n",
    "print(\"bad rows:\", int(bad.sum()))\n",
    "if bad.any():\n",
    "    display(probe.loc[bad].head(20))\n",
    "\n",
    "\n",
    "# Restrict to chromosomes and windows covered by reads\n",
    "_reads = filtered_reads_df[[\"chrom\",\"bed_start\",\"bed_end\"]].copy()\n",
    "\n",
    "# coerce numerics, drop NaN and ±inf\n",
    "_reads[\"bed_start\"] = pd.to_numeric(_reads[\"bed_start\"], errors=\"coerce\")\n",
    "_reads[\"bed_end\"]   = pd.to_numeric(_reads[\"bed_end\"],   errors=\"coerce\")\n",
    "_reads = _reads.replace([np.inf, -np.inf], np.nan).dropna(subset=[\"bed_start\",\"bed_end\"])\n",
    "\n",
    "# map chromosome names\n",
    "def _chrom_ce11_from_df(name: str) -> str:\n",
    "    if not isinstance(name, str) or not name.startswith(\"CHROMOSOME_\"):\n",
    "        return None\n",
    "    suf = name.split(\"_\", 1)[1]\n",
    "    return \"chr\" + {\"I\":\"I\",\"II\":\"II\",\"III\":\"III\",\"IV\":\"IV\",\"V\":\"V\",\"X\":\"X\",\"M\":\"M\"}.get(suf, suf)\n",
    "\n",
    "_reads[\"wig_chrom\"] = _reads[\"chrom\"].map(_chrom_ce11_from_df)\n",
    "_reads = _reads[_reads[\"wig_chrom\"].notna()]\n",
    "\n",
    "# enforce sane windows\n",
    "_reads = _reads[_reads[\"bed_end\"] > _reads[\"bed_start\"]]\n",
    "\n",
    "agg = (\n",
    "    _reads.groupby(\"wig_chrom\", as_index=True)\n",
    "          .agg(lo=(\"bed_start\",\"min\"), hi=(\"bed_end\",\"max\"))\n",
    ")\n",
    "\n",
    "# drop any remaining non-finite groups, then cast\n",
    "finite_mask = np.isfinite(agg[\"lo\"].values) & np.isfinite(agg[\"hi\"].values)\n",
    "if (~finite_mask).any():\n",
    "    print(\"[MNase] Dropping groups with non-finite lo/hi:\",\n",
    "          agg.index[~finite_mask].tolist(), file=sys.stderr)\n",
    "agg = agg.iloc[finite_mask]\n",
    "\n",
    "agg = np.floor(agg).astype(np.int64)  # safe cast\n",
    "chrom_windows = agg.to_dict(orient=\"index\")\n",
    "\n",
    "allowed_chroms = set(chrom_windows.keys())\n",
    "print(\"[MNase] Allowed chromosomes:\", sorted(allowed_chroms))\n",
    "print(\"[MNase] Windows per chromosome:\",\n",
    "      {k:(v[\"lo\"], v[\"hi\"]) for k,v in chrom_windows.items()})\n",
    "\n",
    "if MNASE_DEBUG:\n",
    "    print(f\"[MNase] Allowed chromosomes: {sorted(allowed_chroms)}\", file=sys.stderr)\n",
    "    print(f\"[MNase] Windows per chromosome: \"\n",
    "          f\"{ {k:(v['lo'],v['hi']) for k,v in chrom_windows.items()} }\", file=sys.stderr)\n",
    "\n",
    "# =================== WIG PARSE + INDEX ====================== #\n",
    "def _parse_wig_to_index(path, allowed, windows):\n",
    "    \"\"\"Return dict: chrom -> (starts, ends, values) arrays within requested windows.\"\"\"\n",
    "    out = {c: ([], [], []) for c in allowed}\n",
    "    keep = 0\n",
    "    with open(path, \"r\") as fh:\n",
    "        for line in fh:\n",
    "            if not line or line[0] == \"#\":\n",
    "                continue\n",
    "            parts = line.split()\n",
    "            if len(parts) != 4:\n",
    "                continue\n",
    "            chrom, s, e, v = parts[0], int(parts[1]), int(parts[2]), float(parts[3])\n",
    "            if chrom not in allowed:\n",
    "                continue\n",
    "            lo, hi = windows[chrom][\"lo\"], windows[chrom][\"hi\"]\n",
    "            if e <= lo or s >= hi:\n",
    "                continue\n",
    "            s2, e2 = max(s, lo), min(e, hi)\n",
    "            if s2 < e2:\n",
    "                out[chrom][0].append(s2)\n",
    "                out[chrom][1].append(e2)\n",
    "                out[chrom][2].append(v)\n",
    "                keep += 1\n",
    "    for c in list(out.keys()):\n",
    "        starts = np.asarray(out[c][0], dtype=np.int64)\n",
    "        ends   = np.asarray(out[c][1], dtype=np.int64)\n",
    "        vals   = np.asarray(out[c][2], dtype=np.float32)\n",
    "        # ensure sorted\n",
    "        idx = np.argsort(starts, kind=\"mergesort\")\n",
    "        out[c] = (starts[idx], ends[idx], vals[idx])\n",
    "    return out\n",
    "\n",
    "if MNASE_DEBUG: print(f\"[MNase] Parsing WIGs …\", file=sys.stderr)\n",
    "WIG_IDX = []\n",
    "for p in MNASE_WIG_FILES:\n",
    "    if MNASE_DEBUG: print(f\"  ↳ {p}\", file=sys.stderr)\n",
    "    WIG_IDX.append(_parse_wig_to_index(p, allowed_chroms, chrom_windows))\n",
    "if MNASE_DEBUG: print(f\"[MNase] Done parsing.\", file=sys.stderr)\n",
    "\n",
    "def _sample_bedgraph(idx_tuple, pos):\n",
    "    \"\"\"Vectorized lookup for positions 'pos' using (starts, ends, vals).\"\"\"\n",
    "    starts, ends, vals = idx_tuple\n",
    "    if starts.size == 0:\n",
    "        return np.zeros_like(pos, dtype=np.float32)\n",
    "    ii = np.searchsorted(starts, pos, side=\"right\") - 1\n",
    "    ok = (ii >= 0) & (pos < ends[np.clip(ii, 0, len(ends)-1)])\n",
    "    out = np.zeros_like(pos, dtype=np.float32)\n",
    "    out[ok] = vals[ii[ok]]\n",
    "    return out\n",
    "\n",
    "# =============== PER-READ EXTRACTION (PARALLEL) ============== #\n",
    "def _prep_read_records(df):\n",
    "    recs = []\n",
    "    for r in df.itertuples(index=False):\n",
    "        wc = _chrom_ce11_from_df(r.chrom)\n",
    "        if wc is None or wc not in allowed_chroms:\n",
    "            continue\n",
    "        recs.append((r.read_id, r.condition, wc, int(r.bed_start), int(r.bed_end)))\n",
    "    return recs\n",
    "\n",
    "read_records = _prep_read_records(filtered_reads_df[[\"read_id\",\"condition\",\"chrom\",\"bed_start\",\"bed_end\"]])\n",
    "if MNASE_DEBUG: print(f\"[MNase] Reads eligible: {len(read_records):,}\", file=sys.stderr)\n",
    "\n",
    "# Share WIG_IDX to workers\n",
    "def _worker_read_to_mnase(args):\n",
    "    rid, cond, wchr, bstart, bend = args\n",
    "    # skip bad bounds\n",
    "    if bend <= bstart or (bend - bstart) < MNASE_BIN_BP:\n",
    "        return None\n",
    "\n",
    "    # 1) bin centers across the genomic read span [bed_start, bed_end)\n",
    "    centers = np.arange(bstart, bend, MNASE_BIN_BP, dtype=np.int64) + (MNASE_BIN_BP // 2)\n",
    "\n",
    "    # 2) average WIG values across replicates at genomic centers\n",
    "    vals = None\n",
    "    for rep in WIG_IDX:\n",
    "        if wchr not in rep:\n",
    "            return None\n",
    "        v = _sample_bedgraph(rep[wchr], centers)\n",
    "        vals = v if vals is None else (vals + v)\n",
    "    vals = vals / float(len(WIG_IDX))\n",
    "\n",
    "    # 3) convert genomic centers → rel_pos with zero at the read midpoint\n",
    "    if MNASE_RELATIVE_ZERO == \"midpoint\":\n",
    "        mid = 0.5 * (bstart + bend)   # may be .5 when read length is odd\n",
    "    else:  # 'start'\n",
    "        mid = float(bstart)\n",
    "\n",
    "    rel = centers.astype(np.float64) - mid\n",
    "    if MNASE_ROUND_REL:\n",
    "        rel = np.rint(rel).astype(np.int32)      # integer rel_pos like other tracks\n",
    "    else:\n",
    "        rel = rel.astype(np.float32)\n",
    "\n",
    "    # Optional debug on first few reads\n",
    "    # if MNASE_DEBUG:\n",
    "    #     print(f\"[MNase] {rid}: bstart={bstart}, bend={bend}, mid={mid:.1f}, \"\n",
    "    #           f\"centers[0..2]={centers[:3].tolist()}, rel[0..2]={rel[:3].tolist()}\",\n",
    "    #           file=sys.stderr)\n",
    "\n",
    "    return dict(\n",
    "        read_id=rid, condition=cond, wig_chrom=wchr,\n",
    "        bed_start=int(bstart), bed_end=int(bend),\n",
    "        mnase_rel_pos=rel,\n",
    "        mnase_dyad_occupancy=vals.astype(np.float32),\n",
    "    )\n",
    "\n",
    "results = []\n",
    "if read_records:\n",
    "    with mp.Pool(processes=MNASE_NUM_WORKERS) as pool, tqdm(total=len(read_records), desc=\"MNase per-read\") as bar:\n",
    "        for out in pool.imap_unordered(_worker_read_to_mnase, read_records, chunksize=MNASE_CHUNKSIZE):\n",
    "            if out is not None:\n",
    "                results.append(out)\n",
    "            bar.update(1)\n",
    "\n",
    "mnase_seq_nuc_occupancy_df = pd.DataFrame(results)\n",
    "if MNASE_DEBUG:\n",
    "    n_total = len(read_records)\n",
    "    n_kept  = len(mnase_seq_nuc_occupancy_df)\n",
    "    print(f\"[MNase] Built mnase_seq_nuc_occupancy_df: {n_kept:,}/{n_total:,} reads with data.\", file=sys.stderr)\n",
    "\n",
    "if MNASE_CACHE_PATH:\n",
    "    os.makedirs(os.path.dirname(MNASE_CACHE_PATH), exist_ok=True)\n",
    "    mnase_seq_nuc_occupancy_df.to_parquet(MNASE_CACHE_PATH, index=False)\n",
    "    if MNASE_DEBUG: print(f\"[MNase] Cached → {MNASE_CACHE_PATH}\", file=sys.stderr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7194d1fa35fade2c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m = (combined_bed_df['type'].str.contains('MEX', na=False)) & (combined_bed_df['bed_strand']=='-')\n",
    "(combined_bed_df.loc[m, 'bed_start'] > combined_bed_df.loc[m, 'bed_end']).sum()\n",
    "ctr_l = combined_bed_df.loc[m, 'bed_start'] + bed_window\n",
    "ctr_r = combined_bed_df.loc[m, 'bed_end']   - bed_window\n",
    "print((ctr_l - ctr_r).abs().describe())  # should be ~0 if starts are left edges\n",
    "\n",
    "x = combined_bed_df.query(\"type=='MEX_motif' & bed_strand=='-'\").copy()\n",
    "x[\"half_width\"] = (x.bed_end - x.bed_start)/2\n",
    "print(x[\"half_width\"].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8550bbdf5b5138",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ╔══════════════════════════════════════════════════════════════════╗\n",
    "# ║  CELL B: Raster + %m6A + IND profile + normalized occ + MNase    ║\n",
    "# ╚══════════════════════════════════════════════════════════════════╝\n",
    "\n",
    "# ========================== USER CONFIG ========================== #\n",
    "PLOT_COND    = analysis_cond[0]     # primary condition (required)\n",
    "PLOT_COND2   = analysis_cond[5]     # secondary condition (None → off)\n",
    "TYPE_TO_PLOT = \"MEX_motif\"           # set to a string to filter, or None to disable\n",
    "STRAND_TO_PLOT = \"-\" # \"+\" or None or \"-\"\n",
    "CHR_TYPE_FILTER = [\"Autosome\"]   # e.g., [\"X\"] or [\"Autosome\"]; None disables\n",
    "\n",
    "\n",
    "SCATTER_READS_N   = 200\n",
    "PLOT_WINDOW       = 2000\n",
    "MOV_AVG_WINDOW    = 10               # kept for compatibility (unused here)\n",
    "BIN_ENC_BP        = 30               # raster encoding bin\n",
    "M6A_BIN           = 5\n",
    "M6A_MOVAVG_WIN    = 1                # half-window (bins) for the %m6A moving average\n",
    "HIST_BIN          = 10               # density smoothing for occupancy\n",
    "GAUSS_SIG_HIST    = 2\n",
    "\n",
    "# Evenness params (unused in this cell; kept for compatibility)\n",
    "EVEN_WIN          = 90\n",
    "EVEN_BIN          = 30\n",
    "EVEN_STEP         = 30\n",
    "SMOOTH_EVEN_BINS  = 0\n",
    "\n",
    "MIN_GAP_BP        = 1                # ≥1 bp gap on a raster row\n",
    "type_label        = \"edge-grown\"\n",
    "\n",
    "# Tracks\n",
    "TRACK_BIN         = HIST_BIN         # x-bin for nuc occupancy normalization\n",
    "MNASE_PLOT_BIN    = MNASE_BIN_BP if \"MNASE_BIN_BP\" in globals() else 10\n",
    "\n",
    "# IND profile config\n",
    "IND_BIN_BP               = 10        # x-bin for IND(x)\n",
    "MIN_M6A_IN_IND_PROFILE   = 0         # ≥ this many m6A within the gated span; 0 disables\n",
    "INCLUDE_EDGES_IN_IND     = True      # closed vs open interval at boundaries\n",
    "MAX_IND_BP               = 600       # exclude larger values\n",
    "IND_MODE                 = \"edges\"   # \"edges\" for inner-edge gap; \"centers\" for center-to-center\n",
    "\n",
    "# I/O and debug\n",
    "SAVE_FIGS         = True\n",
    "DEBUG             = True\n",
    "\n",
    "# ───────────────── condition→colour helper ───────────────── #\n",
    "from itertools import cycle\n",
    "CLR_SDC = \"#b12537\"; CLR_N2  = \"#4974a5\"; CLR_DPY = \"#47B562\"\n",
    "_GREY_CYCLE, _cond2clr_cache = cycle([\"#4d4d4d\", \"#a3a3a3\"]), {}\n",
    "def cond_color(cond: str) -> str:\n",
    "    key = cond.lower()\n",
    "    if \"sdc\" in key: return CLR_SDC\n",
    "    if \"dpy\" in key: return CLR_DPY\n",
    "    if \"n2\"  in key: return CLR_N2\n",
    "    if key not in _cond2clr_cache: _cond2clr_cache[key] = next(_GREY_CYCLE)\n",
    "    return _cond2clr_cache[key]\n",
    "\n",
    "# ---------- libs & paths ------------------------------------------ #\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import numpy as np, pandas as pd, sys\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.ndimage import uniform_filter1d\n",
    "\n",
    "OUT_DIR_FIG = Path(\"/tmp/edge_grown_qc_plots\"); OUT_DIR_FIG.mkdir(parents=True, exist_ok=True)\n",
    "STAMP       = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# ---------- core-length palette ----------------------------------- #\n",
    "def _apply_type_chr_filters(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df\n",
    "    if STRAND_TO_PLOT:\n",
    "        out = out[out[\"bed_strand\"]==STRAND_TO_PLOT]\n",
    "    if TYPE_TO_PLOT:\n",
    "        out = out[out[\"type\"] == TYPE_TO_PLOT]\n",
    "    if CHR_TYPE_FILTER not in (None, \"\", []):\n",
    "        allowed = {str(x).lower() for x in CHR_TYPE_FILTER}\n",
    "        out = out[out[\"chr_type\"].astype(str).str.lower().isin(allowed)]\n",
    "    return out\n",
    "\n",
    "\n",
    "def _core_color(clen):\n",
    "    if clen <= 100:   return \"#e5c951\"\n",
    "    elif clen <= 128: return \"#d07987\"\n",
    "    elif clen <= 180: return \"#a06ab4\"\n",
    "    else:             return \"#15819a\"\n",
    "\n",
    "# ---------- Raster traces ----------------------------------------- #\n",
    "def raster_traces(df_sub:pd.DataFrame, cond:str):\n",
    "    traces=[]\n",
    "    for rec in df_sub.itertuples(index=False):\n",
    "        rid=rec.row_id\n",
    "        rel=np.asarray(rec.rel_pos); qual=np.asarray(rec.mod_qual_bin)\n",
    "        m_pos=rel[qual==1]; m_pos=m_pos[(m_pos>=-PLOT_WINDOW)&(m_pos<=PLOT_WINDOW)]\n",
    "        if m_pos.size:\n",
    "            traces.append(go.Scatter(x=m_pos,y=[rid]*len(m_pos),\n",
    "                                     mode=\"markers\",\n",
    "                                     marker=dict(size=2,color=\"black\"),\n",
    "                                     hoverinfo=\"skip\",showlegend=False))\n",
    "        for cen,core,*_ in rec.nuc_coords:\n",
    "            clr=_core_color(core)\n",
    "            traces.extend([\n",
    "                go.Scatter(x=[cen-core//2,cen+core//2],y=[rid,rid],mode=\"lines\",\n",
    "                           line=dict(width=4,color=clr),opacity=0.25,\n",
    "                           hoverinfo=\"skip\",showlegend=False),\n",
    "                go.Scatter(x=[cen],y=[rid],mode=\"markers\",\n",
    "                           marker=dict(size=4,color=clr),\n",
    "                           hoverinfo=\"skip\",showlegend=False)\n",
    "            ])\n",
    "        traces.append(go.Scatter(x=[rel.min(),rel.max()],y=[rid,rid],\n",
    "                                 mode=\"lines\",\n",
    "                                 line=dict(color=\"rgba(120,120,120,0.4)\",width=1),\n",
    "                                 hoverinfo=\"skip\",showlegend=False))\n",
    "    return traces\n",
    "\n",
    "def build_plot_df(cond:str)->pd.DataFrame:\n",
    "    df = filtered_reads_df.loc[filtered_reads_df[\"condition\"]==cond].reset_index(drop=True)\n",
    "    pre_n = len(df)\n",
    "    df = _apply_type_chr_filters(df)\n",
    "    if DEBUG:\n",
    "        msg = [f\"[INFO] {cond}: {pre_n:,} reads total\"]\n",
    "        msg.append(f\"[INFO] {cond}: {len(df):,} after type/chr filters\"\n",
    "                   f\" (TYPE_TO_PLOT={TYPE_TO_PLOT}, CHR_TYPE_FILTER={CHR_TYPE_FILTER})\")\n",
    "        print(\"\\n\".join(msg), file=sys.stderr)\n",
    "    if df.empty: return df\n",
    "    # Keep reads with ≥1 nuc in window\n",
    "    df = df[df.nuc_centers.apply(lambda c:any((-PLOT_WINDOW<=x<=PLOT_WINDOW) for x in c))]\n",
    "    if DEBUG: print(f\"[INFO] {cond}: {len(df):,} reads inside window\", file=sys.stderr)\n",
    "    df = df.sample(min(SCATTER_READS_N,len(df)), random_state=0).reset_index(drop=True)\n",
    "\n",
    "    # Greedy row-packing by extents\n",
    "    row_ends, row_ids=[],[]\n",
    "    for rec in df.itertuples(index=False):\n",
    "        r_start, r_end = min(rec.rel_pos), max(rec.rel_pos)\n",
    "        for idx,last_end in enumerate(row_ends):\n",
    "            if r_start > last_end + (MIN_GAP_BP-1):\n",
    "                row_ids.append(idx+1); row_ends[idx]=r_end; break\n",
    "        else:\n",
    "            row_ends.append(r_end); row_ids.append(len(row_ends))\n",
    "    df=df.assign(row_id=row_ids)\n",
    "    return df\n",
    "\n",
    "\n",
    "# ---------- %m6A profile ------------------------------------------ #\n",
    "def m6a_profile(df_full:pd.DataFrame):\n",
    "    edges=np.arange(-PLOT_WINDOW,PLOT_WINDOW+M6A_BIN,M6A_BIN)\n",
    "    bin_centres=(edges[:-1]+edges[1:])/2\n",
    "    c1=np.zeros(len(edges)-1,int); ctot=np.zeros_like(c1)\n",
    "    for rel_pos,qual in df_full[['rel_pos','mod_qual_bin']].itertuples(index=False):\n",
    "        x=np.asarray(rel_pos); q=np.asarray(qual)\n",
    "        mask=(q==1)|(q==0); idx=np.digitize(x[mask],edges)-1\n",
    "        ok=(idx>=0)&(idx<len(c1)); idx=idx[ok]; q=q[mask][ok]\n",
    "        ctot[idx]+=1; c1[idx]+=(q==1)\n",
    "    with np.errstate(divide='ignore',invalid='ignore'):\n",
    "        pct=c1/ctot.astype(float)\n",
    "    if M6A_MOVAVG_WIN > 0:\n",
    "        win = 2 * M6A_MOVAVG_WIN + 1\n",
    "        num = uniform_filter1d(c1.astype(float), size=win, mode=\"nearest\")\n",
    "        den = uniform_filter1d(ctot.astype(float), size=win, mode=\"nearest\")\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            pct = np.divide(num, den, out=np.zeros_like(num), where=den>0)\n",
    "    return bin_centres, pct\n",
    "\n",
    "# ---------- Normalized nuc occupancy ------------------------------- #\n",
    "def normalized_nuc_occupancy(df_full: pd.DataFrame):\n",
    "    centres = np.concatenate(df_full.nuc_centers.values) if len(df_full) else np.array([])\n",
    "    if centres.size == 0:\n",
    "        return np.array([]), np.array([])\n",
    "    bins = np.arange(-PLOT_WINDOW, PLOT_WINDOW + TRACK_BIN, TRACK_BIN)\n",
    "    counts, _ = np.histogram(centres, bins=bins)\n",
    "    avg = counts.mean() if counts.size else 0.0\n",
    "    norm = counts / avg if avg > 0 else np.zeros_like(counts, dtype=float)\n",
    "    bc = (bins[:-1] + bins[1:]) / 2\n",
    "    return bc, norm\n",
    "\n",
    "# ---------- MNase dyad occupancy track ---------------------------- #\n",
    "def mnase_dyad_occupancy_track(df_full: pd.DataFrame):\n",
    "    if \"mnase_seq_nuc_occupancy_df\" not in globals():\n",
    "        if DEBUG: print(\"[MNase] mnase_seq_nuc_occupancy_df not found → skipping track.\", file=sys.stderr)\n",
    "        return np.array([]), np.array([])\n",
    "    use_ids = set(df_full[\"read_id\"])\n",
    "    sub = mnase_seq_nuc_occupancy_df[mnase_seq_nuc_occupancy_df[\"read_id\"].isin(use_ids)]\n",
    "    if sub.empty:\n",
    "        if DEBUG: print(\"[MNase] No MNase entries for reads in this panel.\", file=sys.stderr)\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "    g_edges = np.arange(-PLOT_WINDOW, PLOT_WINDOW + MNASE_PLOT_BIN, MNASE_PLOT_BIN)\n",
    "    g_cent  = (g_edges[:-1] + g_edges[1:]) / 2\n",
    "    sumv = np.zeros_like(g_cent, dtype=float)\n",
    "    cntv = np.zeros_like(g_cent, dtype=int)\n",
    "\n",
    "    for rel, occ in zip(sub[\"mnase_rel_pos\"], sub[\"mnase_dyad_occupancy\"]):\n",
    "        rel = np.asarray(rel); occ = np.asarray(occ, float)\n",
    "        mask = (rel >= g_edges[0]) & (rel < g_edges[-1])\n",
    "        if not mask.any(): continue\n",
    "        idx = np.digitize(rel[mask], g_edges) - 1\n",
    "        np.add.at(sumv, idx, occ[mask])\n",
    "        np.add.at(cntv, idx, 1)\n",
    "\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        meanv = np.divide(sumv, cntv, out=np.zeros_like(sumv), where=cntv>0)\n",
    "    return g_cent, meanv\n",
    "\n",
    "# ─────────────────────────── IND helpers ────────────────────────── #\n",
    "def _centers_and_cores(nuc_coords):\n",
    "    \"\"\"Return sorted centers and matching cores from [(center, core_len, ...), ...].\"\"\"\n",
    "    if not nuc_coords: return np.array([], int), np.array([], int)\n",
    "    cc, kk = [], []\n",
    "    for t in nuc_coords:\n",
    "        try:\n",
    "            c = int(t[0]); k = int(t[1])\n",
    "        except Exception:\n",
    "            continue\n",
    "        cc.append(c); kk.append(k)\n",
    "    if not cc: return np.array([], int), np.array([], int)\n",
    "    cc = np.array(cc, int); kk = np.array(kk, int)\n",
    "    order = np.argsort(cc)\n",
    "    return cc[order], kk[order]\n",
    "\n",
    "def _ind_profile_for_read(\n",
    "    nuc_coords, x_centers, pos1=None, min_m6a=0, include_edges=True,\n",
    "    max_ind_bp=None, mode=\"edges\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Always choose left/right by centers bracketing x.\n",
    "    mode=\"centers\": IND = cR - cL\n",
    "    mode=\"edges\":   IND = (cR - kR//2) - (cL + kL//2)   # inner-edge gap\n",
    "    m6A gating uses the same pair: [cL,cR] for centers, [end_left,start_right] for edges.\n",
    "    \"\"\"\n",
    "    C, K = _centers_and_cores(nuc_coords)\n",
    "    n = C.size\n",
    "    if n < 2:\n",
    "        return np.full_like(x_centers, np.nan, dtype=float)\n",
    "\n",
    "    out = np.full_like(x_centers, np.nan, dtype=float)\n",
    "\n",
    "    pos1_sorted = None\n",
    "    if min_m6a > 0 and pos1 is not None:\n",
    "        p = np.asarray(pos1, int)\n",
    "        if p.size: pos1_sorted = np.sort(p)\n",
    "\n",
    "    for i, x in enumerate(x_centers):\n",
    "        r = np.searchsorted(C, x, side=\"left\")   # first center >= x\n",
    "        l = r - 1                                # last center <  x\n",
    "        if l < 0 or r >= n:\n",
    "            continue\n",
    "\n",
    "        cL, cR = C[l], C[r]\n",
    "        kL, kR = K[l], K[r]\n",
    "\n",
    "        if mode == \"centers\":\n",
    "            ind = cR - cL\n",
    "            lo = cL if include_edges else cL + 1\n",
    "            hi = cR if include_edges else cR - 1\n",
    "        else:\n",
    "            end_left    = cL + (kL // 2)\n",
    "            start_right = cR - (kR // 2)\n",
    "            ind = start_right - end_left\n",
    "            lo = end_left if include_edges else end_left + 1\n",
    "            hi = start_right if include_edges else start_right - 1\n",
    "\n",
    "        if (max_ind_bp is not None) and (ind > max_ind_bp):\n",
    "            continue\n",
    "\n",
    "        if min_m6a > 0 and pos1_sorted is not None and hi >= lo and ind >= 0:\n",
    "            a = np.searchsorted(pos1_sorted, lo, side=\"left\")\n",
    "            b = np.searchsorted(pos1_sorted, hi, side=\"right\")\n",
    "            if (b - a) < min_m6a:\n",
    "                continue\n",
    "\n",
    "        out[i] = float(ind)\n",
    "    return out\n",
    "\n",
    "def ind_profile(df_full: pd.DataFrame, step_bp:int=IND_BIN_BP,\n",
    "                min_m6a:int=MIN_M6A_IN_IND_PROFILE, include_edges:bool=INCLUDE_EDGES_IN_IND,\n",
    "                max_ind_bp=None, mode:str=IND_MODE):\n",
    "    \"\"\"\n",
    "    Aggregate IND(x) across reads for a condition.\n",
    "    Returns: bin_centers, median_IND, valid_fraction\n",
    "    \"\"\"\n",
    "    edges = np.arange(-PLOT_WINDOW, PLOT_WINDOW + step_bp, step_bp)\n",
    "    x_centers = (edges[:-1] + edges[1:]) / 2.0\n",
    "    if df_full.empty:\n",
    "        return x_centers, np.full_like(x_centers, np.nan, float), np.zeros_like(x_centers, float)\n",
    "\n",
    "    ind_matrix = []\n",
    "    for nuc_coords, rel_pos, mod_bin in df_full[[\"nuc_coords\",\"rel_pos\",\"mod_qual_bin\"]].itertuples(index=False):\n",
    "        pos1 = None\n",
    "        if min_m6a > 0 and (rel_pos is not None) and (mod_bin is not None):\n",
    "            rp = np.asarray(rel_pos); mb = np.asarray(mod_bin)\n",
    "            if rp.size and mb.size:\n",
    "                n = min(rp.size, mb.size)\n",
    "                pos1 = rp[:n][mb[:n] == 1]\n",
    "        ind_vec = _ind_profile_for_read(\n",
    "            nuc_coords, x_centers, pos1=pos1,\n",
    "            min_m6a=min_m6a, include_edges=include_edges,\n",
    "            max_ind_bp=max_ind_bp, mode=mode\n",
    "        )\n",
    "        ind_matrix.append(ind_vec)\n",
    "\n",
    "    M = np.vstack(ind_matrix) if ind_matrix else np.empty((0, x_centers.size))\n",
    "    with np.errstate(all='ignore'):\n",
    "        med = np.nanmedian(M, axis=0)\n",
    "        frac = np.nanmean(np.isfinite(M), axis=0) if M.size else np.zeros_like(x_centers, float)\n",
    "    return x_centers, med, frac\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Build dataframes for primary and optional secondary condition\n",
    "# -------------------------------------------------------------------\n",
    "cond_specs=[(PLOT_COND,   build_plot_df(PLOT_COND))]\n",
    "if PLOT_COND2 not in (None,\"\"):\n",
    "    cond_specs.append((PLOT_COND2, build_plot_df(PLOT_COND2)))\n",
    "\n",
    "max_row_A = cond_specs[0][1].row_id.max() if not cond_specs[0][1].empty else 0\n",
    "max_row_B = cond_specs[1][1].row_id.max() if len(cond_specs)==2 and not cond_specs[1][1].empty else 0\n",
    "\n",
    "# ----------------------- assemble traces -------------------------- #\n",
    "mid_traces = {\"m6a\": [], \"ind\": [], \"occ\": [], \"mnase\": []}\n",
    "\n",
    "for cond, _plot_df in cond_specs:\n",
    "    df_full = _apply_type_chr_filters(\n",
    "        filtered_reads_df[filtered_reads_df.condition == cond]\n",
    "    )\n",
    "    if df_full.empty:\n",
    "        continue\n",
    "    col = cond_color(cond)\n",
    "\n",
    "    # % m6A\n",
    "    x_m6a, y_m6a = m6a_profile(df_full)\n",
    "    mid_traces[\"m6a\"].append(go.Scatter(x=x_m6a, y=y_m6a, mode=\"lines\",\n",
    "                                        line=dict(color=col, width=2),\n",
    "                                        name=f\"% m6A ({cond})\"))\n",
    "\n",
    "    # IND profile (median across reads), center-bracketed pairs in both modes\n",
    "    x_ind, y_ind, frac_valid = ind_profile(\n",
    "        df_full, step_bp=IND_BIN_BP,\n",
    "        min_m6a=MIN_M6A_IN_IND_PROFILE,\n",
    "        include_edges=INCLUDE_EDGES_IN_IND,\n",
    "        max_ind_bp=MAX_IND_BP, mode=IND_MODE\n",
    "    )\n",
    "    mid_traces[\"ind\"].append(go.Scatter(\n",
    "        x=x_ind, y=y_ind, mode=\"lines\",\n",
    "        line=dict(color=col, width=2),\n",
    "        name=f\"IND ({'edges' if IND_MODE=='edges' else 'centers'}) ({cond})\",\n",
    "        hovertemplate=\"x=%{x:.0f} bp<br>IND=%{y:.1f} bp<extra></extra>\"\n",
    "    ))\n",
    "\n",
    "    # normalized nuc occupancy\n",
    "    x_occ, y_occ = normalized_nuc_occupancy(df_full)\n",
    "    if y_occ.size:\n",
    "        mid_traces[\"occ\"].append(go.Scatter(x=x_occ, y=y_occ, mode=\"lines\",\n",
    "                                            line=dict(color=col, width=2),\n",
    "                                            name=f\"Normalized nuc occupancy ({cond})\"))\n",
    "\n",
    "    # MNase dyad occupancy\n",
    "    x_mn, y_mn = mnase_dyad_occupancy_track(df_full)\n",
    "    if y_mn.size:\n",
    "        mid_traces[\"mnase\"].append(go.Scatter(x=x_mn, y=y_mn, mode=\"lines\",\n",
    "                                              line=dict(color=col, width=2),\n",
    "                                              name=f\"MNase dyad occupancy ({cond})\"))\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Figure: raster + middle tracks (m6A, IND, norm occ, MNase)\n",
    "# -------------------------------------------------------------------\n",
    "if len(cond_specs) == 2:\n",
    "    rows = 6  # primary raster, m6A, IND, norm occ, MNase, secondary raster\n",
    "    row_heights = [0.38, 0.12, 0.12, 0.12, 0.12, 0.38]\n",
    "    sec_raster_row = 6\n",
    "else:\n",
    "    rows = 5  # primary raster, m6A, IND, norm occ, MNase\n",
    "    row_heights = [0.50, 0.13, 0.13, 0.12, 0.12]\n",
    "\n",
    "fig_raster = make_subplots(rows=rows, cols=1, shared_xaxes=True,\n",
    "                           vertical_spacing=0.02, row_heights=row_heights)\n",
    "\n",
    "# primary raster\n",
    "for tr in raster_traces(cond_specs[0][1], cond_specs[0][0]):\n",
    "    fig_raster.add_trace(tr,row=1,col=1)\n",
    "fig_raster.update_yaxes(title=\"Read #\", range=[0.5, max_row_A + 0.5],\n",
    "                        showticklabels=False, row=1, col=1)\n",
    "\n",
    "# middle panels\n",
    "for tr in mid_traces[\"m6a\"]:  fig_raster.add_trace(tr,row=2,col=1)\n",
    "fig_raster.update_yaxes(title=\"% m6A\", row=2, col=1)\n",
    "\n",
    "for tr in mid_traces[\"ind\"]:  fig_raster.add_trace(tr,row=3,col=1)\n",
    "fig_raster.update_yaxes(\n",
    "    title=\"Edge gap (bp)\" if IND_MODE==\"edges\" else \"Center-to-center (bp)\",\n",
    "    row=3, col=1\n",
    ")\n",
    "\n",
    "for tr in mid_traces[\"occ\"]:  fig_raster.add_trace(tr,row=4,col=1)\n",
    "fig_raster.update_yaxes(title=\"normalized nuc occupancy\", row=4, col=1)\n",
    "\n",
    "if mid_traces[\"mnase\"]:\n",
    "    for tr in mid_traces[\"mnase\"]: fig_raster.add_trace(tr,row=5,col=1)\n",
    "    fig_raster.update_yaxes(title=\"MNase dyad occupancy\", row=5, col=1)\n",
    "else:\n",
    "    if DEBUG: print(\"[MNase] Track absent in this panel.\", file=sys.stderr)\n",
    "\n",
    "# secondary raster (optional)\n",
    "if len(cond_specs) == 2:\n",
    "    for tr in raster_traces(cond_specs[1][1], cond_specs[1][0]):\n",
    "        fig_raster.add_trace(tr, row=sec_raster_row, col=1)\n",
    "    fig_raster.update_yaxes(title=\"Read #\", range=[0.5, max_row_B + 0.5],\n",
    "                            showticklabels=False, row=sec_raster_row, col=1)\n",
    "\n",
    "# axes + layout\n",
    "fig_raster.update_xaxes(title=\"rel_pos (bp)\", range=[-PLOT_WINDOW, PLOT_WINDOW], row=rows, col=1)\n",
    "for ax in fig_raster.layout:\n",
    "    if ax.startswith((\"xaxis\",\"yaxis\")):\n",
    "        fig_raster.layout[ax].showgrid=False\n",
    "\n",
    "title = f\"{PLOT_COND}\" + (f\" vs {PLOT_COND2}\" if len(cond_specs)==2 else \"\")\n",
    "fig_raster.update_layout(template=\"plotly_white\", width=1100,\n",
    "                         height=1040 if rows==6 else 950,\n",
    "                         title=title + \" | raster + %m6A + IND + normalized nuc occupancy + MNase\")\n",
    "\n",
    "fig_raster.show()\n",
    "\n",
    "if SAVE_FIGS:\n",
    "    fig_raster.write_image(OUT_DIR_FIG/f\"raster_{STAMP}.png\", scale=2)\n",
    "    fig_raster.write_image(OUT_DIR_FIG/f\"raster_{STAMP}.svg\")\n",
    "    if DEBUG: print(f\"[SAVE] raster_{STAMP}.png / .svg → {OUT_DIR_FIG}\", file=sys.stderr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afcacb55ac06382",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ╔══════════════════════════════════════════════════════════════════╗\n",
    "# ║  STAND‑ALONE QC CELL – template profile + single‑read deep‑dive  ║\n",
    "# ╚══════════════════════════════════════════════════════════════════╝\n",
    "#\n",
    "#   Generates only:\n",
    "#     1.  All 1st‑pass templates on a centred profile plot\n",
    "#     2.  A single‑read diagnostic panel (m6A + weighted Pearson‑r)\n",
    "#   then shows & (optionally) saves the figures.\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# ============================== CONFIG ============================== #\n",
    "PLOT_COND      = analysis_cond[0]          # condition to draw from\n",
    "TYPE_TO_PLOT   = None                      # e.g. \"MOTIFS_rex48\" or None\n",
    "MOV_AVG_WINDOW = 10                        # r‑smoothing\n",
    "PLOT_WINDOW    = 1000                      # for single‑read axes\n",
    "SAVE_FIGS      = True\n",
    "DEBUG          = True\n",
    "COMPOSITE_R_SMOOTH = 20   # set to an integer window size (odd), or 0 to disable\n",
    "\n",
    "# ── colour controls ------------------------------------------------ #\n",
    "TEMPLATE_COLOR_CYCLE = [\"#333333\", \"#555555\", \"#777777\", \"#999999\"]#[\"#e5c951\", \"#d07987\", \"#a06ab4\", \"#15819a\"]\n",
    "DINUC_PARENT_CORE    = 298\n",
    "DINUC_CLR            = \"#FF6DB6\"\n",
    "TEMPLATE_COLOR_MAP = {\n",
    "    tpl: TEMPLATE_COLOR_CYCLE[i % len(TEMPLATE_COLOR_CYCLE)]\n",
    "    for i, tpl in enumerate(_tpl_stats.keys())\n",
    "}\n",
    "CORE_COLOR_MAP = {tpl[1]: clr for tpl, clr in TEMPLATE_COLOR_MAP.items()}\n",
    "CORE_COLOR_MAP[DINUC_PARENT_CORE] = DINUC_CLR\n",
    "\n",
    "# ── exports -------------------------------------------------------- #\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "OUT_DIR_FIG = Path(\"/Data1/git/meyer-nanopore/scripts/analysis/images_20250707/qc_plots\")\n",
    "OUT_DIR_FIG.mkdir(parents=True, exist_ok=True)\n",
    "STAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# ============================== LIBS =============================== #\n",
    "import numpy as np, pandas as pd, math, sys\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.ndimage import uniform_filter1d\n",
    "\n",
    "# ====================== HELPERS / UTILITIES ======================== #\n",
    "# ── colour controls ------------------------------------------------ #\n",
    "RANGE_COLORS = [\"#333333\", \"#555555\", \"#777777\", \"#999999\"] #[\"#333333\", \"#555555\", \"#777777\", \"#999999\"]\n",
    "\n",
    "def color_by_core(core_len: int) -> str:\n",
    "    if core_len < 100:         return RANGE_COLORS[0]\n",
    "    if core_len <= 124:        return RANGE_COLORS[1]\n",
    "    if core_len <= 169:        return RANGE_COLORS[2]\n",
    "    return RANGE_COLORS[3]\n",
    "\n",
    "DINUC_PARENT_CORE = 298        # unchanged\n",
    "DINUC_CLR          = color_by_core(DINUC_PARENT_CORE)  # bright for halves\n",
    "NAVY               = \"#1f4489\"\n",
    "\n",
    "def _hex_to_rgb(h): h=h.lstrip(\"#\"); return tuple(int(h[i:i+2],16) for i in (0,2,4))\n",
    "def _add_core_shapes(fig, core_list, y0, y1, row, col):\n",
    "    for item in core_list:\n",
    "        cen, core = item[:2]\n",
    "        parent    = item[2] if len(item) == 3 else core\n",
    "        clr_hex   = DINUC_CLR if parent == DINUC_PARENT_CORE else color_by_core(parent)\n",
    "        r, g, b   = _hex_to_rgb(clr_hex)\n",
    "        fig.add_shape(\n",
    "            type=\"rect\",\n",
    "            x0=cen-core//2, x1=cen+core//2,\n",
    "            y0=y0, y1=y1,\n",
    "            xref=f\"x{col}\" if col > 1 else \"x\",\n",
    "            yref=f\"y{row}\",\n",
    "            fillcolor=f\"rgba({r},{g},{b},0.4)\",\n",
    "            line_width=0,\n",
    "        )\n",
    "\n",
    "\n",
    "# ============================ DATA SET ============================= #\n",
    "df_plot = filtered_reads_df.loc[filtered_reads_df[\"condition\"] == PLOT_COND]\n",
    "if TYPE_TO_PLOT:\n",
    "    df_plot = df_plot[df_plot[\"type\"] == TYPE_TO_PLOT]\n",
    "if DEBUG: print(f\"[INFO] {len(df_plot):,} reads in subset\", file=sys.stderr)\n",
    "df_plot = df_plot.reset_index(drop=True)\n",
    "\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# 1. TEMPLATE PROFILE — centred & padded\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "max_tpl_len = max(len(v[0]) for v in _tpl_stats.values())\n",
    "x_tpl       = np.arange(max_tpl_len) - max_tpl_len // 2\n",
    "\n",
    "fig_tpl = go.Figure()\n",
    "for tpl_key, (tpl_arr, *_ ) in _tpl_stats.items():\n",
    "    pad_left  = (max_tpl_len - len(tpl_arr)) // 2\n",
    "    pad_right = max_tpl_len - len(tpl_arr) - pad_left\n",
    "    fig_tpl.add_trace(\n",
    "        go.Scatter(\n",
    "            x=x_tpl,\n",
    "            y=np.pad(tpl_arr, (pad_left, pad_right), constant_values=np.nan),\n",
    "            mode=\"lines\",\n",
    "            line=dict(color=TEMPLATE_COLOR_MAP[tpl_key], width=4),\n",
    "            name=f\"u={tpl_key[0]}, core={tpl_key[1]}, d={tpl_key[2]}\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig_tpl.update_layout(\n",
    "    template=\"plotly_white\", width=700, height=350,\n",
    "    title=\"1st‑pass templates (centred & padded)\",\n",
    "    xaxis_title=\"relative position (bp)\", yaxis_title=\"value\",\n",
    "    showlegend=True\n",
    ")\n",
    "fig_tpl.update_xaxes(showgrid=False); fig_tpl.update_yaxes(showgrid=False)\n",
    "\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# 2. SINGLE‑READ DIAGNOSTIC PANEL\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# ────────────────── choose a read with 50‑100 nucleosomes ───────── #\n",
    "# (fallback: longest read >500 bp if none meet the nuc‑count filter)\n",
    "cand_idx = [\n",
    "    i for i, r in df_plot.iterrows()\n",
    "    if r[\"read_length\"] > 500 and 50 <= len(r[\"nuc_coords\"]) <= 100\n",
    "]\n",
    "\n",
    "if cand_idx:\n",
    "    first_idx = cand_idx[0]            # take the first qualifying read\n",
    "    if DEBUG:\n",
    "        print(f\"[INFO] picked read {df_plot.loc[first_idx,'read_id']} \"\n",
    "              f\"with {len(df_plot.loc[first_idx,'nuc_coords'])} nucleosomes\",\n",
    "              file=sys.stderr)\n",
    "else:\n",
    "    first_idx = next(i for i, r in df_plot.iterrows() if r[\"read_length\"] > 500)\n",
    "    if DEBUG:\n",
    "        print(\"[WARN] no read with 50‑100 nucleosomes found; \"\n",
    "              \"using first read >500 bp instead\", file=sys.stderr)\n",
    "\n",
    "rec = df_plot.iloc[first_idx]\n",
    "\n",
    "x   = np.asarray(rec[\"rel_pos\"], int)\n",
    "sig = np.asarray(rec[\"mod_qual_bin\"], float)\n",
    "\n",
    "vec = np.full(x.ptp() + 1, np.nan); vec[x - x.min()] = sig\n",
    "if PERFORM_FILLING: vec = _fill_met_domains(vec, MET_DOMAIN_WIDTH)\n",
    "if PERFORM_INTERP:  vec = _mode_interpolate(vec, INTERP_WINDOW)\n",
    "\n",
    "# Pearson‑r per template\n",
    "# ── build a composite max‑r curve (smoothing applied per template) ──\n",
    "max_r_by_pos = {}          # x‑coord -> running max r\n",
    "\n",
    "for tpl_key, (tpl_arr, tpl_m, tpl_s, half_len) in _tpl_stats.items():\n",
    "    r_raw = _corr_window(vec, tpl_arr, tpl_m, tpl_s, cond=rec[\"condition\"])\n",
    "    if r_raw.size == 0:\n",
    "        continue\n",
    "    r_s  = uniform_filter1d(r_raw, MOV_AVG_WINDOW, mode=\"nearest\")\n",
    "    x_r  = np.arange(x.min(), x.min() + r_raw.size) + half_len\n",
    "    for x_pos, r_val in zip(x_r, r_s):\n",
    "        # keep the maximum r seen so far at this position\n",
    "        if (x_pos not in max_r_by_pos) or (r_val > max_r_by_pos[x_pos]):\n",
    "            max_r_by_pos[x_pos] = r_val\n",
    "\n",
    "# convert the dict to sorted arrays for plotting\n",
    "comp_x, comp_r = map(np.array, zip(*sorted(max_r_by_pos.items())))\n",
    "# → optional moving‑average on the composite max‑r curve\n",
    "if COMPOSITE_R_SMOOTH and COMPOSITE_R_SMOOTH > 1:\n",
    "    comp_r = uniform_filter1d(comp_r, size=COMPOSITE_R_SMOOTH, mode=\"nearest\")\n",
    "    \n",
    "fig_single = make_subplots(\n",
    "    rows=2, cols=1, shared_xaxes=True,\n",
    "    row_heights=[0.25, 0.75], vertical_spacing=0.02\n",
    ")\n",
    "\n",
    "# ── m6A markers & bars ─────────────────────────────────────────── #\n",
    "mask1 = sig == 1; mask0 = sig == 0\n",
    "fig_single.add_trace(\n",
    "    go.Bar(\n",
    "        x=x[mask1], y=[1] * mask1.sum(),\n",
    "        base=0, width=2, marker_color=\"rgba(0,0,0,1)\",\n",
    "        hoverinfo=\"skip\", showlegend=False),\n",
    "    row=1, col=1\n",
    ")\n",
    "fig_single.add_trace(\n",
    "    go.Scatter(\n",
    "        x=x[mask1], y=sig[mask1],\n",
    "        mode=\"markers\", marker=dict(symbol=\"circle\", size=4, color=\"black\"),\n",
    "        name=\"m6A = 1\"),\n",
    "    row=1, col=1\n",
    ")\n",
    "fig_single.add_trace(\n",
    "    go.Scatter(\n",
    "        x=x[mask0], y=sig[mask0],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(symbol=\"circle-open\", size=4, color=\"black\",\n",
    "                    line=dict(width=0.25)),\n",
    "        name=\"m6A = 0\"),\n",
    "    row=1, col=1\n",
    ")\n",
    "_add_core_shapes(fig_single, rec[\"nuc_coords\"], -0.05, 1.05, 1, 1)\n",
    "fig_single.update_yaxes(range=[-0.1, 1.1], title=\"m6A\", row=1, col=1)\n",
    "\n",
    "fig_single.add_trace(\n",
    "    go.Scatter(\n",
    "        x=comp_x, y=comp_r, mode=\"lines\",\n",
    "        line=dict(color=\"black\", width=2),\n",
    "        name=\"max r (composite)\"),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "_add_core_shapes(fig_single, rec[\"nuc_coords\"], -1.05, 1.05, 2, 1)\n",
    "fig_single.update_yaxes(range=[-1.05, 1.05], title=\"weighted r\", row=2, col=1)\n",
    "\n",
    "fig_single.update_layout(\n",
    "    template=\"plotly_white\", width=1100, height=600,\n",
    "    title=f\"Read {rec['read_id']} ({rec['read_length']} bp)\")\n",
    "for ax in (\"xaxis\", \"xaxis2\", \"yaxis\", \"yaxis2\"):\n",
    "    fig_single.layout[ax].showgrid = False\n",
    "\n",
    "# ============================== SHOW ============================== #\n",
    "fig_tpl.show()\n",
    "fig_single.show()\n",
    "\n",
    "# ============================== SAVE ============================== #\n",
    "if SAVE_FIGS:\n",
    "    for name, fig in [(\"template\", fig_tpl),\n",
    "                      (f\"single_{rec['read_id']}\", fig_single)]:\n",
    "        png_path = OUT_DIR_FIG / f\"{name}_{STAMP}.png\"\n",
    "        svg_path = OUT_DIR_FIG / f\"{name}_{STAMP}.svg\"\n",
    "        fig.write_image(png_path, scale=2)\n",
    "        fig.write_image(svg_path)\n",
    "        if DEBUG:\n",
    "            print(f\"[SAVE] {png_path.name} / {svg_path.name} → {OUT_DIR_FIG}\",\n",
    "                  file=sys.stderr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fecf3dd984ebdf",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ╔══════════════════════════════════════════════════════════════════╗\n",
    "# ║  CELL 3 (re‑styled) – QC plots + optional exp_id filtering       ║\n",
    "# ╚══════════════════════════════════════════════════════════════════╝\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import numpy as np, pandas as pd, math, random\n",
    "import plotly.express as px, plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.ndimage import uniform_filter1d, gaussian_filter1d\n",
    "from scipy.stats import gaussian_kde\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# ========================== USER CONFIG ========================== #\n",
    "PLOT_COND_A      = analysis_cond[0]          # primary condition\n",
    "PLOT_COND_B      = analysis_cond[2]          # secondary condition\n",
    "MOV_AVG_WINDOW   = 10                        # r‑smoothing\n",
    "SCATTER_READS_N  = 50                        # raster subsample\n",
    "CORE_HIST_BIN    = 1                         # histogram bin (bp)\n",
    "BIN_ENC_BP, PLOT_WINDOW = 20, 1000\n",
    "SAVE_FIGS        = True\n",
    "FIG_DIR          = Path(\"/tmp/nuc_qc_plots\"); FIG_DIR.mkdir(exist_ok=True, parents=True)\n",
    "STAMP            = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# —— TEMPORARY exclusion of specific exp_id values ———————— #\n",
    "EXCLUDE_EXP_IDS = [\n",
    "    # \"exp12345\", \"exp98765\"\n",
    "]\n",
    "# ------------------------------------------------------------------ #\n",
    "df_base = filtered_reads_df.loc[~filtered_reads_df[\"exp_id\"].isin(EXCLUDE_EXP_IDS)].copy()\n",
    "if df_base.empty:\n",
    "    raise ValueError(\"All rows filtered out by EXCLUDE_EXP_IDS – nothing to plot.\")\n",
    "\n",
    "# ───── 7‑bin discrete core‑length palette ───────────────────────── #\n",
    "_COLOR_ANCHORS = { 80:\"#e5c951\", 110:\"#d07987\", 147:\"#a06ab4\", 200:\"#15819a\" }\n",
    "def _hex_to_rgb(h): h=h.lstrip(\"#\"); return tuple(int(h[i:i+2],16) for i in (0,2,4))\n",
    "def _rgb_to_hex(rgb): return f\"#{rgb[0]:02x}{rgb[1]:02x}{rgb[2]:02x}\"\n",
    "def _mid_hex(c0,c1):\n",
    "    r0,g0,b0=_hex_to_rgb(c0); r1,g1,b1=_hex_to_rgb(c1)\n",
    "    return _rgb_to_hex(((r0+r1)//2,(g0+g1)//2,(b0+b1)//2))\n",
    "_anchor_lens=sorted(_COLOR_ANCHORS); _anchor_hex=[_COLOR_ANCHORS[L] for L in _anchor_lens]\n",
    "_discrete_lens=[]; _discrete_cols=[]\n",
    "for i,L in enumerate(_anchor_lens):\n",
    "    _discrete_lens.append(L); _discrete_cols.append(_anchor_hex[i])\n",
    "    if i<len(_anchor_lens)-1:\n",
    "        _discrete_lens.append((L+_anchor_lens[i+1])/2)\n",
    "        _discrete_cols.append(_mid_hex(_anchor_hex[i],_anchor_hex[i+1]))\n",
    "_boundaries=[(_discrete_lens[i]+_discrete_lens[i+1])/2 for i in range(len(_discrete_lens)-1)]\n",
    "def _core_color(clen):\n",
    "    if clen<=_boundaries[0]: return _discrete_cols[0]\n",
    "    if clen> _boundaries[-1]:return _discrete_cols[-1]\n",
    "    return _discrete_cols[np.digitize(clen,_boundaries)]\n",
    "\n",
    "def _add_core_shapes(fig, core_list, y0, y1, row, col):\n",
    "    for item in core_list:\n",
    "        cen, core = item[:2]\n",
    "        clr=_core_color(core); r,g,b=_hex_to_rgb(clr)\n",
    "        fig.add_shape(type=\"rect\",x0=cen-core//2,x1=cen+core//2,\n",
    "                      y0=y0,y1=y1,\n",
    "                      xref=f\"x{col}\" if col>1 else \"x\",yref=f\"y{row}\",\n",
    "                      fillcolor=f\"rgba({r},{g},{b},0.4)\",line_width=0)\n",
    "        #fig.add_vline(x=cen,row=row,col=col,line=dict(color=f\"rgb({r},{g},{b})\",width=2))\n",
    "\n",
    "# ====================== 1) core‑length histogram ================== #\n",
    "core_by_cond={}\n",
    "for cond,grp in df_base.groupby(\"condition\"):\n",
    "    lengths=[core for coords in grp[\"nuc_coords\"] for core in (c[1] for c in coords)]\n",
    "    core_by_cond[cond]=np.array(lengths,int)\n",
    "all_lengths=np.concatenate(list(core_by_cond.values()))\n",
    "min_len,max_len=all_lengths.min(),all_lengths.max()\n",
    "bins=np.arange(min_len,max_len+CORE_HIST_BIN,CORE_HIST_BIN)\n",
    "\n",
    "fig_hist=go.Figure(); palette=px.colors.qualitative.Set2+px.colors.qualitative.Set1\n",
    "for i,(cond,lens) in enumerate(core_by_cond.items()):\n",
    "    counts,_=np.histogram(lens,bins=bins)\n",
    "    pct=100*(counts.astype(float)+0.5)/counts.sum()\n",
    "    centers=(bins[:-1]+bins[1:])/2\n",
    "    fig_hist.add_trace(go.Bar(x=centers,y=pct,width=CORE_HIST_BIN,\n",
    "                              marker_color=palette[i%len(palette)],\n",
    "                              opacity=0.3,showlegend=False))\n",
    "    kde=gaussian_kde(lens,bw_method=0.05)\n",
    "    x_kde=np.linspace(min_len,max_len,500); y_kde=kde(x_kde)*CORE_HIST_BIN*100\n",
    "    fig_hist.add_trace(go.Scatter(x=x_kde,y=y_kde,mode=\"lines\",\n",
    "                                  line=dict(color=palette[i%len(palette)],width=2),\n",
    "                                  name=f\"{cond} KDE\"))\n",
    "fig_hist.update_layout(template=\"plotly_white\",barmode=\"group\",\n",
    "    title=\"Nucleosome core‑length distribution (% of nucs)\",\n",
    "    xaxis_title=\"core length (bp)\",yaxis_title=\"percentage of nucleosomes\",\n",
    "    width=1000,height=500); fig_hist.show()\n",
    "\n",
    "# =========================== 2) single read ======================= #\n",
    "df_cond=df_base.query(\"condition == @PLOT_COND_A\")\n",
    "i_example=next(i for i,r in df_cond.iterrows() if r.read_length>3500)\n",
    "rec=df_cond.iloc[i_example]\n",
    "x=np.asarray(rec.rel_pos,int); sig=np.asarray(rec.mod_qual_bin,float)\n",
    "vec=np.full(x.ptp()+1,np.nan); vec[x-x.min()]=sig\n",
    "if PERFORM_FILLING: vec=_fill_met_domains(vec,MET_DOMAIN_WIDTH)\n",
    "if PERFORM_INTERP:  vec=_mode_interpolate(vec,INTERP_WINDOW)\n",
    "\n",
    "fig_single=make_subplots(rows=2,cols=1,shared_xaxes=True,row_heights=[0.55,0.45],\n",
    "                         vertical_spacing=0.03)\n",
    "mask1=sig==1; mask0=sig==0\n",
    "fig_single.add_trace(go.Bar(x=x[mask1],y=[1]*mask1.sum(),base=0,width=2,\n",
    "                            marker_color=\"rgba(0,0,0,1)\",hoverinfo=\"skip\"),row=1,col=1)\n",
    "fig_single.add_trace(go.Scatter(x=x[mask1],y=sig[mask1],mode=\"markers\",\n",
    "                    marker=dict(symbol=\"circle\",size=4,color=\"black\"),name=\"m6A = 1\"),row=1,col=1)\n",
    "fig_single.add_trace(go.Scatter(x=x[mask0],y=sig[mask0],mode=\"markers\",\n",
    "                    marker=dict(symbol=\"circle-open\",size=4,color=\"black\",\n",
    "                                line=dict(width=0.25)),name=\"m6A = 0\"),row=1,col=1)\n",
    "_add_core_shapes(fig_single,rec.nuc_coords,-0.05,1.05,1,1)\n",
    "fig_single.update_yaxes(range=[-0.1,1.1],title=\"m6A\",row=1,col=1)\n",
    "\n",
    "for tpl_key,(tpl_arr,tpl_m,tpl_s,half_len) in _tpl_stats.items():\n",
    "    r_raw=_corr_window(vec,tpl_arr,tpl_m,tpl_s,cond=rec.condition)\n",
    "    if r_raw.size:\n",
    "        r_s=uniform_filter1d(r_raw,MOV_AVG_WINDOW,mode=\"nearest\")\n",
    "        x_r=np.arange(x.min(),x.min()+r_raw.size)+half_len\n",
    "        fig_single.add_trace(go.Scatter(x=x_r,y=r_s,mode=\"lines\",\n",
    "                            line=dict(color=_core_color(tpl_key[1])),\n",
    "                            name=f\"r (core={tpl_key[1]})\"),row=2,col=1)\n",
    "_add_core_shapes(fig_single,rec.nuc_coords,-1.05,1.05,2,1)\n",
    "fig_single.update_yaxes(title=\"weighted r\",row=2,col=1)\n",
    "fig_single.update_layout(template=\"plotly_white\",width=900,height=500,\n",
    "        title=f\"{rec.read_id} ({rec.read_length} bp, {PLOT_COND_A})\")\n",
    "for ax in (\"xaxis\",\"xaxis2\",\"yaxis\",\"yaxis2\"): fig_single.layout[ax].showgrid=False\n",
    "fig_single.show()\n",
    "\n",
    "# ========================= 3) raster etc. ========================= #\n",
    "grid_edges=np.arange(-PLOT_WINDOW,PLOT_WINDOW+BIN_ENC_BP,BIN_ENC_BP)\n",
    "n_bins=len(grid_edges)-1; scan=np.arange(-PLOT_WINDOW,PLOT_WINDOW+1,30)\n",
    "\n",
    "def _one_hot(centres):\n",
    "    if not centres: return np.zeros(n_bins,int)\n",
    "    idx=np.digitize(centres,grid_edges)-1; idx=idx[(idx>=0)&(idx<n_bins)]\n",
    "    v=np.zeros(n_bins,int); v[np.unique(idx)]=1; return v\n",
    "\n",
    "def _prep_condition(cond):\n",
    "    df=df_base.query(\"condition == @cond\")\n",
    "    sub=df.sample(min(SCATTER_READS_N,len(df)),random_state=0)\n",
    "\n",
    "    def _closest(v):\n",
    "        if not v: return np.inf,np.inf\n",
    "        arr=np.asarray(v); idx=np.argmin(np.abs(arr))\n",
    "        return abs(arr[idx]), arr[idx]\n",
    "    sub=(sub.assign(order_key=sub.nuc_centers.apply(_closest))\n",
    "            .sort_values(\"order_key\",kind=\"mergesort\").reset_index(drop=True))\n",
    "\n",
    "    mask=sub[\"rel_pos\"].apply(lambda rel:(np.array(rel)>=-PLOT_WINDOW).any() and\n",
    "                                          (np.array(rel)<= PLOT_WINDOW).any())\n",
    "    sub_scatter=sub[mask].reset_index(drop=True)\n",
    "\n",
    "    scat=[]; all_cen=[]\n",
    "    for rid,row in enumerate(sub_scatter.itertuples(index=False),1):\n",
    "        rel=np.asarray(row.rel_pos); qual=np.asarray(row.mod_qual_bin)\n",
    "        m_pos=rel[qual==1]; m_pos=m_pos[(m_pos>=-PLOT_WINDOW)&(m_pos<=PLOT_WINDOW)]\n",
    "        if m_pos.size: scat.append(go.Scatter(x=m_pos,y=[rid]*len(m_pos),\n",
    "                                              mode=\"markers\",marker=dict(size=2,color=\"black\"),\n",
    "                                              hoverinfo=\"skip\",showlegend=False))\n",
    "        for cen,core,*_ in row.nuc_coords:\n",
    "            clr=_core_color(core)\n",
    "            scat.extend([go.Scatter(x=[cen-core//2,cen+core//2],y=[rid,rid],mode=\"lines\",\n",
    "                                     line=dict(width=4,color=clr),opacity=0.25,\n",
    "                                     hoverinfo=\"skip\",showlegend=False),\n",
    "                         go.Scatter(x=[cen],y=[rid],mode=\"markers\",\n",
    "                                     marker=dict(size=4,color=clr),\n",
    "                                     hoverinfo=\"skip\",showlegend=False)])\n",
    "        all_cen.append(np.asarray(row.nuc_centers))\n",
    "    all_cen=np.concatenate(all_cen) if all_cen else np.array([])\n",
    "    hist_vals,_=np.histogram(all_cen,bins=grid_edges)\n",
    "    hist_pct=hist_vals/len(sub) if len(sub) else np.zeros_like(hist_vals,dtype=float)\n",
    "    hist_sm=uniform_filter1d(hist_pct,size=3,mode=\"nearest\") if hist_pct.size else hist_pct\n",
    "    even=[]\n",
    "    for c in scan:\n",
    "        rel=all_cen-c\n",
    "        counts,_=np.histogram(rel,bins=np.arange(-90,90,30)); counts=counts+0.5\n",
    "        p=counts/counts.sum(); H=-(p*np.log(p)).sum()\n",
    "        even.append(1-math.exp(H)/len(counts))\n",
    "    return sub,scat,hist_sm,even\n",
    "\n",
    "sub_A,scat_A,hist_A,even_A=_prep_condition(PLOT_COND_A)\n",
    "sub_B,scat_B,hist_B,even_B=_prep_condition(PLOT_COND_B)\n",
    "bin_centres=(grid_edges[:-1]+grid_edges[1:])/2\n",
    "\n",
    "fig_raster=make_subplots(rows=4,cols=1,shared_xaxes=True,vertical_spacing=0.02,\n",
    "                         row_heights=[0.45,0.25,0.15,0.45])\n",
    "for tr in scat_A: fig_raster.add_trace(tr,1,1)\n",
    "fig_raster.update_yaxes(title=f\"{PLOT_COND_A} read #\",showticklabels=False,row=1,col=1)\n",
    "for tr in scat_B: fig_raster.add_trace(tr,4,1)\n",
    "fig_raster.update_yaxes(title=f\"{PLOT_COND_B} read #\",showticklabels=False,row=4,col=1)\n",
    "fig_raster.add_trace(go.Scatter(x=bin_centres,y=hist_A,mode=\"lines\",\n",
    "                                line=dict(color=\"black\",width=3),\n",
    "                                name=f\"{PLOT_COND_A} cores/read\"),row=2,col=1)\n",
    "fig_raster.add_trace(go.Scatter(x=bin_centres,y=hist_B,mode=\"lines\",\n",
    "                                line=dict(color=\"black\",width=3,dash=\"dot\"),\n",
    "                                name=f\"{PLOT_COND_B} cores/read\"),row=2,col=1)\n",
    "fig_raster.update_yaxes(title=\"# cores / read\",row=2,col=1)\n",
    "fig_raster.add_trace(go.Scatter(x=scan,y=even_A,mode=\"lines\",\n",
    "                                line=dict(color=\"black\",width=2),\n",
    "                                name=f\"{PLOT_COND_A} clustering\"),row=3,col=1)\n",
    "fig_raster.add_trace(go.Scatter(x=scan,y=even_B,mode=\"lines\",\n",
    "                                line=dict(color=\"black\",width=2,dash=\"dot\"),\n",
    "                                name=f\"{PLOT_COND_B} clustering\"),row=3,col=1)\n",
    "fig_raster.update_yaxes(title=\"clustering\",row=3,col=1)\n",
    "fig_raster.update_xaxes(title=\"rel_pos (bp)\",range=[-PLOT_WINDOW,PLOT_WINDOW],row=3,col=1)\n",
    "for ax in (\"xaxis\",\"xaxis2\",\"xaxis3\",\"xaxis4\",\"yaxis\",\"yaxis2\",\"yaxis3\",\"yaxis4\"):\n",
    "    fig_raster.layout[ax].showgrid=False\n",
    "fig_raster.update_layout(template=\"plotly_white\",width=1050,height=1200,\n",
    "    title=f\"{PLOT_COND_A} vs {PLOT_COND_B} · raster / cores/read / clustering\",\n",
    "    legend=dict(yanchor=\"bottom\",y=-0.05,xanchor=\"left\",x=0.01))\n",
    "fig_raster.show()\n",
    "\n",
    "# ============================== SAVE ============================== #\n",
    "if SAVE_FIGS:\n",
    "    for name,fig in [(\"core_len_hist\",fig_hist),\n",
    "                     (f\"single_{rec['read_id']}\",fig_single),\n",
    "                     (\"raster_100reads\",fig_raster)]:\n",
    "        fig.write_image(FIG_DIR/f\"{name}_{STAMP}.png\",scale=2)\n",
    "        print(f\"[SAVE] {name}_{STAMP}.png → {FIG_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa75a3573af1410",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#  Integrated nucleosome analytics – EVENNESS-ONLY edition                     #\n",
    "#  (run immediately after “CELL 1”)                                            #\n",
    "################################################################################\n",
    "import numpy as np, pandas as pd, plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.ndimage import uniform_filter1d, gaussian_filter1d\n",
    "from scipy.signal import find_peaks   # for dynamic-seed helper\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import math, colorsys                  # only used later\n",
    "\n",
    "# ───────────────────────── USER CONFIG ───────────────────────── #\n",
    "COND_A, COND_B, COND_C = analysis_cond[0], analysis_cond[2], analysis_cond[1]\n",
    "EXP_ID_SUBSET = [\n",
    "    # \"BM_05_30_24_SMACseq_R10_rep1\",\n",
    "    # \"AG-22_11_30_23\"\n",
    "]\n",
    "\n",
    "print(f\"Conditions: {COND_A} vs {COND_B}   (balanced with {COND_C})\")\n",
    "# 1) READ-FILTER / IQR-PLOTS\n",
    "FILTER_MODE      = \"all\"\n",
    "THRESH_DIST      = 88\n",
    "MOTIF_FILTER_N   = 1\n",
    "MAX_READS        = 100000\n",
    "READ_SORT_KEY    = \"type\"\n",
    "\n",
    "# 2) CONSENSUS / ENTROPY\n",
    "CONS_START_POS   = (-0, 0)     # fixed –ve / +ve seed\n",
    "NRL              = 10          # bp ladder spacing\n",
    "CONS_WINDOW_BP   = 90\n",
    "entropy_bin      = 10\n",
    "SMOOTH_COUNTS_HALF_BINS = 0\n",
    "ENT_SMOOTH_WINDOW= 1\n",
    "ENT_IQR_LINE_W   = 1\n",
    "ENT_MED_LINE_W   = 4\n",
    "BOOTSTRAP_SPLITS = 1\n",
    "# ───────────────────────── USER CONFIG ───────────────────────── #\n",
    "# Normalisation strategy for comparing COND_A vs COND_B (and COND_C for balancing)\n",
    "#   \"centres\"        → down‑sample so each (type, bed_start) contributes\n",
    "#                       the same *number of nucleosome centres*.\n",
    "#   \"scaled\"         → keep all reads and scale per‑rung histograms so the\n",
    "#                       effective centre counts match (no read loss).\n",
    "#   \"equal_read_len\" → down‑sample so each (type, bed_start) contributes the\n",
    "#                       same total cumulative rel_read_length.\n",
    "READ_BALANCE_MODE = \"scaled\"         # choose: \"centres\" | \"scaled\" | \"equal_read_len\"\n",
    "\n",
    "# ╔══════════════  SYMMETRY / MIRRORING  ══════════════╗\n",
    "SAVE_PER_TYPE = False     # ← toggle ON / OFF\n",
    "CONSIDER_BED_START = True\n",
    "\n",
    "\n",
    "MIRROR_ABS_DISTANCE = False   # True → pool –x & +x into |distance|\n",
    "                              # False → keep signed positions\n",
    "# ╚════════════════════════════════════════════════════╝\n",
    "\n",
    "# ───────── MOTIF LINES (optional) ───────── #\n",
    "SHOW_MOTIF_LINES = False          # dashed grey guides on plots\n",
    "MOTIF_LINE_STYLE = dict(color=\"grey\", width=1, dash=\"dash\")\n",
    "SHOW_TSS_Q4_LINES  = False                     # green guides for TSS_q4\n",
    "TSS_Q4_LINE_STYLE  = dict(color=\"green\", width=1, dash=\"dash\")\n",
    "SHOW_INDIVIDUAL_LINES = False    # if True, plot the thin per‑replicate traces\n",
    "\n",
    "# ─────────────────────────────────────────── #\n",
    "# 3) GLOBAL & EXPORTS\n",
    "PLOT_WINDOW      = 5000\n",
    "SHOW_DEBUG       = True\n",
    "CLR_A , CLR_B    = \"#b12537\", \"#4974a5\"\n",
    "STAMP = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "OUT_DIR_NEW      = Path(\"/Data1/git/meyer-nanopore/scripts/analysis\"\n",
    "                        \"/images_20250710/SDC2_N2_univnucX/bg2\")\n",
    "OUT_DIR_NEW.mkdir(parents=True, exist_ok=True)\n",
    "rng              = np.random.default_rng(seed=43)\n",
    "################################################################################\n",
    "#                               Helper utilities                               #\n",
    "################################################################################\n",
    "def dbg(msg):           # light wrapper so we can silence easily\n",
    "    if SHOW_DEBUG:\n",
    "        print(msg)\n",
    "\n",
    "def centred_ma(arr, k):\n",
    "    if k == 0:\n",
    "        return arr.astype(float)\n",
    "    pad = np.pad(arr.astype(float), k, mode=\"reflect\")\n",
    "    ker = np.ones(2*k + 1) / (2*k + 1)\n",
    "    return np.convolve(pad, ker, mode=\"valid\")\n",
    "\n",
    "# ---------------- motif / read filter (unchanged from previous cell) ---------\n",
    "def motif_pass(row):\n",
    "    if FILTER_MODE == \"all\" or MOTIF_FILTER_N == 0:\n",
    "        return True\n",
    "    if not row.motif_rel_start or not row.nuc_centers:\n",
    "        return False\n",
    "    motifs = sorted(row.motif_rel_start, key=lambda x: abs(x))[:MOTIF_FILTER_N]\n",
    "    centres= np.asarray(row.nuc_centers)\n",
    "    dists  = [np.min(np.abs(centres - m)) for m in motifs]\n",
    "    if FILTER_MODE == \"under\":\n",
    "        return all(d < THRESH_DIST for d in dists)\n",
    "    if not all(d > THRESH_DIST for d in dists):\n",
    "        return False\n",
    "    hits = np.asarray(row.rel_pos)[np.asarray(row.mod_qual_bin) == 1]\n",
    "    if hits.size == 0:\n",
    "        return False\n",
    "    return any(np.min(np.abs(hits - m)) <= THRESH_DIST for m in motifs)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "################################################################################\n",
    "#                      0)  filter & basic subsample                            #\n",
    "################################################################################\n",
    "\n",
    "# ─────────────── OPTIONAL exp_id OVERRIDE ────────────────\n",
    "# ------------------------------------------------------------\n",
    "# Build a temporary view called `work_df` and leave the master\n",
    "# `filtered_reads_df` untouched.\n",
    "# ------------------------------------------------------------\n",
    "work_df = filtered_reads_df.copy()            # default: just a view\n",
    "# work_df[\"type\"] = \"strong_rex\"\n",
    "work_df = work_df.loc[\n",
    "    work_df[\"type\"].isin([\"TSS_q3\"])\n",
    "]                          # <- copy so we can mutate safely\n",
    "\n",
    "# work_df = work_df.loc[\n",
    "#     work_df[\"bed_start\"] == 11821084\n",
    "# ]\n",
    "\n",
    "if EXP_ID_SUBSET:                      # activate only when you list exp_ids\n",
    "    dbg(f\"[exp_id‑filter] keeping {len(EXP_ID_SUBSET)} experiments\")\n",
    "\n",
    "    # if     exp_id = \"N2_biorep1_fiber_old_R10_04_2025\", set it to \"N2_biorep2_fiber_old_R10_04_2025\",\n",
    "    work_df[\"exp_id\"] = work_df[\"exp_id\"].replace(\n",
    "        \"N2_biorep2_fiber_old_R10_04_2025_D\",\n",
    "        \"AG-22_11_30_23\"\n",
    "    )\n",
    "    work_df[\"exp_id\"] = work_df[\"exp_id\"].replace(\n",
    "        \"N2_biorep2_fiber_old_R10_04_2025_B\",\n",
    "        \"AG-22_11_30_23\"\n",
    "    )\n",
    "    work_df[\"exp_id\"] = work_df[\"exp_id\"].replace(\n",
    "        \"BS_10_17_24_SMACseq_R10_rep2\",\n",
    "        \"BM_05_30_24_SMACseq_R10_rep1\"\n",
    "    )\n",
    "\n",
    "\n",
    "    # 1) slice rows  (this returns a *new* frame)\n",
    "    work_df = work_df.loc[\n",
    "        work_df[\"exp_id\"].isin(EXP_ID_SUBSET)\n",
    "    ]\n",
    "    \n",
    "\n",
    "    \n",
    "    # 2) re‑label the grouping column **inside the copy**\n",
    "    work_df[\"condition\"] = work_df[\"exp_id\"]\n",
    "\n",
    "    # 3) redefine labels for plots / masks\n",
    "    if len(EXP_ID_SUBSET) < 2:\n",
    "        raise ValueError(\"EXP_ID_SUBSET needs ≥2 exp_ids\")\n",
    "    COND_A, COND_B = EXP_ID_SUBSET[:2]\n",
    "    COND_C = EXP_ID_SUBSET[2] if len(EXP_ID_SUBSET) > 2 else \"BAL\"\n",
    "\n",
    "    dbg(f\"[exp_id‑filter] A={COND_A}, B={COND_B}, C={COND_C}\")\n",
    "# ──────────────────────────────────────────────────────────\n",
    "\n",
    "# ---------- collect unique motif start positions ----------\n",
    "if SHOW_MOTIF_LINES:\n",
    "    _raw = []\n",
    "    for tup in work_df[\"motif_rel_start\"].dropna():\n",
    "        # tuple / list / scalar → always iterate\n",
    "        _raw.extend(tup if hasattr(tup, \"__iter__\") else [tup])\n",
    "\n",
    "    motif_positions = sorted({\n",
    "        abs(int(p)) if MIRROR_ABS_DISTANCE else int(p)\n",
    "        for p in _raw\n",
    "    })\n",
    "else:\n",
    "    motif_positions = []\n",
    "\n",
    "# ---------- collect unique TSS_q4 start positions ----------  # INSERT\n",
    "if SHOW_TSS_Q4_LINES:\n",
    "    _tss_raw = []\n",
    "    for rels, attrs in work_df[[\"tss_rel_start\", \"tss_attributes\"]].dropna().itertuples(index=False):\n",
    "        for pos, attr in zip(rels, attrs):          # attr = (start, strand, type)\n",
    "            if attr[2] == \"TSS_q4\":                 # quartile‑4 only\n",
    "                _tss_raw.append(pos)\n",
    "            elif attr[2] == \"TSS_q3\":                 # quartile‑4 only\n",
    "                _tss_raw.append(pos)\n",
    "    tss_q4_positions = sorted({\n",
    "        abs(int(p)) if MIRROR_ABS_DISTANCE else int(p)\n",
    "        for p in _tss_raw\n",
    "    })\n",
    "else:\n",
    "    tss_q4_positions = []\n",
    "# ------------------------------------------------------------\n",
    "# ----------------------------------------------------------\n",
    "def add_motif_guides(fig, *, positions, style, yref=\"paper\"):\n",
    "    \"\"\"Draw vertical dashed lines at each motif position.\"\"\"\n",
    "    for pos in positions:\n",
    "        fig.add_shape(\n",
    "            type=\"line\",\n",
    "            x0=pos, x1=pos, y0=0, y1=1,\n",
    "            xref=\"x\", yref=yref,\n",
    "            line=style,\n",
    "            layer=\"below\"          # keep data traces on top\n",
    "        )\n",
    "\n",
    "\n",
    "maskA   = work_df[\"condition\"] == COND_A\n",
    "maskB   = work_df[\"condition\"] == COND_B\n",
    "dfA_all = work_df[maskA & work_df.apply(motif_pass, axis=1)]\n",
    "dfB_all = work_df[maskB & work_df.apply(motif_pass, axis=1)]\n",
    "maskC   = work_df[\"condition\"] == COND_C\n",
    "dfC_all = work_df[maskC & work_df.apply(motif_pass, axis=1)]\n",
    "dbg(f\"{COND_C}: {len(dfC_all)} reads (balancing only)\")\n",
    "dbg(f\"{COND_A}: {len(dfA_all)} reads  | {COND_B}: {len(dfB_all)} reads\")\n",
    "\n",
    "# ─────────────── BALANCE NUCLEOSOME CENTRES ────────────── #\n",
    "# ──────────── BALANCE helper  (handles 2‑ OR 3‑condition input) ────────────\n",
    "def match_total_readlen_three(dfA, dfB, dfC=None, *, mode=\"length\", base_seed=43, verbose=False):\n",
    "    \"\"\"\n",
    "    Down-sample reads so that, within each (type, bed_start) group,\n",
    "    all conditions contribute the same total metric (nucleosome centers or read length)\n",
    "    but only considering regions that overlap the ±PLOT_WINDOW.\n",
    "    \"\"\"\n",
    "    if mode not in {\"length\", \"centres\"}:\n",
    "        raise ValueError(\"mode must be 'length' or 'centres'\")\n",
    "\n",
    "    use_third = dfC is not None and not dfC.empty\n",
    "    if not use_third:\n",
    "        dfC = pd.DataFrame(columns=dfA.columns)\n",
    "\n",
    "    # Window filter for nucleosome centers\n",
    "    def _centres_in_window(centers):\n",
    "        return np.sum((np.asarray(centers) >= -PLOT_WINDOW) &\n",
    "                      (np.asarray(centers) <= PLOT_WINDOW))\n",
    "\n",
    "    # Metric per read: only count centers or read_length within ±PLOT_WINDOW\n",
    "    def _metric_per_read(df):\n",
    "        if mode == \"length\":\n",
    "            return np.array([np.sum((np.asarray(r.rel_pos) >= -PLOT_WINDOW) &\n",
    "                                    (np.asarray(r.rel_pos) <= PLOT_WINDOW))\n",
    "                             for r in df.itertuples(index=False)])\n",
    "        else:\n",
    "            return np.array([_centres_in_window(r.nuc_centers)\n",
    "                             for r in df.itertuples(index=False)])\n",
    "\n",
    "    # Total metric per group\n",
    "    def _total(df):\n",
    "        if df.empty:\n",
    "            return 0\n",
    "        return _metric_per_read(df).sum()\n",
    "\n",
    "    keep_A, keep_B, keep_C = set(), set(), set()\n",
    "    frames_for_grouping = [dfA, dfB] + ([dfC] if use_third else [])\n",
    "    all_groups = pd.concat(frames_for_grouping).groupby([\"type\", \"bed_start\"])\n",
    "\n",
    "    for (typ, bed), _ in all_groups:\n",
    "        grpA = dfA[(dfA.type == typ) & (dfA.bed_start == bed)]\n",
    "        grpB = dfB[(dfB.type == typ) & (dfB.bed_start == bed)]\n",
    "        grpC = dfC[(dfC.type == typ) & (dfC.bed_start == bed)] if use_third else pd.DataFrame()\n",
    "\n",
    "        totA = _total(grpA)\n",
    "        totB = _total(grpB)\n",
    "        totC = _total(grpC) if use_third else np.inf\n",
    "        target = min(totA, totB, totC)\n",
    "\n",
    "        if target == 0 or np.isinf(target):\n",
    "            continue  # skip empty groups\n",
    "\n",
    "        # Deterministic selection based on in-window metrics\n",
    "        def _select(df, cond):\n",
    "            metrics = _metric_per_read(df)\n",
    "            sel, acc = [], 0\n",
    "            local_rng = np.random.default_rng(hash((cond, typ, bed, base_seed)) & 0xFFFFFFFF)\n",
    "            order = local_rng.permutation(len(df))\n",
    "            for idx in order:\n",
    "                m = metrics[idx]\n",
    "                if m == 0:  # skip reads without in-window contribution\n",
    "                    continue\n",
    "                if acc + m > target:\n",
    "                    continue\n",
    "                sel.append(df.index[idx])\n",
    "                acc += m\n",
    "                if acc >= target:\n",
    "                    break\n",
    "            return sel\n",
    "\n",
    "        keep_A.update(_select(grpA, \"A\"))\n",
    "        keep_B.update(_select(grpB, \"B\"))\n",
    "        if use_third:\n",
    "            keep_C.update(_select(grpC, \"C\"))\n",
    "\n",
    "        if verbose:\n",
    "            who = \"A-B-C\" if use_third else \"A-B\"\n",
    "            unit = \"centres\" if mode == \"centres\" else \"bp\"\n",
    "            print(f\"[BAL-{mode[:3].upper()}] ({typ}, {bed}) → {target} {unit} each ({who})\")\n",
    "\n",
    "    dfA_bal = dfA.loc[sorted(keep_A)].reset_index(drop=True)\n",
    "    dfB_bal = dfB.loc[sorted(keep_B)].reset_index(drop=True)\n",
    "    dfC_bal = dfC.loc[sorted(keep_C)].reset_index(drop=True) if use_third else dfC\n",
    "\n",
    "    return dfA_bal, dfB_bal, dfC_bal\n",
    "\n",
    "# ------ choose balancing strategy --------------------------------\n",
    "if READ_BALANCE_MODE == \"centres\":\n",
    "    dbg(\"[MODE] down‑sample by *in‑window nucleosome centres* per type/bed_start\")\n",
    "    dfA_all_bal, dfB_all_bal, dfC_all_bal = match_total_readlen_three(\n",
    "        dfA_all, dfB_all, dfC_all, mode=\"centres\"\n",
    "    )\n",
    "\n",
    "elif READ_BALANCE_MODE == \"equal_read_len\":\n",
    "    dbg(\"[MODE] down‑sample by *in‑window read length* per type/bed_start\")\n",
    "    dfA_all_bal, dfB_all_bal, dfC_all_bal = match_total_readlen_three(\n",
    "        dfA_all, dfB_all, dfC_all, mode=\"length\"\n",
    "    )\n",
    "\n",
    "elif READ_BALANCE_MODE == \"scaled\":\n",
    "    dbg(\"[MODE] keep all reads, use per‑rung histogram scaling (no down‑sampling)\")\n",
    "    dfA_all_bal, dfB_all_bal = dfA_all, dfB_all\n",
    "else:\n",
    "    raise ValueError(f\"Unknown READ_BALANCE_MODE: {READ_BALANCE_MODE}\")\n",
    "\n",
    "# ---------- quick debug: in‑window totals ----------\n",
    "def _tot_centres(df):\n",
    "    return sum(\n",
    "        np.sum((np.asarray(c) >= -PLOT_WINDOW) & (np.asarray(c) <= PLOT_WINDOW))\n",
    "        for c in df.nuc_centers\n",
    "    )\n",
    "\n",
    "def _tot_len(df):\n",
    "    return sum(\n",
    "        np.sum((np.asarray(r.rel_pos) >= -PLOT_WINDOW) &\n",
    "               (np.asarray(r.rel_pos) <=  PLOT_WINDOW))\n",
    "        for r in df.itertuples(index=False)\n",
    "    )\n",
    "\n",
    "dbg(\"── In‑window totals (after balancing) ──\")\n",
    "if READ_BALANCE_MODE == \"centres\":\n",
    "    dbg(f\"{COND_A}: { _tot_centres(dfA_all_bal)} centres   |   \"\n",
    "        f\"{COND_B}: { _tot_centres(dfB_all_bal)} centres\")\n",
    "else:\n",
    "    dbg(f\"{COND_A}: { _tot_len(dfA_all_bal)} bp read length   |   \"\n",
    "        f\"{COND_B}: { _tot_len(dfB_all_bal)} bp read length\")\n",
    "dbg(\"────────────────────────────────────────\")\n",
    "# ------------------------------------------------------\n",
    "\n",
    "###############################################################################\n",
    "#  PRE-COMPUTE BOOTSTRAP GROUPS  (deterministic per cond×type×bed_start)      #\n",
    "###############################################################################\n",
    "def add_bootstrap_idx(df, splits, base_seed=1234):\n",
    "    \"\"\"\n",
    "    Return a *copy* of df with an added 'boot_idx' column ∈ {0 … splits-1}.\n",
    "    The assignment is reproducible and depends only on (cond,type,bed_start).\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    boot_col = np.full(len(df), -1, int)\n",
    "\n",
    "    for (cond, typ, bed), sub_idx in (\n",
    "        df.groupby([\"condition\", \"type\", \"bed_start\"]).groups.items()\n",
    "    ):\n",
    "        # hash→seed so every slice is shuffled the same way,\n",
    "        # independent of how many other slices exist.\n",
    "        slice_seed = (hash((cond, typ, bed, base_seed)) & 0xFFFFFFFF)\n",
    "        local_rng  = np.random.default_rng(slice_seed)\n",
    "\n",
    "        order = local_rng.permutation(len(sub_idx))\n",
    "        split_idx = np.array_split(order, splits)\n",
    "\n",
    "        for j, arr in enumerate(split_idx):\n",
    "            boot_col[sub_idx[arr]] = j   # assign group j\n",
    "\n",
    "    assert (boot_col >= 0).all(), \"bootstrap assignment failed\"\n",
    "    df[\"boot_idx\"] = boot_col\n",
    "    return df\n",
    "\n",
    "df_bal_all = pd.concat([dfA_all_bal, dfB_all_bal], ignore_index=True)\n",
    "df_bal_all = add_bootstrap_idx(df_bal_all, splits=BOOTSTRAP_SPLITS)\n",
    "\n",
    "dfA_all_bal = df_bal_all[df_bal_all[\"condition\"] == COND_A].reset_index(drop=True)\n",
    "dfB_all_bal = df_bal_all[df_bal_all[\"condition\"] == COND_B].reset_index(drop=True)\n",
    "\n",
    "################################################################################\n",
    "#                      1)  consensus ladder set-up                              #\n",
    "################################################################################\n",
    "def grouped_reads(df):\n",
    "    for (cond, typ, bed), sub in df.groupby([\"condition\",\"type\",\"bed_start\"]):\n",
    "        yield cond, typ, bed, sub.reset_index(drop=True)\n",
    "\n",
    "# fixed seeds → build per-group ladder\n",
    "def _build_ladder(seed_pair):\n",
    "    neg_seed, pos_seed = seed_pair\n",
    "    ladder = []\n",
    "\n",
    "    # walk left from neg_seed\n",
    "    p = neg_seed\n",
    "    while p >= -PLOT_WINDOW:\n",
    "        ladder.append(p)\n",
    "        p -= NRL\n",
    "\n",
    "    # walk right from pos_seed\n",
    "    p = pos_seed\n",
    "    while p <= PLOT_WINDOW:\n",
    "        ladder.append(p)\n",
    "        p += NRL\n",
    "\n",
    "    return sorted(set(ladder))\n",
    "\n",
    "GROUP_SEEDS  = {(cond,typ): CONS_START_POS\n",
    "                for cond,typ,_,_ in grouped_reads(\n",
    "                    pd.concat([dfA_all,dfB_all]))}\n",
    "LADDER_GROUP = {k: _build_ladder(v) for k,v in GROUP_SEEDS.items()}\n",
    "shared_len   = min(len(l) for l in LADDER_GROUP.values())\n",
    "for k in LADDER_GROUP:\n",
    "    LADDER_GROUP[k] = LADDER_GROUP[k][:shared_len]\n",
    "\n",
    "consensus   = [float(np.mean([LADDER_GROUP[k][i] for k in LADDER_GROUP]))\n",
    "               for i in range(shared_len)]\n",
    "cat_labels  = [f\"{round(c,1)}\" for c in consensus]\n",
    "DOT_OFFSET  = NRL * 0.1        # used by several plots\n",
    "\n",
    "# identity map (no adaptive shift)\n",
    "CONS_REF = {k:{p:p for p in LADDER_GROUP[k]} for k in LADDER_GROUP}\n",
    "\n",
    "# ── choose the plotting x-axis, respecting the MIRROR flag ──\n",
    "if MIRROR_ABS_DISTANCE:\n",
    "    PLOT_RUNG_AXIS = sorted({abs(c) for c in consensus})\n",
    "    AXIS_LABEL     = \"Distance from recruitment (bp)\"\n",
    "else:\n",
    "    PLOT_RUNG_AXIS = consensus\n",
    "    AXIS_LABEL     = \"Consensus position (bp)\"\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "#  QUICK STATS – nucleosome counts per entropy window (±CONS_WINDOW_BP)       #\n",
    "###############################################################################\n",
    "def _window_counts(df, axis_rungs, win_bp):\n",
    "    \"\"\"\n",
    "    Return {cond: {rung: centre_count}} for every condition in df.\n",
    "    \"\"\"\n",
    "    out = {c: {r: 0 for r in axis_rungs} for c in df[\"condition\"].unique()}\n",
    "    for cond, sub in df.groupby(\"condition\"):\n",
    "        centres = np.concatenate(sub.nuc_centers.to_numpy()) if not sub.empty else np.array([])\n",
    "        for rung in axis_rungs:\n",
    "            out[cond][rung] += np.sum(np.abs(centres - rung) < win_bp)\n",
    "    return out\n",
    "\n",
    "def _summarise(count_dict):\n",
    "    vals = np.asarray(list(count_dict.values()), int)\n",
    "    return dict( min    = int(vals.min()),\n",
    "                 Q1     = float(np.percentile(vals, 25)),\n",
    "                 median = float(np.median(vals)),\n",
    "                 mean   = float(vals.mean()),\n",
    "                 Q3     = float(np.percentile(vals, 75)),\n",
    "                 max    = int(vals.max()) )\n",
    "\n",
    "# choose which dataframe represents the “final” inputs to entropy\n",
    "df_stats_src = pd.concat([dfA_all_bal, dfB_all_bal], ignore_index=True)\n",
    "\n",
    "axis_rungs = (sorted({abs(c) for c in PLOT_RUNG_AXIS})\n",
    "              if MIRROR_ABS_DISTANCE else list(PLOT_RUNG_AXIS))\n",
    "\n",
    "win_counts = _window_counts(df_stats_src, axis_rungs, CONS_WINDOW_BP)\n",
    "\n",
    "dbg(\"\\n─── Nucleosomes per window (±{} bp) ───\".format(CONS_WINDOW_BP))\n",
    "for cond in [COND_A, COND_B]:\n",
    "    stats = _summarise(win_counts[cond])\n",
    "    dbg(f\"{cond}:  n per window  \"\n",
    "        f\"min={stats['min']}, Q1={stats['Q1']:.1f}, \"\n",
    "        f\"median={stats['median']:.1f}, mean={stats['mean']:.1f}, \"\n",
    "        f\"Q3={stats['Q3']:.1f}, max={stats['max']}\")\n",
    "\n",
    "\n",
    "# ################################################################################\n",
    "# #                      2)  offsets to consensus (fig_iqr)                      #\n",
    "# ################################################################################\n",
    "# def collect_offsets_all(df):\n",
    "#     out={COND_A:{c:[] for c in consensus},\n",
    "#          COND_B:{c:[] for c in consensus}}\n",
    "#     for cond,typ,_,sub in grouped_reads(df):\n",
    "#         ladder=LADDER_GROUP[(cond,typ)]\n",
    "#         for row in sub.itertuples(index=False):\n",
    "#             for cen in row.nuc_centers:\n",
    "#                 idx=np.argmin(np.abs(np.asarray(ladder)-cen))\n",
    "#                 if idx>=shared_len: continue\n",
    "#                 ref=CONS_REF[(cond,typ)][ladder[idx]]\n",
    "#                 d = abs(cen-ref)\n",
    "#                 if d<=CONS_WINDOW_BP:\n",
    "#                     out[cond][consensus[idx]].append(d)\n",
    "#     return out\n",
    "#\n",
    "# offsets = collect_offsets_all(pd.concat([dfA_all,dfB_all], ignore_index=True))\n",
    "#\n",
    "# fig_iqr = go.Figure()\n",
    "# for j,(cond,clr) in enumerate(zip([COND_A,COND_B],[CLR_A,CLR_B])):\n",
    "#     q1,med,q3=[],[],[]\n",
    "#     for c in consensus:\n",
    "#         v=offsets[cond][c]\n",
    "#         q1.append(np.nan if not v else np.percentile(v,25))\n",
    "#         med.append(np.nan if not v else np.percentile(v,50))\n",
    "#         q3.append(np.nan if not v else np.percentile(v,75))\n",
    "#     err_up   = np.asarray(q3)-np.asarray(med)\n",
    "#     err_down = np.asarray(med)-np.asarray(q1)\n",
    "#     x = [c-DOT_OFFSET if j==0 else c+DOT_OFFSET for c in consensus]\n",
    "#     fig_iqr.add_trace(go.Scatter(\n",
    "#         x=x,y=med,mode=\"markers\",name=cond,\n",
    "#         marker=dict(size=10,color=clr),\n",
    "#         error_y=dict(type=\"data\",symmetric=False,\n",
    "#                      array=err_up,arrayminus=err_down,\n",
    "#                      thickness=1,color=clr),\n",
    "#         hovertemplate=(\"cons=%{customdata[2]}<br>\"\n",
    "#                        \"Q1=%{customdata[0]:.1f}  \"\n",
    "#                        \"median=%{y:.1f}  \"\n",
    "#                        \"Q3=%{customdata[1]:.1f}\"),\n",
    "#         customdata=np.column_stack([q1,q3,consensus])\n",
    "#     ))\n",
    "# fig_iqr.add_shape(type=\"line\",x0=0,x1=0,y0=0,y1=1,\n",
    "#                   xref=\"x\",yref=\"paper\",\n",
    "#                   line=dict(color=\"black\",dash=\"dash\"))\n",
    "# fig_iqr.add_annotation(x=0,y=1.05,xref=\"x\",yref=\"paper\",\n",
    "#                        text=\"best consensus match motif\",\n",
    "#                        showarrow=False)\n",
    "# fig_iqr.update_xaxes(type=\"linear\",tickmode=\"array\",\n",
    "#                      tickvals=consensus,ticktext=cat_labels,\n",
    "#                      title=\"Consensus position (bp)\")\n",
    "# fig_iqr.update_layout(template=\"plotly_white\",width=1000,height=500,\n",
    "#                       xaxis=dict(showgrid=False,zeroline=False),\n",
    "#                       yaxis=dict(showgrid=False,zeroline=False),\n",
    "#                       title=(f\"Offsets to consensus (NRL={NRL}, \"\n",
    "#                              f\"window=±{CONS_WINDOW_BP} bp)\"),\n",
    "#                       yaxis_title=\"Centre – consensus (bp)\")\n",
    "# fig_iqr.show()\n",
    "#\n",
    "# ################################################################################\n",
    "# #                      3)  σ (dispersion)  – fig_sigma                         #\n",
    "# ################################################################################\n",
    "# def collect_sigma_all(df):\n",
    "#     sig={COND_A:{c:[] for c in consensus},\n",
    "#          COND_B:{c:[] for c in consensus}}\n",
    "#     for cond,typ,_,sub in grouped_reads(df):\n",
    "#         ladder=LADDER_GROUP[(cond,typ)]\n",
    "#         for i,rung in enumerate(ladder):\n",
    "#             pool=[]\n",
    "#             for row in sub.itertuples(index=False):\n",
    "#                 pool.extend([c for c in row.nuc_centers\n",
    "#                              if abs(c-rung)<=CONS_WINDOW_BP])\n",
    "#             if len(pool)>1:\n",
    "#                 sig[cond][consensus[i]].append(float(np.std(pool,ddof=0)))\n",
    "#     return sig\n",
    "#\n",
    "# sigma_vals = collect_sigma_all(pd.concat([dfA_all,dfB_all], ignore_index=True))\n",
    "#\n",
    "# fig_sigma = go.Figure()\n",
    "# for j,(cond,clr) in enumerate(zip([COND_A,COND_B],[CLR_A,CLR_B])):\n",
    "#     q1,med,q3=[],[],[]\n",
    "#     for c in consensus:\n",
    "#         v=sigma_vals[cond][c]\n",
    "#         q1.append(np.nan if not v else np.percentile(v,25))\n",
    "#         med.append(np.nan if not v else np.percentile(v,50))\n",
    "#         q3.append(np.nan if not v else np.percentile(v,75))\n",
    "#     err_up   = np.asarray(q3)-np.asarray(med)\n",
    "#     err_down = np.asarray(med)-np.asarray(q1)\n",
    "#     x=[c-DOT_OFFSET if j==0 else c+DOT_OFFSET for c in consensus]\n",
    "#     fig_sigma.add_trace(go.Scatter(\n",
    "#         x=x,y=med,mode=\"markers\",name=cond,\n",
    "#         marker=dict(size=10,color=clr),\n",
    "#         error_y=dict(type=\"data\",symmetric=False,\n",
    "#                      array=err_up,arrayminus=err_down,\n",
    "#                      thickness=1,color=clr)))\n",
    "# fig_sigma.update_xaxes(type=\"linear\",tickmode=\"array\",\n",
    "#                        tickvals=consensus,ticktext=cat_labels,\n",
    "#                        title=\"Consensus position (bp)\")\n",
    "# fig_sigma.update_layout(template=\"plotly_white\",width=1000,height=500,\n",
    "#                         xaxis=dict(showgrid=False,zeroline=False),\n",
    "#                         yaxis=dict(showgrid=False,zeroline=False),\n",
    "#                         title=(f\"Centre dispersion σ within ±{CONS_WINDOW_BP} bp\"),\n",
    "#                         yaxis_title=\"σ of centre positions (bp)\")\n",
    "# fig_sigma.show()\n",
    "\n",
    "################################################################################\n",
    "#                      4)  Shannon evenness (KL proxy)                         #\n",
    "################################################################################\n",
    "# ───────────────── EVENNESS / KL-bootstrap helper ────────────────── #\n",
    "###############################################################################\n",
    "#  Clustering bootstrap helper  (+ optional ABS-mirroring)                    #\n",
    "###############################################################################\n",
    "def collect_kl_bootstrap_grouped(df_all, *, bin_size=20,\n",
    "                                 splits=2, return_full=False):\n",
    "    \"\"\"\n",
    "    Aggregate clustering (1−evenness) per rung for each\n",
    "    (condition, type, bed_start, boot_idx) replicate.\n",
    "\n",
    "    If READ_LEVEL_BALANCING is False we down-weight histograms so that, for\n",
    "    every rung, A and B contribute the same *effective* number of centres.\n",
    "    \"\"\"\n",
    "    axis_rungs = (sorted({abs(c) for c in PLOT_RUNG_AXIS})\n",
    "                  if MIRROR_ABS_DISTANCE else list(PLOT_RUNG_AXIS))\n",
    "\n",
    "    def _blank(): return {c: [] for c in axis_rungs}\n",
    "    even_out = {COND_A: _blank(), COND_B: _blank()}\n",
    "    full_map = {}\n",
    "\n",
    "    win_bp  = CONS_WINDOW_BP\n",
    "    edges   = np.arange(-win_bp, win_bp + bin_size, bin_size)\n",
    "    n_bins  = len(edges) - 1\n",
    "\n",
    "    # ─────────────── 0. per-window weight table ───────────────\n",
    "    if READ_BALANCE_MODE == \"scaled\":\n",
    "        # raw centre counts per condition&rung\n",
    "        raw_counts = {COND_A: {r: 0 for r in axis_rungs},\n",
    "                      COND_B: {r: 0 for r in axis_rungs}}\n",
    "        for cond, sub in df_all.groupby(\"condition\"):\n",
    "            centres = np.concatenate(sub.nuc_centers.to_numpy())\n",
    "            for rung in axis_rungs:\n",
    "                raw_counts[cond][rung] += np.sum(np.abs(centres - rung) < win_bp)\n",
    "\n",
    "        scale_factor = {COND_A: {}, COND_B: {}}\n",
    "        for rung in axis_rungs:\n",
    "            nA, nB = raw_counts[COND_A][rung], raw_counts[COND_B][rung]\n",
    "            n_min  = min(nA, nB)\n",
    "            for cond, n in [(COND_A, nA), (COND_B, nB)]:\n",
    "                scale_factor[cond][rung] = (n_min / n) if n else 0.0\n",
    "    # ───────────────────────────────────────────────────────────\n",
    "\n",
    "\n",
    "    # ─────────────── 1. loop over replicates ───────────────\n",
    "    for (cond, typ, bed, boot_idx), grp in df_all.groupby(\n",
    "            [\"condition\", \"type\", \"bed_start\", \"boot_idx\"]):\n",
    "\n",
    "        centres = np.concatenate(grp.nuc_centers.to_numpy())\n",
    "        this_rep = {}\n",
    "\n",
    "        for rung in axis_rungs:\n",
    "            rel = centres - rung\n",
    "            rel = rel[np.abs(rel) < win_bp]\n",
    "            if rel.size == 0:\n",
    "                continue\n",
    "\n",
    "            counts, _ = np.histogram(rel, bins=edges)\n",
    "            counts = centred_ma(counts, SMOOTH_COUNTS_HALF_BINS) + 0.5\n",
    "\n",
    "            # -------- weighting toggle --------\n",
    "            if READ_BALANCE_MODE == \"scaled\":\n",
    "                counts = counts * scale_factor[cond][rung]\n",
    "            # ----------------------------------\n",
    "\n",
    "            p_vec   = counts / counts.sum()\n",
    "            H       = -(p_vec[p_vec > 0] * np.log(p_vec[p_vec > 0])).sum()\n",
    "            cluster = 1.0 - math.exp(H) / n_bins\n",
    "\n",
    "            out_key = abs(rung) if MIRROR_ABS_DISTANCE else rung\n",
    "            even_out[cond][out_key].append(cluster)\n",
    "            this_rep[out_key] = cluster\n",
    "\n",
    "        full_map[(cond, typ, bed, boot_idx)] = this_rep\n",
    "\n",
    "    return (even_out, full_map) if return_full else even_out\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "#  NEW helper 2: paired Δ = B − A for matching (type, boot) reps\n",
    "# ------------------------------------------------------------------\n",
    "def paired_deltas(full_map_A, full_map_B):\n",
    "    out = {r: [] for r in PLOT_RUNG_AXIS}\n",
    "    common = set(full_map_A) & set(full_map_B)      # (type,bed,boot)\n",
    "    for key in common:\n",
    "        a_r = full_map_A[key]\n",
    "        b_r = full_map_B[key]\n",
    "        for rung in PLOT_RUNG_AXIS:\n",
    "            if rung in a_r and rung in b_r:\n",
    "                out[rung].append(b_r[rung] - a_r[rung])\n",
    "    return out\n",
    "# ------ helper: bootstrap evenness identical to previous cell ---------------\n",
    "\n",
    "kl_vals, full_map = collect_kl_bootstrap_grouped(\n",
    "    pd.concat([dfA_all_bal, dfB_all_bal], ignore_index=True),\n",
    "    bin_size = entropy_bin,\n",
    "    splits   = BOOTSTRAP_SPLITS,\n",
    "    return_full = True\n",
    ")\n",
    "full_map_A = {(typ, bed, b): v\n",
    "              for (cond, typ, bed, b), v in full_map.items()\n",
    "              if cond == COND_A}\n",
    "\n",
    "full_map_B = {(typ, bed, b): v\n",
    "              for (cond, typ, bed, b), v in full_map.items()\n",
    "              if cond == COND_B}\n",
    "\n",
    "fig_kl = go.Figure()\n",
    "if SHOW_INDIVIDUAL_LINES:\n",
    "    # — add one thin line per (cond, type, bed_start, bootstrap) —\n",
    "    for (cond, typ, bed, boot), rep in full_map.items():\n",
    "        x = PLOT_RUNG_AXIS\n",
    "        y = [rep.get(r, np.nan) for r in x]\n",
    "        fig_kl.add_trace(go.Scatter(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            mode=\"lines\",\n",
    "            line=dict(width=0.5, color=CLR_A if cond==COND_A else CLR_B),\n",
    "            opacity=0.3,\n",
    "            showlegend=False\n",
    "        ))\n",
    "    \n",
    "for cond, clr in zip([COND_A, COND_B], [CLR_A, CLR_B]):\n",
    "    q1, med, q3 = [], [], []\n",
    "    for rung in PLOT_RUNG_AXIS:\n",
    "        vals = kl_vals[cond][rung]\n",
    "        q1 .append(np.nan if not vals else np.percentile(vals, 25))\n",
    "        med.append(np.nan if not vals else np.percentile(vals, 50))\n",
    "        q3 .append(np.nan if not vals else np.percentile(vals, 75))\n",
    "    if ENT_SMOOTH_WINDOW > 1:\n",
    "        q1  = centred_ma(np.asarray(q1),  ENT_SMOOTH_WINDOW)\n",
    "        med = centred_ma(np.asarray(med), ENT_SMOOTH_WINDOW)\n",
    "        q3  = centred_ma(np.asarray(q3),  ENT_SMOOTH_WINDOW)\n",
    "    fig_kl.add_trace(go.Scatter(x=PLOT_RUNG_AXIS, y=q1 ,\n",
    "                                mode=\"lines\", line=dict(width=1, dash=\"dot\",\n",
    "                                color=clr), showlegend=False))\n",
    "    fig_kl.add_trace(go.Scatter(x=PLOT_RUNG_AXIS, y=q3 ,\n",
    "                                mode=\"lines\", line=dict(width=1, dash=\"dot\",\n",
    "                                color=clr), showlegend=False))\n",
    "    fig_kl.add_trace(go.Scatter(x=PLOT_RUNG_AXIS, y=med,\n",
    "                                mode=\"lines\", line=dict(width=ENT_MED_LINE_W,\n",
    "                                color=clr), name=f\"{cond} median\"))\n",
    "fig_kl.update_layout(template=\"plotly_white\", width=1000, height=450,\n",
    "    title=(f\"Clustering of centre distribution (±{CONS_WINDOW_BP} bp window, \"\n",
    "           f\"{entropy_bin} bp bins; 1 pt = one type × bootstrap)\"),\n",
    "    yaxis_title=\"Clustering  (0 → uniform, 1 → clustered)\",\n",
    "    xaxis_title=AXIS_LABEL)\n",
    "fig_kl.update_xaxes(showgrid=False, zeroline=False)\n",
    "fig_kl.update_yaxes(showgrid=False, zeroline=False)\n",
    "\n",
    "if SHOW_MOTIF_LINES:\n",
    "    add_motif_guides(fig_kl,  positions=motif_positions,\n",
    "                                      style=MOTIF_LINE_STYLE)\n",
    "if SHOW_TSS_Q4_LINES:                                    # INSERT\n",
    "    add_motif_guides(fig_kl,  positions=tss_q4_positions,\n",
    "                               style=TSS_Q4_LINE_STYLE)  # INSERT\n",
    "fig_kl.show()\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────\n",
    "def bootstrap_deltas(even_dict_A, even_dict_B, *, max_rows=None):\n",
    "    \"\"\"\n",
    "    Given two evenness dicts (rung → list[cluster values]) coming out of\n",
    "    collect_kl_bootstrap_grouped, return:\n",
    "        diff_per_rung[rung]  -> list[ΔClustering = B – A] (length ≤ max_rows)\n",
    "\n",
    "    The two replicate lists are *paired by index*.  If they are unequal in\n",
    "    length we truncate to the shorter one (or to max_rows if provided).\n",
    "    \"\"\"\n",
    "    out = {r: [] for r in PLOT_RUNG_AXIS }\n",
    "    for rung in PLOT_RUNG_AXIS :\n",
    "        a = np.asarray(even_dict_A[rung], float)\n",
    "        b = np.asarray(even_dict_B[rung], float)\n",
    "        n = min(len(a), len(b))\n",
    "        if max_rows is not None:\n",
    "            n = min(n, max_rows)\n",
    "        if n:\n",
    "            out[rung].extend(b[:n] - a[:n])\n",
    "    return out\n",
    "\n",
    "\n",
    "# ╔═══════════  NEW: ΔEvenness (B–A)  pooled across types  ═══════════╗\n",
    "SAVE_TYPE_DIFF_COMBINED = True          # toggle ON / OFF\n",
    "#   +1  →  B very even,   A very clustered\n",
    "#   -1  →  B very clustered, A very even\n",
    "#    0  →  identical evenness\n",
    "# ╚═══════════════════════════════════════════════════════════════════╝\n",
    "if SAVE_TYPE_DIFF_COMBINED:\n",
    "    pooled_deltas = {r: [] for r in PLOT_RUNG_AXIS }\n",
    "\n",
    "    delta_vals = paired_deltas(full_map_A, full_map_B)\n",
    "\n",
    "    q1  = [np.nanpercentile(delta_vals[r], 25) if delta_vals[r] else np.nan\n",
    "           for r in PLOT_RUNG_AXIS]\n",
    "    med = [np.nanmedian   (delta_vals[r])      if delta_vals[r] else np.nan\n",
    "           for r in PLOT_RUNG_AXIS]\n",
    "    q3  = [np.nanpercentile(delta_vals[r], 75) if delta_vals[r] else np.nan\n",
    "           for r in PLOT_RUNG_AXIS]\n",
    "    if ENT_SMOOTH_WINDOW > 1:\n",
    "        q1  = centred_ma(np.asarray(q1),  ENT_SMOOTH_WINDOW)\n",
    "        med = centred_ma(np.asarray(med), ENT_SMOOTH_WINDOW)\n",
    "        q3  = centred_ma(np.asarray(q3),  ENT_SMOOTH_WINDOW)\n",
    "\n",
    "    fig_diff = go.Figure()\n",
    "    if SHOW_INDIVIDUAL_LINES:\n",
    "        # — add one thin grey line per paired (type, bed_start, bootstrap) group —\n",
    "        common = set(full_map_A) & set(full_map_B)\n",
    "        for key in common:\n",
    "            a_r = full_map_A[key]\n",
    "            b_r = full_map_B[key]\n",
    "            x, y = [], []\n",
    "            for rung in PLOT_RUNG_AXIS:\n",
    "                if rung in a_r and rung in b_r:\n",
    "                    x.append(rung)\n",
    "                    y.append(b_r[rung] - a_r[rung])\n",
    "            fig_diff.add_trace(go.Scatter(\n",
    "                x=x,\n",
    "                y=y,\n",
    "                mode=\"lines\",\n",
    "                line=dict(width=0.5, color=\"grey\"),\n",
    "                opacity=0.3,\n",
    "                showlegend=False\n",
    "            ))\n",
    "    fig_diff.add_trace(go.Scatter(x=PLOT_RUNG_AXIS, y=q1 ,\n",
    "                                  mode=\"lines\", line=dict(color=\"black\",\n",
    "                                  dash=\"dot\", width=1), showlegend=False))\n",
    "    fig_diff.add_trace(go.Scatter(x=PLOT_RUNG_AXIS, y=q3 ,\n",
    "                                  mode=\"lines\", line=dict(color=\"black\",\n",
    "                                  dash=\"dot\", width=1), showlegend=False))\n",
    "    fig_diff.add_trace(go.Scatter(x=PLOT_RUNG_AXIS, y=med,\n",
    "                                  mode=\"lines\", line=dict(color=\"black\",\n",
    "                                  width=ENT_MED_LINE_W),\n",
    "                                  name=\"median ΔClustering\"))\n",
    "    fig_diff.add_shape(type=\"line\",\n",
    "                       x0=min(PLOT_RUNG_AXIS), x1=max(PLOT_RUNG_AXIS),\n",
    "                       y0=0, y1=0,\n",
    "                       line=dict(color=\"grey\", dash=\"dash\"))\n",
    "    fig_diff.update_layout(template=\"plotly_white\", width=1000, height=450,\n",
    "        title=(f\"ΔClustering (B – A), paired by type & bootstrap\"),\n",
    "        xaxis_title=AXIS_LABEL,\n",
    "        yaxis_title=\"<0 → B more clustered<br>>0 → A more clustered\")\n",
    "    fig_diff.update_xaxes(showgrid=False, zeroline=False)\n",
    "    fig_diff.update_yaxes(showgrid=False, zeroline=False)\n",
    "    if SHOW_MOTIF_LINES: add_motif_guides(fig_diff, positions=motif_positions,\n",
    "                                      style=MOTIF_LINE_STYLE)\n",
    "    if SHOW_TSS_Q4_LINES:                                    # INSERT\n",
    "        add_motif_guides(fig_diff,  positions=tss_q4_positions,\n",
    "                                   style=TSS_Q4_LINE_STYLE)  # INSERT\n",
    "    fig_diff.show()\n",
    "# end SAVE_TYPE_DIFF_COMBINED\n",
    "\n",
    "# ---------- B. per-type Evenness & ΔEvenness (optional) ----------\n",
    "\n",
    "\n",
    "if SAVE_PER_TYPE:\n",
    "    # build list of “groups” to iterate\n",
    "    if CONSIDER_BED_START:\n",
    "        # all unique (type, bed_start) pairs present in A or B\n",
    "        groups = sorted(\n",
    "            set(tuple(x) for x in pd.concat([dfA_all_bal, dfB_all_bal])[\n",
    "                [\"type\",\"bed_start\"]\n",
    "             ].drop_duplicates().to_numpy()),\n",
    "            key=lambda x: (x[0], x[1])\n",
    "        )\n",
    "    else:\n",
    "        # just unique types\n",
    "        groups = sorted(set(dfA_all_bal[\"type\"]) | set(dfB_all_bal[\"type\"]))\n",
    "\n",
    "    for grp in groups:\n",
    "        if CONSIDER_BED_START:\n",
    "            typ, bed = grp\n",
    "            sub_A = dfA_all_bal[(dfA_all_bal.type == typ) & (dfA_all_bal.bed_start == bed)]\n",
    "            sub_B = dfB_all_bal[(dfB_all_bal.type == typ) & (dfB_all_bal.bed_start == bed)]\n",
    "            title_suffix = f\"{typ}_bed{bed}\"\n",
    "        else:\n",
    "            typ = grp\n",
    "            sub_A = dfA_all_bal[dfA_all_bal.type == typ]\n",
    "            sub_B = dfB_all_bal[dfB_all_bal.type == typ]\n",
    "            title_suffix = typ\n",
    "\n",
    "        if sub_A.empty and sub_B.empty:\n",
    "            continue\n",
    "\n",
    "        sub_df_bal = pd.concat([sub_A, sub_B], ignore_index=True)\n",
    "        even_vals = collect_kl_bootstrap_grouped(\n",
    "            sub_df_bal,\n",
    "            bin_size    = entropy_bin,\n",
    "            splits      = BOOTSTRAP_SPLITS,\n",
    "            return_full = False\n",
    "        )\n",
    "\n",
    "        # ── helper to get Q1/median/Q3 for one condition ──\n",
    "        def _quartiles(e_dict):\n",
    "            q1, med, q3 = [], [], []\n",
    "            for rung in PLOT_RUNG_AXIS:\n",
    "                vals = e_dict[rung]\n",
    "                if vals:\n",
    "                    _q1, _m, _q3 = np.percentile(vals, [25, 50, 75])\n",
    "                else:\n",
    "                    _q1 = _m = _q3 = np.nan\n",
    "                q1.append(_q1); med.append(_m); q3.append(_q3)\n",
    "            if ENT_SMOOTH_WINDOW > 1:\n",
    "                q1  = centred_ma(np.asarray(q1),  ENT_SMOOTH_WINDOW)\n",
    "                med = centred_ma(np.asarray(med), ENT_SMOOTH_WINDOW)\n",
    "                q3  = centred_ma(np.asarray(q3),  ENT_SMOOTH_WINDOW)\n",
    "            return q1, med, q3\n",
    "\n",
    "        q1_A, med_A, q3_A = _quartiles(even_vals[COND_A])\n",
    "        q1_B, med_B, q3_B = _quartiles(even_vals[COND_B])\n",
    "\n",
    "        # ---------- build paired-bootstrap Δ lists ----------\n",
    "        delta_vals = bootstrap_deltas(even_vals[COND_A], even_vals[COND_B])\n",
    "\n",
    "        # quartiles of the Δ distribution\n",
    "        diff_q1  = [np.nanpercentile(delta_vals[r], 25) if delta_vals[r] else np.nan\n",
    "                    for r in PLOT_RUNG_AXIS]\n",
    "        diff_med = [np.nanmedian   (delta_vals[r])       if delta_vals[r] else np.nan\n",
    "                    for r in PLOT_RUNG_AXIS]\n",
    "        diff_q3  = [np.nanpercentile(delta_vals[r], 75) if delta_vals[r] else np.nan\n",
    "                    for r in PLOT_RUNG_AXIS]\n",
    "\n",
    "        if ENT_SMOOTH_WINDOW > 1:\n",
    "            diff_q1  = centred_ma(np.asarray(diff_q1),  ENT_SMOOTH_WINDOW)\n",
    "            diff_med = centred_ma(np.asarray(diff_med), ENT_SMOOTH_WINDOW)\n",
    "            diff_q3  = centred_ma(np.asarray(diff_q3),  ENT_SMOOTH_WINDOW)\n",
    "\n",
    "        # ─────────────── figure layout ───────────────\n",
    "        from plotly.subplots import make_subplots\n",
    "\n",
    "        fig_t = make_subplots(\n",
    "            rows=2, cols=1,\n",
    "            shared_xaxes=True,\n",
    "            vertical_spacing=0.05,\n",
    "            row_heights=[0.6, 0.4]\n",
    "        )\n",
    "\n",
    "        # ── Row 1 : Evenness per condition ──\n",
    "        for cond, clr, q1, med, q3 in [\n",
    "            (COND_A, CLR_A, q1_A, med_A, q3_A),\n",
    "            (COND_B, CLR_B, q1_B, med_B, q3_B)\n",
    "        ]:\n",
    "            fig_t.add_trace(\n",
    "                go.Scatter(x=PLOT_RUNG_AXIS, y=q1,\n",
    "                           mode=\"lines\", line=dict(color=clr, dash=\"dot\", width=1),\n",
    "                           showlegend=False),\n",
    "                row=1, col=1)\n",
    "            fig_t.add_trace(\n",
    "                go.Scatter(x=PLOT_RUNG_AXIS, y=q3,\n",
    "                           mode=\"lines\", line=dict(color=clr, dash=\"dot\", width=1),\n",
    "                           showlegend=False),\n",
    "                row=1, col=1)\n",
    "            fig_t.add_trace(\n",
    "                go.Scatter(x=PLOT_RUNG_AXIS, y=med,\n",
    "                           mode=\"lines\", line=dict(color=clr, width=ENT_MED_LINE_W),\n",
    "                           name=f\"{cond} median\"),\n",
    "                row=1, col=1)\n",
    "\n",
    "        # ── Row 2 : ΔEvenness (B – A) ──\n",
    "        fig_t.add_trace(\n",
    "            go.Scatter(x=PLOT_RUNG_AXIS, y=diff_q1,\n",
    "                       mode=\"lines\", line=dict(color=\"black\", dash=\"dot\", width=1),\n",
    "                       showlegend=False),\n",
    "            row=2, col=1)\n",
    "        fig_t.add_trace(\n",
    "            go.Scatter(x=PLOT_RUNG_AXIS, y=diff_q3,\n",
    "                       mode=\"lines\", line=dict(color=\"black\", dash=\"dot\", width=1),\n",
    "                       showlegend=False),\n",
    "            row=2, col=1)\n",
    "        fig_t.add_trace(\n",
    "            go.Scatter(x=PLOT_RUNG_AXIS, y=diff_med,\n",
    "                       mode=\"lines\", line=dict(color=\"black\", width=ENT_MED_LINE_W),\n",
    "                       name=\"median ΔEvenness\"),\n",
    "            row=2, col=1)\n",
    "\n",
    "        # zero reference line\n",
    "        fig_t.add_shape(type=\"line\",\n",
    "                        x0=min(PLOT_RUNG_AXIS), x1=max(PLOT_RUNG_AXIS),\n",
    "                        y0=0, y1=0,\n",
    "                        line=dict(color=\"grey\", dash=\"dash\"),\n",
    "                        row=2, col=1)\n",
    "\n",
    "        # ── cosmetics ──\n",
    "        fig_t.update_layout(\n",
    "            template     = \"plotly_white\",\n",
    "            width        = 1000,\n",
    "            height       = 600,\n",
    "            title        = f\"Clustering & ΔClustering (MT – WT) (type = {typ})\",\n",
    "            xaxis_title  = AXIS_LABEL,\n",
    "            yaxis_title  = \"1 → clustered, 0 → uniform)\",\n",
    "            yaxis2_title = \"<0 → ET more clustered<br> >0 → MT more clustered\"\n",
    "        )\n",
    "        fig_t.update_xaxes(showgrid=False, zeroline=False)\n",
    "        fig_t.update_yaxes(showgrid=False, zeroline=False)\n",
    "        fig_t.update_yaxes(showgrid=False, zeroline=False, row=2, col=1)\n",
    "        if SHOW_MOTIF_LINES: add_motif_guides(fig_t,   positions=motif_positions,\n",
    "                                      style=MOTIF_LINE_STYLE)\n",
    "        if SHOW_TSS_Q4_LINES:                                    # INSERT\n",
    "            add_motif_guides(fig_t,  positions=tss_q4_positions,\n",
    "                                       style=TSS_Q4_LINE_STYLE)  # INSERT\n",
    "\n",
    "        # ── save files ──\n",
    "        tag = f\"evenness_{title_suffix}_{COND_A}_vs_{COND_B}_{STAMP}\"\n",
    "        fig_t.write_image(OUT_DIR_NEW / f\"{tag}.png\", scale=2)\n",
    "        fig_t.write_image(OUT_DIR_NEW / f\"{tag}.svg\")\n",
    "        dbg(f\"[SAVE] per-type {title_suffix}: {tag}.png / .svg\")\n",
    "# end if SAVE_PER_TYPE\n",
    "\n",
    "################################################################################\n",
    "#                           EXPORTS  (unchanged)                               #\n",
    "################################################################################\n",
    "base_tag = f\"evenness_{COND_A}_vs_{COND_B}_{STAMP}\"\n",
    "fig_kl    .write_image(OUT_DIR_NEW/f\"{base_tag}_kl.png\"   ,scale=2)\n",
    "fig_kl    .write_image(OUT_DIR_NEW/f\"{base_tag}_kl.svg\")\n",
    "dbg(f\"[SAVE] all evenness plots → {OUT_DIR_NEW}\")\n",
    "################################################################################\n",
    "#                           EXPORTS  (unchanged)                               #\n",
    "################################################################################\n",
    "base_tag = f\"evenness_diff_{COND_A}_vs_{COND_B}_{STAMP}\"\n",
    "fig_diff    .write_image(OUT_DIR_NEW/f\"{base_tag}_kl.png\"   ,scale=2)\n",
    "fig_diff    .write_image(OUT_DIR_NEW/f\"{base_tag}_kl.svg\")\n",
    "dbg(f\"[SAVE] all evenness plots → {OUT_DIR_NEW}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e742161aab63902",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ╔════════════════════════════════════════════════════════════════╗\n",
    "# ║  CELL Y++++: CV · min‑CV · Δmin‑CV · nuc‑per‑read ratio        ║\n",
    "# ╚════════════════════════════════════════════════════════════════╝\n",
    "#\n",
    "#  • The CV / min‑CV logic (panels 1‑3) is **unchanged** and still uses\n",
    "#    DIST_WINDOW_BP, CV_STEP, MIN_WINDOW_BP, SMOOTH_HALF_WIN.\n",
    "#  • The nuc‑count panels (4‑5) now have their *own* bin‑size & smoothing\n",
    "#    controls:  NUC_STEP  and  NUC_SMOOTH_HALF_WIN.\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "import numpy as np, pandas as pd, plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# ───────── USER CONFIG – CV / min‑CV (unchanged) ───────── #\n",
    "DIST_WINDOW_BP      = 80      # ± window (bp) for CV calculations\n",
    "CV_STEP             = 5      # grid spacing (bp)\n",
    "MIN_WINDOW_BP       = 80     # ± window for local‑min CV search\n",
    "SMOOTH_HALF_WIN     = 5       # MA half‑width for CV curves\n",
    "# ───────── USER CONFIG – nuc‑count panels (NEW) ────────── #\n",
    "NUC_STEP            = 10       # grid spacing (bp) for nuc‑count curves\n",
    "NUC_SMOOTH_HALF_WIN = 1      # MA half‑width for nuc‑count curves\n",
    "# --------------------------------------------------------- #\n",
    "\n",
    "CLR_MAP = {COND_A: CLR_A, COND_B: CLR_B}\n",
    "\n",
    "# ── shared helpers ────────────────────────────────────────\n",
    "def _smooth(arr, half_win):\n",
    "    arr = np.asarray(arr, float)\n",
    "    if half_win == 0 or arr.size == 0:\n",
    "        return arr\n",
    "    pad = np.pad(arr, half_win, mode=\"edge\")\n",
    "    ker = np.ones(2*half_win + 1) / (2*half_win + 1)\n",
    "    out = np.convolve(pad, ker, mode=\"valid\")\n",
    "    out[np.isnan(arr)] = np.nan\n",
    "    return out\n",
    "\n",
    "def _w_read(row):\n",
    "    return 1 if READ_BALANCE_MODE in {\"centres\", \"equal_read_len\"} else row.read_length\n",
    "# --------------------------------------------------------- #\n",
    "\n",
    "# ───────── grids ─────────\n",
    "orig_rungs   = (sorted({abs(c) for c in PLOT_RUNG_AXIS})\n",
    "                if MIRROR_ABS_DISTANCE else list(PLOT_RUNG_AXIS))\n",
    "axis_min, axis_max = min(orig_rungs), max(orig_rungs)\n",
    "\n",
    "axis_pos_cv   = np.arange(axis_min, axis_max + CV_STEP,   CV_STEP)\n",
    "axis_pos_nuc  = np.arange(axis_min, axis_max + NUC_STEP,  NUC_STEP)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────\n",
    "#  1)  replicate‑level CV / min‑CV / ratio curves (old code)\n",
    "# ──────────────────────────────────────────────────────────\n",
    "def _replicate_curves_CV(read_df):\n",
    "    offset_lists = {p: [] for p in axis_pos_cv}\n",
    "    nuc_cnt      = np.zeros_like(axis_pos_cv, float)\n",
    "    read_cnt     = np.zeros_like(axis_pos_cv, float)\n",
    "\n",
    "    for row in read_df.itertuples(index=False):\n",
    "        centres = np.asarray(row.nuc_centers, float)\n",
    "        relpos  = np.asarray(row.rel_pos,      int)\n",
    "        for idx, p in enumerate(axis_pos_cv):\n",
    "            lo, hi = p - DIST_WINDOW_BP, p + DIST_WINDOW_BP\n",
    "            if not ((relpos >= lo) & (relpos <= hi)).any():\n",
    "                continue\n",
    "            w = _w_read(row)\n",
    "            read_cnt[idx] += w\n",
    "            m = (np.abs(centres - p) < DIST_WINDOW_BP)\n",
    "            if m.any():\n",
    "                nuc_cnt[idx] += m.sum() * w\n",
    "                offset_lists[p].extend(np.abs(centres[m] - p))\n",
    "\n",
    "    # CV(|offset|)\n",
    "    cv_raw = np.array([\n",
    "        (np.nan if len(lst) < 2 or np.mean(lst) == 0\n",
    "         else np.std(lst, ddof=0) / np.mean(lst))\n",
    "        for lst in offset_lists.values()\n",
    "    ])\n",
    "    # local‑min CV\n",
    "    half_idx = int(np.ceil(MIN_WINDOW_BP / CV_STEP))\n",
    "    min_raw  = np.array([\n",
    "        (np.nanmin(cv_raw[max(0,i-half_idx):i+half_idx+1])\n",
    "         if np.any(~np.isnan(cv_raw[max(0,i-half_idx):i+half_idx+1])) else np.nan)\n",
    "        for i in range(len(cv_raw))\n",
    "    ])\n",
    "    # nuc/read ratio OR raw counts\n",
    "    if READ_BALANCE_MODE in {\"centres\", \"equal_read_len\"}:\n",
    "        ratio_raw = nuc_cnt\n",
    "    else:\n",
    "        ratio_raw = np.divide(nuc_cnt, read_cnt,\n",
    "                              out=np.full_like(nuc_cnt, np.nan),\n",
    "                              where=read_cnt > 0)\n",
    "    return cv_raw, min_raw, ratio_raw\n",
    "\n",
    "# ──────────────────────────────────────────────────────────\n",
    "#  2)  replicate‑level nuc‑count curves (NEW grid / smooth)\n",
    "# ──────────────────────────────────────────────────────────\n",
    "def _replicate_curves_NUC(read_df):\n",
    "    nuc_cnt = np.zeros_like(axis_pos_nuc, float)\n",
    "    for row in read_df.itertuples(index=False):\n",
    "        centres = np.asarray(row.nuc_centers, float)\n",
    "        for idx, p in enumerate(axis_pos_nuc):\n",
    "            if np.any(np.abs(centres - p) < DIST_WINDOW_BP):\n",
    "                nuc_cnt[idx] += 1  # one per centre\n",
    "    return nuc_cnt\n",
    "\n",
    "# ───────── gather replicate curves ─────────\n",
    "rep_cv   = {COND_A: {}, COND_B: {}}\n",
    "rep_nuc  = {COND_A: {}, COND_B: {}}\n",
    "\n",
    "for cond, df_cond in ((COND_A, dfA_all_bal), (COND_B, dfB_all_bal)):\n",
    "    for key, sub in df_cond.groupby([\"type\", \"bed_start\", \"boot_idx\"]):\n",
    "        rep_cv [cond][key] = _replicate_curves_CV (sub.reset_index(drop=True))\n",
    "        rep_nuc[cond][key] = _replicate_curves_NUC(sub.reset_index(drop=True))\n",
    "\n",
    "# ───────── quartiles helper ─────────\n",
    "def _q123(mat_list, axis_len):\n",
    "    if not mat_list:\n",
    "        return (np.full(axis_len, np.nan),)*3\n",
    "    mat = np.vstack(mat_list)\n",
    "    return tuple(np.nanpercentile(mat, q, axis=0) for q in (25,50,75))\n",
    "\n",
    "# ───────── CV‑family quartiles ─────────\n",
    "cv_Q, min_Q, ratio_Q = {}, {}, {}\n",
    "for cond in (COND_A, COND_B):\n",
    "    cv_Q   [cond] = _q123([v[0] for v in rep_cv[cond].values()],   len(axis_pos_cv))\n",
    "    min_Q  [cond] = _q123([v[1] for v in rep_cv[cond].values()],   len(axis_pos_cv))\n",
    "    ratio_Q[cond] = _q123([v[2] for v in rep_cv[cond].values()],   len(axis_pos_cv))\n",
    "\n",
    "for cond, targ in [(COND_A, cv_Q), (COND_B, cv_Q),\n",
    "                   (COND_A, min_Q), (COND_B, min_Q),\n",
    "                   (COND_A, ratio_Q), (COND_B, ratio_Q)]:\n",
    "    q1, med, q3 = targ[cond]\n",
    "    targ[cond] = (_smooth(q1, SMOOTH_HALF_WIN),\n",
    "                  _smooth(med, SMOOTH_HALF_WIN),\n",
    "                  _smooth(q3, SMOOTH_HALF_WIN))\n",
    "\n",
    "# ───────── nuc‑count quartiles ─────────\n",
    "nuc_Q = {}\n",
    "for cond in (COND_A, COND_B):\n",
    "    nuc_raw = [v for v in rep_nuc[cond].values()]\n",
    "    nuc_Q[cond] = _q123(nuc_raw, len(axis_pos_nuc))\n",
    "    q1, med, q3 = nuc_Q[cond]\n",
    "    nuc_Q[cond] = (_smooth(q1, NUC_SMOOTH_HALF_WIN),\n",
    "                   _smooth(med, NUC_SMOOTH_HALF_WIN),\n",
    "                   _smooth(q3, NUC_SMOOTH_HALF_WIN))\n",
    "\n",
    "# Δ nuc‑count (A−B)\n",
    "delta_nuc = []\n",
    "common = set(rep_nuc[COND_A]) & set(rep_nuc[COND_B])\n",
    "for k in common:\n",
    "    delta_nuc.append(rep_nuc[COND_A][k] - rep_nuc[COND_B][k])\n",
    "dN_Q = _q123(delta_nuc, len(axis_pos_nuc))\n",
    "dN_Q = tuple(_smooth(arr, NUC_SMOOTH_HALF_WIN) for arr in dN_Q)\n",
    "\n",
    "# ───────────────────────── plotting ─────────────────────────\n",
    "fig = make_subplots(\n",
    "    rows=5, cols=1, shared_xaxes=False,\n",
    "    vertical_spacing=0.04,\n",
    "    row_heights=[0.20]*5,\n",
    "    subplot_titles=(\n",
    "        f\"Smoothed CV  (±{DIST_WINDOW_BP} bp)\",\n",
    "        f\"Smoothed local‑min CV  (±{MIN_WINDOW_BP} bp)\",\n",
    "        f\"Δ local‑min CV  ({COND_A} – {COND_B})\",\n",
    "        \"Nucleosome count\",\n",
    "        f\"Δ nuc count  ({COND_A} – {COND_B})\"\n",
    "    )\n",
    ")\n",
    "\n",
    "def _add_q(fig, x, qtuple, color, name, row):\n",
    "    q1, med, q3 = qtuple\n",
    "    fig.add_trace(go.Scatter(x=x, y=q1, mode=\"lines\",\n",
    "                             line=dict(color=color, dash=\"dot\", width=1),\n",
    "                             showlegend=False), row=row, col=1)\n",
    "    fig.add_trace(go.Scatter(x=x, y=q3, mode=\"lines\",\n",
    "                             line=dict(color=color, dash=\"dot\", width=1),\n",
    "                             showlegend=False), row=row, col=1)\n",
    "    fig.add_trace(go.Scatter(x=x, y=med, mode=\"lines\",\n",
    "                             line=dict(color=color, width=3), name=name),\n",
    "                  row=row, col=1)\n",
    "\n",
    "# Panels 1‑3 (CV, min‑CV, Δmin‑CV) use axis_pos_cv\n",
    "_add_q(fig, axis_pos_cv, cv_Q  [COND_A], CLR_A, COND_A, row=1)\n",
    "_add_q(fig, axis_pos_cv, cv_Q  [COND_B], CLR_B, COND_B, row=1)\n",
    "_add_q(fig, axis_pos_cv, min_Q [COND_A], CLR_A, COND_A, row=2)\n",
    "_add_q(fig, axis_pos_cv, min_Q [COND_B], CLR_B, COND_B, row=2)\n",
    "\n",
    "q1_d, med_d, q3_d = _q123(\n",
    "    [v[1] - rep_cv[COND_B][k][1] for k,v in rep_cv[COND_A].items()\n",
    "     if k in rep_cv[COND_B]], len(axis_pos_cv))\n",
    "q1_d = _smooth(q1_d, SMOOTH_HALF_WIN)\n",
    "med_d= _smooth(med_d, SMOOTH_HALF_WIN)\n",
    "q3_d = _smooth(q3_d, SMOOTH_HALF_WIN)\n",
    "for arr in (q1_d, q3_d):\n",
    "    fig.add_trace(go.Scatter(x=axis_pos_cv, y=arr, mode=\"lines\",\n",
    "                             line=dict(color=\"black\", dash=\"dot\", width=1),\n",
    "                             showlegend=False), row=3, col=1)\n",
    "fig.add_trace(go.Scatter(x=axis_pos_cv, y=med_d, mode=\"lines\",\n",
    "                         line=dict(color=\"black\", width=3),\n",
    "                         name=\"median Δ\"), row=3, col=1)\n",
    "fig.add_shape(type=\"line\", x0=axis_min, x1=axis_max, y0=0, y1=0,\n",
    "              line=dict(color=\"grey\", dash=\"dash\"), row=3, col=1)\n",
    "\n",
    "# Panel 4 – nuc count (axis_pos_nuc)\n",
    "_add_q(fig, axis_pos_nuc, nuc_Q[COND_A], CLR_A, COND_A, row=4)\n",
    "_add_q(fig, axis_pos_nuc, nuc_Q[COND_B], CLR_B, COND_B, row=4)\n",
    "fig.update_yaxes(title=\"nuc count\", row=4, col=1)\n",
    "\n",
    "# Panel 5 – Δ nuc count\n",
    "q1_n, med_n, q3_n = dN_Q\n",
    "fig.add_trace(go.Scatter(x=axis_pos_nuc, y=q1_n,\n",
    "                         mode=\"lines\", line=dict(color=\"black\", dash=\"dot\", width=1),\n",
    "                         showlegend=False), row=5, col=1)\n",
    "fig.add_trace(go.Scatter(x=axis_pos_nuc, y=q3_n,\n",
    "                         mode=\"lines\", line=dict(color=\"black\", dash=\"dot\", width=1),\n",
    "                         showlegend=False), row=5, col=1)\n",
    "fig.add_trace(go.Scatter(x=axis_pos_nuc, y=med_n,\n",
    "                         mode=\"lines\", line=dict(color=\"black\", width=3),\n",
    "                         name=\"median Δ nuc\"), row=5, col=1)\n",
    "fig.add_shape(type=\"line\", x0=axis_min, x1=axis_max, y0=0, y1=0,\n",
    "              line=dict(color=\"grey\", dash=\"dash\"), row=5, col=1)\n",
    "fig.update_yaxes(title=\"Δ nuc count\", row=5, col=1)\n",
    "\n",
    "# Cosmetics & guides\n",
    "for r in (1,2,3):\n",
    "    fig.update_yaxes(showgrid=False, zeroline=False, row=r, col=1)\n",
    "fig.update_xaxes(showgrid=False, zeroline=False, row=1, col=1)\n",
    "fig.update_xaxes(title=AXIS_LABEL, row=5, col=1)\n",
    "if SHOW_MOTIF_LINES:\n",
    "    add_motif_guides(fig, positions=motif_positions, style=MOTIF_LINE_STYLE)\n",
    "if SHOW_TSS_Q4_LINES:\n",
    "    add_motif_guides(fig, positions=tss_q4_positions, style=TSS_Q4_LINE_STYLE)\n",
    "\n",
    "fig.update_layout(template=\"plotly_white\", width=800, height=1600,\n",
    "                  showlegend=False)\n",
    "\n",
    "# set x axis range to -900 to 900\n",
    "fig.update_xaxes(range=(-900, 900))\n",
    "fig.show()\n",
    "\n",
    "# ── DEBUG: in‑window nucleosome counts ──\n",
    "def _count_centres_in_window(df):\n",
    "    return sum(np.sum((np.asarray(c)>=-PLOT_WINDOW)&(np.asarray(c)<=PLOT_WINDOW))\n",
    "               for c in df.nuc_centers)\n",
    "\n",
    "dbg(f\"[In‑window Nucs] {COND_A}: {_count_centres_in_window(dfA_all_bal)} | \"\n",
    "    f\"{COND_B}: {_count_centres_in_window(dfB_all_bal)}  (±{PLOT_WINDOW} bp)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe3c9d8b3526859",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ╔════════════════════════════════════════════════════════════════╗\n",
    "# ║  Inter‑nucleosome distance  – single‑plot + per‑group exports  ║\n",
    "# ╚════════════════════════════════════════════════════════════════╝\n",
    "#\n",
    "# ‣ Re‑uses global flags already defined elsewhere:\n",
    "#     • SAVE_PER_TYPE        – toggle per‑(type[/bed_start]) exports\n",
    "#     • CONSIDER_BED_START   – group by type *and* bed_start when True\n",
    "#     • SHOW_MOTIF_LINES / SHOW_TSS_Q4_LINES  – dashed guide lines\n",
    "#\n",
    "# ‣ All CONFIG knobs relevant to *this* cell are (re)stated below.\n",
    "# ‣ Set OUT_DIR_INT to change the export folder just for these plots.\n",
    "# ‣ Requires `add_motif_guides`, `motif_positions`, `tss_q4_positions`,\n",
    "#   and the balanced dataframes `dfA_all_bal`, `dfB_all_bal`\n",
    "#   to be in scope (guaranteed by upstream cells).\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "import numpy as np, pandas as pd, plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from pathlib import Path\n",
    "import multiprocessing as mp\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ───────── USER CONFIG – inter‑nuc distance ───────── #\n",
    "INT_DIST_STEP        = 5       # grid spacing (bp)\n",
    "INT_DIST_MAX         =  1000    # keep pairs with distance ≤ this\n",
    "INT_SMOOTH_HALF_WIN  = 2       # moving‑avg half‑width (0 ⇒ no smoothing)\n",
    "REQUIRE_MOD_IN_GAP   = True   # True → demand ≥1 mod hit between cores\n",
    "INT_XWINDOW          = (-5000, 5000)\n",
    "INT_YWINDOW1         = (160, 320)   # Panel 1 (median distance)\n",
    "INT_YWINDOW2         = (-120,  120)   # Panel 2 (Δ distance)\n",
    "# ─── Robust trimmed statistics ─── #\n",
    "DISPERSION_METHOD = \"iqr\"   # or \"iqr\"\n",
    "TRIM_FRAC         = 0.10        # ignored if DISPERSION_METHOD == \"iqr\"\n",
    "\n",
    "# Saving / export\n",
    "SAVE_PER_TYPE        = False        # (inherited) toggle per‑group files\n",
    "CONSIDER_BED_START   = False   # (inherited) grouping policy\n",
    "OUT_DIR_INT          = Path(\"/Data1/git/meyer-nanopore/scripts/analysis/images_20250725/DPY27_N2_dyad_to_dyad_900bp_strong_rex\")   # ← change if needed\n",
    "STAMP                = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "# ---------------------------------------------------- #\n",
    "# ───────── MOTIF LINES (optional) ───────── #\n",
    "SHOW_MOTIF_LINES = False          # dashed grey guides on plots\n",
    "MOTIF_LINE_STYLE = dict(color=\"grey\", width=1, dash=\"dash\")\n",
    "SHOW_TSS_Q4_LINES  = False                     # green guides for TSS_q4\n",
    "TSS_Q4_LINE_STYLE  = dict(color=\"grey\", width=1, dash=\"dashdot\")\n",
    "\n",
    "OUT_DIR_INT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ╔═════════════════  REQUIRED INPUTS  (assertions)  ═════════════╗\n",
    "for name in (\"dfA_all_bal\", \"dfB_all_bal\", \"PLOT_WINDOW\",\n",
    "             \"COND_A\", \"COND_B\", \"CLR_A\", \"CLR_B\"):\n",
    "    assert name in globals(), f\"Variable {name} not defined – run earlier cells.\"\n",
    "# ╚══════════════════════════════════════════════════════════════╝\n",
    "# ===============================================================\n",
    "# (A) ONE‑TIME definition – paste right after the USER CONFIG box\n",
    "#     (just before the assertions are run is a good spot)\n",
    "# ===============================================================\n",
    "# Decide the fields that uniquely identify a “replicate”\n",
    "#   – includes bed_start only when CONSIDER_BED_START is True\n",
    "KEY_FIELDS = [\"type\", \"bed_start\", \"boot_idx\"] if CONSIDER_BED_START else \\\n",
    "             [\"type\", \"boot_idx\"]\n",
    "# ===============================================================\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "#  NEW helper – consistent condition → colour mapping\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "from itertools import cycle\n",
    "\n",
    "CLR_SDC = \"#b12537\"     # red\n",
    "CLR_N2  = \"#4974a5\"     # blue\n",
    "CLR_DPY = \"#47B562\"     # green\n",
    "\n",
    "_GREY_CYCLE = cycle([\"#4d4d4d\", \"#a3a3a3\"])\n",
    "_cond2clr_cache = {}\n",
    "\n",
    "def cond_color(cond: str) -> str:\n",
    "    \"\"\"\n",
    "    Return the plotting colour for a condition label.\n",
    "      * contains 'SDC' → red\n",
    "      * contains 'DPY' → green\n",
    "      * contains 'N2'  → blue\n",
    "      * else           → next shade of grey (stable per unknown label)\n",
    "    \"\"\"\n",
    "    key = cond.lower()\n",
    "    if \"sdc\" in key:\n",
    "        return CLR_SDC\n",
    "    if \"dpy\" in key:\n",
    "        return CLR_DPY\n",
    "    if \"n2\" in key:\n",
    "        return CLR_N2\n",
    "\n",
    "    # unseen ‘other’ label → assign next grey in the cycle\n",
    "    if key not in _cond2clr_cache:\n",
    "        _cond2clr_cache[key] = next(_GREY_CYCLE)\n",
    "    return _cond2clr_cache[key]\n",
    "\n",
    "# ─── replace the previous “col_A / col_B” definitions with:\n",
    "col_A = cond_color(COND_A)\n",
    "col_B = cond_color(COND_B)\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "#  Recompute MOTIF / TSS guide‑line positions for this cell only\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "src_df = pd.concat([dfA_all_bal, dfB_all_bal], ignore_index=True)\n",
    "\n",
    "if SHOW_MOTIF_LINES and \"motif_rel_start\" in src_df.columns:\n",
    "    _raw = []\n",
    "    for tup in src_df[\"motif_rel_start\"].dropna():\n",
    "        _raw.extend(tup if hasattr(tup, \"__iter__\") else [tup])\n",
    "    motif_positions = sorted({\n",
    "        abs(int(p)) if globals().get(\"MIRROR_ABS_DISTANCE\", False) else int(p)\n",
    "        for p in _raw\n",
    "    })\n",
    "else:\n",
    "    motif_positions = []\n",
    "\n",
    "if SHOW_TSS_Q4_LINES and {\"tss_rel_start\", \"tss_attributes\"} <= set(src_df.columns):\n",
    "    _tss_raw = []\n",
    "    for rels, attrs in src_df[[\"tss_rel_start\", \"tss_attributes\"]].dropna().itertuples(index=False):\n",
    "        for pos, attr in zip(rels, attrs):              # attr = (start, strand, type)\n",
    "            if attr[2] in (\"TSS_q4\", \"TSS_q3\"):\n",
    "                _tss_raw.append(pos)\n",
    "    tss_q4_positions = sorted({\n",
    "        abs(int(p)) if globals().get(\"MIRROR_ABS_DISTANCE\", False) else int(p)\n",
    "        for p in _tss_raw\n",
    "    })\n",
    "else:\n",
    "    tss_q4_positions = []\n",
    "\n",
    "def add_motif_guides(fig, *, positions, style, yref=\"paper\"):\n",
    "    \"\"\"Vertical dashed lines at motif / TSS positions.\"\"\"\n",
    "    for pos in positions:\n",
    "        fig.add_shape(type=\"line\", x0=pos, x1=pos, y0=0, y1=1,\n",
    "                      xref=\"x\", yref=yref, line=style, layer=\"below\")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "#  Helper utilities\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "def _smooth(arr, k):\n",
    "    if k == 0:\n",
    "        return arr.astype(float)\n",
    "    pad = np.pad(arr.astype(float), k, mode=\"edge\")\n",
    "    ker = np.ones(2 * k + 1, float) / (2 * k + 1)\n",
    "    return np.convolve(pad, ker, mode=\"valid\")\n",
    "\n",
    "def _q123(list_of_arrays, length):\n",
    "    if not list_of_arrays:\n",
    "        return (np.full(length, np.nan),\n",
    "                np.full(length, np.nan),\n",
    "                np.full(length, np.nan))\n",
    "    stack = np.vstack([np.asarray(a, float) for a in list_of_arrays])\n",
    "    with np.errstate(all=\"ignore\"):\n",
    "        q1  = np.nanpercentile(stack, 25, axis=0)\n",
    "        med = np.nanpercentile(stack, 50, axis=0)\n",
    "        q3  = np.nanpercentile(stack, 75, axis=0)\n",
    "    return q1, med, q3\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "#  Axis grid for distance curves\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "axis_min, axis_max = INT_XWINDOW\n",
    "axis_pos_int = np.arange(axis_min, axis_max + INT_DIST_STEP, INT_DIST_STEP)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "#  Core computation: replicate → (median distance, pair count)\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "#  NEW: compute q1 / median / q3 inside each replicate\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "#  NEW: trimmed‑mean ± trimmed‑SD curves for one replicate\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "#  Helper: human‑readable label for plot titles\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "def disp_label():\n",
    "    return (\"Median ± IQR\" if DISPERSION_METHOD == \"iqr\"\n",
    "            else f\"Trimmed mean ± SD (trim {int(TRIM_FRAC*100)} %)\")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "#  Compute (lower, centre, upper) curves for one replicate\n",
    "#    • \"iqr\"      → (q1, median, q3)\n",
    "#    • \"trimmed\"  → (mean‑SD, mean, mean+SD) after trimming\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "from scipy.stats import trim_mean\n",
    "\n",
    "def _replicate_curves_INT(read_df):\n",
    "    dist_lists = {p: [] for p in axis_pos_int}\n",
    "    pair_cnt   = np.zeros_like(axis_pos_int, float)\n",
    "\n",
    "    for row in read_df.itertuples(index=False):\n",
    "        centres = np.asarray(row.nuc_centers, float)\n",
    "        if centres.size < 2:\n",
    "            continue\n",
    "\n",
    "        relpos = np.asarray(row.rel_pos,      int)\n",
    "        mods   = np.asarray(row.mod_qual_bin, int)\n",
    "\n",
    "        in_win = centres[(centres >= -PLOT_WINDOW) & (centres <= PLOT_WINDOW)]\n",
    "        if in_win.size < 2:\n",
    "            continue\n",
    "\n",
    "        for idx, p in enumerate(axis_pos_int):\n",
    "            ups   = in_win[in_win <= p]\n",
    "            downs = in_win[in_win >= p]\n",
    "            if ups.size == 0 or downs.size == 0:\n",
    "                continue\n",
    "\n",
    "            dist = downs.min() - ups.max()\n",
    "            if not (0 < dist <= INT_DIST_MAX):\n",
    "                continue\n",
    "\n",
    "            if REQUIRE_MOD_IN_GAP:\n",
    "                in_gap = (relpos > ups.max()) & (relpos < downs.min())\n",
    "                if not (in_gap.any() and (mods[in_gap] == 1).any()):\n",
    "                    continue\n",
    "\n",
    "            dist_lists[p].append(dist)\n",
    "            pair_cnt[idx] += 1\n",
    "\n",
    "    lo, mid, hi = [], [], []\n",
    "    for vals in dist_lists.values():\n",
    "        if not vals:\n",
    "            lo.append(np.nan); mid.append(np.nan); hi.append(np.nan); continue\n",
    "        vals = np.asarray(vals, float)\n",
    "\n",
    "        if DISPERSION_METHOD == \"iqr\":\n",
    "            q1, med, q3 = np.nanpercentile(vals, [25, 50, 75])\n",
    "            lo.append(q1); mid.append(med); hi.append(q3)\n",
    "        else:  # \"trimmed\"\n",
    "            vals = np.sort(vals)\n",
    "            n    = len(vals)\n",
    "            k    = int(np.floor(TRIM_FRAC * n))\n",
    "            if 2 * k >= n - 1:\n",
    "                lo.append(np.nan); mid.append(np.nan); hi.append(np.nan); continue\n",
    "            trimmed = vals[k:-k]\n",
    "            mu = trimmed.mean()\n",
    "            sd = trimmed.std(ddof=0)\n",
    "            lo.append(mu - sd); mid.append(mu); hi.append(mu + sd)\n",
    "\n",
    "    return (np.array(lo), np.array(mid), np.array(hi)), pair_cnt\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "#  Parallel gathering of all replicate curves\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "def _task_replicate(args):\n",
    "    cond, key, subdf = args\n",
    "    return cond, key, _replicate_curves_INT(subdf)\n",
    "# ===============================================================\n",
    "tasks = []\n",
    "for cond, df_cond in ((COND_A, dfA_all_bal), (COND_B, dfB_all_bal)):\n",
    "    # group by the key we just defined\n",
    "    for key, sub in df_cond.groupby(KEY_FIELDS, sort=False):\n",
    "        # key is a tuple whose length matches KEY_FIELDS\n",
    "        tasks.append((cond, key, sub.reset_index(drop=True)))\n",
    "# ===============================================================\n",
    "\n",
    "rep_int = {COND_A: {}, COND_B: {}}\n",
    "with mp.Pool(max(2, mp.cpu_count() - 1)) as pool:\n",
    "    for cond, key, result in tqdm(pool.imap_unordered(_task_replicate, tasks),\n",
    "                                  total=len(tasks), desc=\"INT‑distance curves\"):\n",
    "        rep_int[cond][key] = result\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "#  Pooled quartiles (median ± IQR) & Δ‑distance\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "#  Pooled (mean‑of‑groups) IQR bands & Δ‑distance\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "dist_Q, pair_Q = {}, {}\n",
    "for cond in (COND_A, COND_B):\n",
    "    # collect each group's q‑tuple and pair‑count\n",
    "    q_tuples = [v[0] for v in rep_int[cond].values()]  # (q1,med,q3)\n",
    "    pair_raw = [v[1] for v in rep_int[cond].values()]\n",
    "\n",
    "    if q_tuples:\n",
    "        q1_mean  = np.nanmean([t[0] for t in q_tuples], axis=0)\n",
    "        med_mean = np.nanmean([t[1] for t in q_tuples], axis=0)\n",
    "        q3_mean  = np.nanmean([t[2] for t in q_tuples], axis=0)\n",
    "        dist_Q[cond] = tuple(\n",
    "            _smooth(a, INT_SMOOTH_HALF_WIN)\n",
    "            for a in (q1_mean, med_mean, q3_mean)\n",
    "        )\n",
    "    else:\n",
    "        dist_Q[cond] = (np.full_like(axis_pos_int, np.nan),) * 3\n",
    "\n",
    "    if pair_raw:\n",
    "        pair_mean = np.nanmean(pair_raw, axis=0)\n",
    "        pair_Q[cond] = (pair_mean, pair_mean, pair_mean)  # single curve\n",
    "    else:\n",
    "        pair_Q[cond] = (np.full_like(axis_pos_int, np.nan),) * 3\n",
    "\n",
    "# averaged‑median Δ and its averaged IQR bounds\n",
    "delta_q1  = dist_Q[COND_A][0] - dist_Q[COND_B][0]\n",
    "delta_med = dist_Q[COND_A][1] - dist_Q[COND_B][1]\n",
    "delta_q3  = dist_Q[COND_A][2] - dist_Q[COND_B][2]\n",
    "dD_Q = tuple(_smooth(a, INT_SMOOTH_HALF_WIN)\n",
    "             for a in (delta_q1, delta_med, delta_q3))\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "#  Plot helpers\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "def _add_q(fig, x, qtuple, color, name, row):\n",
    "    q1, med, q3 = qtuple\n",
    "    fig.add_trace(go.Scatter(x=x, y=q1, mode=\"lines\",\n",
    "                             line=dict(color=color, dash=\"dot\", width=1),\n",
    "                             showlegend=False), row=row, col=1)\n",
    "    fig.add_trace(go.Scatter(x=x, y=q3, mode=\"lines\",\n",
    "                             line=dict(color=color, dash=\"dot\", width=1),\n",
    "                             showlegend=False), row=row, col=1)\n",
    "    fig.add_trace(go.Scatter(x=x, y=med, mode=\"lines\",\n",
    "                             line=dict(color=color, width=3),\n",
    "                             name=name), row=row, col=1)\n",
    "\n",
    "# ╔═══════════════════  Pooled figure  ═════════════════════════╗\n",
    "fig_int = make_subplots(\n",
    "    rows=3, cols=1, shared_xaxes=False,\n",
    "    vertical_spacing=0.05,\n",
    "    row_heights=[0.33, 0.33, 0.34],\n",
    "    subplot_titles=(\n",
    "        f\"Inter‑nucleosome distance – {disp_label()} (≤ {INT_DIST_MAX} bp)\",\n",
    "        f\"Δ distance  ({COND_A} – {COND_B})\",\n",
    "        \"Valid pair count\"\n",
    "    )\n",
    "\n",
    ")\n",
    "_add_q(fig_int, axis_pos_int, dist_Q[COND_A], col_A, COND_A, row=1)\n",
    "_add_q(fig_int, axis_pos_int, dist_Q[COND_B], col_B, COND_B, row=1)\n",
    "fig_int.update_yaxes(title=\"Dyad‑dyad dist (bp)\",\n",
    "                     range=INT_YWINDOW1, row=1, col=1)\n",
    "\n",
    "q1_d, med_d, q3_d = dD_Q\n",
    "for arr in (q1_d, q3_d):\n",
    "    fig_int.add_trace(go.Scatter(x=axis_pos_int, y=arr, mode=\"lines\",\n",
    "                                 line=dict(color=\"black\", dash=\"dot\", width=1),\n",
    "                                 showlegend=False), row=2, col=1)\n",
    "fig_int.add_trace(go.Scatter(x=axis_pos_int, y=med_d, mode=\"lines\",\n",
    "                             line=dict(color=\"black\", width=3),\n",
    "                             name=\"Dyad‑dyad dist (bp)\"), row=2, col=1)\n",
    "fig_int.add_shape(type=\"line\", x0=axis_min, x1=axis_max, y0=0, y1=0,\n",
    "                  line=dict(color=\"grey\", dash=\"dash\"), row=2, col=1)\n",
    "fig_int.update_yaxes(title=\"Δ median per rex (bp)\",\n",
    "                     range=INT_YWINDOW2, row=2, col=1)\n",
    "\n",
    "_add_q(fig_int, axis_pos_int, pair_Q[COND_A], col_A, COND_A, row=3)\n",
    "_add_q(fig_int, axis_pos_int, pair_Q[COND_B], col_B, COND_B, row=3)\n",
    "fig_int.update_yaxes(title=\"# pairs\", row=3, col=1)\n",
    "\n",
    "for r in (1, 2, 3):\n",
    "    fig_int.update_xaxes(range=INT_XWINDOW, showgrid=False, zeroline=False,\n",
    "                         row=r, col=1)\n",
    "    fig_int.update_yaxes(showgrid=False, zeroline=False, row=r, col=1)\n",
    "for t in fig_int.data:\n",
    "    t.showlegend = (t.name in (COND_A, COND_B) and t.line.width == 3)\n",
    "\n",
    "if SHOW_MOTIF_LINES: add_motif_guides(fig_int, positions=motif_positions,\n",
    "                                      style=MOTIF_LINE_STYLE)\n",
    "if SHOW_TSS_Q4_LINES: add_motif_guides(fig_int, positions=tss_q4_positions,\n",
    "                                       style=TSS_Q4_LINE_STYLE)\n",
    "\n",
    "fig_int.update_layout(\n",
    "    template=\"plotly_white\", width=800, height=1200,\n",
    "    legend=dict(orientation='h', x=0.5, y=1.02,\n",
    "                xanchor='center', yanchor='bottom'),\n",
    "    margin=dict(t=100)\n",
    ")\n",
    "fig_int.show()\n",
    "\n",
    "# Save the pooled inter‑nuc distance figure\n",
    "base_tag = f\"intdist_pooled_{COND_A}_vs_{COND_B}_{STAMP}\"\n",
    "fig_int.write_image(OUT_DIR_INT / f\"{base_tag}.png\", scale=2)\n",
    "fig_int.write_image(OUT_DIR_INT / f\"{base_tag}.svg\")\n",
    "print(f\"[SAVE] pooled plot: {base_tag}.png / .svg → {OUT_DIR_INT}\")\n",
    "\n",
    "# ╔═══════════  Per‑type(/bed_start) exports  ══════════════════╗\n",
    "# ╔═══════════  Per-type(/bed) exports ═════════════╗\n",
    "if SAVE_PER_TYPE:\n",
    "    if CONSIDER_BED_START:\n",
    "        groups=sorted({tuple(x) for x in src_df[[\"type\",\"bed_start\"]].drop_duplicates().to_numpy()})\n",
    "    else:\n",
    "        groups=sorted(src_df[\"type\"].unique())\n",
    "    for grp in groups:\n",
    "        if CONSIDER_BED_START:\n",
    "            typ,bed=grp\n",
    "            df_grp=src_df[(src_df.type==typ)&(src_df.bed_start==bed)]\n",
    "            sel=lambda k: k[0]==typ and k[1]==bed\n",
    "            suffix=f\"{typ}_bed{bed}\"\n",
    "        else:\n",
    "            typ=grp\n",
    "            df_grp=src_df[src_df.type==typ]\n",
    "            sel=lambda k: k[0]==typ\n",
    "            suffix=typ\n",
    "\n",
    "        # group-specific motif/TSS\n",
    "        if SHOW_MOTIF_LINES and \"motif_rel_start\" in df_grp:\n",
    "            raw=[]\n",
    "            for tup in df_grp[\"motif_rel_start\"].dropna():\n",
    "                raw.extend(tup if hasattr(tup,\"__iter__\") else [tup])\n",
    "            mot_pos=sorted({abs(int(p)) if globals().get(\"MIRROR_ABS_DISTANCE\",False)\n",
    "                            else int(p) for p in raw})\n",
    "        else:\n",
    "            mot_pos=[]\n",
    "        if SHOW_TSS_Q4_LINES and {\"tss_rel_start\",\"tss_attributes\"}<=set(df_grp):\n",
    "            raw=[]\n",
    "            for rels,attrs in df_grp[[\"tss_rel_start\",\"tss_attributes\"]].dropna().itertuples(index=False):\n",
    "                for pos,attr in zip(rels,attrs):\n",
    "                    if attr[2] in (\"TSS_q4\",\"TSS_q3\"):\n",
    "                        raw.append(pos)\n",
    "            tss_pos=sorted({abs(int(p)) if globals().get(\"MIRROR_ABS_DISTANCE\",False)\n",
    "                             else int(p) for p in raw})\n",
    "        else:\n",
    "            tss_pos=[]\n",
    "\n",
    "        # quartiles for this group\n",
    "        dist_Qg, pair_Qg = {}, {}\n",
    "        for cond in (COND_A,COND_B):\n",
    "            D=[v[0] for k,v in rep_int[cond].items() if sel(k)]\n",
    "            C=[v[1] for k,v in rep_int[cond].items() if sel(k)]\n",
    "            if D or C:\n",
    "                dist_Qg[cond]=tuple(_smooth(a,INT_SMOOTH_HALF_WIN)\n",
    "                                    for a in _q123(D,len(axis_pos_int)))\n",
    "                pair_Qg[cond]=tuple(_smooth(a,INT_SMOOTH_HALF_WIN)\n",
    "                                    for a in _q123(C,len(axis_pos_int)))\n",
    "\n",
    "        if not dist_Qg: continue\n",
    "\n",
    "        # Δ for group\n",
    "        if COND_A in dist_Qg and COND_B in dist_Qg:\n",
    "            delta_g=[dist_Qg[COND_A][1]-dist_Qg[COND_B][1]]\n",
    "            dQg=tuple(_smooth(a,INT_SMOOTH_HALF_WIN)\n",
    "                      for a in _q123(delta_g,len(axis_pos_int)))\n",
    "        else:\n",
    "            dQg=dD_Q\n",
    "\n",
    "        # build figure\n",
    "        fig=make_subplots(rows=3,cols=1,shared_xaxes=False,vertical_spacing=0.05,\n",
    "                          row_heights=[0.33,0.33,0.34],\n",
    "                          subplot_titles=[\n",
    "                              f\"Inter‑nuc distance (median±IQR, ≤{INT_DIST_MAX}bp)\",\n",
    "                              f\"Δ distance ({COND_A}–{COND_B})\",\"# pairs\"])\n",
    "        _add_q(fig,axis_pos_int,dist_Qg.get(COND_A,dist_Q[COND_A]),col_A,COND_A,1)\n",
    "        _add_q(fig,axis_pos_int,dist_Qg.get(COND_B,dist_Q[COND_B]),col_B,COND_B,1)\n",
    "        fig.update_yaxes(range=INT_YWINDOW1,title=\"Dyad‑dyad dist\",row=1,col=1)\n",
    "\n",
    "        # ── Row 2 : Δ distance  (median ± IQR) ──\n",
    "        q1_sub, med_sub, q3_sub = dQg          # unpack\n",
    "\n",
    "        # IQR bounds (dotted)\n",
    "        for y_arr in (q1_sub, q3_sub):\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=axis_pos_int,\n",
    "                    y=y_arr,\n",
    "                    mode=\"lines\",\n",
    "                    line=dict(color=\"black\", dash=\"dot\", width=1),\n",
    "                    showlegend=False,\n",
    "                ),\n",
    "                row=2,\n",
    "                col=1,\n",
    "            )\n",
    "\n",
    "        # Median Δ line (solid)\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=axis_pos_int,\n",
    "                y=med_sub,\n",
    "                mode=\"lines\",\n",
    "                line=dict(color=\"black\", width=3),\n",
    "                name=\"Δ median\",\n",
    "            ),\n",
    "            row=2,\n",
    "            col=1,\n",
    "        )\n",
    "\n",
    "        # zero reference\n",
    "        fig.add_shape(\n",
    "            type=\"line\",\n",
    "            x0=axis_min, x1=axis_max, y0=0, y1=0,\n",
    "            line=dict(color=\"grey\", dash=\"dash\"),\n",
    "            row=2,\n",
    "            col=1,\n",
    "        )\n",
    "        fig.update_yaxes(range=INT_YWINDOW2, title=\"Δ median per rex (bp)\",\n",
    "                         row=2, col=1)\n",
    "        fig.update_yaxes(range=INT_YWINDOW2,title=\"Δ (bp)\",row=2,col=1)\n",
    "\n",
    "        _add_q(fig,axis_pos_int,pair_Qg.get(COND_A,pair_Q[COND_A]),col_A,COND_A,3)\n",
    "        _add_q(fig,axis_pos_int,pair_Qg.get(COND_B,pair_Q[COND_B]),col_B,COND_B,3)\n",
    "        fig.update_yaxes(title=\"# pairs\",row=3,col=1)\n",
    "\n",
    "        for r in (1,2,3):\n",
    "            fig.update_xaxes(range=INT_XWINDOW,showgrid=False,zeroline=False,row=r,col=1)\n",
    "            fig.update_yaxes(showgrid=False,zeroline=False,row=r,col=1)\n",
    "        for t in fig.data: t.showlegend=(t.name in (COND_A,COND_B) and t.line.width==3)\n",
    "\n",
    "        if SHOW_MOTIF_LINES: add_motif_guides(fig,positions=mot_pos,style=MOTIF_LINE_STYLE)\n",
    "        if SHOW_TSS_Q4_LINES:add_motif_guides(fig,positions=tss_pos,style=TSS_Q4_LINE_STYLE)\n",
    "\n",
    "        fig.update_layout(template=\"plotly_white\",width=800,height=1200,\n",
    "                          legend=dict(orientation='h',x=0.5,y=1.02,\n",
    "                                      xanchor='center',yanchor='bottom'),\n",
    "                          margin=dict(t=100),\n",
    "                          title=f\"Inter‑nuc distance – {suffix}\")\n",
    "        tag=f\"intdist_{suffix}_{COND_A}_vs_{COND_B}_{STAMP}\"\n",
    "        fig.write_image(OUT_DIR_INT/f\"{tag}.png\",scale=2)\n",
    "        fig.write_image(OUT_DIR_INT/f\"{tag}.svg\")\n",
    "        print(f\"[SAVE] {suffix}: {tag}.png/.svg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5be1e294d31960",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ╔════════════════════════════════════════════════════════════════╗\n",
    "# ║  CELL X: 1st→other inter‑nuc distance · NRL (± SD)             ║\n",
    "# ╚════════════════════════════════════════════════════════════════╝\n",
    "import numpy as np, plotly.graph_objects as go\n",
    "from scipy.stats import gaussian_kde\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "# ───────────────────────── USER CONFIG ────────────────────────── #\n",
    "MAX_CENTER_ABS  = 5000      # keep centres with |coord| < this\n",
    "DIST_X_MAX      = 750       # x‑axis upper bound (bp)\n",
    "GRID_STEP_BP    = 5         # grid step for KDE & x‑axis\n",
    "KDE_BW_FACTOR   = 0.20      # gaussian_kde bandwidth scaling\n",
    "PEAK_MIN_PROM   = 0.001     # min prominence for peak calling\n",
    "BOOT_N          = 50       # bootstrap replicates for ± SD\n",
    "# ───────────────────────────────────────────────────────────────── #\n",
    "\n",
    "# helper: grey‑override when needed\n",
    "def _pick_color(name, base, first):\n",
    "    if (\"N2\" in name) or (\"SDC\" in name):\n",
    "        return base\n",
    "    return \"#4d4d4d\" if first else \"#bfbfbf\"\n",
    "\n",
    "# assemble condition‑definitions\n",
    "cond_defs = [\n",
    "    (COND_A, dfA_all_bal, CLR_A),\n",
    "    (COND_B, dfB_all_bal, CLR_B),\n",
    "]\n",
    "if \"dfC_all_bal\" in globals():\n",
    "    cond_defs.append((COND_C, dfC_all_bal, globals().get(\"CLR_C\", \"#d95f02\")))\n",
    "\n",
    "# ─────────────────── Distance extraction per condition ─────────────────── #\n",
    "dist_by_cond, col_by_cond = {}, {}\n",
    "for idx, (cond, df_cond, base_col) in enumerate(cond_defs):\n",
    "    dists = []\n",
    "    for row in df_cond.itertuples(index=False):\n",
    "        centres = np.asarray(row.nuc_centers, float)\n",
    "        centres = centres[np.abs(centres) < MAX_CENTER_ABS]\n",
    "        if centres.size < 2:\n",
    "            continue\n",
    "\n",
    "        # ── NEGATIVE side (centres < 0) ──\n",
    "        neg = centres[centres < 0]\n",
    "        if neg.size >= 2:\n",
    "            first_neg = neg.min()                 # most upstream (most negative)\n",
    "            dists.extend(np.abs(neg[neg != first_neg] - first_neg))\n",
    "\n",
    "        # ── POSITIVE side (centres > 0) ──\n",
    "        pos = centres[centres > 0]\n",
    "        if pos.size >= 2:\n",
    "            first_pos = pos.min()                # closest to origin\n",
    "            dists.extend(np.abs(pos[pos != first_pos] - first_pos))\n",
    "\n",
    "    dists = np.asarray([d for d in dists if d <= DIST_X_MAX])\n",
    "    if dists.size:\n",
    "        dist_by_cond[cond] = dists\n",
    "        col_by_cond[cond]  = _pick_color(cond, base_col, idx == 0)\n",
    "\n",
    "x_grid = np.arange(0, DIST_X_MAX + GRID_STEP_BP, GRID_STEP_BP)\n",
    "\n",
    "# containers for peaks & NRL estimates\n",
    "peaks_by_cond, slope_by_cond, sd_by_cond = {}, {}, {}\n",
    "\n",
    "# ─────────────────────────── KDE + peak calling ─────────────────────────── #\n",
    "fig_kde = go.Figure()\n",
    "for cond, dvec in dist_by_cond.items():\n",
    "    kde  = gaussian_kde(dvec, bw_method=KDE_BW_FACTOR)\n",
    "    pdf  = kde(x_grid)\n",
    "    pct  = pdf * GRID_STEP_BP * 100\n",
    "    fig_kde.add_trace(go.Scatter(\n",
    "        x=x_grid, y=pct, mode=\"lines\",\n",
    "        name=cond, line=dict(width=3, color=col_by_cond[cond])\n",
    "    ))\n",
    "\n",
    "    # peaks\n",
    "    peaks_idx, _ = find_peaks(pct, prominence=PEAK_MIN_PROM)\n",
    "    peaks_x = x_grid[peaks_idx]\n",
    "    peaks_by_cond[cond] = peaks_x\n",
    "\n",
    "    for px in peaks_x:\n",
    "        fig_kde.add_shape(\n",
    "            type=\"line\", x0=px, x1=px, y0=0, y1=pct.max(),\n",
    "            line=dict(color=col_by_cond[cond], dash=\"dot\", width=1)\n",
    "        )\n",
    "        fig_kde.add_annotation(\n",
    "            x=px, y=pct.max(), text=f\"{px:.0f}\", showarrow=False,\n",
    "            yshift=4, font=dict(size=10, color=col_by_cond[cond])\n",
    "        )\n",
    "\n",
    "fig_kde.update_layout(\n",
    "    template=\"plotly_white\",\n",
    "    title=\"Inter‑nucleosome distance from first centre (KDE + peaks)\",\n",
    "    xaxis_title=\"distance from first centre (bp)\",\n",
    "    yaxis_title=f\"% of distances per {GRID_STEP_BP} bp\",\n",
    "    width=800, height=500,\n",
    "    legend=dict(orientation=\"h\", x=0.5, y=-0.25, xanchor=\"center\"),\n",
    "    margin=dict(b=100)\n",
    ")\n",
    "fig_kde.update_xaxes(range=[100, DIST_X_MAX], showgrid=False, zeroline=False)\n",
    "fig_kde.update_yaxes(showgrid=False, zeroline=False)\n",
    "fig_kde.show()\n",
    "\n",
    "# ───────────────────── Scatter + linear NRL fit (± SD) ───────────────────── #\n",
    "fig_nrl = go.Figure()\n",
    "\n",
    "for cond, peaks_x in peaks_by_cond.items():\n",
    "    if len(peaks_x) < 2:\n",
    "        continue\n",
    "    orders = np.arange(1, len(peaks_x) + 1)\n",
    "\n",
    "    # OLS slope\n",
    "    slope, intercept = np.polyfit(orders, peaks_x, 1)\n",
    "    slope_by_cond[cond] = slope\n",
    "\n",
    "    # bootstrap SD\n",
    "    boot_slopes = []\n",
    "    rng = np.random.default_rng(seed=0)\n",
    "    dvec = dist_by_cond[cond]\n",
    "\n",
    "    for _ in range(BOOT_N):\n",
    "        resample = rng.choice(dvec, size=dvec.size, replace=True)\n",
    "        kde_rs   = gaussian_kde(resample, bw_method=KDE_BW_FACTOR)(x_grid)\n",
    "        pk_idx_rs, _ = find_peaks(kde_rs * GRID_STEP_BP * 100,\n",
    "                                  prominence=PEAK_MIN_PROM)\n",
    "        pk_rs = x_grid[pk_idx_rs]\n",
    "        if len(pk_rs) < 2:\n",
    "            continue\n",
    "        ord_rs = np.arange(1, len(pk_rs) + 1)\n",
    "        s_rs, _ = np.polyfit(ord_rs, pk_rs, 1)\n",
    "        boot_slopes.append(s_rs)\n",
    "\n",
    "    sd = np.std(boot_slopes, ddof=1) if boot_slopes else np.nan\n",
    "    sd_by_cond[cond] = sd\n",
    "\n",
    "    # scatter + trend\n",
    "    fig_nrl.add_trace(go.Scatter(\n",
    "        x=orders, y=peaks_x, mode=\"markers\",\n",
    "        marker=dict(color=col_by_cond[cond], size=8),\n",
    "        name=f\"{cond} peaks\", showlegend=False\n",
    "    ))\n",
    "    y_fit = slope * orders + intercept\n",
    "    fig_nrl.add_trace(go.Scatter(\n",
    "        x=orders, y=y_fit, mode=\"lines\",\n",
    "        line=dict(color=col_by_cond[cond], width=3),\n",
    "        name=f\"{cond}  NRL = {slope:.1f} ± {sd:.1f} bp\"\n",
    "    ))\n",
    "\n",
    "fig_nrl.update_layout(\n",
    "    template=\"plotly_white\",\n",
    "    title=\"NRL estimation from peak order vs. distance\",\n",
    "    xaxis_title=\"Nucleosome repeats (x)\",\n",
    "    yaxis_title=\"dyad-dyad distance (bp)\",\n",
    "    width=800, height=500,\n",
    "    legend=dict(orientation=\"h\", x=0.5, y=-0.25, xanchor=\"center\"),\n",
    "    margin=dict(b=100)\n",
    ")\n",
    "fig_nrl.update_xaxes(showgrid=False, zeroline=False)\n",
    "fig_nrl.update_yaxes(showgrid=False, zeroline=False)\n",
    "fig_nrl.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7161afe608f0de78",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ╔════════════════════════════════════════════════════════════════╗\n",
    "# ║  NEW: inter‑nucleosome distance panels  – BOOTSTRAP VERSION    ║\n",
    "# ╚════════════════════════════════════════════════════════════════╝\n",
    "import numpy as np, pandas as pd, plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import multiprocessing as mp\n",
    "from tqdm.auto import tqdm\n",
    "from functools import partial\n",
    "from random import Random\n",
    "\n",
    "# ───────── USER CONFIG – inter‑nuc distance ───────── #\n",
    "INT_DIST_STEP       = 5       # grid spacing (bp)\n",
    "INT_DIST_MAX        = 500     # keep pairs with distance ≤ this\n",
    "INT_SMOOTH_HALF_WIN = 1       # moving‑avg half‑width (0 ⇒ no smoothing)\n",
    "REQUIRE_MOD_IN_GAP  = False   # False ⇒ ignore modification filter\n",
    "INT_XWINDOW         = (-900, +900)\n",
    "\n",
    "INT_YWINDOW1        = (150, 275)   # Panel 1 y‑range\n",
    "INT_YWINDOW2        = (-20,  60)   # Panel 2 y‑range\n",
    "\n",
    "BOOT_N              = 20      # ◀── number of bootstrap groups\n",
    "BOOT_SEED           = 42      # reproducible shuffling\n",
    "# ---------------------------------------------------- #\n",
    "\n",
    "axis_pos_int = np.arange(axis_min, axis_max + INT_DIST_STEP, INT_DIST_STEP)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────\n",
    "#  Helper: one dataframe → median distance & pair‑count curves\n",
    "# ──────────────────────────────────────────────────────────────────\n",
    "def _curves_INT(read_df):\n",
    "    dist_lists = {p: [] for p in axis_pos_int}\n",
    "    pair_cnt   = np.zeros_like(axis_pos_int, float)\n",
    "\n",
    "    for row in read_df.itertuples(index=False):\n",
    "        centres = np.asarray(row.nuc_centers, float)\n",
    "        if centres.size < 2:\n",
    "            continue\n",
    "\n",
    "        relpos = np.asarray(row.rel_pos,      int)\n",
    "        mods   = np.asarray(row.mod_qual_bin, int)\n",
    "\n",
    "        in_win = centres[(centres >= -PLOT_WINDOW) & (centres <= PLOT_WINDOW)]\n",
    "        if in_win.size < 2:\n",
    "            continue\n",
    "\n",
    "        for idx, p in enumerate(axis_pos_int):\n",
    "            ups   = in_win[in_win <= p]\n",
    "            downs = in_win[in_win >= p]\n",
    "            if (ups.size == 0) or (downs.size == 0):\n",
    "                continue\n",
    "            up_c, down_c = ups.max(), downs.min()\n",
    "            dist = down_c - up_c\n",
    "            if not (0 < dist <= INT_DIST_MAX):\n",
    "                continue\n",
    "\n",
    "            if REQUIRE_MOD_IN_GAP:\n",
    "                in_gap = (relpos > up_c) & (relpos < down_c)\n",
    "                if not (in_gap.any() and (mods[in_gap] == 1).any()):\n",
    "                    continue\n",
    "\n",
    "            dist_lists[p].append(dist)\n",
    "            pair_cnt[idx] += 1\n",
    "\n",
    "    dist_med = np.array([np.median(v) if v else np.nan\n",
    "                         for v in dist_lists.values()])\n",
    "    return dist_med, pair_cnt\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────\n",
    "#  Split each condition into BOOT_N groups  (random, no replacement)\n",
    "# ──────────────────────────────────────────────────────────────────\n",
    "def _partition_df(df, n_groups, rand):\n",
    "    idx = list(df.index)\n",
    "    rand.shuffle(idx)\n",
    "    chunks = np.array_split(idx, n_groups)\n",
    "    return [df.loc[c].reset_index(drop=True) for c in chunks]\n",
    "\n",
    "rand = Random(BOOT_SEED)\n",
    "groups = {COND_A: _partition_df(dfA_all_bal, BOOT_N, rand),\n",
    "          COND_B: _partition_df(dfB_all_bal, BOOT_N, rand)}\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────\n",
    "#  Compute curves in parallel\n",
    "# ──────────────────────────────────────────────────────────────────\n",
    "def _task(subdf):\n",
    "    return _curves_INT(subdf)\n",
    "\n",
    "results = {COND_A: [], COND_B: []}\n",
    "n_workers = max(2, mp.cpu_count() - 1)\n",
    "with mp.Pool(n_workers) as pool:\n",
    "    for cond in (COND_A, COND_B):\n",
    "        for med, cnt in tqdm(pool.imap_unordered(_task, groups[cond]),\n",
    "                             total=BOOT_N, desc=f\"bootstrap {cond}\"):\n",
    "            results[cond].append((med, cnt))\n",
    "\n",
    "# stacks: shape = (BOOT_N, n_positions)\n",
    "stack_med  = {c: np.stack([v[0] for v in results[c]]) for c in results}\n",
    "stack_cnt  = {c: np.stack([v[1] for v in results[c]]) for c in results}\n",
    "\n",
    "# helper to smooth 1‑d arrays\n",
    "def _smooth(arr, half_win):\n",
    "    if half_win <= 0 or np.isnan(arr).all():\n",
    "        return arr\n",
    "    pad   = np.pad(arr, half_win, mode=\"edge\")\n",
    "    kern  = np.ones(2*half_win+1) / (2*half_win+1)\n",
    "    sm    = np.convolve(pad, kern, mode=\"valid\")\n",
    "    sm[arr.size:] = np.nan  # preserve trailing NaNs if axis shorter\n",
    "    return sm[:arr.size]\n",
    "\n",
    "# median & IQR across bootstrap groups (per position)\n",
    "def _bootstrap_qtuple(stack, smooth_hw):\n",
    "    q1  = _smooth(np.nanpercentile(stack, 25, axis=0), smooth_hw)\n",
    "    med = _smooth(np.nanmedian    (stack,      axis=0), smooth_hw)\n",
    "    q3  = _smooth(np.nanpercentile(stack, 75, axis=0), smooth_hw)\n",
    "    return q1, med, q3\n",
    "\n",
    "dist_Q = {c: _bootstrap_qtuple(stack_med[c], INT_SMOOTH_HALF_WIN)\n",
    "          for c in (COND_A, COND_B)}\n",
    "pair_Q = {c: _bootstrap_qtuple(stack_cnt[c], INT_SMOOTH_HALF_WIN)\n",
    "          for c in (COND_A, COND_B)}\n",
    "\n",
    "# Δ‑panel : use paired bootstrap index (0..BOOT_N‑1) for A – B\n",
    "delta_stack = stack_med[COND_A] - stack_med[COND_B]\n",
    "dD_Q = _bootstrap_qtuple(delta_stack, INT_SMOOTH_HALF_WIN)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────\n",
    "#  Plotting\n",
    "# ──────────────────────────────────────────────────────────────────\n",
    "fig_int = make_subplots(\n",
    "    rows=3, cols=1, shared_xaxes=False,\n",
    "    vertical_spacing=0.05,\n",
    "    row_heights=[0.33, 0.33, 0.34],\n",
    "    subplot_titles=(\n",
    "        f\"Inter‑nucleosome distance (median ± IQR, ≤ {INT_DIST_MAX} bp)\",\n",
    "        f\"Δ distance ({COND_A} – {COND_B})\",\n",
    "        \"Valid pair count (all bootstrap groups)\"\n",
    "    )\n",
    ")\n",
    "\n",
    "def _add_q(fig, x, qtuple, color, name, row):\n",
    "    q1, med, q3 = qtuple\n",
    "    fig.add_trace(go.Scatter(x=x, y=q1, mode=\"lines\",\n",
    "                             line=dict(color=color, dash=\"dot\", width=1),\n",
    "                             showlegend=False), row=row, col=1)\n",
    "    fig.add_trace(go.Scatter(x=x, y=q3, mode=\"lines\",\n",
    "                             line=dict(color=color, dash=\"dot\", width=1),\n",
    "                             showlegend=False), row=row, col=1)\n",
    "    fig.add_trace(go.Scatter(x=x, y=med, mode=\"lines\",\n",
    "                             line=dict(color=color, width=3),\n",
    "                             name=name), row=row, col=1)\n",
    "\n",
    "# Panel 1 – distance\n",
    "_add_q(fig_int, axis_pos_int, dist_Q[COND_A], CLR_A, COND_A, row=1)\n",
    "_add_q(fig_int, axis_pos_int, dist_Q[COND_B], CLR_B, COND_B, row=1)\n",
    "fig_int.update_yaxes(title=\"median dist (bp)\", row=1, col=1)\n",
    "\n",
    "# Panel 2 – Δ distance\n",
    "q1_d, med_d, q3_d = dD_Q\n",
    "for arr in (q1_d, q3_d):\n",
    "    fig_int.add_trace(go.Scatter(x=axis_pos_int, y=arr, mode=\"lines\",\n",
    "                                 line=dict(color=\"black\", dash=\"dot\", width=1),\n",
    "                                 showlegend=False), row=2, col=1)\n",
    "fig_int.add_trace(go.Scatter(x=axis_pos_int, y=med_d, mode=\"lines\",\n",
    "                             line=dict(color=\"black\", width=3),\n",
    "                             name=\"median Δ\"), row=2, col=1)\n",
    "fig_int.add_shape(type=\"line\", x0=axis_min, x1=axis_max, y0=0, y1=0,\n",
    "                  line=dict(color=\"grey\", dash=\"dash\"), row=2, col=1)\n",
    "fig_int.update_yaxes(title=\"Δ distance (bp)\", row=2, col=1)\n",
    "\n",
    "# Panel 3 – pair counts\n",
    "# thin lines for every bootstrap group (optional, comment out if too busy)\n",
    "def _rgba(hexclr, alpha):\n",
    "    h = hexclr.lstrip(\"#\")\n",
    "    r, g, b = tuple(int(h[i:i+2], 16) for i in (0, 2, 4))\n",
    "    return f\"rgba({r},{g},{b},{alpha})\"\n",
    "\n",
    "for cond, color in ((COND_A, CLR_A), (COND_B, CLR_B)):\n",
    "    rgba = _rgba(color, 0.25)\n",
    "    # for cnt in stack_cnt[cond]:\n",
    "    #     fig_int.add_trace(go.Scatter(x=axis_pos_int, y=cnt, mode=\"lines\",\n",
    "    #                                  line=dict(color=rgba, width=1),\n",
    "    #                                  showlegend=False), row=3, col=1)\n",
    "    _add_q(fig_int, axis_pos_int, pair_Q[cond], color, cond, row=3)\n",
    "\n",
    "fig_int.update_yaxes(title=\"# pairs\", row=3, col=1)\n",
    "\n",
    "# Cosmetics\n",
    "for r in (1, 2, 3):\n",
    "    fig_int.update_xaxes(showgrid=False, zeroline=False, range=INT_XWINDOW,\n",
    "                         row=r, col=1)\n",
    "    fig_int.update_yaxes(showgrid=False, zeroline=False, row=r, col=1)\n",
    "\n",
    "fig_int.update_yaxes(range=INT_YWINDOW1, row=1, col=1)\n",
    "fig_int.update_yaxes(range=INT_YWINDOW2, row=2, col=1)\n",
    "# for row 4 default y axis\n",
    "# panel 3: no explicit range → default autoscale\n",
    "# ──────────────────────────────────────────────────────────────────\n",
    "#  Dynamically set y‑range for Panel 3 based on INT_XWINDOW\n",
    "# ──────────────────────────────────────────────────────────────────\n",
    "import numpy as np\n",
    "\n",
    "# make a mask of the x‑positions within the zoom window\n",
    "mask = (axis_pos_int >= INT_XWINDOW[0]) & (axis_pos_int <= INT_XWINDOW[1])\n",
    "\n",
    "# collect all y‑values in panel 3 within that x‑range\n",
    "ys = []\n",
    "for cond in (COND_A, COND_B):\n",
    "    # raw bootstrap counts\n",
    "    ys.append(stack_cnt[cond][:, mask])\n",
    "    # envelope lines (q1 and q3)\n",
    "    q1, _, q3 = pair_Q[cond]\n",
    "    ys.append(q1[mask])\n",
    "    ys.append(q3[mask])\n",
    "\n",
    "# flatten and compute min/max, then pad by 5%\n",
    "all_vals = np.concatenate([arr.ravel() for arr in ys])\n",
    "ymin, ymax = np.nanmin(all_vals), np.nanmax(all_vals)\n",
    "\n",
    "# apply to row 3\n",
    "fig_int.update_yaxes(range=(ymin , ymax ), row=3, col=1)\n",
    "\n",
    "# dotted 175‑bp reference line\n",
    "# fig_int.add_hline(y=175, line_dash=\"dot\", line_color=\"grey\", line_width=1,\n",
    "#                   row=1, col=1)\n",
    "# fig_int.add_annotation(xref=\"x domain\", yref=\"y\", x=1.0, y=175,\n",
    "#                        text=\"175 bp\", showarrow=False,\n",
    "#                        xanchor=\"left\", yanchor=\"bottom\", row=1, col=1)\n",
    "\n",
    "fig_int.update_layout(template=\"plotly_white\",\n",
    "                      width=800, height=1200,\n",
    "                      showlegend=False)\n",
    "\n",
    "fig_int.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28b1c4771930f68",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%────────────────────────────────────────────────────────────────────────────\n",
    "#  PLOT 100 RANDOM READS (one condition) + ENTROPY (J) UNDERNEATH\n",
    "#  – Semi-transparent core / linker tracks per read\n",
    "#  – mod_qual_bin==1 hits as coloured markers\n",
    "#  – Entropy computed *only* from the same reads\n",
    "# -----------------------------------------------------------------------------#\n",
    "import numpy as np, pandas as pd, plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.ndimage import uniform_filter1d\n",
    "\n",
    "# ───────── USER CONFIG ───────── #\n",
    "CONDITION_TO_PLOT = COND_A          # ← choose one of your conditions\n",
    "NUM_READS_PLOT    = 100             # how many reads to display\n",
    "RAND_SEED         = 22\n",
    "# visual\n",
    "CORE_CLR   = \"rgba(0,0,0,0.45)\"\n",
    "LINK_CLR   = \"rgba(160,160,160,0.45)\"\n",
    "HIT_CLR    = \"#E31A1C\"\n",
    "MA_WINDOW  = 3                      # moving-average half-window for J curve\n",
    "# entropy windowing (reuse globals)\n",
    "HALF_WIN   = WIN_BP // 2\n",
    "CENTRES    = np.arange(-PLOT_WINDOW, PLOT_WINDOW+1, STEP_BP)\n",
    "# ─────────────────────────────── #\n",
    "\n",
    "rng = np.random.default_rng(RAND_SEED)\n",
    "\n",
    "# ╔══════════════ 1. SAMPLE READS ═══════════════╗\n",
    "reads_sub = (\n",
    "    filtered_reads_df[filtered_reads_df.condition == CONDITION_TO_PLOT]\n",
    "      .sample(n=min(NUM_READS_PLOT,\n",
    "                    (filtered_reads_df.condition == CONDITION_TO_PLOT).sum()),\n",
    "              random_state=RAND_SEED)\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# make sure core_pos / link_pos exist  (should, from earlier cell)\n",
    "if \"core_pos\" not in reads_sub.columns:\n",
    "    raise RuntimeError(\"core_pos/link_pos columns missing – execute state-map cell first.\")\n",
    "\n",
    "# ╔══════════════ 2. COMPOSE READ-LEVEL TRACES ═══════════════╗\n",
    "def _segments(arr):\n",
    "    \"\"\"yield (start,end) for contiguous runs in a sorted integer array\"\"\"\n",
    "    if arr.size == 0:\n",
    "        return\n",
    "    run_start = arr[0]\n",
    "    prev = arr[0]\n",
    "    for x in arr[1:]:\n",
    "        if x != prev + 1:\n",
    "            yield run_start, prev\n",
    "            run_start = x\n",
    "        prev = x\n",
    "    yield run_start, prev\n",
    "\n",
    "scatter_traces = []\n",
    "for ridx, row in reads_sub.iterrows():\n",
    "    y = ridx + 1                        # 1-based on y-axis\n",
    "\n",
    "    # core (dark)\n",
    "    for s, e in _segments(np.sort(row.core_pos)):\n",
    "        scatter_traces.append(go.Scatter(\n",
    "            x=[s, e], y=[y, y],\n",
    "            mode=\"lines\", line=dict(color=CORE_CLR, width=3),\n",
    "            hoverinfo=\"skip\", showlegend=False\n",
    "        ))\n",
    "    # linker (light grey)\n",
    "    for s, e in _segments(np.sort(row.link_pos)):\n",
    "        scatter_traces.append(go.Scatter(\n",
    "            x=[s, e], y=[y, y],\n",
    "            mode=\"lines\", line=dict(color=LINK_CLR, width=3),\n",
    "            hoverinfo=\"skip\", showlegend=False\n",
    "        ))\n",
    "    # accessible hits\n",
    "    hits = [p for p, q in zip(row.rel_pos, row.mod_qual_bin) if q == 1]\n",
    "    if hits:\n",
    "        scatter_traces.append(go.Scatter(\n",
    "            x=hits, y=[y]*len(hits),\n",
    "            mode=\"markers\",\n",
    "            marker=dict(symbol=\"circle\", size=4, color=HIT_CLR),\n",
    "            hoverinfo=\"skip\", showlegend=False\n",
    "        ))\n",
    "\n",
    "# ╔══════════════ 3. ENTROPY (J) FOR THESE READS ═══════════════╗\n",
    "J_vals = []\n",
    "for cpos in CENTRES:\n",
    "    lo, hi = cpos - HALF_WIN, cpos + HALF_WIN - 1\n",
    "    core_cnt = linker_cnt = 0\n",
    "    for _, r in reads_sub.iterrows():\n",
    "        core_cnt   += ((r.core_pos >= lo) & (r.core_pos <= hi)).sum()\n",
    "        linker_cnt += ((r.link_pos >= lo) & (r.link_pos <= hi)).sum()\n",
    "    if core_cnt == 0 or linker_cnt == 0:\n",
    "        J_vals.append(np.nan)\n",
    "    else:\n",
    "        tot = core_cnt + linker_cnt\n",
    "        p1, p0 = core_cnt / tot, linker_cnt / tot\n",
    "        H = - (p1*np.log(p1) + p0*np.log(p0))\n",
    "        J_vals.append(H / np.log(2))\n",
    "\n",
    "# optional smoothing\n",
    "if MA_WINDOW > 1:\n",
    "    pad = MA_WINDOW // 2\n",
    "    J_sm = uniform_filter1d(np.where(np.isfinite(J_vals), J_vals, 0.0),\n",
    "                            MA_WINDOW, mode=\"nearest\")\n",
    "    cnt  = uniform_filter1d(np.isfinite(J_vals).astype(int),\n",
    "                            MA_WINDOW, mode=\"nearest\")\n",
    "    J_plot = np.where(cnt > 0, J_sm / np.maximum(cnt, 1), np.nan)\n",
    "else:\n",
    "    J_plot = J_vals\n",
    "\n",
    "# ╔══════════════ 4. BUILD FIGURE ═══════════════╗\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=1,\n",
    "    shared_xaxes=True,\n",
    "    row_heights=[0.7, 0.3],\n",
    "    vertical_spacing=0.03\n",
    ")\n",
    "\n",
    "# top panel: reads\n",
    "for tr in scatter_traces:\n",
    "    fig.add_trace(tr, row=1, col=1)\n",
    "\n",
    "fig.update_yaxes(title=\"Read #\", row=1, col=1,\n",
    "                 range=[0, len(reads_sub)+1], autorange=\"reversed\")\n",
    "\n",
    "# bottom panel: entropy\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=CENTRES, y=J_plot,\n",
    "    mode=\"lines\",\n",
    "    line=dict(width=4, color=\"#1F78B4\"),\n",
    "    name=\"J (core vs linker)\"\n",
    "), row=2, col=1)\n",
    "\n",
    "fig.update_yaxes(title=\"Shannon J\", row=2, col=1)\n",
    "\n",
    "# shared x-axis\n",
    "fig.update_xaxes(range=[-PLOT_WINDOW, PLOT_WINDOW],\n",
    "                 title=\"rel_pos (bp)\", row=2, col=1)\n",
    "\n",
    "fig.update_layout(\n",
    "    template=\"plotly_white\",\n",
    "    width=1000, height=700,\n",
    "    title=(f\"{NUM_READS_PLOT} random reads – {CONDITION_TO_PLOT} \"\n",
    "           f\"(cores/linkers & mod hits) + entropy\")\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d8603ffc810dd2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#  Sliding-window variance profile  +  variance-difference significance        #\n",
    "#  – One figure, two traces: σ_A(x), σ_B(x)  on left axis                      #\n",
    "#    and −log10(q) on right axis                                               #\n",
    "################################################################################\n",
    "import numpy as np, pandas as pd, plotly.graph_objects as go\n",
    "from scipy.stats import levene\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from tqdm.auto import tqdm   # progress bar (silently falls back if tqdm missing)\n",
    "\n",
    "# ───────────────────────── USER CONFIG ────────────────────────── #\n",
    "WINDOW_BP      = 88           # half-window used to collect centres (bp)\n",
    "STEP_BP        = 5           # stride of evaluation grid (bp)\n",
    "PVALUE_METHOD  = \"fdr_bh\"     # p-value correction ('fdr_bh', 'holm', …)\n",
    "ALPHA          = 0.05         # significance threshold on adjusted p\n",
    "PLOT_TEMPLATE  = \"plotly_white\"\n",
    "DEBUG          = True\n",
    "# ───────────────────────────────────────────────────────────────── #\n",
    "\n",
    "def dbg(msg):\n",
    "    if DEBUG:\n",
    "        print(msg)\n",
    "\n",
    "# 1) Gather ALL dyad positions for each condition ──────────────────────────── #\n",
    "def centres_from_df(df):\n",
    "    \"\"\"Return a flat NumPy array of all nucleosome centres in the dataframe.\"\"\"\n",
    "    return np.concatenate([r.nuc_centers for r in df.itertuples(index=False)\n",
    "                           if r.nuc_centers]).astype(float)\n",
    "\n",
    "centA = centres_from_df(dfA_all)\n",
    "centB = centres_from_df(dfB_all)\n",
    "dbg(f\"[DATA] {len(centA):,} centres in {COND_A}, {len(centB):,} in {COND_B}\")\n",
    "\n",
    "# 2) Evaluation grid along the x-axis ──────────────────────────────────────── #\n",
    "xs = np.arange(-PLOT_WINDOW, PLOT_WINDOW + 1, STEP_BP)\n",
    "\n",
    "σA, σB, pvals = [], [], []\n",
    "\n",
    "# 3) Sliding-window variance + Levene test ─────────────────────────────────── #\n",
    "for x in tqdm(xs, desc=\"sliding var\"):\n",
    "    winA = centA[np.abs(centA - x) <= WINDOW_BP]\n",
    "    winB = centB[np.abs(centB - x) <= WINDOW_BP]\n",
    "\n",
    "    # if either condition has < 3 centres in window, mark as NaN\n",
    "    if winA.size < 3 or winB.size < 3:\n",
    "        σA.append(np.nan); σB.append(np.nan); pvals.append(np.nan); continue\n",
    "\n",
    "\n",
    "    σA.append(np.std(winA, ddof=1))\n",
    "    σB.append(np.std(winB, ddof=1))\n",
    "\n",
    "    # Brown-Forsythe = Levene with center='median'\n",
    "    stat, p = levene(winA, winB, center='median')\n",
    "    pvals.append(p)\n",
    "\n",
    "σA, σB, pvals = map(np.asarray, (σA, σB, pvals))\n",
    "\n",
    "# 4) Multiple-testing correction ───────────────────────────────────────────── #\n",
    "mask_p = np.isfinite(pvals)\n",
    "_, qvals, _, _ = multipletests(pvals[mask_p], alpha=ALPHA, method=PVALUE_METHOD)\n",
    "q_full = np.full_like(pvals, np.nan); q_full[mask_p] = qvals\n",
    "sig = (q_full < ALPHA)\n",
    "\n",
    "dbg(f\"[STATS] {sig.sum()} / {mask_p.sum()} windows significant (FDR<{ALPHA})\")\n",
    "\n",
    "# 5) Build Plotly figure ───────────────────────────────────────────────────── #\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=xs, y=σA, mode=\"lines\", name=f\"σ {COND_A}\",\n",
    "    line=dict(color=CLR_A, width=2)\n",
    "))\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=xs, y=σB, mode=\"lines\", name=f\"σ {COND_B}\",\n",
    "    line=dict(color=CLR_B, width=2)\n",
    "))\n",
    "\n",
    "# secondary axis: −log10(q)\n",
    "with np.errstate(divide='ignore'):\n",
    "    neglogQ = -np.log10(q_full)\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=xs, y=neglogQ, mode=\"lines\",\n",
    "    name=\"−log10(q)\", line=dict(color=\"black\", dash=\"dot\"), yaxis=\"y2\"\n",
    "))\n",
    "\n",
    "# highlight significant windows\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=xs[sig], y=neglogQ[sig], mode=\"markers\",\n",
    "    marker=dict(size=6, color=\"red\"),\n",
    "    name=f\"FDR<{ALPHA}\"\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    template=PLOT_TEMPLATE,\n",
    "    width=1000, height=500,\n",
    "    title=f\"Sliding-window variance (window=±{WINDOW_BP} bp, step={STEP_BP} bp)\",\n",
    "    xaxis_title=\"Genomic rel_pos (bp)\",\n",
    "    yaxis_title=\"σ (bp)\",\n",
    "    yaxis2=dict(\n",
    "        title=\"−log10(q)\",\n",
    "        overlaying=\"y\", side=\"right\", showgrid=False\n",
    "    ),\n",
    "    legend=dict(x=0.02, y=0.98)\n",
    ")\n",
    "fig.show()\n",
    "################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34698a55cbd748d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%  ───────────────────────────────────────────────────────────────\n",
    "#  Peak-aligned per-type variance profile (mean ± IQR, adj_rel_pos)\n",
    "#  ───────────────────────────────────────────────────────────────────\n",
    "import numpy as np, pandas as pd, plotly.graph_objects as go\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ────────── USER CONFIG ADDITIONS ──────────\n",
    "ALIGN_RANGE   = (-200, 200)   # bp window in which to seek the peak variance\n",
    "FIG_W, FIG_H  = 1000, 500\n",
    "# ───────────────────────────────────────────\n",
    "\n",
    "def dbg(msg):\n",
    "    if DEBUG:\n",
    "        print(msg)\n",
    "\n",
    "# 1) Centres per type  (same helper as before) ─────────────────────\n",
    "def centres_by_type(df):\n",
    "    groups = {}\n",
    "    for t, sub in df.groupby(\"type\", sort=False):\n",
    "        groups[t] = np.concatenate(\n",
    "            [r.nuc_centers for r in sub.itertuples(index=False) if r.nuc_centers]\n",
    "        ).astype(float)\n",
    "    return groups\n",
    "\n",
    "centA_by_type = centres_by_type(dfA_all)\n",
    "centB_by_type = centres_by_type(dfB_all)\n",
    "type_list     = sorted(centA_by_type.keys())   # all types identical by premise\n",
    "\n",
    "dbg(f\"[DATA] {len(type_list)} types shared between conditions\")\n",
    "\n",
    "# 2) Evaluation grid ───────────────────────────────────────────────\n",
    "xs  = np.arange(-PLOT_WINDOW, PLOT_WINDOW + 1, STEP_BP)\n",
    "zero_idx = np.where(xs == 0)[0][0]\n",
    "\n",
    "# 3) Utility: shift array w/ NaNs (no wrap) ────────────────────────\n",
    "def shift_no_wrap(arr, shift):\n",
    "    \"\"\"Shift 1-D array by `shift` (int indices); new positions get NaN.\"\"\"\n",
    "    out = np.full_like(arr, np.nan, dtype=float)\n",
    "    if shift > 0:\n",
    "        out[shift:] = arr[:-shift]\n",
    "    elif shift < 0:\n",
    "        out[:shift] = arr[-shift:]\n",
    "    else:\n",
    "        out[:] = arr\n",
    "    return out\n",
    "\n",
    "# 4) Build variance matrix [n_types × n_xs] with peak alignment ───\n",
    "def var_matrix(cent_dict):\n",
    "    mats = []\n",
    "    for t in tqdm(type_list, desc=\"per-type variance\"):\n",
    "        centres = cent_dict[t]\n",
    "        # sliding-window variance\n",
    "        var_vec = []\n",
    "        for x in xs:\n",
    "            win = centres[np.abs(centres - x) <= WINDOW_BP]\n",
    "            var_vec.append(np.var(win, ddof=1) if win.size >= 2 else np.nan)\n",
    "        var_vec = np.asarray(var_vec)\n",
    "\n",
    "        # determine peak within ALIGN_RANGE\n",
    "        mask = (xs >= ALIGN_RANGE[0]) & (xs <= ALIGN_RANGE[1]) & np.isfinite(var_vec)\n",
    "        if mask.any():\n",
    "            peak_idx = np.where(mask)[0][np.nanargmax(var_vec[mask])]\n",
    "            shift    = peak_idx - zero_idx      # >0 ⇒ peak right of 0\n",
    "            var_vec  = shift_no_wrap(var_vec, -shift)\n",
    "        # if nothing finite in the window, leave unshifted\n",
    "\n",
    "        mats.append(var_vec)\n",
    "    return np.vstack(mats)     # shape: (n_types, n_xs)\n",
    "\n",
    "matA = var_matrix(centA_by_type)   # σ² values per type, aligned\n",
    "matB = var_matrix(centB_by_type)\n",
    "\n",
    "# 5) Aggregate across types (ignoring NaNs) ────────────────────────\n",
    "def summarise(mat):\n",
    "    mean = np.nanmean(mat, axis=0)\n",
    "    q1   = np.nanpercentile(mat, 25, axis=0)\n",
    "    q3   = np.nanpercentile(mat, 75, axis=0)\n",
    "    return mean, q1, q3\n",
    "\n",
    "meanA, q1A, q3A = summarise(matA)\n",
    "meanB, q1B, q3B = summarise(matB)\n",
    "\n",
    "# 6) Plot on adjusted axis (adj_rel_pos == xs) ─────────────────────\n",
    "fig = go.Figure()\n",
    "\n",
    "# Condition A\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=xs, y=meanA, mode=\"lines\", name=f\"Mean σ² {COND_A}\",\n",
    "    line=dict(color=CLR_A, width=2)\n",
    "))\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=xs, y=q1A, mode=\"lines\", name=f\"Q1 σ² {COND_A}\",\n",
    "    line=dict(color=CLR_A, width=1, dash=\"dot\")\n",
    "))\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=xs, y=q3A, mode=\"lines\", name=f\"Q3 σ² {COND_A}\",\n",
    "    line=dict(color=CLR_A, width=1, dash=\"dot\")\n",
    "))\n",
    "\n",
    "# Condition B\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=xs, y=meanB, mode=\"lines\", name=f\"Mean σ² {COND_B}\",\n",
    "    line=dict(color=CLR_B, width=2)\n",
    "))\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=xs, y=q1B, mode=\"lines\", name=f\"Q1 σ² {COND_B}\",\n",
    "    line=dict(color=CLR_B, width=1, dash=\"dot\")\n",
    "))\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=xs, y=q3B, mode=\"lines\", name=f\"Q3 σ² {COND_B}\",\n",
    "    line=dict(color=CLR_B, width=1, dash=\"dot\")\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    template=PLOT_TEMPLATE,\n",
    "    width=FIG_W, height=FIG_H,\n",
    "    title=(f\"Peak-aligned per-type sliding-window variance \"\n",
    "           f\"(window ±{WINDOW_BP} bp, step {STEP_BP} bp)\"),\n",
    "    xaxis_title=\"adj_rel_pos (bp, peak variance aligned at 0)\",\n",
    "    yaxis_title=\"σ² of centres (bp²)\",\n",
    "    legend=dict(x=0.02, y=0.98)\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ffde1b9897af51",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "#  Cell 2b – Fast-start “filtered_reads_df” builder (no nucleosome calling)   #\n",
    "#  ▸ Generates the filtered read table that Cell 3 expects                    #\n",
    "#  ▸ Adds empty nuc_centers / nuc_coords columns                             #\n",
    "#  ▸ Defines analysis_cond (edit as needed)                                   #\n",
    "###############################################################################\n",
    "import os, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ───────────────────────── USER CONFIG ────────────────────────── #\n",
    "# hard-coded condition aliases (Cell 3 expects these)\n",
    "COND_A, COND_B = analysis_cond[0], analysis_cond[2]\n",
    "# 1. Which three conditions do you want Cell 3 to use?\n",
    "ANALYSIS_CONDS = [COND_A, COND_B]\n",
    "\n",
    "DOWNSAMPLE_READS   = True   # False → keep native counts (default behaviour)\n",
    "DOWNSAMPLE_SEED    = 123    # reproducible RNG for the sampling step\n",
    "BALANCE_BY_TOTAL_BP  = True       # NEW → False keeps old count-based mode\n",
    "\n",
    "\n",
    "\n",
    "# 2. Metadata filters (should match Cell 1’s originals)\n",
    "TYPES_TO_INCLUDE      = []       # \"ALL\" or list or \"intergenic_control\"\n",
    "CHR_TYPE_INCLUDE      = []                        # [] → keep all \"Autosome\"\n",
    "CHIP_RANK_CUTOFF      = 80\n",
    "ABOVE_FLAG            = True\n",
    "MIN_READ_LENGTH       = 500\n",
    "REL_POS_RANGE         = 2000\n",
    "PLOT_WINDOW = REL_POS_RANGE\n",
    "REQUIRE_CENTRAL       = False\n",
    "CLR_A , CLR_B    = \"#b12537\", \"#4974a5\"\n",
    "\n",
    "# Path to rex_chiprank.bed (used only if TYPES_TO_INCLUDE is empty)\n",
    "CHIPRANK_PATH         = \"/Data1/reference/rex_chiprank.bed\"\n",
    "\n",
    "# ───────────────────────── sanity checks ───────────────────────── #\n",
    "if \"merged_df\" not in globals():\n",
    "    raise RuntimeError(\n",
    "        \"Cell 2b needs a DataFrame named `merged_df` in memory.\\n\"\n",
    "        \"Make sure you’ve loaded it earlier in the notebook.\"\n",
    "    )\n",
    "\n",
    "# ───────────────────────── helper ───────────────────────── #\n",
    "def _dbg(msg):\n",
    "    print(f\"[Cell 2b] {msg}\")\n",
    "\n",
    "# ───────────────────────── helper: bp-aware sampler ───────────── #\n",
    "def _sample_to_bp(df, target_bp, rng):\n",
    "    \"\"\"\n",
    "    Randomly shuffle reads and keep the smallest prefix whose\n",
    "    cumulative read_length ≥ target_bp.\n",
    "    Guarantees ≥ target bp (may overshoot by ≤ max(read_length)).\n",
    "    \"\"\"\n",
    "    idx = df.index.to_numpy()\n",
    "    rng.shuffle(idx)\n",
    "\n",
    "    cum_len = np.cumsum(df.loc[idx, \"read_length\"].to_numpy())\n",
    "    keep_up_to = np.searchsorted(cum_len, target_bp, side=\"left\") + 1\n",
    "    return idx[:keep_up_to]\n",
    "\n",
    "\n",
    "# ───────────────────────── chip-rank lookup (optional) ───────────────────────── #\n",
    "if (not TYPES_TO_INCLUDE) or (TYPES_TO_INCLUDE == [\"ALL\"]):\n",
    "    chiprank_df = pd.read_csv(CHIPRANK_PATH, sep=r\"\\s+\")\n",
    "    chiprank_df[\"type\"] = \"MOTIFS_\" + chiprank_df[\"type\"].astype(str)\n",
    "    chip_rank_lookup = dict(zip(chiprank_df[\"type\"], chiprank_df[\"chip_rank\"] * 100))\n",
    "else:\n",
    "    chip_rank_lookup = {}\n",
    "\n",
    "# ───────────────────────── build filtered_reads_df ───────────────────────── #\n",
    "keep_conds = set(ANALYSIS_CONDS)\n",
    "\n",
    "# --- decide motif types to keep --------------------------------------------\n",
    "if TYPES_TO_INCLUDE == [\"ALL\"]:\n",
    "    keep_types = set(merged_df[\"type\"].unique())\n",
    "elif TYPES_TO_INCLUDE:                          # explicit list\n",
    "    keep_types = set(TYPES_TO_INCLUDE)\n",
    "else:                                           # chip-rank threshold\n",
    "    keep_types = {\n",
    "        t for t, rk in chip_rank_lookup.items()\n",
    "        if (rk >= CHIP_RANK_CUTOFF) == ABOVE_FLAG\n",
    "    }\n",
    "\n",
    "# --- chromosomes ------------------------------------------------------------\n",
    "keep_chr = set(CHR_TYPE_INCLUDE) if CHR_TYPE_INCLUDE else set(merged_df[\"chr_type\"].unique())\n",
    "\n",
    "# --- metadata filter --------------------------------------------------------\n",
    "df0 = merged_df.query(\n",
    "    \"condition in @keep_conds and type in @keep_types and chr_type in @keep_chr\"\n",
    ").copy()\n",
    "_dbg(f\"after metadata filter: {len(df0)} reads\")\n",
    "\n",
    "# --- overlap with REL_POS_RANGE --------------------------------------------\n",
    "mask_overlap = df0[\"rel_pos\"].apply(\n",
    "    lambda arr: ((arr >= -REL_POS_RANGE) & (arr <= REL_POS_RANGE)).any()\n",
    ")\n",
    "df0 = df0[mask_overlap].reset_index(drop=True)\n",
    "_dbg(f\"after overlap filter:  {len(df0)} reads\")\n",
    "\n",
    "# --- length / central coverage ---------------------------------------------\n",
    "if REQUIRE_CENTRAL:\n",
    "    half = MIN_READ_LENGTH // 2\n",
    "    mask_central = df0[\"rel_pos\"].apply(\n",
    "        lambda arr: (arr.min() <= -half) and (arr.max() >= half)\n",
    "    )\n",
    "    df1 = df0[mask_central]\n",
    "else:\n",
    "    df1 = df0[df0[\"read_length\"] >= MIN_READ_LENGTH]\n",
    "df1 = df1.reset_index(drop=True)\n",
    "_dbg(f\"after length/central filter: {len(df1)} reads\")\n",
    "\n",
    "# --- methylation content ----------------------------------------------------\n",
    "mask_valid = df1[\"mod_qual_bin\"].apply(\n",
    "    lambda x: isinstance(x, (list, np.ndarray)) and np.nansum(x) > 0\n",
    ")\n",
    "filtered_reads_df = df1[mask_valid].reset_index(drop=True)\n",
    "_dbg(f\"after methylation filter: {len(filtered_reads_df)} reads\")\n",
    "\n",
    "# ──────────────────── NEW: per-locus balancing key ──────────────────── #\n",
    "# (insert just **before** the DOWNSAMPLE_READS block)\n",
    "\n",
    "BAL_KEY_COL = \"__bal_type\"                         # temporary helper col\n",
    "\n",
    "filtered_reads_df[BAL_KEY_COL] = np.where(\n",
    "    filtered_reads_df[\"type\"] == \"intergenic_control\",\n",
    "    # include bed_start to distinguish each intergenic region\n",
    "    filtered_reads_df[\"type\"] + \"_\" + filtered_reads_df[\"bed_start\"].astype(str),\n",
    "    filtered_reads_df[\"type\"]\n",
    ")\n",
    "\n",
    "# # temporarily keep only rows where type == the first unique type\n",
    "# if len(filtered_reads_df[BAL_KEY_COL].unique()) > 1:\n",
    "#     first_type = filtered_reads_df[BAL_KEY_COL].unique()[0]\n",
    "#     filtered_reads_df = filtered_reads_df[\n",
    "#         filtered_reads_df[BAL_KEY_COL] == first_type\n",
    "#     ].reset_index(drop=True\n",
    "#     )\n",
    "# _dbg(f\"after balancing key: {len(filtered_reads_df)} reads\")\n",
    "\n",
    "# ─── per-read bp covered *inside* the ±REL_POS_RANGE window ───\n",
    "def _bp_in_window(rel):\n",
    "    rel = np.asarray(rel, dtype=int)\n",
    "    m   = (rel >= -REL_POS_RANGE) & (rel <= REL_POS_RANGE)\n",
    "    # unique() ⇒ one bp counted once even if called multiple times\n",
    "    return np.unique(rel[m]).size\n",
    "\n",
    "filtered_reads_df[\"bp_window\"] = filtered_reads_df[\"rel_pos\"].apply(_bp_in_window)\n",
    "\n",
    "# ──────────────────── MODIFIED down-sampling loop ───────────────────── #\n",
    "if DOWNSAMPLE_READS:\n",
    "    rng = np.random.RandomState(DOWNSAMPLE_SEED)\n",
    "    kept_idx   = []\n",
    "    rows_drop  = 0\n",
    "    groups_trim = 0                                 # rename for clarity\n",
    "\n",
    "    print(\"\\n[Cell 2b] === DOWN-SAMPLING START (seed=\", DOWNSAMPLE_SEED, \") ===\")\n",
    "\n",
    "     ##############################################################################\n",
    "    #   BALANCE EACH (type or intergenic_control_locus) ACROSS CONDITIONS        #\n",
    "    #   — aim for equal bp; optional secondary match on read count               #\n",
    "    ##############################################################################\n",
    "    REQUIRE_EQUAL_READS = False      # set True if you also need n_reads balanced\n",
    "\n",
    "    for bal_key, df_t in filtered_reads_df.groupby(BAL_KEY_COL, sort=False):\n",
    "\n",
    "        bp_by_cond = (\n",
    "            df_t.groupby(\"condition\")[\"bp_window\"].sum()\n",
    "                .reindex(ANALYSIS_CONDS, fill_value=0)\n",
    "        )\n",
    "        target_bp = bp_by_cond.min()\n",
    "\n",
    "        print(f\"   {bal_key!r}: bp A={bp_by_cond.iloc[0]:,}  \"\n",
    "              f\"bp B={bp_by_cond.iloc[1]:,}  → target={target_bp:,} bp\")\n",
    "\n",
    "        for cond, g in df_t.groupby(\"condition\", sort=False):\n",
    "\n",
    "            # random order for fairness\n",
    "            g_shuf = g.sample(frac=1, random_state=rng)\n",
    "\n",
    "            # cumulate bp within the window\n",
    "            cum_bp = g_shuf[\"bp_window\"].cumsum()\n",
    "\n",
    "            keep_mask = cum_bp <= target_bp          # still under budget?\n",
    "\n",
    "            if not keep_mask.any():\n",
    "                # every read is longer than target_bp → keep the shortest\n",
    "                shortest_idx = g_shuf[\"bp_window\"].idxmin()\n",
    "                kept_idx.append(shortest_idx)\n",
    "                rows_drop   += len(g_shuf) - 1\n",
    "                groups_trim += 1\n",
    "                continue\n",
    "\n",
    "            # keep all reads that keep us under budget\n",
    "            kept_idx.extend(g_shuf.loc[keep_mask].index)\n",
    "\n",
    "            if REQUIRE_EQUAL_READS:\n",
    "                n_target = keep_mask.sum()\n",
    "                if len(g_shuf) > n_target:\n",
    "                    rows_drop   += len(g_shuf) - n_target\n",
    "                    groups_trim += 1\n",
    "            else:\n",
    "                rows_drop   += len(g_shuf) - keep_mask.sum()\n",
    "                groups_trim += 1\n",
    "\n",
    "    filtered_reads_df = filtered_reads_df.loc[kept_idx].reset_index(drop=True)\n",
    "    # ─── QUICK DIAGNOSTIC: are we balancing by reads or by bp? ───\n",
    "    def _balance_snapshot(df):\n",
    "        snap = (\n",
    "            df.groupby([BAL_KEY_COL, \"condition\"])\n",
    "              .agg(n_reads=(\"bp_window\", \"size\"),\n",
    "                   sum_bp =(\"bp_window\", \"sum\"),\n",
    "                   mean_bp=(\"bp_window\", \"mean\"))\n",
    "              .unstack(\"condition\", fill_value=0)\n",
    "              .sort_index()\n",
    "        )\n",
    "        return snap\n",
    "\n",
    "    print(\"\\n[DEBUG]  Post-down-sampling balance check  (first 10 loci):\")\n",
    "    snap = _balance_snapshot(filtered_reads_df)\n",
    "    display(snap.head(10))     # if in Jupyter, else print(snap.head(10))\n",
    "\n",
    "    # Summaries\n",
    "    same_reads = (snap.xs(\"n_reads\", axis=1, level=0).diff(axis=1).abs().sum().sum() == 0)\n",
    "    same_bp    = (snap.xs(\"sum_bp\",  axis=1, level=0).diff(axis=1).abs().sum().sum() == 0)\n",
    "    print(f\"\\n  • Equal #reads per locus?  {same_reads}\")\n",
    "    print(f\"  • Equal bp per locus?      {same_bp}\\n\")\n",
    "\n",
    "    print(f\"[Cell 2b] === DOWN-SAMPLING DONE: total dropped {rows_drop} rows \"\n",
    "          f\"from {groups_trim} (key×cond) combos ===\\n\")\n",
    "else:\n",
    "    print(\"[Cell 2b] down-sampling disabled – keeping native read counts\\n\")\n",
    "\n",
    "# ───── clean-up helper column ─────\n",
    "filtered_reads_df.drop(columns=BAL_KEY_COL, inplace=True)\n",
    "\n",
    "for cond, df_cond in filtered_reads_df.groupby(\"condition\"):\n",
    "    tot_bp = df_cond[\"read_length\"].sum()\n",
    "    _dbg(f\"{cond}: {len(df_cond)} reads, {tot_bp} bp after balancing\")\n",
    "\n",
    "# ───────────────────────── add empty nuc columns ──────────────────────────── #\n",
    "filtered_reads_df[\"nuc_centers\"] = [[] for _ in range(len(filtered_reads_df))]\n",
    "filtered_reads_df[\"nuc_coords\"]  = [[] for _ in range(len(filtered_reads_df))]\n",
    "\n",
    "_dbg(\"filtered_reads_df ready for Cell 3\")\n",
    "_dbg(f\"columns: {list(filtered_reads_df.columns)}\")\n",
    "\n",
    "# ╔═════════════════════ EXTRA GLOBALS NEEDED BY LATER CELLS ══════════════════╗\n",
    "\n",
    "\n",
    "# basic per-condition DataFrames (no motif filtering here;\n",
    "# later cells can refine them further if they wish)\n",
    "dfA_all = filtered_reads_df[filtered_reads_df[\"condition\"] == COND_A].copy()\n",
    "dfB_all = filtered_reads_df[filtered_reads_df[\"condition\"] == COND_B].copy()\n",
    "\n",
    "_dbg(f\"{COND_A}: {len(dfA_all)} reads\")\n",
    "_dbg(f\"{COND_B}: {len(dfB_all)} reads\")\n",
    "_dbg(\"Cell 2b complete – you can run the next analysis cell now.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cb21191d8d015",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "from numpy.lib.stride_tricks import as_strided\n",
    "from scipy.ndimage import uniform_filter1d   # at top with other imports\n",
    "# import gaussian filter 1d\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "SMOOTH_LINE_BP = 25       # width of running mean applied to y-traces\n",
    "# width (in bp) of uniform box-car smoothing applied to all heat-maps (0 = no smoothing)\n",
    "SMOOTH_HEAT_BP = 25\n",
    "\n",
    "# ────────── OCCUPANCY THRESHOLD OPTIONS ──────────\n",
    "#\n",
    "# How do we decide whether a window is “occupied”?\n",
    "#   • \"fixed\"      → use OCC_THRESH_FIXED for every condition\n",
    "#   • \"percentile\" → per-condition threshold = P-th percentile\n",
    "#   • \"mean_sd\"    → threshold = mean + K · sd   (per condition)\n",
    "# ───── USER CONFIG toggles ─────\n",
    "USE_FRAC_POS = True      # False → mean r  |  True → % reads with r>0\n",
    "OCC_THRESH_METHOD      = \"fixed\"   # \"fixed\" | \"percentile\" | \"mean_sd\"\n",
    "OCC_THRESH_FIXED       = 0.1           # only if method == \"fixed\"\n",
    "OCC_THRESH_PERCENTILE  = 80             # only if method == \"percentile\"\n",
    "OCC_THRESH_K_SD        = 1.0            # only if method == \"mean_sd\"\n",
    "\n",
    "# ───────────────────────── USER CONFIG ────────────────────────── #\n",
    "BIN_WIDTH_M6A      = 1      # bp per bin\n",
    "SMOOTH_M6A_BINS    = 1     # Gaussian σ in bins\n",
    "USE_THREADS        = True\n",
    "N_WORKERS          = min(32, len(dfA_all))\n",
    "\n",
    "### TEMPLATE DESIGN\n",
    "PEAK_FRAC   = 0.18   # 0 – 1\n",
    "TROUGH_FRAC = 0.8   # 0 – 1   (peak+trough must be < 1)\n",
    "\n",
    "LINKER_OUTER_BP    = 15     # left & right outer linkers\n",
    "TRANSITION_BP      = 1    # width of each cosine transition (ignored for binary)\n",
    "\n",
    "TEMPLATE_FORM  = \"single\"      # \"single\" | \"dual\" | \"triple\"\n",
    "EDGE_STYLE     = \"binary\"    # \"cosine\" | \"binary\"\n",
    "\n",
    "OUT_LINKER_BP  = 15          # outer linkers for \"dual\"\n",
    "CORE_WIDTH_BP  = 145         # each core for \"dual\"\n",
    "TRANSITION_BP  = 1          # cosine edge width if EDGE_STYLE==\"cosine\"\n",
    "\n",
    "\n",
    "# HEATMAP SETTINGS\n",
    "# --------------------------------------------------- #\n",
    "# define period grid\n",
    "PERIOD_MIN         = 175 #10\n",
    "PERIOD_MAX         = 175 #50\n",
    "PERIOD_STEP        = 1\n",
    "PERIODS_BP         = np.arange(PERIOD_MIN, PERIOD_MAX + 1, PERIOD_STEP)\n",
    "# ── y-axis values for the heat-maps ────────────────────────────────\n",
    "if TEMPLATE_FORM.lower() in (\"dual\", \"triple\"):\n",
    "    PERIODS_DISPLAY = PERIODS_BP / 2          # core-to-core distance\n",
    "    Y_AXIS_LABEL    = \"Half-period (bp)\"\n",
    "else:                                         # \"single\"\n",
    "    PERIODS_DISPLAY = PERIODS_BP\n",
    "    Y_AXIS_LABEL    = \"Period (bp)\"\n",
    "\n",
    "\n",
    "# specify which period to plot in the final line plot\n",
    "SELECT_PERIOD      = 175\n",
    "PERIOD_IDX = int(np.where(PERIODS_BP == SELECT_PERIOD)[0])   # ← NEW\n",
    "\n",
    "SKIP_ZERO_WINDOWS = True     # ← NEW: ignore windows whose mod_qual_bin is all 0\n",
    "\n",
    "# ───────────────────────── SHARED GRID ────────────────────────── #\n",
    "bins_m6a = np.arange(-PLOT_WINDOW, PLOT_WINDOW + BIN_WIDTH_M6A, BIN_WIDTH_M6A)\n",
    "cent_m6a = (bins_m6a[:-1] + bins_m6a[1:]) / 2\n",
    "\n",
    "# -----------------------COMPUTE r Threshold per condition---------------------------- #\n",
    "def _compute_occ_thresh(heats, condition_name):\n",
    "    \"\"\"Return a single scalar threshold for one condition & print debug.\"\"\"\n",
    "    finite_vals = heats[np.isfinite(heats)]\n",
    "    if finite_vals.size == 0:           # fallback\n",
    "        print(f\"[DEBUG] {condition_name}: no finite r – using 0.0\")\n",
    "        return 0.0\n",
    "\n",
    "    if OCC_THRESH_METHOD == \"fixed\":\n",
    "        thr = OCC_THRESH_FIXED\n",
    "\n",
    "    elif OCC_THRESH_METHOD == \"percentile\":\n",
    "        thr = np.percentile(finite_vals, OCC_THRESH_PERCENTILE)\n",
    "\n",
    "    elif OCC_THRESH_METHOD == \"mean_sd\":\n",
    "        thr = finite_vals.mean() + OCC_THRESH_K_SD * finite_vals.std()\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"unknown OCC_THRESH_METHOD '{OCC_THRESH_METHOD}'\")\n",
    "\n",
    "    print(f\"[DEBUG] {condition_name}: occupancy threshold = {thr:.4f} \"\n",
    "          f\"({OCC_THRESH_METHOD})\")\n",
    "    return thr\n",
    "\n",
    "\n",
    "def _make_template(period_bp: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Build a mean-centred template of *exactly* `period_bp` bp.\n",
    "\n",
    "    TEMPLATE_FORM == \"single\"\n",
    "        • Flat peak (PEAK_FRAC) / flat trough (TROUGH_FRAC) / cosine transitions.\n",
    "\n",
    "    TEMPLATE_FORM == \"dual\"\n",
    "        • Pattern 1–0–1–0–1:\n",
    "              out_linker  |  core  |  mid_linker  |  core  |  out_linker\n",
    "        • OUT_LINKER_BP and CORE_WIDTH_BP are fixed.\n",
    "        • mid_linker grows/shrinks with the period.\n",
    "        • EDGE_STYLE == \"cosine\" → smooth edges (width = TRANSITION_BP);\n",
    "          else hard binary.\n",
    "    \"\"\"\n",
    "    n_bins_tot = int(round(period_bp / BIN_WIDTH_M6A))\n",
    "\n",
    "    # ------------------------------------------------ SINGLE -----------\n",
    "    if TEMPLATE_FORM.lower() == \"single\":\n",
    "        if PEAK_FRAC + TROUGH_FRAC >= 1:\n",
    "            raise ValueError(\"PEAK_FRAC + TROUGH_FRAC must be < 1\")\n",
    "\n",
    "        peak_bins   = int(round(n_bins_tot * PEAK_FRAC / 2)) * 2  # split L/R\n",
    "        trough_bins = int(round(n_bins_tot * TROUGH_FRAC))\n",
    "        trans_bins  = (n_bins_tot - peak_bins - trough_bins) // 2\n",
    "        # segments\n",
    "        peak_L = np.ones(peak_bins // 2)\n",
    "        peak_R = np.ones_like(peak_L)\n",
    "        trough = np.zeros(trough_bins)\n",
    "        x = np.linspace(0, np.pi, trans_bins, endpoint=False)\n",
    "        fall  = 0.5 * (1 + np.cos(x))        # 1 → 0\n",
    "        rise  = fall[::-1]                   # 0 → 1\n",
    "        tpl = np.concatenate([peak_L, fall, trough, rise, peak_R])\n",
    "\n",
    "    # ------------------------------------------------ DUAL -------------\n",
    "    elif TEMPLATE_FORM.lower() == \"dual\":\n",
    "        out_bins  = int(round(OUT_LINKER_BP / BIN_WIDTH_M6A))\n",
    "        core_bins = int(round(CORE_WIDTH_BP  / BIN_WIDTH_M6A))\n",
    "        trans_bins= int(round(TRANSITION_BP  / BIN_WIDTH_M6A)) if EDGE_STYLE==\"cosine\" else 0\n",
    "\n",
    "        fixed = 2*out_bins + 2*core_bins + 4*trans_bins\n",
    "        mid_bins = n_bins_tot - fixed\n",
    "        if mid_bins < 1:\n",
    "            raise ValueError(f\"period {period_bp} too small for fixed dimensions.\")\n",
    "\n",
    "        # helpers\n",
    "        def plateau(h, w): return np.full(w, h)\n",
    "        def trans(a, b):\n",
    "            if trans_bins==0 or a==b: return np.empty(0)\n",
    "            x = np.linspace(0, np.pi/2, trans_bins, endpoint=False)\n",
    "            return np.cos(x)**2 if a>b else 1-np.cos(x)**2\n",
    "\n",
    "        tpl = np.concatenate([\n",
    "            plateau(1, out_bins),\n",
    "            trans(1,0),\n",
    "            plateau(0, core_bins),\n",
    "            trans(0,1),\n",
    "            plateau(1, mid_bins),\n",
    "            trans(1,0),\n",
    "            plateau(0, core_bins),\n",
    "            trans(0,1),\n",
    "            plateau(1, out_bins)\n",
    "        ])\n",
    "    # ---------------------------------------------------------------- TRIPLE --\n",
    "    elif TEMPLATE_FORM.lower() == \"triple\":\n",
    "        out_bins   = int(round(OUT_LINKER_BP  / BIN_WIDTH_M6A))\n",
    "        core_bins  = int(round(CORE_WIDTH_BP   / BIN_WIDTH_M6A))\n",
    "        trans_bins = int(round(TRANSITION_BP   / BIN_WIDTH_M6A)) \\\n",
    "                     if EDGE_STYLE == \"cosine\" else 0\n",
    "\n",
    "        def plateau(h, w): return np.full(w, h)\n",
    "        def trans(a, b):\n",
    "            if trans_bins == 0 or a == b:\n",
    "                return np.empty(0, dtype=float)\n",
    "            x = np.linspace(0, np.pi / 2, trans_bins, endpoint=False)\n",
    "            return np.cos(x) ** 2 if a > b else 1 - np.cos(x) ** 2\n",
    "\n",
    "        # one nucleosome block: 1 → 0 → 1\n",
    "        nuc = np.concatenate([\n",
    "            plateau(1, out_bins),\n",
    "            trans(1, 0),\n",
    "            plateau(0, core_bins),\n",
    "            trans(0, 1),\n",
    "            plateau(1, out_bins)\n",
    "        ])\n",
    "\n",
    "        fixed = 2 * nuc.size                # two nucleosomes, no gap yet\n",
    "        gap_bins = n_bins_tot - fixed\n",
    "        if gap_bins < 0:\n",
    "            raise ValueError(f\"period {period_bp} too small for 'triple' form\")\n",
    "\n",
    "        tpl = np.concatenate([nuc, np.full(gap_bins, np.nan), nuc])\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"TEMPLATE_FORM must be 'single', 'dual' or 'triple'.\")\n",
    "\n",
    "    # ----- centre on the *valid* (non-NaN) bins only --------------------------\n",
    "    m = np.nanmean(tpl)\n",
    "    tpl = tpl - m\n",
    "    tpl[np.isnan(tpl)] = np.nan             # keep NaNs\n",
    "\n",
    "    # length fence-post\n",
    "    if tpl.size < n_bins_tot:\n",
    "        tpl = np.pad(tpl, (0, n_bins_tot - tpl.size), constant_values=np.nan)\n",
    "    elif tpl.size > n_bins_tot:\n",
    "        tpl = tpl[:n_bins_tot]\n",
    "\n",
    "    return tpl\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ─────────────────── prebuild all templates ────────────────────── #\n",
    "TEMPLATES = {L: _make_template(L) for L in PERIODS_BP}\n",
    "\n",
    "\n",
    "# ─────────── show five example templates ─────────── #\n",
    "idxs            = np.linspace(0, len(PERIODS_BP)-1, 5, dtype=int)\n",
    "example_periods = PERIODS_BP[idxs]\n",
    "\n",
    "fig_templates = go.Figure()\n",
    "for L in example_periods:\n",
    "    tpl = TEMPLATES[L]\n",
    "    # center x around zero\n",
    "    x   = (np.arange(tpl.size) - tpl.size//2) * BIN_WIDTH_M6A\n",
    "    fig_templates.add_trace(go.Scatter(\n",
    "        x=x, y=tpl, mode=\"lines\", name=f\"{L} bp\",connectgaps=False,\n",
    "    ))\n",
    "fig_templates.update_layout(\n",
    "    template=\"plotly_white\",\n",
    "    title=f\"Example {TEMPLATE_FORM.title()} Templates\",\n",
    "    xaxis_title=\"rel_pos within window (bp)\",\n",
    "    yaxis_title=\"Mean-centered amplitude\",\n",
    "    width = 800\n",
    ")\n",
    "fig_templates.show()\n",
    "\n",
    "# ─────────────────── m6A occupancy track per read ────────────────── #\n",
    "def _m6a_track(rel_pos, mod_bin):\n",
    "    idx = np.digitize(rel_pos[np.asarray(mod_bin)==1], bins_m6a) - 1\n",
    "    idx = idx[(idx>=0) & (idx<len(cent_m6a))]\n",
    "    #track = np.zeros(len(cent_m6a), float) # background = 0\n",
    "    track = np.full(len(cent_m6a), -1.0)     # background = -1\n",
    "\n",
    "    if idx.size:\n",
    "        track[np.unique(idx)] = 1.0\n",
    "    # ───── Gaussian smoothing instead of uniform ─────\n",
    "    if SMOOTH_M6A_BINS > 0:\n",
    "        # SMOOTH_M6A_BINS now acts as the Gaussian sigma\n",
    "        track = gaussian_filter1d(track, sigma=SMOOTH_M6A_BINS, mode=\"nearest\")\n",
    "    return track\n",
    "\n",
    "# ─────────── normalised sliding correlation (Pearson r) ─────────── #\n",
    "def _normxcorr1d(track: np.ndarray, tpl: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Sliding Pearson-r with optional NaNs in tpl (ignored in r).\n",
    "    \"\"\"\n",
    "    m, n = tpl.size, track.size\n",
    "    if n < m:\n",
    "        return np.full(n, np.nan)\n",
    "\n",
    "    # pre-mask once – tpl NaNs are the same for every window\n",
    "    mask      = ~np.isnan(tpl)\n",
    "    tpl_valid = tpl[mask]                   # 1-D view without NaNs\n",
    "    tpl_zm    = tpl_valid - tpl_valid.mean()\n",
    "    tpl_norm  = np.linalg.norm(tpl_zm)\n",
    "\n",
    "    # stride windows, but keep only the masked columns\n",
    "    shape   = (n - m + 1, m)\n",
    "    strides = (track.strides[0], track.strides[0])\n",
    "    windows = as_strided(track, shape, strides)[:, mask]      # (win, k)\n",
    "    w_cent  = windows - windows.mean(axis=1, keepdims=True)\n",
    "\n",
    "    numer = (w_cent * tpl_zm).sum(axis=1)\n",
    "    denom = np.linalg.norm(w_cent, axis=1) * tpl_norm\n",
    "\n",
    "    if SKIP_ZERO_WINDOWS:\n",
    "        corr = np.divide(numer, denom,\n",
    "                         out=np.full_like(numer, np.nan),\n",
    "                         where=denom > 0)\n",
    "    else:\n",
    "        safe_denom = np.where(denom == 0, 1, denom)\n",
    "        corr = numer / safe_denom\n",
    "        corr[denom == 0] = 0.0\n",
    "\n",
    "    out = np.full(n, np.nan)\n",
    "    out[m // 2 : m // 2 + corr.size] = corr\n",
    "    return out\n",
    "\n",
    "\n",
    "# ───────────── single-read periodicity heat-map ───────────── #\n",
    "def _per_read_heat(track):\n",
    "    return np.vstack([_normxcorr1d(track, TEMPLATES[L]) for L in PERIODS_BP])\n",
    "\n",
    "# ───────────── wrapper for threading ───────────── #\n",
    "def _process_m6a(args):\n",
    "    track = _m6a_track(*args)\n",
    "    heat  = _per_read_heat(track)              # (len(PERIODS_BP), len(rel_pos))\n",
    "    # store which windows we skipped for SELECT_PERIOD\n",
    "    zero_mask = np.isnan(heat[PERIOD_IDX]) if SKIP_ZERO_WINDOWS \\\n",
    "                else (heat[PERIOD_IDX] == 0)\n",
    "    return heat, zero_mask\n",
    "\n",
    "\n",
    "# ───────────────────────── condition-level helper ───────────────────────── #\n",
    "def _condition_heat(df, *, thresh_val=None,return_tot=False):\n",
    "    payload = [(np.asarray(r.rel_pos,      dtype=int),\n",
    "                np.asarray(r.mod_qual_bin, dtype=int))\n",
    "               for r in df.itertuples(index=False)]\n",
    "\n",
    "    worker = _process_m6a\n",
    "    if USE_THREADS and N_WORKERS > 1:\n",
    "        with ThreadPool(N_WORKERS) as pool:\n",
    "            results = list(tqdm(pool.imap_unordered(worker, payload),\n",
    "                                total=len(payload),\n",
    "                                desc=\"m6A correlation\"))\n",
    "    else:\n",
    "        results = [worker(args) for args in tqdm(payload,\n",
    "                                                 desc=\"m6A correlation\")]\n",
    "\n",
    "    heats, masks = zip(*results)\n",
    "    heats = np.array(heats)            # (n_reads, n_periods, n_pos)\n",
    "    masks = np.array(masks)            # (n_reads, n_pos)\n",
    "\n",
    "    pct_zero = 100 * masks.sum(axis=0) / masks.shape[0]\n",
    "\n",
    "    if USE_FRAC_POS:\n",
    "        if thresh_val is None:                   # compute exactly once\n",
    "            thresh_val = _compute_occ_thresh(heats, df.iloc[0].condition)\n",
    "\n",
    "\n",
    "        valid   = ~np.isnan(heats)\n",
    "        pos_cnt = np.sum((heats > thresh_val) & valid, axis=0)\n",
    "        tot_cnt = np.sum(valid, axis=0)\n",
    "        frac_pos = 100 * pos_cnt / np.where(tot_cnt == 0, np.nan, tot_cnt)\n",
    "        \n",
    "        if return_tot:\n",
    "            return frac_pos, pct_zero, thresh_val, tot_cnt[PERIOD_IDX]\n",
    "        else:\n",
    "            return frac_pos, pct_zero, thresh_val\n",
    "    else:\n",
    "        mean_heat = np.nanmean(heats, axis=0) # consider nanmean\n",
    "        return mean_heat, pct_zero, None\n",
    "\n",
    "# ╔════════════════════════════════════════════════════════════════╗\n",
    "# ║           BUILD & PLOT HEAT-MAPS –  Pearson correlation        ║\n",
    "# ╚════════════════════════════════════════════════════════════════╝\n",
    "# ═════════ BUILD & PLOT HEAT-MAPS ═════════\n",
    "print(\"[m6A-PERIODICITY] computing …\")\n",
    "\n",
    "# ---- full-condition calls (threshold computed & cached) ----------\n",
    "hA, pctA, THR_A = _condition_heat(dfA_all)\n",
    "hB, pctB, THR_B = _condition_heat(dfB_all)\n",
    "hD              = hB - hA\n",
    "\n",
    "# Store in a dict for later bootstrap/type slices\n",
    "COND_THRESH = {COND_A: THR_A, COND_B: THR_B}\n",
    "\n",
    "# ───────────── helper to make symmetric heat-maps ───────────── #\n",
    "def _heat(fig_title, Z, fixed_lim=None, y_vals=PERIODS_DISPLAY):\n",
    "    \"\"\"\n",
    "    Draw a heat-map.  z-range priority:\n",
    "      1) caller-supplied fixed_lim\n",
    "      2) %-reads mode → [0, 100]\n",
    "      3) dynamic 5th-/95th-percentile clip (≈ middle 90 %)\n",
    "    \"\"\"\n",
    "    # ───── apply optional box-car smoothing along rel_pos axis ─────\n",
    "    if SMOOTH_HEAT_BP > 1:\n",
    "        win = max(1, int(round(SMOOTH_HEAT_BP / BIN_WIDTH_M6A)))\n",
    "        Z_sm = np.empty_like(Z)\n",
    "        for i in range(Z.shape[0]):\n",
    "            row = Z[i]\n",
    "            mask = ~np.isnan(row)\n",
    "            csum = np.cumsum(np.where(mask, row, 0.0))\n",
    "            cnt  = np.cumsum(mask.astype(int))\n",
    "            # sliding-window subtraction\n",
    "            csum[win:] -= csum[:-win]\n",
    "            cnt[win:]  -= cnt[:-win]\n",
    "            smooth = csum / np.where(cnt == 0, np.nan, cnt)\n",
    "            smooth[:win-1] = np.nan\n",
    "            Z_sm[i] = smooth\n",
    "        Z = Z_sm\n",
    "\n",
    "    # 1) explicit limits from caller ↓\n",
    "    if fixed_lim is not None:\n",
    "        zmin, zmax = fixed_lim\n",
    "        zmid       = 0\n",
    "\n",
    "    # 3) dynamic clip to 5th–95th percentiles ↓\n",
    "    else:\n",
    "        p5, p95 = np.nanpercentile(Z, [5, 95])\n",
    "        # keep symmetry for signed maps so colours stay intuitive\n",
    "        if (p5 < 0) and (p95 > 0):\n",
    "            v = max(abs(p5), abs(p95))\n",
    "            zmin, zmax = -v, v\n",
    "            zmid       = 0\n",
    "        else:\n",
    "            zmin, zmax = p5, p95\n",
    "            zmid       = None\n",
    "\n",
    "    # colour-bar label\n",
    "    cbar_title = (\"% reads\\n(r > 0)\" if USE_FRAC_POS and \"Δ\" not in fig_title\n",
    "                  else (\"Pearson r\" if \"Δ\" not in fig_title else \"Δ r or Δ %\"))\n",
    "\n",
    "    fig = go.Figure(go.Heatmap(\n",
    "        x          = cent_m6a,\n",
    "        y          = y_vals,\n",
    "        z          = Z,\n",
    "        colorscale = \"RdBu\",\n",
    "        zmid       = zmid,\n",
    "        zmin       = zmin,\n",
    "        zmax       = zmax,\n",
    "        colorbar   = dict(title=cbar_title)\n",
    "    ))\n",
    "    fig.update_layout(\n",
    "        template    = \"plotly_white\",\n",
    "        title       = fig_title,\n",
    "        xaxis_title = \"rel_pos (bp)\",\n",
    "        yaxis_title = Y_AXIS_LABEL,\n",
    "        xaxis       = dict(range=[-PLOT_WINDOW, PLOT_WINDOW]),\n",
    "        width       = 800\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "\n",
    "\n",
    "# single-condition maps use the same fixed limits (-1…1)\n",
    "#corr_lim = (-0.02, 0.08)\n",
    "fig_hA = _heat(f\"{COND_A} – Pearson correlation\", hA)#, corr_lim)\n",
    "fig_hB = _heat(f\"{COND_B} – Pearson correlation\", hB)#, corr_lim)\n",
    "\n",
    "# difference map scales to its own max (could be up to ±2)\n",
    "fig_hD = _heat(\"Δ Pearson r (B – A)\", hD)\n",
    "\n",
    "fig_hA.show(); fig_hB.show(); fig_hD.show()\n",
    "print(\"[m6A-PERIODICITY] done.\")\n",
    "#\n",
    "# # ─────────────────── Example individual reads ─────────────────── #\n",
    "# import plotly.graph_objects as go\n",
    "#\n",
    "# # pick two example reads (first two, or .sample(...))\n",
    "# sample_reads = dfA_all.head(2)\n",
    "#\n",
    "# for row in sample_reads.itertuples(index=False):\n",
    "#     # 1) build the occupancy track\n",
    "#     track = _m6a_track(\n",
    "#         np.asarray(row.rel_pos,      dtype=int),\n",
    "#         np.asarray(row.mod_qual_bin, dtype=int)\n",
    "#     )\n",
    "#\n",
    "#     # plot raw occupancy\n",
    "#     fig_track = go.Figure(go.Scatter(\n",
    "#         x=cent_m6a, y=track,\n",
    "#         mode=\"lines\", name=\"m6A occupancy\"\n",
    "#     ))\n",
    "#     fig_track.update_layout(\n",
    "#         template=\"plotly_white\",\n",
    "#         title=f\"m6A occupancy – read {row.read_id}\",\n",
    "#         xaxis_title=\"rel_pos (bp)\",\n",
    "#         yaxis_title=\"Occupancy\",\n",
    "#         xaxis=dict(range=[-PLOT_WINDOW, PLOT_WINDOW]),\n",
    "#         width = 800\n",
    "#     )\n",
    "#     fig_track.show()\n",
    "#\n",
    "#     # 2) compute and plot per‐read heatmap\n",
    "#     H = _per_read_heat(track)\n",
    "#     # use same fixed limits as single‐cond (–1…1)\n",
    "#     fig_read = _heat(f\"Pearson r – read {row.read_id}\", H, fixed_lim=(-1,1))\n",
    "#     fig_read.show()\n",
    "\n",
    "# ─────────────────── Correlation vs rel_pos – slider plot ─────────────────── #\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "def _nan_running_mean(arr, win):\n",
    "    \"\"\"\n",
    "    Windowed mean that ignores NaNs.\n",
    "    The first (win-1) positions are NaN so the output aligns\n",
    "    with a centred boxcar of width `win`.\n",
    "    \"\"\"\n",
    "    arr   = np.asarray(arr, float)\n",
    "    mask  = ~np.isnan(arr)\n",
    "    csum  = np.cumsum(np.where(mask, arr, 0.0))\n",
    "    cnt   = np.cumsum(mask.astype(int))\n",
    "\n",
    "    # standard cumulative-sum window trick\n",
    "    csum[win:] = csum[win:] - csum[:-win]\n",
    "    cnt[win:]  = cnt[win:]  - cnt[:-win]\n",
    "\n",
    "    out = csum / np.where(cnt == 0, np.nan, cnt)\n",
    "    out[:win-1] = np.nan        # left-pad so length stays the same\n",
    "    return out\n",
    "\n",
    "# ─────────── Plot correlation vs rel_pos for one period ────────── #\n",
    "idx = np.argwhere(PERIODS_BP == SELECT_PERIOD).item()\n",
    "\n",
    "# raw per-condition series (may contain NaNs from skipped windows)\n",
    "yA_raw = hA[idx]\n",
    "yB_raw = hB[idx]\n",
    "\n",
    "# ---- smooth with NaN-aware running mean --------------------------\n",
    "win_bins = max(1, int(round(SMOOTH_LINE_BP / BIN_WIDTH_M6A)))\n",
    "yA  = _nan_running_mean(yA_raw, win_bins)\n",
    "yB  = _nan_running_mean(yB_raw, win_bins)\n",
    "yD  = yB - yA                       # Δ after smoothing\n",
    "\n",
    "# ---- dynamic y-axis limits ---------------------------------------\n",
    "if USE_FRAC_POS:\n",
    "    top_label  = \"% reads r>0\"\n",
    "    diff_label = \"Δ % (B−A)\"\n",
    "    y_top_rng  = [0, 100]\n",
    "else:\n",
    "    top_label  = \"Pearson r\"\n",
    "    diff_label = \"Δ Pearson r\"\n",
    "    ymin, ymax = np.nanmin([yA, yB]), np.nanmax([yA, yB])\n",
    "    pad        = 0.05 * (ymax - ymin) if ymax > ymin else 0.05\n",
    "    y_top_rng  = [ymin - pad, ymax + pad]\n",
    "\n",
    "diff_max = np.nanmax(np.abs(yD))\n",
    "y_diff_rng = [-diff_max * 1.05, diff_max * 1.05]\n",
    "\n",
    "# ---- build figure ------------------------------------------------\n",
    "fig = make_subplots(\n",
    "    rows=3, cols=1, shared_xaxes=True,\n",
    "    row_heights=[0.6, 0.25, 0.15],\n",
    "    vertical_spacing=0.08\n",
    ")\n",
    "\n",
    "# row 1: smoothed condition traces\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=cent_m6a, y=yA,\n",
    "    mode=\"lines\", name=COND_A,\n",
    "    line=dict(color=CLR_A)\n",
    "), row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=cent_m6a, y=yB,\n",
    "    mode=\"lines\", name=COND_B,\n",
    "    line=dict(color=CLR_B)\n",
    "), row=1, col=1)\n",
    "\n",
    "# row 2: smoothed Δ\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=cent_m6a, y=yD,\n",
    "    mode=\"lines\", name=f\"{COND_B} − {COND_A}\",\n",
    "    line=dict(color=\"black\", dash=\"dash\")\n",
    "), row=2, col=1)\n",
    "\n",
    "# row 3: % skipped (unchanged)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=cent_m6a, y=pctA,\n",
    "    mode=\"lines\", name=f\"% zero {COND_A}\",\n",
    "    line=dict(color=CLR_A, dash=\"dot\")\n",
    "), row=3, col=1)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=cent_m6a, y=pctB,\n",
    "    mode=\"lines\", name=f\"% zero {COND_B}\",\n",
    "    line=dict(color=CLR_B, dash=\"dot\")\n",
    "), row=3, col=1)\n",
    "\n",
    "# ---- axes & layout ----------------------------------------------\n",
    "fig.update_yaxes(title_text=top_label,  row=1, col=1, range=y_top_rng)\n",
    "fig.update_yaxes(title_text=diff_label, row=2, col=1, range=y_diff_rng)\n",
    "fig.update_yaxes(title_text=\"% skipped\\n(r = 0)\", row=3, col=1, range=[0, 100])\n",
    "\n",
    "fig.update_xaxes(title_text=\"rel_pos (bp)\",\n",
    "                 row=3, col=1,\n",
    "                 range=[-PLOT_WINDOW, PLOT_WINDOW])\n",
    "\n",
    "fig.update_layout(\n",
    "    template=\"plotly_white\",\n",
    "    title=(f\"Period = {SELECT_PERIOD} bp  –  \"\n",
    "           f\"{SMOOTH_LINE_BP} bp running mean\"),\n",
    "    height=700, width=800,\n",
    "    showlegend=True\n",
    ")\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78249c39a0efeebc",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%  ───────────────────────────────────────────────────────────────\n",
    "#  Box-plots of per-type×bootstrap “% Phased” + motif histogram\n",
    "#  Keys switch to (type, bed_start) when ‘intergenic_control’ present\n",
    "#  ───────────────────────────────────────────────────────────────────\n",
    "\n",
    "# ─────── DISABLE ALL PROGRESS BARS ───────\n",
    "import tqdm\n",
    "tqdm = lambda iterable, **kwargs: iterable\n",
    "# ───────── USER-CONFIG (add / update) ─────────\n",
    "BIN_STEP_BP         = 10\n",
    "BIN_WIDTH_BP        = 200          # full width of each analysis window\n",
    "FIRST_BIN_CENTER_BP = 0\n",
    "N_BOOT_GROUPS       = 2\n",
    "BOOT_GROUP_SEED     = 43\n",
    "\n",
    "SUB_BIN_SIZE_BP     = 20          # NEW ⟶ width of sub-bins inside each window\n",
    "USE_WINDOW_ENTROPY  = True       # NEW ⟶ True ⇒ use Shannon entropy metric\n",
    "# ──────────────────────────────────────────────\n",
    "\n",
    "# ───────── ENTROPY BASELINE CONFIG ─────────\n",
    "BASELINE_PCTL = 0      # percentile to subtract (0–100)\n",
    "CLIP_BELOW_0 = True    # clip negatives to 0 before entropy\n",
    "# -------------------------------------------\n",
    "\n",
    "\n",
    "# ───────── PLOT-WINDOW (display only) ─────────\n",
    "PLOT_HALF_WINDOW_BP = 1000          # ← set any positive integer\n",
    "PLOT_MOTIF_HIST     = False      # toggle the motif histogram on/off\n",
    "\n",
    "# ─── decide key granularity ───\n",
    "has_ic = ((dfA_all[\"type\"] == \"intergenic_control\").any() or\n",
    "          (dfB_all[\"type\"] == \"intergenic_control\").any())\n",
    "\n",
    "KEY_COLS = [\"type\", \"bed_start\"] if has_ic else [\"type\"]\n",
    "print(f\"[DEBUG] grouping keys =\", KEY_COLS)\n",
    "\n",
    "# ───────────────── helper: assign bootstrap groups ────────────────\n",
    "def _subbin_stat(vec, mask, *, tlabel, cond):\n",
    "    \"\"\"\n",
    "    Metric for a sub-bin minus its condition×type baseline.\n",
    "    • vec[mask]  → raw metric values (may contain NaNs)\n",
    "    • tlabel, cond  → look up baseline\n",
    "    \"\"\"\n",
    "    if mask.sum() == 0:\n",
    "        raw = 0.0\n",
    "    else:\n",
    "        raw = np.nanmean(vec[mask]) if USE_FRAC_POS else np.nanmean(vec[mask])\n",
    "                                                # same either way\n",
    "    # subtract baseline\n",
    "    base = (BASE_A if cond == COND_A else BASE_B)[tlabel]\n",
    "    adj  = raw - base\n",
    "    if CLIP_BELOW_0:\n",
    "        adj = max(adj, 0.0)\n",
    "    return adj\n",
    "\n",
    "\n",
    "\n",
    "def _assign_bootstrap_groups(df, n_groups, seed=42):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    df  = df.copy()\n",
    "    for key_vals, idx in df.groupby(KEY_COLS).groups.items():\n",
    "        shuffled = rng.permutation(idx)\n",
    "        df.loc[shuffled, \"__boot\"] = np.arange(len(shuffled)) % n_groups\n",
    "    df[\"__boot\"] = df[\"__boot\"].astype(int)\n",
    "    return df\n",
    "\n",
    "_LN = np.log                                     # natural log (base e)\n",
    "\n",
    "def _shannon_entropy01(vals):\n",
    "    \"\"\"\n",
    "    Normalised Shannon entropy, 0 ≤ H ≤ 1.\n",
    "    `vals` can be any non-negative numbers (need not sum to 1).\n",
    "    \"\"\"\n",
    "    v   = np.asarray(vals, float)\n",
    "    v[v < 0] = 0\n",
    "    tot = v.sum()\n",
    "    if tot == 0 or np.isnan(tot):\n",
    "        return np.nan\n",
    "    p        = v / tot\n",
    "    nz       = p > 0\n",
    "    h_raw    = -(p[nz] * _LN(p[nz])).sum()       # 0‒ln(n)\n",
    "    h_max    = _LN(len(vals))                    # ln(n)\n",
    "    return h_raw / h_max if h_max > 0 else np.nan   # 0‒1\n",
    "\n",
    "# NEW – convert entropy → order once, everywhere\n",
    "def _order_score(vals):\n",
    "    h = _shannon_entropy01(vals)\n",
    "    return 1 - h if np.isfinite(h) else np.nan\n",
    "\n",
    "# ───────────────── SELECT_PERIOD series per (key, boot) ───────────\n",
    "def _per_key_boot_series(df):\n",
    "    out = {}\n",
    "    grp_cols = KEY_COLS + [\"__boot\"]\n",
    "    for key_tuple, g in df.groupby(grp_cols, sort=False):\n",
    "        if g.empty:\n",
    "            continue\n",
    "        thr = COND_THRESH[g.iloc[0].condition]\n",
    "        heat, _ , _ = _condition_heat(g, thresh_val=thr)\n",
    "        out[key_tuple] = heat[PERIOD_IDX]     # 1-D trace\n",
    "    return out\n",
    "\n",
    "dfA_boot = _assign_bootstrap_groups(dfA_all, N_BOOT_GROUPS, BOOT_GROUP_SEED)\n",
    "dfB_boot = _assign_bootstrap_groups(dfB_all, N_BOOT_GROUPS, BOOT_GROUP_SEED)\n",
    "\n",
    "series_A = _per_key_boot_series(dfA_boot)\n",
    "series_B = _per_key_boot_series(dfB_boot)\n",
    "\n",
    "keys_all = sorted(series_A.keys() | series_B.keys())\n",
    "print(f\"[DEBUG] {len(keys_all)} group×boot combos – first five:\", keys_all[:5])\n",
    "\n",
    "def _build_baseline(series_dict, condition_name):\n",
    "    \"\"\"\n",
    "    Return {type_label: baseline_value} where baseline is the\n",
    "    P-th percentile (configurable) of the selected metric across\n",
    "    *all* reads / positions / boots for that (condition, type).\n",
    "    \"\"\"\n",
    "    pools = {}\n",
    "    for (tlabel, *_), vec in series_dict.items():   # key[0] is type\n",
    "        pools.setdefault(tlabel, []).append(vec)\n",
    "\n",
    "    baselines = {}\n",
    "    for tlabel, vecs in pools.items():\n",
    "        all_vals = np.concatenate([v[~np.isnan(v)] for v in vecs])\n",
    "        if all_vals.size == 0:\n",
    "            baselines[tlabel] = 0.0\n",
    "        else:\n",
    "            # if BASELINE_PCTL != 0 then  baselines[tlabel] = np.nanpercentile(all_vals, BASELINE_PCTL)\n",
    "            if BASELINE_PCTL != 0:\n",
    "                baselines[tlabel] = np.nanpercentile(all_vals, BASELINE_PCTL)\n",
    "            else:\n",
    "                # subtract 0\n",
    "                baselines[tlabel] = 0.0\n",
    "\n",
    "\n",
    "        print(f\"[DEBUG] {condition_name} | {tlabel}: \"\n",
    "              f\"{BASELINE_PCTL}th-pct baseline = {baselines[tlabel]:.4f}\")\n",
    "    return baselines\n",
    "\n",
    "BASE_A = _build_baseline(series_A, COND_A)   # dict {type: value}\n",
    "BASE_B = _build_baseline(series_B, COND_B)\n",
    "\n",
    "\n",
    "# ─────────────────── build window grid (patched) ────────────────\n",
    "pos_centres = np.arange(FIRST_BIN_CENTER_BP,\n",
    "                        PLOT_HALF_WINDOW_BP + 1,\n",
    "                        BIN_STEP_BP)\n",
    "\n",
    "# If the first positive bin is zero, skip that same zero on the\n",
    "# negative side to avoid duplication.\n",
    "if FIRST_BIN_CENTER_BP == 0:\n",
    "    neg_centres = -pos_centres[1:][::-1]   # skip the 0\n",
    "else:\n",
    "    neg_centres = -pos_centres[::-1]\n",
    "\n",
    "centres = np.concatenate([neg_centres, pos_centres])\n",
    "edges   = [(c - BIN_WIDTH_BP/2, c + BIN_WIDTH_BP/2) for c in centres]\n",
    "labels  = [f\"{int(lo)} to {int(hi)}\" for lo, hi in edges]\n",
    "\n",
    "# ─────────────────── % Phased per window (unchanged) ──────────────\n",
    "def _window_metric(series_dict, cond):\n",
    "    \"\"\"\n",
    "    Metric per window for one condition (A or B), after\n",
    "    type-specific baseline subtraction.\n",
    "    \"\"\"\n",
    "    metric = [[] for _ in edges]\n",
    "\n",
    "    for key_tuple, vec in series_dict.items():\n",
    "        tlabel = key_tuple[0]                 # first element is type\n",
    "\n",
    "        for idx, (lo, hi) in enumerate(edges):\n",
    "            win_mask = (cent_m6a >= lo) & (cent_m6a < hi)\n",
    "            if not win_mask.any():\n",
    "                continue\n",
    "\n",
    "            if not USE_WINDOW_ENTROPY:\n",
    "                win_val = np.nanmax(vec[win_mask])\n",
    "            else:\n",
    "                sub_edges = np.arange(lo, hi, SUB_BIN_SIZE_BP)\n",
    "                sub_means = []\n",
    "                for s_lo in sub_edges:\n",
    "                    s_hi = min(s_lo + SUB_BIN_SIZE_BP, hi)\n",
    "                    m = win_mask & (cent_m6a >= s_lo) & (cent_m6a < s_hi)\n",
    "                    sub_means.append(\n",
    "                        _subbin_stat(vec, m, tlabel=tlabel, cond=cond)\n",
    "                    )\n",
    "                win_val = _order_score(sub_means)       # ← was  _shannon_entropy01(...)\n",
    "\n",
    "            metric[idx].append(win_val)\n",
    "\n",
    "    return metric\n",
    "\n",
    "\n",
    "\n",
    "phased_A = _window_metric(series_A, COND_A)\n",
    "phased_B = _window_metric(series_B, COND_B)\n",
    "\n",
    "\n",
    "# pretty-print string used in titles / y-labels\n",
    "BASE_TXT = (f\" – baseline {BASELINE_PCTL}th pct\"\n",
    "            if BASELINE_PCTL else \"\")\n",
    "# ─────────────────── motif-density histogram (unchanged) ──────────\n",
    "unique_pairs = set()\n",
    "for df in (dfA_all, dfB_all):\n",
    "    for t, starts in zip(df[\"type\"], df[\"motif_rel_start\"]):\n",
    "        for pos in starts:\n",
    "            unique_pairs.add((t, pos))\n",
    "\n",
    "hist_counts = [0] * len(edges)\n",
    "for _, pos in unique_pairs:\n",
    "    for i, (lo, hi) in enumerate(edges):\n",
    "        if lo <= pos < hi:\n",
    "            hist_counts[i] += 1\n",
    "            break\n",
    "\n",
    "# ─────────────────── debug first key (unchanged) ──────────────────\n",
    "first_key = keys_all[0]\n",
    "lo0, hi0  = edges[0]\n",
    "m0        = (cent_m6a >= lo0) & (cent_m6a < hi0)\n",
    "vals0_A   = series_A[first_key][m0]\n",
    "rng0      = np.nanmax(vals0_A) - np.nanmin(vals0_A)\n",
    "pct0      = 100*(rng0/100) if USE_FRAC_POS else 100*(rng0/2)\n",
    "print(f\"[DEBUG] {first_key}, window '{labels[0]}': range={rng0:.4f}, % Phased={pct0:.2f}\")\n",
    "\n",
    "# ─── helper: 3-bin running mean that ignores NaNs ─────────────────\n",
    "import numpy as np\n",
    "def _nan_smooth(arr, k=3):\n",
    "    \"\"\"Return array smoothed with a centred k-bin mean, NaN-aware.\"\"\"\n",
    "    a = np.asarray(arr, float)\n",
    "    out = np.full_like(a, np.nan)\n",
    "\n",
    "    # build a (len, k) sliding window view\n",
    "    idx = np.arange(len(a))\n",
    "    pads = k // 2\n",
    "    a_p = np.pad(a, (pads, pads), constant_values=np.nan)\n",
    "    for i in idx:\n",
    "        win = a_p[i : i + k]\n",
    "        if np.all(np.isnan(win)):\n",
    "            continue\n",
    "        out[i] = np.nanmean(win)\n",
    "    return out\n",
    "\n",
    "# ─────────────────── figure (unchanged) ───────────────────────────\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# decide how many rows\n",
    "nrows = 2 if PLOT_MOTIF_HIST else 1\n",
    "row_heights = [0.7, 0.3] if PLOT_MOTIF_HIST else [1.0]\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=nrows, cols=1, shared_xaxes=True,\n",
    "    row_heights=row_heights,\n",
    "    vertical_spacing=0.07\n",
    ")\n",
    "\n",
    "# row 1 – median + IQR lines (as per the previous update)\n",
    "for idx, (cond, clr, ranges) in enumerate(\n",
    "        zip([COND_A, COND_B], [CLR_A, CLR_B], [phased_A, phased_B])):\n",
    "\n",
    "    q1s, meds, q3s = [], [], []\n",
    "    for win_vals in ranges:\n",
    "        if win_vals:\n",
    "            q1, med, q3 = np.percentile(win_vals, [25, 50, 75])\n",
    "        else:\n",
    "            q1 = med = q3 = np.nan\n",
    "        q1s.append(q1); meds.append(med); q3s.append(q3)\n",
    "\n",
    "    # --- 3-bin smoothing ----------------------------------------\n",
    "    q1s  = _nan_smooth(q1s)\n",
    "    meds = _nan_smooth(meds)\n",
    "    q3s  = _nan_smooth(q3s)\n",
    "\n",
    "    x_pos = [c - DOT_OFFSET if idx == 0 else c + DOT_OFFSET for c in centres]\n",
    "\n",
    "    # median\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=x_pos, y=meds, mode=\"lines\",\n",
    "        name=f\"{cond} median\",\n",
    "        line=dict(color=clr, width=4)\n",
    "    ), row=1, col=1)\n",
    "\n",
    "    # Q1 & Q3\n",
    "    for bound in (q1s, q3s):\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=x_pos, y=bound, mode=\"lines\",\n",
    "            showlegend=False,\n",
    "            line=dict(color=clr, width=0.5, dash=\"dash\")\n",
    "        ), row=1, col=1)\n",
    "\n",
    "# row 2 – motif histogram (optional)\n",
    "if PLOT_MOTIF_HIST:\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=centres,\n",
    "        y=hist_counts,\n",
    "        width=BIN_WIDTH_BP,\n",
    "        marker_color=\"grey\",\n",
    "        name=\"unique motif starts / type\"\n",
    "    ), row=2, col=1)\n",
    "\n",
    "# ─── axes & layout ───\n",
    "# ───────── USER-CONFIG (add this just below the other config vars) ─────────\n",
    "X_TICK_BP = 100          # spacing (bp) between major x-ticks\n",
    "# ───────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "\n",
    "# ─── 1. Windowed %-phased / IQR figure ────────────────────────────\n",
    "for r in range(1, nrows + 1):\n",
    "    fig.update_xaxes(\n",
    "        row=r, col=1,\n",
    "        type=\"linear\",\n",
    "        tickmode=\"linear\",\n",
    "        dtick=X_TICK_BP,                 # ← use configured spacing\n",
    "        range=[-PLOT_HALF_WINDOW_BP, PLOT_HALF_WINDOW_BP],\n",
    "        showgrid=False, zeroline=False\n",
    "    )\n",
    "\n",
    "BASE_TXT  = \"\" if BASELINE_PCTL == 0 else f\" – baseline {BASELINE_PCTL}th pct\"\n",
    "ylab      = \"Order score (0–1)\" if USE_WINDOW_ENTROPY else \"Max Pearson r\"\n",
    "\n",
    "fig.update_yaxes(\n",
    "    title = ylab, row = 1, col = 1,\n",
    "    showgrid = False, zeroline = False,\n",
    "    range = [0, 1] if USE_WINDOW_ENTROPY else None\n",
    ")\n",
    "fig.update_layout(\n",
    "    template = \"plotly_white\",\n",
    "    width    = 1000, height = 650,\n",
    "    title    = (f\"Windowed {BIN_WIDTH_BP} bp analysis{BASE_TXT} \"\n",
    "                f\"(centre ±{FIRST_BIN_CENTER_BP} bp; step {BIN_STEP_BP} bp; \"\n",
    "                f\"{N_BOOT_GROUPS} bootstrap groups)\")\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "# %%  ───────────────────────────────────────────────────────────────\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────\n",
    "#  NEW 3-in-1 COVERAGE FIGURE\n",
    "#      • total overlap\n",
    "#      • overlap of reads with r > thresh\n",
    "#      • overlap of reads with NaN r\n",
    "# ──────────────────────────────────────────────────────────────────\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# ---------- helper: mean ± SEM -----------------------------------\n",
    "def _mean_sem(vecs):\n",
    "    M    = np.vstack(vecs)               # (n_groups, n_windows)\n",
    "    mean = np.nanmean(M, axis=0)\n",
    "    n    = np.sum(~np.isnan(M), axis=0)\n",
    "    sem  = np.nanstd(M, axis=0, ddof=1) / np.sqrt(n)\n",
    "    return mean, sem\n",
    "\n",
    "def _hex_to_rgba(hexclr, alpha=0.2):\n",
    "    h = hexclr.lstrip('#')\n",
    "    r, g, b = (int(h[i:i+2], 16) for i in (0, 2, 4))\n",
    "    return f\"rgba({r},{g},{b},{alpha})\"\n",
    "\n",
    "# ---------- per-(key,boot) coverage arrays -----------------------\n",
    "def _per_key_boot_cov3(df):\n",
    "    \"\"\"\n",
    "    Return three dicts keyed by (type, bed_start, __boot):\n",
    "        total_cov , thresh_cov , nan_cov\n",
    "    Each value is a length-len(edges) int array.\n",
    "    \"\"\"\n",
    "    n_win = len(edges)\n",
    "    out_tot, out_thr, out_nan = {}, {}, {}\n",
    "\n",
    "    for key_tuple, g in df.groupby(KEY_COLS + [\"__boot\"], sort=False):\n",
    "        tot  = np.zeros(n_win, int)\n",
    "        thr  = np.zeros(n_win, int)\n",
    "        nan  = np.zeros(n_win, int)\n",
    "\n",
    "        threshold = COND_THRESH[g.iloc[0].condition]\n",
    "\n",
    "        for row in g.itertuples(index=False):\n",
    "            rel = np.asarray(row.rel_pos, dtype=int)\n",
    "            mod = np.asarray(row.mod_qual_bin, dtype=int)\n",
    "\n",
    "            # pre-compute r once for this read\n",
    "            track = _m6a_track(rel, mod)\n",
    "            r_vec = _normxcorr1d(track, TEMPLATES[SELECT_PERIOD])\n",
    "\n",
    "            for w, (lo, hi) in enumerate(edges):\n",
    "                overlap = ((rel >= lo) & (rel < hi)).any()\n",
    "                if not overlap:\n",
    "                    continue\n",
    "\n",
    "                # --- raw coverage ---\n",
    "                tot[w] += 1\n",
    "\n",
    "                # bins of r that correspond to this genomic window\n",
    "                msk   = (cent_m6a >= lo) & (cent_m6a < hi)\n",
    "                r_win = r_vec[msk]\n",
    "\n",
    "                # --- classification within the overlapped reads ---\n",
    "                if np.all(np.isnan(r_win)):\n",
    "                    nan[w] += 1\n",
    "                elif (threshold is not None) and (np.nanmax(r_win) > threshold):\n",
    "                    thr[w] += 1\n",
    "                # else: falls into “overlap but r ≤ threshold”\n",
    "\n",
    "        out_tot[key_tuple]  = tot\n",
    "        out_thr[key_tuple]  = thr\n",
    "        out_nan[key_tuple]  = nan\n",
    "\n",
    "    return out_tot, out_thr, out_nan\n",
    "\n",
    "\n",
    "covA_tot, covA_thr, covA_nan = _per_key_boot_cov3(dfA_boot)\n",
    "covB_tot, covB_thr, covB_nan = _per_key_boot_cov3(dfB_boot)\n",
    "\n",
    "# ─── if we're plotting mean r (not % reads > thresh), use tot − nan ───\n",
    "if not USE_FRAC_POS:\n",
    "    covA_thr = { k: covA_tot[k] - covA_nan[k] for k in covA_tot }\n",
    "    covB_thr = { k: covB_tot[k] - covB_nan[k] for k in covB_tot }\n",
    "\n",
    "# ---------- aggregate to condition-level mean ± SEM --------------\n",
    "meanA_tot, semA_tot = _mean_sem(covA_tot.values())\n",
    "meanB_tot, semB_tot = _mean_sem(covB_tot.values())\n",
    "\n",
    "meanA_thr, semA_thr = _mean_sem(covA_thr.values())\n",
    "meanB_thr, semB_thr = _mean_sem(covB_thr.values())\n",
    "\n",
    "meanA_nan, semA_nan = _mean_sem(covA_nan.values())\n",
    "meanB_nan, semB_nan = _mean_sem(covB_nan.values())\n",
    "\n",
    "\n",
    "\n",
    "# ---------- build 3-row figure -----------------------------------\n",
    "fig_cov = make_subplots(\n",
    "    rows=3, cols=1, shared_xaxes=True,\n",
    "    row_heights=[0.4, 0.3, 0.3],\n",
    "    vertical_spacing=0.05\n",
    ")\n",
    "\n",
    "def _add(row, mean, sem, name, clr, dash=None):\n",
    "    # main line\n",
    "    fig_cov.add_trace(\n",
    "        go.Scatter(\n",
    "            x=centres, y=mean, mode=\"lines\",\n",
    "            name=name, line=dict(color=clr, dash=dash, width=2),\n",
    "            hovertemplate=\"window=%{x}<br>mean=%{y:.1f} ± %{customdata:.1f}\",\n",
    "            customdata=sem\n",
    "        ),\n",
    "        row=row, col=1\n",
    "    )\n",
    "    # ribbon\n",
    "    fig_cov.add_trace(\n",
    "        go.Scatter(\n",
    "            x=np.concatenate([centres, centres[::-1]]),\n",
    "            y=np.concatenate([mean - sem, (mean + sem)[::-1]]),\n",
    "            fill=\"toself\", fillcolor=_hex_to_rgba(clr, 0.2),\n",
    "            line=dict(color=\"rgba(0,0,0,0)\"), hoverinfo=\"skip\",\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=row, col=1\n",
    "    )\n",
    "\n",
    "# row 1 – raw coverage\n",
    "_add(1, meanA_tot,  semA_tot,  f\"{COND_A} total\",  CLR_A)\n",
    "_add(1, meanB_tot,  semB_tot,  f\"{COND_B} total\",  CLR_B, dash=\"dash\")\n",
    "\n",
    "# row 2 – r > threshold\n",
    "_add(2, meanA_thr,  semA_thr,  f\"{COND_A} r>thr\", CLR_A)\n",
    "_add(2, meanB_thr,  semB_thr,  f\"{COND_B} r>thr\", CLR_B, dash=\"dash\")\n",
    "\n",
    "# row 3 – NaN coverage\n",
    "_add(3, meanA_nan, semA_nan, f\"{COND_A} NaN\", CLR_A)\n",
    "_add(3, meanB_nan, semB_nan, f\"{COND_B} NaN\", CLR_B, dash=\"dash\")\n",
    "\n",
    "# ---------- axes & layout ----------------------------------------\n",
    "# ─── 2. 3-row coverage figure ────────────────────────────────────\n",
    "for r in (1, 2, 3):\n",
    "    fig_cov.update_xaxes(\n",
    "        row=r, col=1,\n",
    "        type=\"linear\",\n",
    "        tickmode=\"linear\",\n",
    "        dtick=X_TICK_BP,                 # ← use configured spacing\n",
    "        range=[-PLOT_HALF_WINDOW_BP, PLOT_HALF_WINDOW_BP],\n",
    "        showgrid=False, zeroline=False\n",
    "    )\n",
    "\n",
    "fig_cov.update_yaxes(title=\"raw coverage\",             row=1, col=1, rangemode=\"tozero\")\n",
    "fig_cov.update_yaxes(title=\"coverage (r > thr)\",       row=2, col=1, rangemode=\"tozero\")\n",
    "fig_cov.update_yaxes(title=\"coverage (r = NaN)\",       row=3, col=1, rangemode=\"tozero\")\n",
    "\n",
    "\n",
    "\n",
    "fig_cov.update_layout(\n",
    "    template=\"plotly_white\",\n",
    "    width=1000, height=850,\n",
    "    title=(f\"Coverage metrics per {BIN_WIDTH_BP} bp window \"\n",
    "           f\"({N_BOOT_GROUPS} bootstrap group\"\n",
    "           f\"{'s' if N_BOOT_GROUPS>1 else ''}; keys = {', '.join(KEY_COLS)})\"),\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig_cov.show()\n",
    "\n",
    "\n",
    "# PE TYPE PLOTS\n",
    "# %%────────────────────────────────────────────────────────────────────────────\n",
    "#  PER-TYPE WINDOWED %-PHASED LINES  +  MOTIF MARKERS\n",
    "#  (Figure 2 – leaves the combined plot & coverage plots untouched)\n",
    "# -----------------------------------------------------------------------------#\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# ─── gather the list of types present in either condition ────────────────────\n",
    "types_all = sorted(\n",
    "    set(dfA_all[\"type\"].unique()).union(dfB_all[\"type\"].unique())\n",
    ")\n",
    "\n",
    "# ─── helper: phased-metric arrays restricted to one type ──────────────────────\n",
    "def _window_metric_for_type(series_dict, type_label, cond):\n",
    "    \"\"\"Same as above, restricted to one type.\"\"\"\n",
    "    out = [[] for _ in edges]\n",
    "    for key, vec in series_dict.items():\n",
    "        if key[0] != type_label:\n",
    "            continue\n",
    "        for idx, (lo, hi) in enumerate(edges):\n",
    "            win_mask = (cent_m6a >= lo) & (cent_m6a < hi)\n",
    "            if not win_mask.any():\n",
    "                continue\n",
    "\n",
    "            if not USE_WINDOW_ENTROPY:\n",
    "                val = np.nanmax(vec[win_mask])\n",
    "            else:\n",
    "                sub_edges = np.arange(lo, hi, SUB_BIN_SIZE_BP)\n",
    "                sub_means = []\n",
    "                for s_lo in sub_edges:\n",
    "                    s_hi = min(s_lo + SUB_BIN_SIZE_BP, hi)\n",
    "                    m = win_mask & (cent_m6a >= s_lo) & (cent_m6a < s_hi)\n",
    "                    sub_means.append(\n",
    "                        _subbin_stat(vec, m, tlabel=type_label, cond=cond)\n",
    "                    )\n",
    "                val = _shannon_entropy01(sub_means)\n",
    "            out[idx].append(val)\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "# ─── motif positions per type (set for quick lookup) ──────────────────────────\n",
    "motifs_by_type = {}\n",
    "for t, pos in unique_pairs:         # `unique_pairs` built earlier\n",
    "    motifs_by_type.setdefault(t, set()).add(pos)\n",
    "\n",
    "# ─── build per-type figure ───────────────────────────────────────\n",
    "n_rows = len(types_all)\n",
    "fig_t  = make_subplots(\n",
    "    rows=n_rows, cols=1,\n",
    "    shared_xaxes=True,\n",
    "    specs=[[{\"secondary_y\": True}] for _ in range(n_rows)],  # ← NEW\n",
    "    vertical_spacing=0.04,\n",
    "    row_heights=[1 / n_rows] * n_rows\n",
    ")\n",
    "\n",
    "def _window_baseline_adj(series_dict, type_label, cond):\n",
    "    \"\"\"Return ⟨metric − baseline⟩ per window for one type & condition.\"\"\"\n",
    "    means = np.full(len(edges), np.nan)\n",
    "    baseline = (BASE_A if cond == COND_A else BASE_B)[type_label]\n",
    "\n",
    "    # pool all vecs belonging to this type\n",
    "    vecs = [v for k, v in series_dict.items() if k[0] == type_label]\n",
    "    if not vecs:\n",
    "        return means\n",
    "\n",
    "    for idx, (lo, hi) in enumerate(edges):\n",
    "        win_mask = (cent_m6a >= lo) & (cent_m6a < hi)\n",
    "        vals = np.concatenate([v[win_mask] for v in vecs])\n",
    "        vals = vals[~np.isnan(vals)]\n",
    "        if vals.size:\n",
    "            means[idx] = np.nanmean(vals) - baseline\n",
    "            if CLIP_BELOW_0:\n",
    "                means[idx] = max(means[idx], 0.0)\n",
    "    return means\n",
    "\n",
    "\n",
    "for row_idx, tlabel in enumerate(types_all, start=1):\n",
    "    # centred at each subplot’s vertical mid-point\n",
    "    y_pos = 1 - (row_idx - 0.5) / n_rows\n",
    "    # ---- compute median + IQR for each condition for this type --------------\n",
    "    phased_tA = _window_metric_for_type(series_A, tlabel, COND_A)\n",
    "    phased_tB = _window_metric_for_type(series_B, tlabel, COND_B)\n",
    "\n",
    "\n",
    "    for cond_idx, (cond, clr, ranges) in enumerate(\n",
    "            zip([COND_A, COND_B], [CLR_A, CLR_B], [phased_tA, phased_tB])):\n",
    "\n",
    "        q1s, meds, q3s = [], [], []\n",
    "        for win_vals in ranges:\n",
    "            if win_vals:\n",
    "                q1, med, q3 = np.percentile(win_vals, [25, 50, 75])\n",
    "            else:\n",
    "                q1 = med = q3 = np.nan\n",
    "            q1s.append(q1); meds.append(med); q3s.append(q3)\n",
    "\n",
    "        q1s  = _nan_smooth(q1s)\n",
    "        meds = _nan_smooth(meds)\n",
    "        q3s  = _nan_smooth(q3s)\n",
    "\n",
    "        x_pos = [c - DOT_OFFSET if cond_idx == 0 else c + DOT_OFFSET\n",
    "                 for c in centres]\n",
    "\n",
    "        # median line\n",
    "        fig_t.add_trace(go.Scatter(\n",
    "            x=x_pos, y=meds, mode=\"lines\",\n",
    "            name=f\"{tlabel} – {cond} median\",\n",
    "            legendgroup=f\"{cond}\", showlegend=(row_idx == 1),\n",
    "            line=dict(color=clr, width=4)\n",
    "        ), row=row_idx, col=1)\n",
    "\n",
    "        # IQR bounds\n",
    "        for bound in (q1s, q3s):\n",
    "            fig_t.add_trace(go.Scatter(\n",
    "                x=x_pos, y=bound, mode=\"lines\",\n",
    "                showlegend=False, legendgroup=f\"{cond}\",\n",
    "                line=dict(color=clr, width=0.5, dash=\"dash\")\n",
    "            ), row=row_idx, col=1)\n",
    "\n",
    "        # ➋ baseline-subtracted binned metric on secondary axis\n",
    "        pct_A = _window_baseline_adj(series_A, tlabel, COND_A)\n",
    "        pct_B = _window_baseline_adj(series_B, tlabel, COND_B)\n",
    "\n",
    "        fig_t.add_trace(go.Scatter(\n",
    "            x=centres, y=pct_A, mode=\"lines\",\n",
    "            name=f\"{tlabel} – {COND_A} adj\", legendgroup=f\"{COND_A}\",\n",
    "            line=dict(color=CLR_A, width=2, dash=\"dot\")\n",
    "        ), row=row_idx, col=1, secondary_y=True)\n",
    "\n",
    "        fig_t.add_trace(go.Scatter(\n",
    "            x=centres, y=pct_B, mode=\"lines\",\n",
    "            name=f\"{tlabel} – {COND_B} adj\", legendgroup=f\"{COND_B}\",\n",
    "            line=dict(color=CLR_B, width=2, dash=\"dot\")\n",
    "        ), row=row_idx, col=1, secondary_y=True)\n",
    "\n",
    "    # replace the previous annotation block inside the for-loop\n",
    "    y_pos = 1 - (row_idx - 0.5) / n_rows          # vertical centre of row\n",
    "    fig_t.add_annotation(\n",
    "        text        = f\"<b>{tlabel}</b>\",\n",
    "        xref=\"paper\", x = 0.01,                    # left margin\n",
    "        yref=\"paper\", y = y_pos,\n",
    "        showarrow   = False,\n",
    "        font        = dict(size = 12)\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    # ---- vertical dashed lines for motif starts -----------------------------\n",
    "    for mpos in motifs_by_type.get(tlabel, []):\n",
    "        fig_t.add_vline(\n",
    "            x=mpos, line_dash=\"dash\", line_color=\"grey\",\n",
    "            row=row_idx, col=1, opacity=0.5\n",
    "        )\n",
    "\n",
    "    # figure 1 (combined)\n",
    "    ylab = \"Normalised Shannon entropy (0–1)\" if USE_WINDOW_ENTROPY \\\n",
    "           else \"Max Pearson r (mean per rex site)\"\n",
    "    y_rng = [0, 1] if USE_WINDOW_ENTROPY else None\n",
    "\n",
    "    fig_t.update_layout(\n",
    "    title=(f\"Windowed {BIN_WIDTH_BP} bp analysis – per type{BASE_TXT} \"\n",
    "           f\"(centre ±{FIRST_BIN_CENTER_BP} bp; step {BIN_STEP_BP} bp)\"))\n",
    "\n",
    "order_label = \"Order score (0–1)\" if USE_WINDOW_ENTROPY else \"Max Pearson r\"\n",
    "for r in range(1, n_rows + 1):\n",
    "    fig_t.update_yaxes(\n",
    "        title_text = order_label if r == 1 else \"\",\n",
    "        row        = r, col = 1,\n",
    "        showgrid   = False, zeroline = False,\n",
    "        range      = [0, 1] if USE_WINDOW_ENTROPY else None\n",
    "    )\n",
    "    # secondary (baseline-adjusted) axis on the right\n",
    "    fig_t.update_yaxes(\n",
    "        title_text = \"Adj. metric\",\n",
    "        row        = r, col = 1, secondary_y = True,\n",
    "        showgrid   = False, zeroline = False\n",
    "    )\n",
    "\n",
    "\n",
    "# ─── shared x-axis settings ---------------------------------------------------\n",
    "fig_t.update_xaxes(\n",
    "    type=\"linear\", tickmode=\"linear\", dtick=X_TICK_BP,\n",
    "    range=[-PLOT_HALF_WINDOW_BP, PLOT_HALF_WINDOW_BP],\n",
    "    showgrid=False, zeroline=False\n",
    ")\n",
    "\n",
    "fig_t.update_layout(\n",
    "    template=\"plotly_white\",\n",
    "    width=1000, height=300 + 175 * n_rows,\n",
    "    title=(f\"Windowed {BIN_WIDTH_BP} bp analysis – per type \"\n",
    "           f\"(centre ±{FIRST_BIN_CENTER_BP} bp; step {BIN_STEP_BP} bp)\"),\n",
    "    legend_tracegroupgap=60\n",
    ")\n",
    "\n",
    "fig_t.show()\n",
    "# %%────────────────────────────────────────────────────────────────────────────\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd7ddef2841752f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%  ───────────────────────────────────────────────────────────────\n",
    "#  DEBUG: key with the most reads-considered  +  chr_type sanity     #\n",
    "# ────────────────────────────────────────────────────────────────────\n",
    "import numpy as np, itertools, collections\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────\n",
    "# 1)   find key with max mean reads-considered                       |\n",
    "# ────────────────────────────────────────────────────────────────────\n",
    "def _heaviest_key(count_dicts):\n",
    "    best_key, best_mean = None, -np.inf\n",
    "    for d in count_dicts:\n",
    "        for k, vec in d.items():\n",
    "            m = np.nanmean(vec)\n",
    "            if m > best_mean:\n",
    "                best_key, best_mean = k, m\n",
    "    return best_key, best_mean\n",
    "\n",
    "key_max, mean_max = _heaviest_key([counts_A_cons, counts_B_cons])\n",
    "type_key = \"|\".join(map(str, key_max[:-1]))\n",
    "boot_idx = key_max[-1]\n",
    "\n",
    "print(f\"[DEBUG] heaviest key → '{type_key}', boot={boot_idx}  \"\n",
    "      f\"(mean reads ≈ {mean_max:.1f})\")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────\n",
    "# 2)   helper → % r>thr  &  tot reads for SELECT_PERIOD              |\n",
    "# ────────────────────────────────────────────────────────────────────\n",
    "def _frac_tot(df_boot, cond):\n",
    "    mask = (df_boot[\"__boot\"] == boot_idx) & (df_boot[\"type\"] == key_max[0])\n",
    "    if len(key_max) == 3:                           # includes bed_start\n",
    "        mask &= df_boot[\"bed_start\"] == key_max[1]\n",
    "    df_sel = df_boot[mask]\n",
    "    if df_sel.empty:                                # not present in cond\n",
    "        return None, None\n",
    "    thr = COND_THRESH[cond]\n",
    "    frac, _, _, tot = _condition_heat(\n",
    "        df_sel, thresh_val=thr, return_tot=True\n",
    "    )\n",
    "    return frac[PERIOD_IDX], tot\n",
    "\n",
    "fracA, totA = _frac_tot(dfA_boot, COND_A)\n",
    "fracB, totB = _frac_tot(dfB_boot, COND_B)\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────\n",
    "# 3)   build two-row plot                                             |\n",
    "# ────────────────────────────────────────────────────────────────────\n",
    "fig_key = make_subplots(\n",
    "    rows=2, cols=1, shared_xaxes=True,\n",
    "    row_heights=[0.55, 0.45], vertical_spacing=0.07\n",
    ")\n",
    "\n",
    "if fracA is not None:\n",
    "    fig_key.add_trace(go.Scatter(\n",
    "        x=cent_m6a, y=fracA, mode=\"lines\",\n",
    "        name=f\"{COND_A} – % r>thr\", line=dict(color=CLR_A)\n",
    "    ), row=1, col=1)\n",
    "    fig_key.add_trace(go.Scatter(\n",
    "        x=cent_m6a, y=totA, mode=\"lines\",\n",
    "        name=f\"{COND_A} – tot\", line=dict(color=CLR_A, dash=\"dot\")\n",
    "    ), row=2, col=1)\n",
    "\n",
    "if fracB is not None:\n",
    "    fig_key.add_trace(go.Scatter(\n",
    "        x=cent_m6a, y=fracB, mode=\"lines\",\n",
    "        name=f\"{COND_B} – % r>thr\", line=dict(color=CLR_B)\n",
    "    ), row=1, col=1)\n",
    "    fig_key.add_trace(go.Scatter(\n",
    "        x=cent_m6a, y=totB, mode=\"lines\",\n",
    "        name=f\"{COND_B} – tot\", line=dict(color=CLR_B, dash=\"dot\")\n",
    "    ), row=2, col=1)\n",
    "\n",
    "fig_key.update_yaxes(title=\"% reads r>thr\",       row=1, col=1, range=[0, 100])\n",
    "fig_key.update_yaxes(title=\"# reads considered\", row=2, col=1, rangemode=\"tozero\")\n",
    "fig_key.update_xaxes(title=\"rel_pos (bp)\",\n",
    "                     range=[-PLOT_WINDOW, PLOT_WINDOW],\n",
    "                     row=2, col=1)\n",
    "\n",
    "fig_key.update_layout(\n",
    "    template=\"plotly_white\",\n",
    "    width=900, height=600,\n",
    "    title=(f\"Key with most reads: '{type_key}', boot={boot_idx}  \"\n",
    "           f\"(period {SELECT_PERIOD} bp; thresh={OCC_THRESH_METHOD})\")\n",
    ")\n",
    "\n",
    "fig_key.show()\n",
    "# %%  ───────────────────────────────────────────────────────────────\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74961396bf7bfcee",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ╔═══════════════════════════════════════════════════════════════╗\n",
    "# ║  SPLIT-AND-FLIP  +  POSITIVE-SIDE ANALYTICS (SELF-CONTAINED)  ║\n",
    "# ╚═══════════════════════════════════════════════════════════════╝\n",
    "import numpy as np, pandas as pd\n",
    "from copy import deepcopy\n",
    "from scipy.ndimage import uniform_filter1d\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# ───────────────────────── USER CONFIG ───────────────────────── #\n",
    "# – Split-read-specific options live here so you can tweak them without\n",
    "#   touching the first (full-read) cell.\n",
    "SPLIT_POSITIVE_ONLY       = True        # keep only rel_pos ≥ 0\n",
    "SPLIT_CONS_START_POS      = 115         # first seed for consensus (bp)\n",
    "SPLIT_NRL                 = 174         # nucleosome repeat length (bp)\n",
    "SPLIT_CONSENSUS_MODE      = \"per_condition_mean\"   # \"static\" | \"per_condition_mean\"\n",
    "SPLIT_CONS_ADAPT_WINDOW   = 75          # half-window for per-cond averaging\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "# (the rest of the global constants – BIN_WIDTH_OCC, PLOT_WINDOW, etc. –\n",
    "#  are reused from the first cell and assumed to be already defined)\n",
    "\n",
    "# ─── sanity check: keep an untouched copy of the original DataFrame ───\n",
    "orig_filtered = filtered_reads_df.copy(deep=True)\n",
    "dbg(f\"[SANITY]  start  filtered_reads_df.shape = {filtered_reads_df.shape}\")\n",
    "\n",
    "# ───────────────────── split + optional flip ───────────────────── #\n",
    "# (unchanged helper definitions omitted for brevity)\n",
    "# … _split_mask, _filter_motifs, _flip, _flip_np, _flip_coords, _relims …\n",
    "\n",
    "# --- split the reads ---------------------------------------------------------\n",
    "split_reads_df = split_and_flip(orig_filtered)\n",
    "dbg(f\"[SPLIT]  {len(orig_filtered)} → {len(split_reads_df)} rows\")\n",
    "\n",
    "# --- enforce positive-side only ----------------------------------------------\n",
    "if SPLIT_POSITIVE_ONLY:\n",
    "    split_reads_df = split_reads_df.loc[split_reads_df[\"rel_read_end\"] >= 0]\n",
    "    dbg(f\"[POS-SIDE] kept {len(split_reads_df)} rows with rel_read_end ≥ 0\")\n",
    "\n",
    "# ───────────────────── bin grids (positive side) ───────────────────── #\n",
    "BIN_MIN = 0 if SPLIT_POSITIVE_ONLY else -PLOT_WINDOW\n",
    "bins_occ = np.arange(BIN_MIN, PLOT_WINDOW + BIN_WIDTH_OCC, BIN_WIDTH_OCC)\n",
    "cent_occ = (bins_occ[:-1] + bins_occ[1:]) / 2\n",
    "\n",
    "bins_ctr = np.arange(BIN_MIN, PLOT_WINDOW + BIN_WIDTH_CTR, BIN_WIDTH_CTR)\n",
    "cent_ctr = (bins_ctr[:-1] + bins_ctr[1:]) / 2\n",
    "\n",
    "# ───────────────────── masking by condition / motifs ────────────────── #\n",
    "maskA = (split_reads_df[\"condition\"] == COND_A)\n",
    "maskB = (split_reads_df[\"condition\"] == COND_B)\n",
    "dfA_all = split_reads_df[maskA & split_reads_df.apply(motif_pass, axis=1)]\n",
    "dfB_all = split_reads_df[maskB & split_reads_df.apply(motif_pass, axis=1)]\n",
    "\n",
    "# optional sorting for scatter\n",
    "def _subset(df):\n",
    "    if READ_SORT_KEY and READ_SORT_KEY in df.columns:\n",
    "        df = df.sort_values(READ_SORT_KEY)\n",
    "    return df.head(MAX_READS).reset_index(drop=True)\n",
    "\n",
    "dfA, dfB = _subset(dfA_all), _subset(dfB_all)\n",
    "dbg(f\"[SPLIT-PLOT] {COND_A}: {len(dfA_all)} reads ({len(dfA)} plotted)\")\n",
    "dbg(f\"[SPLIT-PLOT] {COND_B}: {len(dfB_all)} reads ({len(dfB)} plotted)\")\n",
    "\n",
    "# ───────────────────── coverage + centre fractions ─────────────────── #\n",
    "fA_occ, fB_occ = (frac_core(dfA_all, bins_occ, cent_occ),\n",
    "                  frac_core(dfB_all, bins_occ, cent_occ))\n",
    "d_occ   = fB_occ - fA_occ\n",
    "col_occ = [BAR_POS if x > 0 else BAR_NEG for x in d_occ]\n",
    "\n",
    "rA_ctr, rB_ctr = (frac_cent(dfA_all, bins_ctr, cent_ctr),\n",
    "                  frac_cent(dfB_all, bins_ctr, cent_ctr))\n",
    "fA_ctr = uniform_filter1d(rA_ctr, SMOOTH_BINS_CTR, mode=\"nearest\")\n",
    "fB_ctr = uniform_filter1d(rB_ctr, SMOOTH_BINS_CTR, mode=\"nearest\")\n",
    "d_ctr  = fB_ctr - fA_ctr\n",
    "col_ctr = [BAR_POS if x > 0 else BAR_NEG for x in d_ctr]\n",
    "\n",
    "# ───────────────────── periodicity heat-maps ──────────────────────── #\n",
    "def periodicity_heat(series):\n",
    "    lag_bins = np.round(PERIODS_BP / BIN_WIDTH_CTR).astype(int)\n",
    "    win_bins = PERIOD_WINDOW_BP // BIN_WIDTH_CTR\n",
    "    H = np.full((len(PERIODS_BP), len(series)), np.nan)\n",
    "    for i in range(len(series)):\n",
    "        seg = series[max(0,i-win_bins):min(len(series),i+win_bins+1)]\n",
    "        if np.isnan(seg).all():     # skip all-NaN windows\n",
    "            continue\n",
    "        seg = np.nan_to_num(seg) - np.nanmean(seg)   # demean\n",
    "        for j,L in enumerate(lag_bins):\n",
    "            if L < len(seg):\n",
    "                H[j,i] = np.dot(seg[:-L], seg[L:])\n",
    "    return H\n",
    "\n",
    "hA_raw, hB_raw = periodicity_heat(fA_ctr), periodicity_heat(fB_ctr)\n",
    "row_scale = np.nanmax(np.abs(np.c_[hA_raw, hB_raw]), axis=1)\n",
    "row_scale[row_scale == 0] = 1      # avoid division by zero → keeps row at 0\n",
    "hA, hB = hA_raw / row_scale[:,None], hB_raw / row_scale[:,None]\n",
    "hD = hB - hA\n",
    "\n",
    "fig_hA = make_heatmap(hA, f\"Periodicity – {COND_A} (split)\")\n",
    "fig_hB = make_heatmap(hB, f\"Periodicity – {COND_B} (split)\")\n",
    "fig_hD = make_heatmap(hD,  \"Δ Periodicity (B–A) · split\")\n",
    "fig_hA.show(); fig_hB.show(); fig_hD.show()\n",
    "\n",
    "# ───────────────────── multi-panel scatter + curves ────────────────── #\n",
    "fig = make_subplots(\n",
    "    rows=6, cols=1, shared_xaxes=True, vertical_spacing=0.02,\n",
    "    row_heights=[0.22,0.07,0.07,0.07,0.07,0.22]\n",
    ")\n",
    "for tr in make_scatter(dfA, CLR_A): fig.add_trace(tr, row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=cent_occ, y=fA_occ, name=f\"{COND_A} core\",\n",
    "                         line=dict(color=CLR_A,width=2)), row=2,col=1)\n",
    "fig.add_trace(go.Scatter(x=cent_occ, y=fB_occ, name=f\"{COND_B} core\",\n",
    "                         line=dict(color=CLR_B,width=2)), row=2,col=1)\n",
    "fig.add_trace(go.Bar(x=cent_occ, y=d_occ, marker_color=col_occ,\n",
    "                     showlegend=False), row=3,col=1)\n",
    "fig.add_trace(go.Scatter(x=cent_ctr, y=fA_ctr, name=f\"{COND_A} centre\",\n",
    "                         line=dict(color=CLR_A,width=2)), row=4,col=1)\n",
    "fig.add_trace(go.Scatter(x=cent_ctr, y=fB_ctr, name=f\"{COND_B} centre\",\n",
    "                         line=dict(color=CLR_B,width=2)), row=4,col=1)\n",
    "fig.add_trace(go.Bar(x=cent_ctr, y=d_ctr, marker_color=col_ctr,\n",
    "                     showlegend=False), row=5,col=1)\n",
    "for tr in make_scatter(dfB, CLR_B): fig.add_trace(tr, row=6, col=1)\n",
    "\n",
    "fig.update_xaxes(range=[BIN_MIN, PLOT_WINDOW],\n",
    "                 title=\"Genomic rel_pos (bp)\", row=6,col=1)\n",
    "fig.update_yaxes(title=\"Read #\", row=1,col=1); fig.update_yaxes(title=\"Read #\", row=6,col=1)\n",
    "fig.update_yaxes(title=\"% reads\\ncore\",   tickformat=\".0%\", row=2,col=1)\n",
    "fig.update_yaxes(title=\"Δ core\",          tickformat=\".0%\", row=3,col=1)\n",
    "fig.update_yaxes(title=\"% reads\\ncentre\", tickformat=\".0%\", row=4,col=1)\n",
    "fig.update_yaxes(title=\"Δ centre\",        tickformat=\".0%\", row=5,col=1)\n",
    "\n",
    "fig.update_layout(template=\"plotly_white\", height=950, width=1000,\n",
    "    title=(f\"Split fragments – A:{COND_A} | B:{COND_B} \"\n",
    "           f\"(filter={FILTER_MODE}, thresh={THRESH_DIST}, motifN={MOTIF_FILTER_N})\"))\n",
    "fig.show()\n",
    "\n",
    "# ───────────────────── consensus offsets (positive side) ──────────── #\n",
    "# build static consensus seeds (positive only)\n",
    "consensus = np.arange(SPLIT_CONS_START_POS, PLOT_WINDOW+1, SPLIT_NRL)\n",
    "consensus = list(consensus)  # keep original order\n",
    "\n",
    "def calc_condition_consensus(df, static_pos, half_win=SPLIT_CONS_ADAPT_WINDOW):\n",
    "    out = {}\n",
    "    for pos in static_pos:\n",
    "        hits = [c for r in df.itertuples(index=False)\n",
    "                   for c in r.nuc_centers if abs(c-pos) <= half_win]\n",
    "        out[pos] = np.mean(hits) if hits else pos\n",
    "    return out\n",
    "\n",
    "if SPLIT_CONSENSUS_MODE == \"per_condition_mean\":\n",
    "    CONS_REF = {COND_A: calc_condition_consensus(dfA_all, consensus),\n",
    "                COND_B: calc_condition_consensus(dfB_all, consensus)}\n",
    "else:   # \"static\"\n",
    "    CONS_REF = {COND_A:{p:p for p in consensus},\n",
    "                COND_B:{p:p for p in consensus}}\n",
    "\n",
    "def collect_offsets(df, cond_key):\n",
    "    ref_map = CONS_REF[cond_key]\n",
    "    offs = {p: [] for p in consensus}\n",
    "    for r in df.itertuples(index=False):\n",
    "        for cen in r.nuc_centers:\n",
    "            idx = np.argmin(np.abs(np.asarray(consensus) - cen))\n",
    "            ref = ref_map[consensus[idx]]\n",
    "            diff = abs(cen - ref)\n",
    "            if diff <= CONS_WINDOW_BP:\n",
    "                offs[consensus[idx]].append(diff)\n",
    "    return offs\n",
    "\n",
    "offsets = {COND_A: collect_offsets(dfA_all, COND_A),\n",
    "           COND_B: collect_offsets(dfB_all, COND_B)}\n",
    "\n",
    "cat_lbls = [str(c) for c in consensus]\n",
    "\n",
    "# debug print of consensus positions for first 5 consensus positions\n",
    "dbg(f\"[SANITY] consensus positions:\")\n",
    "dbg(f\"[SANITY] {COND_A}: {[round(CONS_REF[COND_A][p],1) for p in consensus]}\")\n",
    "\n",
    "\n",
    "# ╔════════════════════════════════════════════════════════════════╗\n",
    "# ║  Consensus offsets – dot + IQR whisker  +  coverage bar row    ║\n",
    "# ╚════════════════════════════════════════════════════════════════╝\n",
    "# ────────────────────────── IQR MEDIAN DOTS + OFFSET + ANNOTATION ──────────────────────────\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# small fraction of the NRL for horizontal jitter\n",
    "DOT_OFFSET = NRL * 0.1\n",
    "\n",
    "fig_iqr = go.Figure()\n",
    "\n",
    "for idx, (cond, clr) in enumerate(zip([COND_A, COND_B], [CLR_A, CLR_B])):\n",
    "    # compute quartiles for each consensus position\n",
    "    meds, q1s, q3s = [], [], []\n",
    "    for cpos in consensus:\n",
    "        vals = offsets[cond][cpos]\n",
    "        if vals:\n",
    "            q1, med, q3 = np.percentile(vals, [25, 50, 75])\n",
    "        else:\n",
    "            q1 = med = q3 = np.nan\n",
    "        q1s.append(q1); meds.append(med); q3s.append(q3)\n",
    "\n",
    "    # error bars for whiskers\n",
    "    err_up   = np.asarray(q3s) - np.asarray(meds)\n",
    "    err_down = np.asarray(meds) - np.asarray(q1s)\n",
    "\n",
    "    # numeric x positions with slight offset left/right\n",
    "    x_pos = [c - DOT_OFFSET if idx == 0 else c + DOT_OFFSET for c in consensus]\n",
    "\n",
    "    fig_iqr.add_trace(go.Scatter(\n",
    "        x            = x_pos,\n",
    "        y            = meds,\n",
    "        mode         = \"markers\",\n",
    "        name         = cond,\n",
    "        marker       = dict(size=10, color=clr),\n",
    "        error_y      = dict(\n",
    "                          type=\"data\",\n",
    "                          symmetric=False,\n",
    "                          array=err_up,\n",
    "                          arrayminus=err_down,\n",
    "                          thickness=1,\n",
    "                          color=clr\n",
    "                       ),\n",
    "        hovertemplate = (\n",
    "            \"cons=%{customdata[2]}<br>\" +\n",
    "            \"Q1=%{customdata[0]:.1f}  median=%{y:.1f}  Q3=%{customdata[1]:.1f}\"\n",
    "        ),\n",
    "        customdata = np.column_stack([q1s, q3s, consensus])\n",
    "    ))\n",
    "\n",
    "# vertical line at x=0\n",
    "fig_iqr.add_shape(\n",
    "    type=\"line\",\n",
    "    x0=0, x1=0,\n",
    "    y0=0, y1=1,\n",
    "    xref=\"x\", yref=\"paper\",\n",
    "    line=dict(color=\"black\", dash=\"dash\")\n",
    ")\n",
    "fig_iqr.add_annotation(\n",
    "    x=0, y=1.05,\n",
    "    xref=\"x\", yref=\"paper\",\n",
    "    text=\"best consensus match motif\",\n",
    "    showarrow=False,\n",
    "    font=dict(color=\"black\")\n",
    ")\n",
    "\n",
    "# relabel x-axis as categorical\n",
    "fig_iqr.update_xaxes(\n",
    "    type=\"linear\",\n",
    "    tickmode=\"array\",\n",
    "    tickvals=consensus,\n",
    "    ticktext=[str(c) for c in consensus],\n",
    "    title=\"Consensus position (bp)\"\n",
    ")\n",
    "fig_iqr.update_layout(\n",
    "    template=\"plotly_white\",\n",
    "    width=1000,\n",
    "    height=500,\n",
    "    # drop gridlines\n",
    "    xaxis=dict(showgrid=False, zeroline=False),\n",
    "    yaxis=dict(showgrid=False, zeroline=False),\n",
    "    title=(\n",
    "        f\"Offsets to consensus (neg_start={neg_start}, \"\n",
    "        f\"pos_start={pos_start}, NRL={NRL}, window=±{CONS_WINDOW_BP} bp)\"\n",
    "    ),\n",
    "    yaxis_title=\"Centre – consensus (bp)\"\n",
    ")\n",
    "fig_iqr.show()\n",
    "\n",
    "\n",
    "# ───────────────────── EXPORTS: PNG, SVG, CSVs ───────────────────── #\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# ---------- config ----------\n",
    "OUT_DIR   = Path(\"/Data1/git/meyer-nanopore/scripts/analysis/images_20250604\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RUN_TAG   = \"split\"\n",
    "BASE_TAG  = f\"iqr_{COND_A}_vs_{COND_B}_{RUN_TAG}\"\n",
    "\n",
    "\n",
    "# ---------- tidy CSV: consensus-offsets (one row per offset) ----------\n",
    "rows_offsets = [\n",
    "    {\"condition\": cond, \"consensus_bp\": cpos, \"offset_bp\": off}\n",
    "    for cond in [COND_A, COND_B]\n",
    "    for cpos, offs in offsets[cond].items()\n",
    "    for off in offs\n",
    "]\n",
    "pd.DataFrame(rows_offsets).to_csv(\n",
    "    OUT_DIR / f\"offsets_{COND_A}_vs_{COND_B}_{RUN_TAG}.csv\", index=False\n",
    ")\n",
    "\n",
    "# ---------- tidy CSV: median + IQR ----------\n",
    "rows_quart = []\n",
    "for cond, clr in zip([COND_A, COND_B], [CLR_A, CLR_B]):\n",
    "    meds, q1s, q3s = [], [], []\n",
    "    for cpos in consensus:\n",
    "        vals = offsets[cond][cpos]\n",
    "        if vals:\n",
    "            q1, med, q3 = np.percentile(vals, [25, 50, 75])\n",
    "        else:\n",
    "            q1 = med = q3 = np.nan\n",
    "        rows_quart.append({\n",
    "            \"condition\":  cond,\n",
    "            \"consensus_bp\": cpos,\n",
    "            \"q1\":       q1,\n",
    "            \"median\":   med,\n",
    "            \"q3\":       q3\n",
    "        })\n",
    "pd.DataFrame(rows_quart).to_csv(\n",
    "    OUT_DIR / f\"iqr_stats_{COND_A}_vs_{COND_B}_{RUN_TAG}.csv\", index=False\n",
    ")\n",
    "\n",
    "print(f\"[EXPORT] saved PNG, SVG, offsets CSV, and IQR CSV → {OUT_DIR}\")\n",
    "# ─────────────────────────────────────────────────────────────────── #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a176995537453ed7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ───────────────── Motif‑accessibility (0‑4 empty motifs) ─────────────────\n",
    "import numpy as np, pandas as pd, plotly.graph_objects as go\n",
    "\n",
    "# ---------------- USER KNOBS ----------------\n",
    "CONDITIONS   = analysis_cond\n",
    "THRESH_DIST  = 70          # bp\n",
    "NEAREST_N    = 4\n",
    "BUFFER_BP    = 20\n",
    "SHOW_DEBUG   = True\n",
    "# -------------------------------------------\n",
    "\n",
    "def dbg(m):\n",
    "    if SHOW_DEBUG:\n",
    "        print(m)\n",
    "\n",
    "# ── helper: count empty motifs (return None if read fails span check) ──\n",
    "def empty_cnt(row):\n",
    "    motifs = list(row.motif_rel_start)\n",
    "    if not motifs or not row.nuc_centers:\n",
    "        return None\n",
    "    nearest = sorted(motifs, key=lambda x: abs(x))[:NEAREST_N]\n",
    "    span_ok = (row.rel_read_start <= min(nearest)-BUFFER_BP) and \\\n",
    "              (row.rel_read_end   >= max(nearest)+BUFFER_BP)\n",
    "    if not span_ok:\n",
    "        return None\n",
    "    cnt = sum(\n",
    "        np.min(np.abs(np.asarray(row.nuc_centers) - m)) > THRESH_DIST\n",
    "        for m in nearest\n",
    "    )\n",
    "    return cnt\n",
    "\n",
    "# ── collect fractions: cond × k (0…4) → list[fractions per motif type] ──\n",
    "data = {c:{k:[] for k in range(NEAREST_N+1)} for c in CONDITIONS}\n",
    "\n",
    "for cond in CONDITIONS:\n",
    "    df_c = filtered_reads_df[filtered_reads_df[\"condition\"] == cond]\n",
    "    for typ in df_c[\"type\"].unique():\n",
    "        df_t = df_c[df_c[\"type\"] == typ]\n",
    "        eligible = 0\n",
    "        counts   = np.zeros(NEAREST_N+1, int)\n",
    "        for row in df_t.itertuples(index=False):\n",
    "            k = empty_cnt(row)\n",
    "            if k is None:\n",
    "                continue\n",
    "            k = min(k, NEAREST_N)\n",
    "            counts[k] += 1\n",
    "            eligible  += 1\n",
    "        if eligible == 0:\n",
    "            continue\n",
    "        for k in range(NEAREST_N+1):\n",
    "            if counts[k]:\n",
    "                data[cond][k].append(counts[k]/eligible)\n",
    "        dbg(f\"{cond} | {typ}: eligible={eligible}  \" +\n",
    "            \", \".join(f\"k{k}={counts[k]}\" for k in range(NEAREST_N+1)))\n",
    "\n",
    "# ── make grouped boxplots ──\n",
    "palette = [\"#1b9e77\",\"#d95f02\",\"#7570b3\",\"#e7298a\",\"#66a61e\"]  # 0→4\n",
    "fig = go.Figure()\n",
    "\n",
    "for k in range(NEAREST_N, -1, -1):       # plot tall boxes last→first\n",
    "    for cond in CONDITIONS:\n",
    "        vals = data[cond][k]\n",
    "        if not vals:\n",
    "            continue\n",
    "        fig.add_trace(go.Box(\n",
    "            x=[cond]*len(vals),\n",
    "            y=vals,                       # fractions (0–1)\n",
    "            name=f\"{k} motifs accessible\",\n",
    "            legendgroup=f\"k{k}\",\n",
    "            marker_color=palette[k],\n",
    "            boxpoints=\"all\", jitter=0.3, pointpos=0,\n",
    "            hovertemplate=\"%{y:.1%}\"\n",
    "        ))\n",
    "\n",
    "fig.update_layout(\n",
    "    template=\"plotly_white\",\n",
    "    boxmode=\"group\",\n",
    "    title=(f\"% reads with 0–{NEAREST_N} motifs accessible \"\n",
    "           f\"(nuc centre > {THRESH_DIST} bp; reads span motifs±{BUFFER_BP} bp)\"),\n",
    "    xaxis_title=\"Condition\",\n",
    "    yaxis_title=\"% reads with k motifs accessible\",\n",
    "    height=550, width=850,\n",
    "    showlegend=True\n",
    ")\n",
    "fig.update_yaxes(tickformat=\".0%\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4e4cf56d4dc976",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import gamma\n",
    "from scipy.ndimage import uniform_filter1d\n",
    "from scipy.signal import find_peaks\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# ── PARAMETERS ───────────────────────────────────────────────────────────────\n",
    "DEBUG                  = True\n",
    "bin_size               = CENTER_HIST_BIN\n",
    "bins                   = np.arange(-REL_POS_RANGE, REL_POS_RANGE + bin_size, bin_size)\n",
    "bin_centres            = (bins[:-1] + bins[1:]) / 2\n",
    "SMOOTH_BINS            = 5\n",
    "NEAREST_CENTER_THRESH  = 70\n",
    "EB_ALPHA0              = 1.0\n",
    "EB_BETA0               = 1.0\n",
    "MIN_PEAK_DIST_BP       = 120   # minimum distance between consensus peaks (bp)\n",
    "MAX_PEAK_GAP_BP        = 200   # maximum allowed gap before filling (bp)\n",
    "\n",
    "conds                  = sorted(filtered_reads_df[\"condition\"].unique())\n",
    "# pick which conditions to use for consensus:\n",
    "# e.g. conds               → all conditions\n",
    "#      [analysis_cond[0]]  → just the first\n",
    "CONSENSUS_CONDS        = conds\n",
    "\n",
    "# ── DEBUG UTILITY ─────────────────────────────────────────────────────────────\n",
    "def dbg(msg):\n",
    "    if DEBUG:\n",
    "        print(f\"[DEBUG] {msg}\")\n",
    "\n",
    "# ── 1) COMPUTE POSTERIORS ─────────────────────────────────────────────────────\n",
    "def compute_posteriors(df):\n",
    "    dbg(f\"compute_posteriors: {len(df)} reads\")\n",
    "    counts = {c: np.zeros(len(bin_centres), dtype=int) for c in conds}\n",
    "    denom  = {c: np.zeros(len(bin_centres), dtype=int) for c in conds}\n",
    "\n",
    "    for c in conds:\n",
    "        grp = df[df[\"condition\"]==c]\n",
    "        dbg(f\"  '{c}': {len(grp)} reads\")\n",
    "        for row in grp.itertuples():\n",
    "            rel = np.asarray(row.rel_pos)\n",
    "            if rel.size==0: continue\n",
    "            covered = (bins[:-1]>=rel.min()) & (bins[1:]<=rel.max())\n",
    "            denom[c][covered] += 1\n",
    "            centers = np.asarray(row.nuc_centers)\n",
    "            if centers.size==0: continue\n",
    "            idxs = np.floor((centers - bins[0])/bin_size).astype(int)\n",
    "            idxs = idxs[(idxs>=0)&(idxs<len(bin_centres))]\n",
    "            for i in np.unique(idxs):\n",
    "                counts[c][i] += 1\n",
    "\n",
    "    post_mean, post_lower, post_upper = {}, {}, {}\n",
    "    for c in conds:\n",
    "        k = counts[c]; n = denom[c]\n",
    "        dbg(f\"  posterior '{c}': k.sum()={k.sum()}, n.sum()={n.sum()}\")\n",
    "        a_post = EB_ALPHA0 + k\n",
    "        b_post = EB_BETA0 + n\n",
    "        m = a_post / b_post\n",
    "        l = gamma.ppf(0.025, a=a_post, scale=1/b_post)\n",
    "        u = gamma.ppf(0.975, a=a_post, scale=1/b_post)\n",
    "        post_mean [c] = uniform_filter1d(m, size=SMOOTH_BINS, mode=\"nearest\")\n",
    "        post_lower[c] = uniform_filter1d(l, size=SMOOTH_BINS, mode=\"nearest\")\n",
    "        post_upper[c] = uniform_filter1d(u, size=SMOOTH_BINS, mode=\"nearest\")\n",
    "    return post_mean, post_lower, post_upper\n",
    "\n",
    "pm_all, pl_all, pu_all = compute_posteriors(filtered_reads_df)\n",
    "\n",
    "# ── 2) FIND CONSENSUS FROM PEAKS IN POSTERIOR MEAN ────────────────────────────\n",
    "# aggregate the posterior means for the chosen conditions\n",
    "agg = np.zeros_like(bin_centres)\n",
    "for c in CONSENSUS_CONDS:\n",
    "    agg += pm_all[c]\n",
    "agg /= len(CONSENSUS_CONDS)\n",
    "dbg(f\"Aggregated posterior mean over {CONSENSUS_CONDS}\")\n",
    "\n",
    "# convert bp thresholds into bin‐units\n",
    "min_dist_bins = int(np.ceil(MIN_PEAK_DIST_BP / bin_size))\n",
    "max_gap_bp    = MAX_PEAK_GAP_BP\n",
    "\n",
    "# find initial peaks\n",
    "peak_idxs    = find_peaks(agg, distance=min_dist_bins)[0]\n",
    "consensus    = bin_centres[peak_idxs]\n",
    "dbg(f\"Initial consensus peaks: {consensus}\")\n",
    "\n",
    "# fill gaps > MAX_PEAK_GAP_BP\n",
    "while True:\n",
    "    diffs = np.diff(consensus)\n",
    "    gaps  = np.where(diffs > max_gap_bp)[0]\n",
    "    if gaps.size==0:\n",
    "        break\n",
    "    for i in reversed(gaps):\n",
    "        lo, hi = consensus[i], consensus[i+1]\n",
    "        mask    = (bin_centres>lo)&(bin_centres<hi)\n",
    "        if mask.any():\n",
    "            sub_vals = agg[mask]\n",
    "            sub_pos  = bin_centres[mask]\n",
    "            newp     = sub_pos[np.argmax(sub_vals)]\n",
    "            dbg(f\"  Filling gap ({lo},{hi}) with peak at {newp}\")\n",
    "        else:\n",
    "            newp = (lo+hi)/2\n",
    "            dbg(f\"  Filling gap ({lo},{hi}) with midpoint {newp}\")\n",
    "        consensus = np.insert(consensus, i+1, newp)\n",
    "dbg(f\"Final consensus centers: {consensus}\")\n",
    "\n",
    "# ── 3) FORM SUBSETS FOR BOXPLOTS ────────────────────────────────────────────────\n",
    "def has_valid(c): return np.asarray(c).size>0\n",
    "valid_df = filtered_reads_df[filtered_reads_df[\"nuc_centers\"].apply(has_valid)]\n",
    "\n",
    "nearest = np.array([np.min(np.abs(c)) for c in valid_df[\"nuc_centers\"]])\n",
    "subsets = [\n",
    "    (\"All reads\",      valid_df),\n",
    "    (f\"Nearest < {NEAREST_CENTER_THRESH}\", valid_df[nearest<NEAREST_CENTER_THRESH]),\n",
    "    (f\"Nearest ≥ {NEAREST_CENTER_THRESH}\", valid_df[nearest>=NEAREST_CENTER_THRESH]),\n",
    "]\n",
    "conds = sorted(valid_df[\"condition\"].unique())\n",
    "\n",
    "# ── 4) PLOT TWO BOXPLOTS PER SUBSET ────────────────────────────────────────────\n",
    "for title, df in subsets:\n",
    "    dbg(f\"\\nSubset '{title}': {len(df)} reads\")\n",
    "\n",
    "    # a) Inter-nuc distances vs consensus\n",
    "    data_mid = {c:[] for c in conds}\n",
    "    data_gap = {c:[] for c in conds}\n",
    "    for c in conds:\n",
    "        grp = df[df[\"condition\"]==c]\n",
    "        for centers in grp[\"nuc_centers\"]:\n",
    "            arr = np.sort(np.asarray(centers))\n",
    "            if arr.size<2: continue\n",
    "            diffs = np.diff(arr)\n",
    "            mids  = (arr[:-1]+arr[1:])/2\n",
    "            idxs  = np.argmin(np.abs(mids[:,None]-consensus[None,:]), axis=1)\n",
    "            for i,d in zip(idxs,diffs):\n",
    "                data_mid[c].append(consensus[i])\n",
    "                data_gap[c].append(d)\n",
    "        dbg(f\"  {c}: {len(data_gap[c])} gaps\")\n",
    "\n",
    "    fig1 = go.Figure()\n",
    "    for i,c in enumerate(conds):\n",
    "        fig1.add_trace(go.Box(\n",
    "            x=data_mid[c], y=data_gap[c], name=c,\n",
    "            marker_color=DEFAULT_READ_CLRS[i%len(DEFAULT_READ_CLRS)],\n",
    "            boxpoints=\"outliers\"\n",
    "        ))\n",
    "    fig1.update_layout(\n",
    "        template=\"plotly_white\",\n",
    "        title=f\"{title}: Inter-nuc distances (consensus from {CONSENSUS_CONDS})\",\n",
    "        xaxis_title=\"Consensus center (bp)\",\n",
    "        yaxis_title=\"Gap size (bp)\"\n",
    "    )\n",
    "    fig1.show()\n",
    "\n",
    "    # b) Position-precision (|offset| vs consensus)\n",
    "    data_off = {c: [] for c in conds}\n",
    "    for c in conds:\n",
    "        grp = df[df[\"condition\"] == c]\n",
    "        for centers in grp[\"nuc_centers\"]:\n",
    "            arr = np.asarray(centers)\n",
    "            if arr.size == 0:\n",
    "                continue\n",
    "            idxs = np.argmin(np.abs(arr[:, None] - consensus[None, :]), axis=1)\n",
    "            for i, val in zip(idxs, arr):\n",
    "                offset = abs(val - consensus[i])\n",
    "                data_off[c].append((consensus[i], offset))\n",
    "        dbg(f\"  {c}: {len(data_off[c])} absolute offsets\")\n",
    "\n",
    "    fig2 = go.Figure()\n",
    "    for i, c in enumerate(conds):\n",
    "        # unpack x and y from the stored tuples\n",
    "        xvals = [center for center, _ in data_off[c]]\n",
    "        yvals = [offset for _, offset in data_off[c]]\n",
    "        fig2.add_trace(go.Box(\n",
    "            x=xvals,\n",
    "            y=yvals,\n",
    "            name=c,\n",
    "            marker_color=DEFAULT_READ_CLRS[i % len(DEFAULT_READ_CLRS)],\n",
    "            boxpoints=\"outliers\"\n",
    "        ))\n",
    "    fig2.update_layout(\n",
    "        template=\"plotly_white\",\n",
    "        title=f\"{title}: Precision vs. consensus (|spread|)\",\n",
    "        xaxis_title=\"Consensus center (bp)\",\n",
    "        yaxis_title=\"|Offset from consensus| (bp)\"\n",
    "    )\n",
    "    fig2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0822253897ddadf",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "#  Pairwise Δ‑distance box‑plots + separate Median‑IQR line plots\n",
    "#  • Conditions are colored:\n",
    "#      analysis_cond[0] → blue (#1F78B4)\n",
    "#      analysis_cond[1] → red  (#E31A1C)\n",
    "#      analysis_cond[2] → green (#33A02C)\n",
    "#  • Box‑plots remain unchanged visually (no fill, transparent bg)\n",
    "#  • Median‑IQR figure is a separate plot below each box‑plot\n",
    "###############################################################################\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from tqdm.auto import tqdm  # progress bar (optional)\n",
    "\n",
    "# ─────────────────── CONFIG ─────────────────── #\n",
    "MAX_FLANK      = 3      # nucleosomes on each side\n",
    "DEBUG_PROGRESS = True   # show tqdm bar\n",
    "FIG_TEMPLATE   = \"plotly_white\"\n",
    "Y_MIN          = 100    # fixed lower bound for y‑axis\n",
    "\n",
    "# ── Color mapping for conditions ────────────────────────────────────────────\n",
    "# Assumes `analysis_cond` is a list like: [ \"condA\", \"condB\", \"condC\", ... ]\n",
    "# and that you want exactly 3 conditions colored as defined below.\n",
    "COLOR_HEX = {\n",
    "    analysis_cond[0]: \"#1F78B4\",  # blue\n",
    "    analysis_cond[1]: \"#E31A1C\",  # red\n",
    "    analysis_cond[2]: \"#33A02C\"   # green\n",
    "}\n",
    "# ────────────────────────────────────────────────────────────────────────────── #\n",
    "\n",
    "# ---------- 1) helpers ------------------------------------------------------- #\n",
    "def _adjacent_distances(centers, max_flank):\n",
    "    centers = sorted(centers)\n",
    "    negs = [c for c in centers if c < 0]\n",
    "    poss = [c for c in centers if c > 0]\n",
    "    # require ≥max_flank on one side AND ≥1 on the other\n",
    "    if not ((len(negs) >= max_flank and len(poss) >= 1) or\n",
    "            (len(poss) >= max_flank and len(negs) >= 1)):\n",
    "        return None\n",
    "    n1, p1 = negs[-1], poss[0]\n",
    "    d = {\"n1\": n1, \"p1\": p1}\n",
    "    # negative side adjacent pairs\n",
    "    for k in range(max_flank, 1, -1):\n",
    "        if len(negs) >= k:\n",
    "            d[f\"n-{k} to n-{k-1}\"] = abs(negs[-k] - negs[-k + 1])\n",
    "    # dyad gap\n",
    "    d[\"n-1 to n+1\"] = abs(n1 - p1)\n",
    "    # positive side adjacent pairs\n",
    "    for i in range(1, max_flank):\n",
    "        if len(poss) >= i + 1:\n",
    "            d[f\"n+{i} to n+{i+1}\"] = abs(poss[i] - poss[i - 1])\n",
    "    return d\n",
    "\n",
    "def build_per_read_df(reads_df, max_flank):\n",
    "    records = []\n",
    "    for row in tqdm(\n",
    "        reads_df.itertuples(index=False),\n",
    "        total=len(reads_df),\n",
    "        disable=not DEBUG_PROGRESS,\n",
    "        desc=\"scanning reads\"\n",
    "    ):\n",
    "        d = _adjacent_distances(row.nuc_centers, max_flank)\n",
    "        if d is not None:\n",
    "            records.append({\n",
    "                \"read_id\":   row.read_id,\n",
    "                \"condition\": row.condition,\n",
    "                **d\n",
    "            })\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "def make_category_order(max_flank):\n",
    "    neg = [f\"n-{k} to n-{k-1}\" for k in range(max_flank, 1, -1)]\n",
    "    pos = [f\"n+{i} to n+{i+1}\" for i in range(1, max_flank)]\n",
    "    return neg + [\"n-1 to n+1\"] + pos\n",
    "\n",
    "def median_iqr_stats(df, cond, categories):\n",
    "    rows = []\n",
    "    for cat in categories:\n",
    "        if cat not in df.columns:\n",
    "            continue\n",
    "        vals = df[cat].dropna()\n",
    "        if vals.empty:\n",
    "            continue\n",
    "        rows.append({\n",
    "            \"condition\": cond,\n",
    "            \"category\":  cat,\n",
    "            \"q1\":        vals.quantile(0.25),\n",
    "            \"median\":    vals.quantile(0.5),\n",
    "            \"q3\":        vals.quantile(0.75)\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def hex_to_rgb(hex_str):\n",
    "    \"\"\"Convert '#RRGGBB' → (r, g, b) tuple of ints.\"\"\"\n",
    "    h = hex_str.lstrip(\"#\")\n",
    "    return tuple(int(h[i:i+2], 16) for i in (0, 2, 4))\n",
    "\n",
    "def add_iqr_band(fig, x, q1, q3, rgb):\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=x + x[::-1],\n",
    "        y=q3 + q1[::-1],\n",
    "        fill=\"toself\",\n",
    "        fillcolor=f\"rgba{rgb + (0.15,)}\",\n",
    "        line=dict(color=\"rgba(0,0,0,0)\"),\n",
    "        showlegend=False,\n",
    "        hoverinfo=\"skip\"\n",
    "    ))\n",
    "\n",
    "def add_median_line(fig, x, med, rgb, name):\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=x,\n",
    "        y=med,\n",
    "        mode=\"lines+markers\",\n",
    "        line=dict(color=f\"rgb{rgb}\"),\n",
    "        name=name\n",
    "    ))\n",
    "\n",
    "# ---------- 2) build per‑read DataFrame ------------------------------------- #\n",
    "df_reads = build_per_read_df(filtered_reads_df, MAX_FLANK)\n",
    "if df_reads.empty:\n",
    "    raise ValueError(f\"No reads span ±{MAX_FLANK} nucleosomes!\")\n",
    "\n",
    "cat_order = make_category_order(MAX_FLANK)\n",
    "\n",
    "# compute global Q3 for y‑axis upper bound\n",
    "all_vals = df_reads[[c for c in cat_order if c in df_reads.columns]]\n",
    "global_q3 = all_vals.quantile(0.75).max()\n",
    "\n",
    "# ---------- 3) pairwise plotting ------------------------------------------------ #\n",
    "all_conditions = sorted(df_reads[\"condition\"].unique())\n",
    "pairs = list(itertools.combinations(all_conditions, 2))\n",
    "\n",
    "for c1, c2 in pairs:\n",
    "    sub = df_reads[df_reads[\"condition\"].isin([c1, c2])]\n",
    "\n",
    "    # -- 3a) Box‑plot figure (no changes here, just add color map) -------- #\n",
    "    long_df = sub.melt(\n",
    "        id_vars=[\"read_id\", \"condition\"],\n",
    "        value_vars=[c for c in cat_order if c in sub.columns],\n",
    "        var_name=\"category\",\n",
    "        value_name=\"distance\"\n",
    "    )\n",
    "    fig_box = px.box(\n",
    "        long_df,\n",
    "        x=\"category\",\n",
    "        y=\"distance\",\n",
    "        color=\"condition\",\n",
    "        category_orders={\"category\": cat_order},\n",
    "        color_discrete_map=COLOR_HEX,  # enforce specific colors\n",
    "        template=FIG_TEMPLATE,\n",
    "        points=False,\n",
    "        title=f\"Box‑plots: {c1} vs {c2}\"\n",
    "    )\n",
    "    # remove box fill and set transparent background\n",
    "    fig_box.update_traces(fillcolor=\"rgba(0,0,0,0)\")\n",
    "    fig_box.update_layout(\n",
    "        plot_bgcolor=\"rgba(0,0,0,0)\",\n",
    "        paper_bgcolor=\"rgba(0,0,0,0)\",\n",
    "        xaxis_title=\"Adjacent‑pair category\",\n",
    "        yaxis_title=\"Distance (bp)\",\n",
    "        width=900\n",
    "    )\n",
    "    #fig_box.update_yaxes(range=[Y_MIN, global_q3])\n",
    "    fig_box.show()\n",
    "\n",
    "    # -- 3b) Median‑line + IQR band figure (separate) ------------------ #\n",
    "    fig_med = go.Figure()\n",
    "    for cond in [c1, c2]:\n",
    "        dfc = sub[sub[\"condition\"] == cond]\n",
    "        stats = median_iqr_stats(dfc, cond, cat_order)\n",
    "        if stats.empty:\n",
    "            continue\n",
    "        x   = stats[\"category\"].tolist()\n",
    "        med = stats[\"median\"].tolist()\n",
    "        q1  = stats[\"q1\"].tolist()\n",
    "        q3  = stats[\"q3\"].tolist()\n",
    "        rgb = hex_to_rgb(COLOR_HEX[cond])\n",
    "\n",
    "        add_iqr_band(fig_med, x, q1, q3, rgb)\n",
    "        add_median_line(fig_med, x, med, rgb, f\"{cond} median\")\n",
    "\n",
    "    fig_med.update_layout(\n",
    "        title=f\"Median ± IQR: {c1} vs {c2}\",\n",
    "        xaxis_title=\"Adjacent‑pair category\",\n",
    "        yaxis_title=\"Distance (bp)\",\n",
    "        template=FIG_TEMPLATE,\n",
    "        plot_bgcolor=\"rgba(0,0,0,0)\",\n",
    "        paper_bgcolor=\"rgba(0,0,0,0)\",\n",
    "        width=900\n",
    "    )\n",
    "    #fig_med.update_yaxes(range=[Y_MIN, global_q4])\n",
    "    fig_med.update_xaxes(categoryorder=\"array\", categoryarray=cat_order)\n",
    "    fig_med.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1d834a5d158a0a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "#  INTER‑NUCLEOSOME‑DISTANCE HISTOGRAMS  +  COMBINED KDE\n",
    "#    • One histogram (% of all inter‑nucleosome gaps) per condition\n",
    "#    • Combined KDE overlay (all conditions) on a separate figure\n",
    "#\n",
    "#  Configurable:\n",
    "#      DIST_LOWER_BOUND   – exclude gaps < this (bp)          (default 147)\n",
    "#      DIST_UPPER_BOUND   – exclude gaps > this (bp)          (default 400)\n",
    "#      DIST_BIN_WIDTH     – histogram bin width (bp)          (default 1)\n",
    "#      KDE_BW             – gaussian_kde bandwidth            (default 0.005)\n",
    "#      REL_POS_WINDOWS    – list[(start,end)] to *keep*       (default whole range)\n",
    "###############################################################################\n",
    "\n",
    "# ─────────────────── 8A)  CONFIG ─────────────────── #\n",
    "DIST_LOWER_BOUND   = 100          # bp\n",
    "DIST_UPPER_BOUND   = 400          # bp\n",
    "DIST_BIN_WIDTH     = 1            # bp\n",
    "KDE_BW             = 0.01        # gaussian_kde bw_method\n",
    "\n",
    "# Windows in rel_pos coordinates that *keep* nucleosome centres.\n",
    "# Example: [(-1000, -200), (200, 1000)]\n",
    "# Leave empty → keep all centres in [-REL_POS_RANGE, +REL_POS_RANGE].\n",
    "REL_POS_WINDOWS    = [(-1000, -200), (200, 1000)]\n",
    "\n",
    "# ─────────────────── 8B)  HELPERS ─────────────────── #\n",
    "def _centres_in_windows(centres):\n",
    "    \"\"\"Filter centres so they fall *inside* any window in REL_POS_WINDOWS.\"\"\"\n",
    "    if not REL_POS_WINDOWS:                    # keep all\n",
    "        return centres\n",
    "    keep = []\n",
    "    for c in centres:\n",
    "        for lo, hi in REL_POS_WINDOWS:\n",
    "            if lo <= c <= hi:\n",
    "                keep.append(c)\n",
    "                break\n",
    "    return sorted(keep)\n",
    "\n",
    "def _adjacent_diffs(sorted_centres):\n",
    "    \"\"\"Return inter‑nuc distances between *adjacent* centres.\"\"\"\n",
    "    if len(sorted_centres) < 2:\n",
    "        return []\n",
    "    arr = np.diff(sorted_centres)\n",
    "    # keep distances in [DIST_LOWER_BOUND, DIST_UPPER_BOUND]\n",
    "    mask = (arr >= DIST_LOWER_BOUND) & (arr <= DIST_UPPER_BOUND)\n",
    "    return arr[mask].tolist()\n",
    "\n",
    "def _collect_inter_nuc_dists():\n",
    "    \"\"\"\n",
    "    Build dict[cond] → list[distances] with all qualifying gaps.\n",
    "    Distances are measured *within the same read* and *within the same window*.\n",
    "    \"\"\"\n",
    "    dbg(\"collecting inter‑nucleosome distances …\")\n",
    "    by_cond = {c: [] for c in filtered_reads_df[\"condition\"].unique()}\n",
    "\n",
    "    for row in filtered_reads_df.itertuples():\n",
    "        cond = row.condition\n",
    "        centres = _centres_in_windows(row.nuc_centers)\n",
    "        # Split centres by window so gaps across windows are ignored\n",
    "        if REL_POS_WINDOWS:\n",
    "            for lo, hi in REL_POS_WINDOWS:\n",
    "                win_centres = [c for c in centres if lo <= c <= hi]\n",
    "                by_cond[cond].extend(_adjacent_diffs(win_centres))\n",
    "        else:\n",
    "            by_cond[cond].extend(_adjacent_diffs(centres))\n",
    "\n",
    "    for cond, lst in by_cond.items():\n",
    "        dbg(f\"[{cond}] kept {len(lst)} gaps\", always=False)\n",
    "    return by_cond\n",
    "\n",
    "# ─────────────────── 8C)  PLOTTER ─────────────────── #\n",
    "def plot_inter_nuc_dist_hist_and_kde(\n",
    "    lower=DIST_LOWER_BOUND, upper=DIST_UPPER_BOUND,\n",
    "    bin_width=DIST_BIN_WIDTH, kde_bw=KDE_BW):\n",
    "    \"\"\"\n",
    "    One bar‑histogram per condition (rows), y = % of all gaps in that condition.\n",
    "    Combined KDE overlay (all conds) beneath.\n",
    "    \"\"\"\n",
    "    dbg(\"plotting inter‑nucleosome distance histograms …\")\n",
    "    dists_by_cond = _collect_inter_nuc_dists()\n",
    "    conds = sorted(dists_by_cond)\n",
    "\n",
    "    # ---------- build histogram data ---------- #\n",
    "    bins = np.arange(lower, upper + bin_width, bin_width)\n",
    "    bin_centres = (bins[:-1] + bins[1:]) / 2\n",
    "    n_bins = len(bin_centres)\n",
    "\n",
    "    pct_by_cond = {}\n",
    "    ymax = 0.0\n",
    "    for cond in conds:\n",
    "        dist_arr = np.asarray(dists_by_cond[cond])\n",
    "        hist, _ = np.histogram(dist_arr, bins=bins)\n",
    "        total = hist.sum()\n",
    "        pct = hist / total if total else np.zeros_like(hist, dtype=float)\n",
    "        pct_by_cond[cond] = pct\n",
    "        ymax = max(ymax, pct.max())\n",
    "\n",
    "    # ---------- figure 1 – histograms ---------- #\n",
    "    rows = len(conds)\n",
    "    hist_fig = make_subplots(rows=rows, cols=1, shared_xaxes=True,\n",
    "                             vertical_spacing=0.03,\n",
    "                             specs=[[{}] for _ in conds])\n",
    "\n",
    "    for r, cond in enumerate(conds, start=1):\n",
    "        colour = DEFAULT_READ_CLRS[(r-1) % len(DEFAULT_READ_CLRS)]\n",
    "        hist_fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=bin_centres,\n",
    "                y=pct_by_cond[cond],\n",
    "                width=bin_width,\n",
    "                marker=dict(color=\"rgba(0,0,0,0)\",\n",
    "                            line=dict(color=colour)),\n",
    "                name=cond,\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=r, col=1\n",
    "        )\n",
    "        hist_fig.update_yaxes(\n",
    "            range=[0, ymax * 1.05],\n",
    "            tickformat=\".1%\",\n",
    "            title_text=cond,\n",
    "            row=r, col=1\n",
    "        )\n",
    "\n",
    "    hist_fig.update_xaxes(range=[lower, upper], dtick=25,\n",
    "                          title_text=\"Inter‑nucleosome distance (bp)\")\n",
    "    hist_fig.update_layout(\n",
    "        template=FIG_TEMPLATE,\n",
    "        width=900,\n",
    "        height=250 * rows,\n",
    "        title=\"% of inter‑nucleosome gaps per condition\"\n",
    "    )\n",
    "    hist_fig.show()\n",
    "\n",
    "    # ---------- figure 2 – combined KDE ---------- #\n",
    "    dbg(\"plotting KDE overlay …\")\n",
    "    x_grid = np.linspace(lower, upper, KDE_POINTS)\n",
    "    kde_fig = go.Figure()\n",
    "    for idx, cond in enumerate(conds):\n",
    "        if not dists_by_cond[cond]:\n",
    "            continue\n",
    "        colour = DEFAULT_READ_CLRS[idx % len(DEFAULT_READ_CLRS)]\n",
    "        kde = gaussian_kde(dists_by_cond[cond], bw_method=kde_bw)\n",
    "        kde_fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=x_grid,\n",
    "                y=kde.evaluate(x_grid),\n",
    "                mode=\"lines\",\n",
    "                line=dict(width=2, color=colour),\n",
    "                name=cond\n",
    "            )\n",
    "        )\n",
    "\n",
    "    kde_fig.update_layout(\n",
    "        template=FIG_TEMPLATE,\n",
    "        width=900,\n",
    "        height=350,\n",
    "        title=\"Inter‑nucleosome distance density (KDE)\",\n",
    "        xaxis_title=\"Inter‑nucleosome distance (bp)\",\n",
    "        yaxis_title=\"Density\",\n",
    "        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02,\n",
    "                    xanchor=\"right\", x=1)\n",
    "    )\n",
    "    kde_fig.update_xaxes(range=[lower, upper], dtick=25)\n",
    "    kde_fig.show()\n",
    "\n",
    "    dbg(\"…done!\", always=True)\n",
    "\n",
    "# ─────────────────── 8D)  CALL PLOTTER ─────────────────── #\n",
    "# Uncomment this line (or call elsewhere) to generate the figures:\n",
    "plot_inter_nuc_dist_hist_and_kde()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fb5ecd676b8d23",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "#  CONSENSUS NUCLEOSOME PEAKS  ➜  Δ‑position box‑plot (relative to 0th condition)\n",
    "#  + DEBUG KDE PLOTS\n",
    "#\n",
    "#  • For *one* type (the first in sorted order), produce a debug KDE plot\n",
    "#    for each condition with vertical lines at the chosen summits.\n",
    "#\n",
    "#  • Then run the usual “find summits per type–condition” logic, build\n",
    "#    peaks_long_df, compute Δ_pos relative to the “0th” (baseline) condition\n",
    "#    for each (type, nuc_index), and draw the box‑plot.\n",
    "###############################################################################\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.stats  import gaussian_kde\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from IPython.display import display\n",
    "\n",
    "# ------------ parameters -----------------------------------------------------\n",
    "kde_bw      = 0.005                      # same bandwidth used before\n",
    "x_grid      = np.linspace(-REL_POS_RANGE, REL_POS_RANGE, KDE_POINTS)\n",
    "peak_sep    = CORE_SIZE                  # 147 bp minimum distance between summits\n",
    "peak_height = 0                          # accept all peaks, filter by height later\n",
    "\n",
    "# ------------ helper: peak selection ----------------------------------------\n",
    "def _select_peak_centers(x: np.ndarray, y: np.ndarray):\n",
    "    \"\"\"\n",
    "    Identify all peaks in the KDE curve `y(x)` using `find_peaks`, then for each peak:\n",
    "      1. Let h = y[p] be the peak height at index p.\n",
    "      2. Define half_h = h/2.\n",
    "      3. Scan left from p until y dips below half_h (or we reach array start) → index l.\n",
    "      4. Scan right from p until y dips below half_h (or we reach array end) → index r.\n",
    "      5. The “center” of that peak is (x[l] + x[r]) / 2.\n",
    "    Finally, keep only those peak‐centers that remain ≥ CORE_SIZE apart,\n",
    "    selecting the tallest peaks first.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List of floating‐point center positions (in the same coordinate system as x).\n",
    "    \"\"\"\n",
    "    # 1) find all local maxima (above zero)\n",
    "    peaks, props = find_peaks(y, height=0.0)\n",
    "    if peaks.size == 0:\n",
    "        return []\n",
    "\n",
    "    # 2) collect tuples (peak_index, peak_height)\n",
    "    peak_info = sorted(\n",
    "        [(p, props[\"peak_heights\"][i]) for i, p in enumerate(peaks)],\n",
    "        key=lambda x: -x[1]   # sort by height descending\n",
    "    )\n",
    "\n",
    "    selected_centers = []\n",
    "    for p, h in peak_info:\n",
    "        half_h = h * 0.5\n",
    "\n",
    "        # 3) scan left from p until y < half_h (or we hit index 0)\n",
    "        l = p\n",
    "        while l > 0 and y[l] >= half_h:\n",
    "            l -= 1\n",
    "        # if we stepped one below half_h, step back up by one\n",
    "        if y[l] < half_h and l < p:\n",
    "            l += 1\n",
    "\n",
    "        # 4) scan right from p until y < half_h (or we hit last index)\n",
    "        r = p\n",
    "        while r + 1 < len(y) and y[r] >= half_h:\n",
    "            r += 1\n",
    "        if y[r] < half_h and r > p:\n",
    "            r -= 1\n",
    "\n",
    "        # 5) compute the midpoint between x[l] and x[r]\n",
    "        center_x = 0.5 * (x[l] + x[r])\n",
    "\n",
    "        # 6) reject if within CORE_SIZE of any already‐accepted center\n",
    "        if any(abs(center_x - c) < CORE_SIZE for c in selected_centers):\n",
    "            continue\n",
    "\n",
    "        selected_centers.append(center_x)\n",
    "\n",
    "    # sort so that final list is left→right\n",
    "    selected_centers.sort()\n",
    "    return selected_centers\n",
    "\n",
    "# ------------ pick the first type and plot debug KDE per condition ----------\n",
    "all_types = sorted(filtered_reads_df[\"type\"].unique())\n",
    "if not all_types:\n",
    "    raise ValueError(\"No types found in filtered_reads_df\")\n",
    "first_type = all_types[0]\n",
    "\n",
    "for cond in sorted(filtered_reads_df[\"condition\"].unique()):\n",
    "    # collect all centres for this type–condition pair\n",
    "    centres = list(itertools.chain.from_iterable(\n",
    "        filtered_reads_df.loc[\n",
    "            (filtered_reads_df[\"type\"] == first_type) &\n",
    "            (filtered_reads_df[\"condition\"] == cond),\n",
    "            \"nuc_centers\"\n",
    "        ].tolist()\n",
    "    ))\n",
    "    if not centres:\n",
    "        continue\n",
    "\n",
    "    kde = gaussian_kde(centres, bw_method=kde_bw)\n",
    "    density = kde.evaluate(x_grid)\n",
    "    summits = _select_peak_centers(x_grid, density)\n",
    "\n",
    "    # build debug plot\n",
    "    fig_dbg = go.Figure()\n",
    "    fig_dbg.add_trace(\n",
    "        go.Scatter(\n",
    "            x=x_grid,\n",
    "            y=density,\n",
    "            mode=\"lines\",\n",
    "            line=dict(width=2, color=\"blue\"),\n",
    "            name=f\"KDE ({first_type}, {cond})\"\n",
    "        )\n",
    "    )\n",
    "    for s in summits:\n",
    "        fig_dbg.add_vline(\n",
    "            x=s,\n",
    "            line=dict(color=\"red\", width=1, dash=\"dash\"),\n",
    "            annotation_text=f\"{int(np.round(s))}\",\n",
    "            annotation_position=\"top right\"\n",
    "        )\n",
    "    fig_dbg.update_layout(\n",
    "        width=900,\n",
    "        height=300,\n",
    "        title=f\"Debug KDE – type={first_type}, condition={cond}\",\n",
    "        xaxis_title=\"Relative Position (bp)\",\n",
    "        yaxis_title=\"Density\"\n",
    "    )\n",
    "    fig_dbg.show()\n",
    "\n",
    "# ------------ collect consensus peaks across all types × conditions ----------\n",
    "rows = []\n",
    "for (typ, cond), grp in filtered_reads_df.groupby([\"type\", \"condition\"]):\n",
    "    centres = list(itertools.chain.from_iterable(grp[\"nuc_centers\"].tolist()))\n",
    "    if not centres:\n",
    "        continue\n",
    "\n",
    "    kde = gaussian_kde(centres, bw_method=kde_bw)\n",
    "    density = kde.evaluate(x_grid)\n",
    "    peaks   = _select_peak_centers(x_grid, density)\n",
    "    if not peaks:\n",
    "        continue\n",
    "\n",
    "    # assign nuc_index: n-1, n-2, … for negative; n+1, n+2, … for positive\n",
    "    neg_peaks = [p for p in peaks if p < 0]\n",
    "    pos_peaks = [p for p in peaks if p > 0]\n",
    "\n",
    "    # sort negative in increasing order (more negative → smaller index)\n",
    "    neg_peaks.sort()\n",
    "    # but nuc_index n-1 should be the *closest* negative to zero, so reverse after sorting\n",
    "    neg_peaks = neg_peaks[::-1]\n",
    "\n",
    "    for i, p in enumerate(neg_peaks, start=1):\n",
    "        rows.append((cond, typ, f\"n-{i}\", p))\n",
    "    for i, p in enumerate(sorted(pos_peaks), start=1):\n",
    "        rows.append((cond, typ, f\"n+{i}\", p))\n",
    "\n",
    "peaks_long_df = pd.DataFrame(rows, columns=[\"condition\", \"type\", \"nuc_index\", \"nuc_pos\"])\n",
    "if peaks_long_df.empty:\n",
    "    raise ValueError(\"No consensus peaks found in any type–condition group\")\n",
    "\n",
    "# ------------ compute Δ_pos relative to 0th (baseline) condition -------------\n",
    "# Determine \"0th condition\" per type: the first in sorted order of conditions\n",
    "conds_sorted = sorted(filtered_reads_df[\"condition\"].unique())\n",
    "baseline_condition = conds_sorted[1]  # treat this as 0th\n",
    "\n",
    "# Build a lookup: (type, nuc_index) -> nuc_pos of baseline_condition\n",
    "baseline_positions = {}\n",
    "for (typ, cond), sub in peaks_long_df.groupby([\"type\", \"condition\"]):\n",
    "    if cond != baseline_condition:\n",
    "        continue\n",
    "    for idx, row in sub.iterrows():\n",
    "        key = (typ, row[\"nuc_index\"])\n",
    "        # if multiple baseline rows for same (typ, nuc_index), take their median/mean\n",
    "        baseline_positions.setdefault(key, []).append(row[\"nuc_pos\"])\n",
    "\n",
    "# reduce lists to single baseline value (mean) per key\n",
    "for key, vals in baseline_positions.items():\n",
    "    baseline_positions[key] = float(np.mean(vals))\n",
    "\n",
    "# Now subtract baseline from every row in peaks_long_df\n",
    "def compute_delta(row):\n",
    "    key = (row[\"type\"], row[\"nuc_index\"])\n",
    "    base = baseline_positions.get(key, np.nan)\n",
    "    return row[\"nuc_pos\"] - base\n",
    "\n",
    "peaks_long_df[\"Δ_pos\"] = peaks_long_df.apply(compute_delta, axis=1)\n",
    "\n",
    "# For rows whose baseline was missing (NaN), Δ_pos becomes NaN—they'll be dropped in plotting:\n",
    "peaks_long_df = peaks_long_df.dropna(subset=[\"Δ_pos\"]).copy()\n",
    "\n",
    "# ------------ box‑plot of Δ_pos by nuc_index, colored by condition ------------\n",
    "fig = px.box(\n",
    "    peaks_long_df,\n",
    "    x=\"nuc_index\",\n",
    "    y=\"Δ_pos\",\n",
    "    color=\"condition\",\n",
    "    points=\"all\",\n",
    "    title=\"Deviation from baseline (0th condition) nucleosome position\",\n",
    "    labels={\"Δ_pos\": \"nuc_pos – baseline (bp)\"}\n",
    ")\n",
    "fig.update_layout(\n",
    "    template=FIG_TEMPLATE,\n",
    "    width=900,\n",
    "    height=600\n",
    ")\n",
    "\n",
    "# ensure x‑axis categories are ordered from smallest index to largest index:\n",
    "unique_indices = sorted(\n",
    "    peaks_long_df[\"nuc_index\"].unique(),\n",
    "    key=lambda s: int(s.replace(\"n\", \"\"))\n",
    ")\n",
    "fig.update_xaxes(categoryorder=\"array\", categoryarray=unique_indices)\n",
    "fig.show()\n",
    "\n",
    "# Optional: display first few rows for verification\n",
    "display(peaks_long_df.head())\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "# ------------ existing boxplot code up to fig creation ------------\n",
    "fig = px.box(\n",
    "    peaks_long_df,\n",
    "    x=\"nuc_index\",\n",
    "    y=\"Δ_pos\",\n",
    "    color=\"condition\",\n",
    "    points=\"all\",\n",
    "    title=\"Deviation from baseline (0th condition) nucleosome position\",\n",
    "    labels={\"Δ_pos\": \"nuc_pos – baseline (bp)\"}\n",
    ")\n",
    "fig.update_layout(\n",
    "    template=FIG_TEMPLATE,\n",
    "    width=900,\n",
    "    height=600\n",
    ")\n",
    "\n",
    "unique_indices = sorted(\n",
    "    peaks_long_df[\"nuc_index\"].unique(),\n",
    "    key=lambda s: int(s.replace(\"n\", \"\").replace(\"+\", \"\").replace(\"-\", \"\")) * (1 if \"+\" in s else -1)\n",
    ")\n",
    "fig.update_xaxes(categoryorder=\"array\", categoryarray=unique_indices)\n",
    "\n",
    "# Extract color mapping: condition -> color\n",
    "color_map = {}\n",
    "for trace in fig.data:\n",
    "    # Box traces have name equal to condition; one trace per condition\n",
    "    cond_name = trace.name\n",
    "    if cond_name not in color_map:\n",
    "        # For box, the fillcolor attribute holds RGBA; extract the line color\n",
    "        # trace.marker.color gives box color\n",
    "        color_map[cond_name] = trace.marker.color\n",
    "\n",
    "# ------------ compute median, Q1, Q3 per condition and nuc_index ------------\n",
    "summary = (\n",
    "    peaks_long_df\n",
    "    .groupby([\"condition\", \"nuc_index\"])[\"Δ_pos\"]\n",
    "    .agg(Q1=lambda x: np.percentile(x, 25),\n",
    "         median=\"median\",\n",
    "         Q3=lambda x: np.percentile(x, 75))\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# ------------ add median lines and shaded IQR for each condition ------------\n",
    "for cond in summary[\"condition\"].unique():\n",
    "    cond_df = summary[summary[\"condition\"] == cond].copy()\n",
    "    # Ensure ordering by nuc_index according to unique_indices\n",
    "    cond_df[\"order\"] = cond_df[\"nuc_index\"].apply(lambda x: unique_indices.index(x))\n",
    "    cond_df = cond_df.sort_values(\"order\")\n",
    "\n",
    "    idxs = cond_df[\"nuc_index\"].tolist()\n",
    "    q1_vals = cond_df[\"Q1\"].tolist()\n",
    "    q3_vals = cond_df[\"Q3\"].tolist()\n",
    "    med_vals = cond_df[\"median\"].tolist()\n",
    "    col = color_map.get(cond, \"black\")\n",
    "\n",
    "    # Trace for Q1 (invisible)\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=idxs,\n",
    "            y=q1_vals,\n",
    "            mode=\"lines\",\n",
    "            line=dict(color='rgba(0,0,0,0)', width=0),\n",
    "            showlegend=False,\n",
    "            hoverinfo='skip'\n",
    "        )\n",
    "    )\n",
    "    # Trace for Q3 with fill to Q1\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=idxs,\n",
    "            y=q3_vals,\n",
    "            mode=\"lines\",\n",
    "            line=dict(color='rgba(0,0,0,0)', width=0),\n",
    "            fill='tonexty',\n",
    "            fillcolor=f'rgba({int(col[1:3],16)},{int(col[3:5],16)},{int(col[5:7],16)},0.2)',\n",
    "            showlegend=False,\n",
    "            hoverinfo='skip'\n",
    "        )\n",
    "    )\n",
    "    # Median line\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=idxs,\n",
    "            y=med_vals,\n",
    "            mode=\"lines+markers\",\n",
    "            line=dict(color=col, width=2),\n",
    "            marker=dict(color=col, size=6),\n",
    "            name=f\"{cond} median\",\n",
    "            hoverinfo='skip'\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Show final figure\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c58503cb105776",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "#  CONSISTENCY OFFSETS USING *EXISTING* CONSENSUS PEAKS\n",
    "#  • “Consensus” positions come from `peaks_long_df` computed in the KDE cell\n",
    "#  • Toggle consensus scope with:\n",
    "#        CONS_BY_CONDITION = False  → (type, nuc_index)     ← previous behaviour\n",
    "#        CONS_BY_CONDITION = True   → (type, condition, nuc_index)\n",
    "#\n",
    "#  • Toggle distance definition with USE_CONS_DIST (unchanged).\n",
    "#  • Optional: restrict analysis to SELECT_TYPES.\n",
    "###############################################################################\n",
    "from itertools import combinations\n",
    "import numpy as np, pandas as pd, plotly.graph_objects as go\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ───── user‑configurable flags ───── #\n",
    "SELECT_TYPES       = []       # empty → all types\n",
    "CONS_BY_CONDITION  = True      # ⬅️ NEW: per‑condition consensus?\n",
    "MAX_OFFSET         = 60        # bp inclusion window around consensus peak\n",
    "USE_CONS_DIST      = True     # True → |centre – consensus| ; False → pairwise\n",
    "FIG_WIDTH          = 900\n",
    "DEBUG_CONS_OFFSETS = True\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "# 1) BUILD a dict of consensus centres\n",
    "#    key = (type, nuc_index)                     if CONS_BY_CONDITION is False\n",
    "#    key = (type, condition, nuc_index)          if CONS_BY_CONDITION is True\n",
    "# --------------------------------------------------------------------------- #\n",
    "cons_dict = {}   # key → consensus_pos\n",
    "\n",
    "if CONS_BY_CONDITION:\n",
    "    # separate consensus per condition & type\n",
    "    for (typ, cond, idx), sub in (\n",
    "        peaks_long_df.groupby([\"type\", \"condition\", \"nuc_index\"])\n",
    "    ):\n",
    "        if SELECT_TYPES and typ not in SELECT_TYPES:\n",
    "            continue\n",
    "        cons_dict[(typ, cond, idx)] = float(sub[\"nuc_pos\"].median())\n",
    "else:\n",
    "    # single consensus across conditions (original logic)\n",
    "    for (typ, idx), sub in peaks_long_df.groupby([\"type\", \"nuc_index\"]):\n",
    "        if SELECT_TYPES and typ not in SELECT_TYPES:\n",
    "            continue\n",
    "        # prefer baseline condition if available, else median of all\n",
    "        base_rows = sub[sub[\"condition\"] == baseline_condition]\n",
    "        if not base_rows.empty:\n",
    "            cons_dict[(typ, idx)] = float(base_rows[\"nuc_pos\"].median())\n",
    "        else:\n",
    "            cons_dict[(typ, idx)] = float(sub[\"nuc_pos\"].median())\n",
    "\n",
    "# optional sanity print\n",
    "if DEBUG_CONS_OFFSETS:\n",
    "    scope = \"(type,cond,nuc_index)\" if CONS_BY_CONDITION else \"(type,nuc_index)\"\n",
    "    print(f\"[consensus peaks] scope = {scope}\")\n",
    "    for k, pos in sorted(cons_dict.items()):\n",
    "        print(f\"  {k}: {pos:8.1f} bp\")\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "# 2) SUBSET reads\n",
    "# --------------------------------------------------------------------------- #\n",
    "df_subset = (\n",
    "    filtered_reads_df[filtered_reads_df[\"type\"].isin(SELECT_TYPES)].copy()\n",
    "    if SELECT_TYPES else\n",
    "    filtered_reads_df.copy()\n",
    ")\n",
    "if df_subset.empty:\n",
    "    raise ValueError(f\"No reads found for type(s): {SELECT_TYPES}\")\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "# 3) COLLECT offsets\n",
    "# --------------------------------------------------------------------------- #\n",
    "records = []\n",
    "for (cond, typ), grp in tqdm(\n",
    "        df_subset.groupby([\"condition\", \"type\"]),\n",
    "        desc=\"computing offsets\", disable=not DEBUG_CONS_OFFSETS):\n",
    "\n",
    "    all_centres = np.concatenate(grp[\"nuc_centers\"].values)\n",
    "    if all_centres.size == 0:\n",
    "        continue\n",
    "\n",
    "    # Build list of nuc_indices for this (typ, cond), depending on scope\n",
    "    if CONS_BY_CONDITION:\n",
    "        # Keys are (typ, cond, idx)\n",
    "        nuc_indices = [\n",
    "            key[2]\n",
    "            for key in cons_dict.keys()\n",
    "            if (key[0] == typ and key[1] == cond)\n",
    "        ]\n",
    "    else:\n",
    "        # Keys are (typ, idx)\n",
    "        nuc_indices = [\n",
    "            key[1]\n",
    "            for key in cons_dict.keys()\n",
    "            if key[0] == typ\n",
    "        ]\n",
    "\n",
    "    for idx in nuc_indices:\n",
    "        # Lookup consensus position with appropriate key\n",
    "        if CONS_BY_CONDITION:\n",
    "            key = (typ, cond, idx)\n",
    "        else:\n",
    "            key = (typ, idx)\n",
    "\n",
    "        cpos = cons_dict.get(key, None)\n",
    "        if cpos is None:\n",
    "            continue\n",
    "\n",
    "        # Find all centres within ±MAX_OFFSET of this consensus\n",
    "        mask  = (all_centres >= cpos - MAX_OFFSET) & (all_centres <= cpos + MAX_OFFSET)\n",
    "        local = all_centres[mask]\n",
    "        if local.size == 0:\n",
    "            continue\n",
    "\n",
    "        if USE_CONS_DIST:\n",
    "            # One record per centre: distance to consensus\n",
    "            for centre in local:\n",
    "                records.append({\n",
    "                    \"cons_label\": idx,\n",
    "                    \"offset\":     abs(centre - cpos),\n",
    "                    \"condition\":  cond\n",
    "                })\n",
    "        else:\n",
    "            # Pairwise absolute distances between every pair in local\n",
    "            if local.size < 2:\n",
    "                continue\n",
    "            for i, j in combinations(local, 2):\n",
    "                records.append({\n",
    "                    \"cons_label\": idx,\n",
    "                    \"offset\":     abs(i - j),\n",
    "                    \"condition\":  cond\n",
    "                })\n",
    "\n",
    "offset_df = pd.DataFrame(records)\n",
    "if offset_df.empty:\n",
    "    raise ValueError(\"No offset data gathered – check MAX_OFFSET or filters.\")\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "# 4) PLOT (unchanged logic, apart from colour map keyed by cond)\n",
    "# --------------------------------------------------------------------------- #\n",
    "conds   = sorted(offset_df[\"condition\"].unique())\n",
    "clr_map = {c: DEFAULT_READ_CLRS[i % len(DEFAULT_READ_CLRS)] for i, c in enumerate(conds)}\n",
    "\n",
    "def numeric_key(lbl):\n",
    "    # lbl is like \"n-3\" or \"n+2\"\n",
    "    return int(lbl.replace(\"n-\", \"-\").replace(\"n+\", \"\"))\n",
    "\n",
    "ordered_labels = sorted(offset_df[\"cons_label\"].unique(), key=numeric_key)\n",
    "\n",
    "fig = go.Figure()\n",
    "for cond in conds:\n",
    "    sub = offset_df[offset_df[\"condition\"] == cond]\n",
    "    fig.add_trace(\n",
    "        go.Box(\n",
    "            x=sub[\"cons_label\"],\n",
    "            y=sub[\"offset\"],\n",
    "            name=cond,\n",
    "            line=dict(color=clr_map[cond]),\n",
    "            fillcolor=\"rgba(0,0,0,0)\",\n",
    "            boxpoints=False\n",
    "        )\n",
    "    )\n",
    "\n",
    "ylabel = \"|centre − consensus|\" if USE_CONS_DIST else \"|centre₁ − centre₂|\"\n",
    "title  = f\"Distances (±{MAX_OFFSET} bp) — consensus scope: \" + \\\n",
    "         (\"per condition\" if CONS_BY_CONDITION else \"global\")\n",
    "\n",
    "fig.update_layout(\n",
    "    template=FIG_TEMPLATE,\n",
    "    width=FIG_WIDTH,\n",
    "    height=450,\n",
    "    title=title,\n",
    "    xaxis_title=\"Consensus peak (n‑index)\",\n",
    "    yaxis_title=f\"{ylabel} (bp)\",\n",
    "    boxmode=\"group\"\n",
    ")\n",
    "fig.update_xaxes(categoryorder=\"array\", categoryarray=ordered_labels)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65d9a7de9ccefd3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# CELL 1 — per-read autocorrelation for filtered reads (with optional filling)\n",
    "###############################################################################\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "# ─────────────── Filtering & Configuration ───────────────\n",
    "CHIP_RANK_CUTOFF      = 80\n",
    "ABOVE_FLAG            = True\n",
    "TYPES_TO_INCLUDE      = [] #\"MOTIFS_rex32\"\n",
    "CHR_TYPE_INCLUDE      = [\"X\"]\n",
    "CONDITIONS_TO_INCLUDE = [\n",
    "    analysis_cond[0],\n",
    "    analysis_cond[1],\n",
    "    analysis_cond[2],\n",
    "    analysis_cond[3]\n",
    "]\n",
    "\n",
    "MIN_READ_LENGTH = 700          # same meaning as in v5\n",
    "REQUIRE_CENTRAL = True         # toggle central‑overlap filter on/off\n",
    "\n",
    "# ─────────────── Filling algorithm toggle & width ───────────────\n",
    "PERFORM_FILLING       = True\n",
    "MET_DOMAIN_WIDTH      = 10\n",
    "\n",
    "# ────────────────── Core smoothing/interp config ──────────────────\n",
    "REL_POS_RANGE     = 1000\n",
    "LAPSE_WINDOW      = 0\n",
    "PROCESSING_OPTION = 2          # 1 = whole-read; 2 = split & lapse\n",
    "PERFORM_INTERP    = True\n",
    "INTERP_WINDOW     = 5\n",
    "RAW_SMOOTH_KIND   = \"moving\"\n",
    "RAW_MOVING_WIN    = 10\n",
    "RAW_GAUSS_SIGMA   = 5\n",
    "RAW_CLAMP_01      = False\n",
    "FINAL_SMOOTH_KIND = \"moving\"\n",
    "FINAL_MOVING_WIN  = 25\n",
    "FINAL_GAUSS_SIGMA = 2\n",
    "FINAL_CLAMP_01    = False\n",
    "\n",
    "N_WORKERS         = max(2, mp.cpu_count() - 2)\n",
    "DEBUG_PROGRESS    = True\n",
    "\n",
    "# ───────────────────── Filtering ─────────────────────\n",
    "chiprank_df = (\n",
    "    pd.read_csv(\"/Data1/reference/rex_chiprank.bed\", sep=r\"\\s+\")\n",
    "      .assign(type=lambda d: \"MOTIFS_\" + d[\"type\"].astype(str))\n",
    ")\n",
    "chip_rank_lookup = {\n",
    "    t: round(float(rk) * 100, 3)\n",
    "    for t, rk in zip(chiprank_df[\"type\"], chiprank_df[\"chip_rank\"])\n",
    "}\n",
    "\n",
    "keep_conds = set(CONDITIONS_TO_INCLUDE)\n",
    "keep_types = (\n",
    "    set(TYPES_TO_INCLUDE)\n",
    "    if TYPES_TO_INCLUDE else\n",
    "    {t for t, r in chip_rank_lookup.items() if (r >= CHIP_RANK_CUTOFF) == ABOVE_FLAG}\n",
    ")\n",
    "keep_chr = (\n",
    "    set(CHR_TYPE_INCLUDE)\n",
    "    if CHR_TYPE_INCLUDE else\n",
    "    set(merged_df[\"chr_type\"].unique())\n",
    ")\n",
    "\n",
    "# --- base metadata filter ----------------------------------------------------\n",
    "df0 = (\n",
    "    merged_df\n",
    "      .query(\"condition in @keep_conds and type in @keep_types and chr_type in @keep_chr\")\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# --- central‑overlap / length filter (mirrors v5) ----------------------------\n",
    "if REQUIRE_CENTRAL:\n",
    "    half = MIN_READ_LENGTH // 2\n",
    "    mask_central = df0[\"rel_pos\"].apply(\n",
    "        lambda arr: (min(arr) <= -half) and (max(arr) >= half)\n",
    "    )\n",
    "    filtered_reads_df = df0[mask_central].reset_index(drop=True)\n",
    "else:\n",
    "    # fall back to simple read‑length threshold\n",
    "    filtered_reads_df = df0[\n",
    "        df0[\"rel_pos\"].str.len() >= MIN_READ_LENGTH\n",
    "    ].reset_index(drop=True)\n",
    "\n",
    "# --- bookkeeping / sanity prints --------------------------------------------\n",
    "types_kept = sorted(filtered_reads_df['type'].unique())\n",
    "print(f\"[INFO] types included after filtering ({len(types_kept)}): {types_kept}\")\n",
    "\n",
    "if DEBUG_PROGRESS:\n",
    "    missing = keep_types - set(types_kept)\n",
    "    if missing:\n",
    "        print(f\"[WARN] types requested but not found after filtering ({len(missing)}): \"\n",
    "              f\"{sorted(missing)}\")\n",
    "\n",
    "\n",
    "# ───────────────────── Helpers ─────────────────────\n",
    "def _mode_interpolate(arr: np.ndarray, radius: int) -> np.ndarray:\n",
    "    isnan = np.isnan(arr)\n",
    "    if not isnan.any():\n",
    "        return arr\n",
    "    valid = (~isnan).astype(int)\n",
    "    ones  = ((arr == 1) & ~isnan).astype(int)\n",
    "    c_val = np.concatenate(([0], np.cumsum(valid)))\n",
    "    c_one = np.concatenate(([0], np.cumsum(ones)))\n",
    "    def _cnt(c_vec, i):\n",
    "        lo, hi = max(0, i - radius), min(len(arr)-1, i + radius)\n",
    "        return c_vec[hi+1] - c_vec[lo]\n",
    "    filled = arr.copy()\n",
    "    for i in np.where(isnan)[0]:\n",
    "        tot = _cnt(c_val, i)\n",
    "        filled[i] = 0.0 if tot == 0 else (1.0 if _cnt(c_one, i) > tot/2 else 0.0)\n",
    "    return filled\n",
    "\n",
    "def _apply_smoothing(y: np.ndarray, *, kind: str,\n",
    "                     moving_win: int, gauss_sigma: float,\n",
    "                     clamp: bool) -> np.ndarray:\n",
    "    if kind == \"moving\" and moving_win > 1:\n",
    "        y = np.convolve(y, np.ones(moving_win)/moving_win, mode=\"same\")\n",
    "    elif kind == \"gaussian\" and gauss_sigma > 0:\n",
    "        y = gaussian_filter1d(y, sigma=gauss_sigma, mode=\"nearest\")\n",
    "    return np.clip(y, 0, 1) if clamp else y\n",
    "\n",
    "def _fill_met_domains(arr: np.ndarray, width: int) -> np.ndarray:\n",
    "    idx = np.where(arr == 1)[0]\n",
    "    if len(idx) < 2:\n",
    "        return arr\n",
    "    filled = arr.copy()\n",
    "    for a, b in zip(idx, idx[1:]):\n",
    "        if b - a <= width:\n",
    "            filled[a:b+1] = 1.0\n",
    "    return filled\n",
    "\n",
    "raw_smooth = partial(\n",
    "    _apply_smoothing,\n",
    "    kind=RAW_SMOOTH_KIND,\n",
    "    moving_win=RAW_MOVING_WIN,\n",
    "    gauss_sigma=RAW_GAUSS_SIGMA,\n",
    "    clamp=RAW_CLAMP_01\n",
    ")\n",
    "final_smooth = partial(\n",
    "    _apply_smoothing,\n",
    "    kind=FINAL_SMOOTH_KIND,\n",
    "    moving_win=FINAL_MOVING_WIN,\n",
    "    gauss_sigma=FINAL_GAUSS_SIGMA,\n",
    "    clamp=FINAL_CLAMP_01\n",
    ")\n",
    "\n",
    "def _autocorr_vec(x: np.ndarray, max_lag: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    True Pearson-style autocorrelation up to max_lag:\n",
    "     - pairwise-complete (ignores NaNs)\n",
    "     - per-lag separate means & variances\n",
    "    \"\"\"\n",
    "    n = len(x)\n",
    "    ac = np.full(max_lag+1, np.nan)\n",
    "    if n < 2:\n",
    "        return ac\n",
    "\n",
    "    for k in range(0, min(max_lag, n-1) + 1):\n",
    "        x1 = x[:n-k]\n",
    "        x2 = x[k:]\n",
    "        valid = ~np.isnan(x1) & ~np.isnan(x2)\n",
    "        if valid.sum() < 2:\n",
    "            continue\n",
    "\n",
    "        x1v = x1[valid]\n",
    "        x2v = x2[valid]\n",
    "        m1 = x1v.mean()\n",
    "        m2 = x2v.mean()\n",
    "\n",
    "        num = np.sum((x1v - m1) * (x2v - m2))\n",
    "        v1  = np.sum((x1v - m1)**2)\n",
    "        v2  = np.sum((x2v - m2)**2)\n",
    "        denom = np.sqrt(v1 * v2)\n",
    "\n",
    "        ac[k] = num/denom if denom > 0 else np.nan\n",
    "\n",
    "    return ac\n",
    "\n",
    "def _process_read(read_row: pd.Series) -> pd.DataFrame:\n",
    "    meta     = read_row[['read_id','condition','type','chr_type']].to_dict()\n",
    "    rel_pos  = np.asarray(read_row['rel_pos'], dtype=int)\n",
    "    modq_bin = np.asarray(read_row['mod_qual_bin'], dtype=int)\n",
    "\n",
    "    # mask to ±REL_POS_RANGE\n",
    "    mask = (rel_pos >= -REL_POS_RANGE) & (rel_pos <= REL_POS_RANGE)\n",
    "    rel_pos, modq_bin = rel_pos[mask], modq_bin[mask]\n",
    "    if rel_pos.size == 0:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    def process_vec(vec, side_label):\n",
    "        # restrict fill/smooth to covered span\n",
    "        idxs = np.where(~np.isnan(vec))[0]\n",
    "        start, end = idxs.min(), idxs.max()\n",
    "        region = vec[start:end+1].copy()\n",
    "        if PERFORM_FILLING:\n",
    "            region = _fill_met_domains(region, MET_DOMAIN_WIDTH)\n",
    "        if PERFORM_INTERP:\n",
    "            region = _mode_interpolate(region, INTERP_WINDOW)\n",
    "        region = raw_smooth(region)\n",
    "        vec[start:end+1] = region\n",
    "        ac = _autocorr_vec(vec, REL_POS_RANGE)\n",
    "        return pd.DataFrame({\n",
    "            **meta,\n",
    "            'pos_neg': side_label,\n",
    "            'lag':     np.arange(REL_POS_RANGE+1),\n",
    "            'autocorr': ac\n",
    "        })\n",
    "\n",
    "    rows = []\n",
    "    if PROCESSING_OPTION == 1:\n",
    "        full_len = 2 * REL_POS_RANGE + 1\n",
    "        vec = np.full(full_len, np.nan)\n",
    "        idx = rel_pos + REL_POS_RANGE\n",
    "        vec[idx] = modq_bin\n",
    "        rows.append(process_vec(vec, 'whole'))\n",
    "    else:\n",
    "        half = LAPSE_WINDOW // 2\n",
    "        # NEG side\n",
    "        neg_mask = rel_pos <= -(half + 1)\n",
    "        if neg_mask.any():\n",
    "            rp = rel_pos[neg_mask]\n",
    "            mb = modq_bin[neg_mask]\n",
    "            new_pos = -rp\n",
    "            keep = new_pos <= REL_POS_RANGE\n",
    "            vec_neg = np.full(REL_POS_RANGE+1, np.nan)\n",
    "            vec_neg[new_pos[keep]] = mb[keep]\n",
    "            rows.append(process_vec(vec_neg, 'neg'))\n",
    "\n",
    "        # POS side\n",
    "        pos_mask = rel_pos >= (half + 1)\n",
    "        if pos_mask.any():\n",
    "            rp = rel_pos[pos_mask]\n",
    "            mb = modq_bin[pos_mask]\n",
    "            new_pos = rp\n",
    "            keep = new_pos <= REL_POS_RANGE\n",
    "            vec_pos = np.full(REL_POS_RANGE+1, np.nan)\n",
    "            vec_pos[new_pos[keep]] = mb[keep]\n",
    "            rows.append(process_vec(vec_pos, 'pos'))\n",
    "\n",
    "    return pd.concat(rows, ignore_index=True)\n",
    "\n",
    "def _process_read_dict(rd: dict) -> pd.DataFrame:\n",
    "    return _process_read(pd.Series(rd))\n",
    "\n",
    "# ───────────────────────── Run multiprocessing ────────────────────────────\n",
    "read_records = filtered_reads_df.to_dict(orient='records')\n",
    "if DEBUG_PROGRESS:\n",
    "    print(f\"[INFO] Queued reads: {len(read_records):,}\")\n",
    "with mp.Pool(N_WORKERS) as pool:\n",
    "    it = pool.imap_unordered(_process_read_dict, read_records, chunksize=128)\n",
    "    if DEBUG_PROGRESS:\n",
    "        it = tqdm(it, total=len(read_records), desc=\"reads\")\n",
    "    per_read_ac = pd.concat((df for df in it if not df.empty),\n",
    "                             ignore_index=True)\n",
    "print(f\"[INFO] per_read_ac shape: {per_read_ac.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda49a23c6e8be9a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 2 — aggregate, autocorrelation plots (split if PROCESSING_OPTION=2),\n",
    "#          subtraction plots, and debug plots\n",
    "\n",
    "import time, itertools, numpy as np, pandas as pd, plotly.graph_objects as go\n",
    "\n",
    "# ─────────────── Configuration ───────────────\n",
    "MIN_LAG, MAX_LAG       = 100, 600\n",
    "MIN_OVERLAP            = 100    # minimum bases overlapping between read & lagged read\n",
    "ZSCORE_NORMALIZE       = False  # set True to z-score normalize\n",
    "TYPES_TO_PLOT          = []     # [] → include all; else subset\n",
    "SHOW_CI                = False  # set False to disable CI envelope\n",
    "# PROCESSING_OPTION, analysis_cond, per_read_ac, filtered_reads_df\n",
    "# are assumed defined by Cell 1\n",
    "\n",
    "# ─────────── Condition ordering & colour mapping ───────────\n",
    "conditions = [analysis_cond[0], analysis_cond[1],\n",
    "              analysis_cond[2], analysis_cond[3]]\n",
    "COND_COLORS = {c: \"#888888\" for c in conditions}\n",
    "COND_COLORS.update({c: \"#4974a5\" for c in conditions if \"N2\"   in c})\n",
    "COND_COLORS.update({c: \"#51ab4d\" for c in conditions if \"DPY27\" in c})\n",
    "COND_COLORS.update({c: \"#b12537\" for c in conditions if \"SDC2\"  in c})\n",
    "\n",
    "def _hex_rgba(hex_code, alpha=0.20):\n",
    "    \"\"\"Convert hex colour to rgba string with given alpha.\"\"\"\n",
    "    hex_code = hex_code.lstrip('#')\n",
    "    r, g, b = (int(hex_code[i:i+2], 16) for i in (0, 2, 4))\n",
    "    return f\"rgba({r},{g},{b},{alpha})\"\n",
    "\n",
    "# ─────────────── Apply type filter & MIN_OVERLAP filter ───────────────\n",
    "df_ac = per_read_ac.copy()\n",
    "df_fr = filtered_reads_df.copy()\n",
    "\n",
    "# Assume filtered_reads_df has 'read_id' and either:\n",
    "#  • 'read_length' column, or\n",
    "#  • 'start' and 'end' columns so we can compute read_length.\n",
    "if 'read_length' not in df_fr.columns:\n",
    "    # Compute read_length if start/end are present\n",
    "    if {'start', 'end'}.issubset(df_fr.columns):\n",
    "        df_fr['read_length'] = df_fr['end'] - df_fr['start'] + 1\n",
    "    else:\n",
    "        raise RuntimeError(\"filtered_reads_df must contain 'read_length' or both 'start' & 'end' columns.\")\n",
    "\n",
    "# Merge read_length into df_ac (requires per_read_ac to have 'read_id')\n",
    "if 'read_id' not in df_ac.columns:\n",
    "    raise RuntimeError(\"per_read_ac must contain 'read_id' for overlap filtering.\")\n",
    "\n",
    "df_ac = df_ac.merge(\n",
    "    df_fr[['read_id', 'read_length']],\n",
    "    on='read_id', how='left'\n",
    ")\n",
    "\n",
    "# Keep only autocorr rows where overlap ≥ MIN_OVERLAP:\n",
    "#   overlap = read_length − lag  ⇒ require lag ≤ read_length − MIN_OVERLAP\n",
    "df_ac = df_ac[df_ac['lag'] <= (df_ac['read_length'] - MIN_OVERLAP)].reset_index(drop=True)\n",
    "\n",
    "# Now apply type filter\n",
    "if TYPES_TO_PLOT:\n",
    "    df_ac = df_ac[df_ac['type'].isin(TYPES_TO_PLOT)]\n",
    "    df_fr = df_fr[df_fr['type'].isin(TYPES_TO_PLOT)]\n",
    "\n",
    "# ─────────────────────────── Timing start ───────────────────────────\n",
    "t0 = time.time()\n",
    "print(f\"[DEBUG] Cell 2 start at {t0:.2f}s\")\n",
    "\n",
    "# ───── 1. Aggregate across read-ids within each type ─────\n",
    "plot_df_auto = (\n",
    "    df_ac\n",
    "      .groupby(['pos_neg', 'condition', 'type', 'lag'], as_index=False, sort=False)['autocorr']\n",
    "      .mean()\n",
    ")\n",
    "plot_df_auto['autocorr_smooth'] = (\n",
    "    plot_df_auto\n",
    "      .groupby(['pos_neg', 'condition', 'type'])['autocorr']\n",
    "      .transform(final_smooth)\n",
    ")\n",
    "μ, σ = plot_df_auto['autocorr_smooth'].mean(), plot_df_auto['autocorr_smooth'].std(ddof=0)\n",
    "plot_df_auto['plot_val'] = (\n",
    "    (plot_df_auto['autocorr_smooth'] - μ) / σ\n",
    "    if ZSCORE_NORMALIZE else plot_df_auto['autocorr_smooth']\n",
    ")\n",
    "plot_df_auto = plot_df_auto[plot_df_auto['lag'].between(MIN_LAG, MAX_LAG)].reset_index(drop=True)\n",
    "print(f\"[DEBUG] per-type aggregation done in {(time.time() - t0):.2f}s\")\n",
    "\n",
    "# ───── 2. Collapse over type for plotting / Δ-plots ─────\n",
    "plot_df_cond = (\n",
    "    plot_df_auto\n",
    "      .groupby(['pos_neg', 'condition', 'lag'], as_index=False)\n",
    "      .agg(mean_val=('plot_val', 'mean'),\n",
    "           sd_val  =('plot_val', 'std'),\n",
    "           n       =('plot_val', 'count'))\n",
    ")\n",
    "plot_df_cond['sem']   = plot_df_cond['sd_val'] / np.sqrt(plot_df_cond['n'].replace(0, np.nan))\n",
    "plot_df_cond['ci_lo'] = plot_df_cond['mean_val'] - 1.96 * plot_df_cond['sem']\n",
    "plot_df_cond['ci_hi'] = plot_df_cond['mean_val'] + 1.96 * plot_df_cond['sem']\n",
    "\n",
    "y_label = \"Autocorrelation (z-score)\" if ZSCORE_NORMALIZE else \"Mean autocorrelation\"\n",
    "\n",
    "# ───── 3. Helper to add a line + optional CI band ─────\n",
    "def _add_trace_with_ci(fig, sub_df, lag_col, color, name):\n",
    "    # main line\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=sub_df[lag_col], y=sub_df['mean_val'],\n",
    "        mode='lines', name=name, line=dict(color=color)\n",
    "    ))\n",
    "    # CI envelope if enabled & ≥2 types\n",
    "    if SHOW_CI and sub_df['n'].max() > 1:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=pd.concat([sub_df[lag_col], sub_df[lag_col][::-1]]),\n",
    "            y=pd.concat([sub_df['ci_hi'],  sub_df['ci_lo'][::-1]]),\n",
    "            fill='toself', fillcolor=_hex_rgba(color),\n",
    "            line=dict(width=0), hoverinfo='skip', showlegend=False\n",
    "        ))\n",
    "\n",
    "# ───── 4. Main autocorrelation plots ─────\n",
    "if PROCESSING_OPTION == 2:\n",
    "    # — Negative side —\n",
    "    neg_df = plot_df_cond[plot_df_cond['pos_neg'] == 'neg'].copy()\n",
    "    neg_df['plot_lag'] = -neg_df['lag']\n",
    "    fig_neg = go.Figure(layout=dict(template=\"plotly_white\"))\n",
    "    for cond in conditions:\n",
    "        _add_trace_with_ci(\n",
    "            fig_neg,\n",
    "            neg_df[neg_df['condition'] == cond].sort_values('plot_lag'),\n",
    "            'plot_lag', COND_COLORS[cond], cond\n",
    "        )\n",
    "    fig_neg.update_layout(\n",
    "        title=\"Read-level Autocorrelation (negative side)\",\n",
    "        xaxis_title=\"Lag (bp)\", yaxis_title=y_label,\n",
    "        legend_title=\"Condition\", xaxis=dict(range=[-MAX_LAG, -MIN_LAG]),\n",
    "        width=1200, height=600\n",
    "    )\n",
    "    fig_neg.show()\n",
    "    print(f\"[DEBUG] Negative-side plot {(time.time() - t0):.2f}s\")\n",
    "\n",
    "    # — Positive side —\n",
    "    pos_df = plot_df_cond[plot_df_cond['pos_neg'] == 'pos'].copy()\n",
    "    pos_df['plot_lag'] = pos_df['lag']\n",
    "    fig_pos = go.Figure(layout=dict(template=\"plotly_white\"))\n",
    "    for cond in conditions:\n",
    "        _add_trace_with_ci(\n",
    "            fig_pos,\n",
    "            pos_df[pos_df['condition'] == cond].sort_values('plot_lag'),\n",
    "            'plot_lag', COND_COLORS[cond], cond\n",
    "        )\n",
    "    fig_pos.update_layout(\n",
    "        title=\"Read-level Autocorrelation (positive side)\",\n",
    "        xaxis_title=\"Lag (bp)\", yaxis_title=y_label,\n",
    "        legend_title=\"Condition\", xaxis=dict(range=[MIN_LAG, MAX_LAG]),\n",
    "        width=1200, height=600\n",
    "    )\n",
    "    fig_pos.show()\n",
    "    print(f\"[DEBUG] Positive-side plot {(time.time() - t0):.2f}s\")\n",
    "\n",
    "else:\n",
    "    whole_df = plot_df_cond.copy()\n",
    "    fig = go.Figure(layout=dict(template=\"plotly_white\"))\n",
    "    for cond in conditions:\n",
    "        _add_trace_with_ci(\n",
    "            fig,\n",
    "            whole_df[whole_df['condition'] == cond].sort_values('lag'),\n",
    "            'lag', COND_COLORS[cond], cond\n",
    "        )\n",
    "    fig.update_layout(\n",
    "        title=\"Read-level Autocorrelation (filtered)\",\n",
    "        xaxis_title=\"Lag (bp)\", yaxis_title=y_label,\n",
    "        legend_title=\"Condition\", xaxis=dict(range=[MIN_LAG, MAX_LAG]),\n",
    "        width=1000, height=600\n",
    "    )\n",
    "    fig.show()\n",
    "    print(f\"[DEBUG] Whole-read plot {(time.time() - t0):.2f}s\")\n",
    "\n",
    "# ───── 5. Pair-wise subtraction plots (mean-across-types) ─────\n",
    "def _pairwise_diff(base_df, lag_col, xaxis_range, title):\n",
    "    fig = go.Figure(layout=dict(template=\"plotly_white\"))\n",
    "    for c1, c2 in itertools.combinations(conditions, 2):\n",
    "        d1 = base_df[base_df['condition'] == c1].set_index(lag_col)['mean_val']\n",
    "        d2 = base_df[base_df['condition'] == c2].set_index(lag_col)['mean_val']\n",
    "        diff_df = (d1 - d2).reset_index(name='diff')\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=diff_df[lag_col], y=diff_df['diff'],\n",
    "            mode='lines', name=f\"{c1} − {c2}\"\n",
    "        ))\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        xaxis_title=\"Lag (bp)\",\n",
    "        yaxis_title=f\"Δ {y_label}\",\n",
    "        xaxis=dict(range=xaxis_range),\n",
    "        width=1000, height=600\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "if PROCESSING_OPTION == 2:\n",
    "    _pairwise_diff(\n",
    "        neg_df, 'plot_lag', [-MAX_LAG, -MIN_LAG],\n",
    "        \"Pair-wise Differences (negative side)\"\n",
    "    )\n",
    "    _pairwise_diff(\n",
    "        pos_df, 'plot_lag', [MIN_LAG, MAX_LAG],\n",
    "        \"Pair-wise Differences (positive side)\"\n",
    "    )\n",
    "else:\n",
    "    _pairwise_diff(\n",
    "        plot_df_cond, 'lag', [MIN_LAG, MAX_LAG],\n",
    "        \"Pair-wise Differences\"\n",
    "    )\n",
    "\n",
    "print(f\"[DEBUG] Cell 2 complete in {(time.time() - t0):.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233befed2ee774e9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# CELL 3 — NRL‑guided peak detection, distances, amplitudes & Δ‑amplitudes\n",
    "###############################################################################\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# ─────────────── High‑level toggles ───────────────\n",
    "RUN_DISTANCES      = False   # peak→peak Δ\n",
    "RUN_AMPLITUDES     = False   # peak amplitude\n",
    "RUN_AMP_SUBTRACT   = True   # NEW: amplitude – baseline amplitude\n",
    "\n",
    "# ─────────────── Parameters ───────────────\n",
    "NUM_CYCLES   = 4           # how many successive peaks to analyse\n",
    "NRL          = 177         # nominal nucleosome repeat length\n",
    "WINDOW       = 30          # ± bp around each NRL multiple to look for a peak\n",
    "FIG_W, FIG_H = 900, 450\n",
    "BASELINE     = analysis_cond[0]\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1)  PEAK‑SELECTION + METRIC UTILS  (unchanged)\n",
    "# -----------------------------------------------------------------------------\n",
    "def _select_nrl_peaks(lag_arr, val_arr, n_cycles=NUM_CYCLES):\n",
    "    chosen_lags, chosen_vals = [], []\n",
    "    for k in range(1, n_cycles + 1):\n",
    "        lo, hi = k * NRL - WINDOW, k * NRL + WINDOW\n",
    "        mask = (lag_arr >= lo) & (lag_arr <= hi)\n",
    "        if mask.any():\n",
    "            idx_max = np.argmax(val_arr[mask])\n",
    "            sub_lags = lag_arr[mask]\n",
    "            sub_vals = val_arr[mask]\n",
    "            chosen_lags.append(sub_lags[idx_max])\n",
    "            chosen_vals.append(sub_vals[idx_max])\n",
    "    return np.asarray(chosen_lags), np.asarray(chosen_vals)\n",
    "\n",
    "def _peak_metrics(lag_arr, val_arr, n_cycles=NUM_CYCLES):\n",
    "    \"\"\"Return ([distances], [amplitudes]) for successive NRL peaks.\"\"\"\n",
    "    pk_lag, pk_val = _select_nrl_peaks(lag_arr, val_arr, n_cycles)\n",
    "    if pk_lag.size < 2:\n",
    "        return [], []\n",
    "    dists = np.diff(pk_lag)[:n_cycles]\n",
    "\n",
    "    amps, prev_lag = [], 0\n",
    "    for lag, val in zip(pk_lag, pk_val):\n",
    "        seg_mask = (lag_arr >= prev_lag) & (lag_arr <= lag)\n",
    "        trough = np.min(val_arr[seg_mask]) if seg_mask.any() else np.nan\n",
    "        amps.append(val - trough)\n",
    "        prev_lag = lag\n",
    "        if len(amps) == n_cycles:\n",
    "            break\n",
    "    return dists.tolist(), amps[:n_cycles]\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2)  DATA‑FRAME BUILDERS\n",
    "# -----------------------------------------------------------------------------\n",
    "def _build_metrics_df(auto_df, side=None):\n",
    "    \"\"\"Compute distance & amplitude DataFrames for (side · condition · type).\"\"\"\n",
    "    sub = auto_df if side is None else auto_df[auto_df[\"pos_neg\"] == side]\n",
    "    dist_rows, amp_rows = [], []\n",
    "\n",
    "    for (pos_neg, cond, typ), g in sub.groupby([\"pos_neg\", \"condition\", \"type\"]):\n",
    "        g = g.sort_values(\"lag\")\n",
    "        dists, amps = _peak_metrics(g[\"lag\"].to_numpy(), g[\"plot_val\"].to_numpy())\n",
    "        for i, d in enumerate(dists, 1):\n",
    "            dist_rows.append(dict(pos_neg=pos_neg, condition=cond, type=typ,\n",
    "                                  cycle=f\"d{i}\", distance=d))\n",
    "        for i, a in enumerate(amps, 1):\n",
    "            amp_rows.append(dict(pos_neg=pos_neg, condition=cond, type=typ,\n",
    "                                 cycle=f\"a{i}\", amplitude=a))\n",
    "\n",
    "    return pd.DataFrame(dist_rows), pd.DataFrame(amp_rows)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3)  PLOTTING HELPER\n",
    "# -----------------------------------------------------------------------------\n",
    "def _plot_box(df, value_col, ylabel, title, order, colours):\n",
    "    fig = go.Figure(layout=dict(template=\"plotly_white\", width=FIG_W, height=FIG_H))\n",
    "    for cond in analysis_cond:\n",
    "        if cond == BASELINE and \"Δ\" in ylabel:\n",
    "            continue                      # skip baseline in subtraction plot\n",
    "        sub = df[df[\"condition\"] == cond]\n",
    "        if sub.empty:\n",
    "            continue\n",
    "        fig.add_trace(go.Box(\n",
    "            x=sub[\"cycle\"], y=sub[value_col], name=cond,\n",
    "            marker_color=colours.get(cond, \"#888888\"),\n",
    "            boxpoints=\"outliers\", offsetgroup=cond\n",
    "        ))\n",
    "    fig.update_xaxes(categoryorder=\"array\", categoryarray=order, title_text=\"Cycle\")\n",
    "    fig.update_layout(title=title, yaxis_title=ylabel,\n",
    "                      boxmode=\"group\", legend_title=\"Condition\")\n",
    "    fig.show()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4)  BUILD METRICS, UNIFY SIDES (when needed), & PLOT\n",
    "# -----------------------------------------------------------------------------\n",
    "if PROCESSING_OPTION == 2:\n",
    "    # build once per side, then concatenate with signed cycle labels\n",
    "    dist_neg, amp_neg = _build_metrics_df(plot_df_auto, side=\"neg\")\n",
    "    dist_pos, amp_pos = _build_metrics_df(plot_df_auto, side=\"pos\")\n",
    "\n",
    "    # label cycles from NEG side as negative (d-1, a-1 …)\n",
    "    for df in (dist_neg, amp_neg):\n",
    "        df[\"cycle\"] = df[\"cycle\"].str.replace(r\"^([da])\", r\"\\1-\",\n",
    "                                              regex=True)\n",
    "    dist_df = pd.concat([dist_neg, dist_pos], ignore_index=True)\n",
    "    amp_df  = pd.concat([amp_neg,  amp_pos],  ignore_index=True)\n",
    "    side_name = \"Negative & Positive sides\"\n",
    "    order_dist = [f\"d-{i}\" for i in range(NUM_CYCLES, 0, -1)] + \\\n",
    "                 [f\"d{i}\"  for i in range(1, NUM_CYCLES + 1)]\n",
    "    order_amp  = [f\"a-{i}\" for i in range(NUM_CYCLES, 0, -1)] + \\\n",
    "                 [f\"a{i}\"  for i in range(1, NUM_CYCLES + 1)]\n",
    "else:\n",
    "    dist_df, amp_df = _build_metrics_df(plot_df_auto)    # whole read\n",
    "    side_name = \"Whole read\"\n",
    "    order_dist = [f\"d{i}\" for i in range(1, NUM_CYCLES + 1)]\n",
    "    order_amp  = [f\"a{i}\" for i in range(1, NUM_CYCLES + 1)]\n",
    "\n",
    "# ----- Δ‑Amplitude (baseline‑subtracted) -------------------------------------\n",
    "if RUN_AMP_SUBTRACT and not amp_df.empty:\n",
    "    # look‑up table of baseline amplitudes keyed by (pos_neg, type, cycle)\n",
    "    base_amp = amp_df[amp_df[\"condition\"] == BASELINE] \\\n",
    "               .set_index([\"pos_neg\", \"type\", \"cycle\"])[\"amplitude\"]\n",
    "\n",
    "    amp_sub_df = amp_df[amp_df[\"condition\"] != BASELINE].copy()\n",
    "    amp_sub_df[\"amp_sub\"] = amp_sub_df.apply(\n",
    "        lambda r: r[\"amplitude\"] - base_amp.get((r[\"pos_neg\"], r[\"type\"], r[\"cycle\"]),\n",
    "                                                np.nan),\n",
    "        axis=1\n",
    "    )\n",
    "    amp_sub_df = amp_sub_df.dropna(subset=[\"amp_sub\"])\n",
    "\n",
    "# ----- PLOTS -----------------------------------------------------------------\n",
    "if RUN_DISTANCES and not dist_df.empty:\n",
    "    _plot_box(dist_df, \"distance\",\n",
    "              \"Distance between peaks (bp)\",\n",
    "              f\"Peak→Peak Distances – {side_name}\",\n",
    "              order_dist, COND_COLORS)\n",
    "\n",
    "if RUN_AMPLITUDES and not amp_df.empty:\n",
    "    _plot_box(amp_df, \"amplitude\",\n",
    "              \"Amplitude (peak − local trough)\",\n",
    "              f\"Peak Amplitudes – {side_name}\",\n",
    "              order_amp, COND_COLORS)\n",
    "\n",
    "if RUN_AMP_SUBTRACT and not amp_sub_df.empty:\n",
    "    _plot_box(amp_sub_df, \"amp_sub\",\n",
    "              f\"Δ Amplitude vs {BASELINE}\",\n",
    "              f\"Amplitude Subtraction (vs {BASELINE}) – {side_name}\",\n",
    "              order_amp, COND_COLORS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4b03d0776aacab",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# CELL 3c — interactive autocorrelation viewer (2 dropdowns + peak markers)\n",
    "###############################################################################\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import ipywidgets as wd\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# ───────────────────────── CONFIG ───────────────────────── #\n",
    "NUM_CYCLES   = 3\n",
    "NRL          = 177          # expected repeat\n",
    "WINDOW       = 30           # ± bp around k*NRL\n",
    "FIG_W, FIG_H = 1200, 500\n",
    "# plot_df_auto, analysis_cond, COND_COLORS, PROCESSING_OPTION must exist (from Cell 1/2)\n",
    "\n",
    "# ─────────────────── helper: NRL‑guided peaks ─────────────────── #\n",
    "def _nrl_peaks(x, y, n_cycles=NUM_CYCLES):\n",
    "    \"\"\"\n",
    "    Return array of x‑positions of peaks:\n",
    "    one per window [k*NRL ± WINDOW], k=1…n_cycles, picking the highest point.\n",
    "    x must be ascending.\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for k in range(1, n_cycles + 1):\n",
    "        ctr = k * NRL\n",
    "        mask = (x >= ctr - WINDOW) & (x <= ctr + WINDOW)\n",
    "        if mask.any():\n",
    "            seg_x = x[mask]\n",
    "            seg_y = y[mask]\n",
    "            out.append(seg_x[np.argmax(seg_y)])\n",
    "    return np.array(out)\n",
    "\n",
    "# ─────────────────── pre‑compute curves & peaks ─────────────────── #\n",
    "# keys = (type, condition, side)  side = 'neg'|'pos'|'whole'\n",
    "mean_curve = {}   # (ty,cond,side) → (x_vals, y_vals)\n",
    "peak_dict  = {}   # (ty,cond,side) → peak_x array\n",
    "\n",
    "sides = ['whole'] if PROCESSING_OPTION != 2 else ['neg', 'pos']\n",
    "for side in sides:\n",
    "    df_side = (plot_df_auto\n",
    "               if side == 'whole'\n",
    "               else plot_df_auto[plot_df_auto['pos_neg'] == side])\n",
    "\n",
    "    for (typ, cond), grp in df_side.groupby(['type', 'condition']):\n",
    "        g = grp.sort_values('lag')\n",
    "        x_vals = g['lag'].to_numpy()\n",
    "        y_vals = g['plot_val'].to_numpy()\n",
    "\n",
    "        mean_curve[(typ, cond, side)] = (x_vals, y_vals)\n",
    "        peak_dict [(typ, cond, side)] = _nrl_peaks(x_vals, y_vals)\n",
    "\n",
    "# ─────────────────── interactive viewer ─────────────────── #\n",
    "def viewer(side='whole'):\n",
    "    \"\"\"\n",
    "    Build dropdown‑controlled Plotly figure for the chosen side.\n",
    "    \"\"\"\n",
    "    # available combos for this side\n",
    "    combos = [(t, c) for (t, c, s) in mean_curve.keys() if s == side]\n",
    "    if not combos:\n",
    "        print(f\"[WARN] no curves for side={side}\")\n",
    "        return\n",
    "\n",
    "    types      = sorted({t for (t, _) in combos})\n",
    "    conditions = sorted({c for (_, c) in combos})\n",
    "\n",
    "    dd_type  = wd.Dropdown(options=types, value=types[0], description='Type:')\n",
    "    dd_cond  = wd.Dropdown(options=conditions, value=conditions[0],\n",
    "                           description='Condition:')\n",
    "    out_plot = wd.Output()\n",
    "\n",
    "    def _redraw(*_):\n",
    "        typ  = dd_type.value\n",
    "        cond = dd_cond.value\n",
    "        x, y = mean_curve[(typ, cond, side)]\n",
    "        peaks = peak_dict[(typ, cond, side)]\n",
    "\n",
    "        # build vertical‑line shapes for peaks\n",
    "        shapes = [dict(type='line',\n",
    "                       x0=px, x1=px,\n",
    "                       y0=y.min(), y1=y.max(),\n",
    "                       line=dict(color='black', width=1, dash='dot'))\n",
    "                  for px in peaks]\n",
    "\n",
    "        fig = go.Figure(layout=dict(template='plotly_white',\n",
    "                                    width=FIG_W, height=FIG_H,\n",
    "                                    shapes=shapes))\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=x, y=y, mode='lines',\n",
    "            line=dict(color=COND_COLORS.get(cond, '#888888'), width=2),\n",
    "            name=f'{cond} — {typ}'\n",
    "        ))\n",
    "        fig.update_layout(\n",
    "            title=(f\"Autocorrelation ({side.upper() if side!='whole' else 'WHOLE'})\"\n",
    "                   f\" — {typ} | {cond}\"),\n",
    "            xaxis_title='Lag (bp)', yaxis_title='Autocorrelation'\n",
    "        )\n",
    "\n",
    "        with out_plot:\n",
    "            clear_output(wait=True)\n",
    "            display(fig)\n",
    "\n",
    "    # link callbacks\n",
    "    dd_type.observe(_redraw, names='value')\n",
    "    dd_cond.observe(_redraw, names='value')\n",
    "\n",
    "    # initial draw\n",
    "    _redraw()\n",
    "\n",
    "    caption = wd.HTML(f\"<b>{'Whole read' if side=='whole' else side.upper()+' side'}</b>\")\n",
    "    display(wd.VBox([caption, wd.HBox([dd_type, dd_cond]), out_plot]))\n",
    "\n",
    "# ─────────────────── render viewer(s) ─────────────────── #\n",
    "for s in sides:\n",
    "    viewer(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfc374367f7cacf",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# CELL 3 — Per-read FFT analysis, power spectra, & comparative plots\n",
    "###############################################################################\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from scipy.fft import rfft, rfftfreq # No longer needed for metrics if all LS based\n",
    "from scipy.signal import find_peaks, peak_widths, lombscargle\n",
    "import plotly.graph_objects as go\n",
    "from tqdm.auto import tqdm\n",
    "import plotly.colors\n",
    "from typing import Union, Optional, Tuple, List, Dict\n",
    "\n",
    "# ─────────────── FFT Analysis Configuration ───────────────\n",
    "# For dominant peak identification (now from Lomb-Scargle spectrum)\n",
    "NUC_PERIOD_MIN = 150\n",
    "NUC_PERIOD_MAX = 250\n",
    "MIN_VALID_AUTOCORR_POINTS = 20\n",
    "\n",
    "# For plotting average power spectra (using Lomb-Scargle)\n",
    "SPECTRUM_PLOT_PERIOD_MIN = 50\n",
    "SPECTRUM_PLOT_PERIOD_MAX = 500\n",
    "SPECTRUM_PLOT_PERIOD_STEP = 1.0 # Desired 1bp resolution\n",
    "\n",
    "# ───────────────────── Timing start ─────────────────────\n",
    "t_cell3_start = time.time()\n",
    "print(f\"[INFO] Cell 3: Analysis and Plotting started at {t_cell3_start:.2f}s\")\n",
    "\n",
    "# ───────────────── Color Helper ─────────────────\n",
    "def hex_to_rgba_str(hex_color: str, alpha: float) -> str:\n",
    "    hex_color = hex_color.lstrip('#')\n",
    "    r, g, b = tuple(int(hex_color[i:i+2], 16) for i in (0, 2, 4))\n",
    "    return f'rgba({r},{g},{b},{alpha})'\n",
    "\n",
    "# ───────────────── Lomb-Scargle Calculation Functions ─────────────────\n",
    "\n",
    "def extract_power_spectrum_lombscargle(autocorr_values: np.ndarray,\n",
    "                                       sampling_interval: float,\n",
    "                                       plot_period_min: float,\n",
    "                                       plot_period_max: float,\n",
    "                                       period_step: float) -> pd.DataFrame:\n",
    "    N = len(autocorr_values)\n",
    "    if N < 2 or np.all(autocorr_values == 0):\n",
    "        return pd.DataFrame({'period': [], 'power': []})\n",
    "\n",
    "    window = np.hanning(N)\n",
    "    autocorr_windowed = autocorr_values * window\n",
    "    t_points = np.arange(N) * sampling_interval\n",
    "    target_periods = np.arange(float(plot_period_min), float(plot_period_max) + period_step, period_step)\n",
    "\n",
    "    valid_period_mask = target_periods > 1e-9\n",
    "    if not np.any(valid_period_mask): return pd.DataFrame({'period': [], 'power': []})\n",
    "    final_target_periods = target_periods[valid_period_mask]\n",
    "    target_angular_frequencies = 2 * np.pi / final_target_periods\n",
    "\n",
    "    power_values = lombscargle(t_points, autocorr_windowed, target_angular_frequencies, normalize=False)\n",
    "    return pd.DataFrame({'period': final_target_periods, 'power': power_values})\n",
    "\n",
    "def find_dominant_peak_from_lombscargle_spectrum(\n",
    "    lombscargle_spectrum_df: pd.DataFrame,\n",
    "    metric_period_min: float,\n",
    "    metric_period_max: float,\n",
    "    period_step_in_spectrum: float = 1.0 # Assuming 1bp resolution from input spectrum\n",
    ") -> pd.Series:\n",
    "    default_return = pd.Series({'dominant_period': np.nan, 'dominant_power': np.nan, 'sharpness_q': np.nan})\n",
    "    if lombscargle_spectrum_df.empty: return default_return\n",
    "\n",
    "    metric_range_spectrum = lombscargle_spectrum_df[\n",
    "        (lombscargle_spectrum_df['period'] >= metric_period_min) &\n",
    "        (lombscargle_spectrum_df['period'] <= metric_period_max)\n",
    "    ].copy()\n",
    "\n",
    "    if metric_range_spectrum.empty or metric_range_spectrum['power'].isnull().all() or (metric_range_spectrum['power'] < 1e-9).all():\n",
    "        return default_return\n",
    "\n",
    "    idx_max_power_in_slice = metric_range_spectrum['power'].idxmax()\n",
    "    dominant_period = metric_range_spectrum.loc[idx_max_power_in_slice, 'period']\n",
    "    dominant_power = metric_range_spectrum.loc[idx_max_power_in_slice, 'power']\n",
    "\n",
    "    if dominant_power < 1e-9: return default_return # No significant peak\n",
    "\n",
    "    power_values_in_metric_range = metric_range_spectrum['power'].values\n",
    "    peak_idx_relative_to_slice = np.argmax(power_values_in_metric_range) # int position\n",
    "\n",
    "    widths, _, _, _ = peak_widths( # Using _,_ for unused left_ips, right_ips if not interpolating FWHM from them\n",
    "        power_values_in_metric_range,\n",
    "        [peak_idx_relative_to_slice],\n",
    "        rel_height=0.5\n",
    "    )\n",
    "    sharpness_q = np.nan\n",
    "    if widths.size > 0 and not np.isnan(widths[0]) and widths[0] > 0:\n",
    "        delta_period_fwhm = widths[0] * period_step_in_spectrum # Width in samples * period step\n",
    "        if delta_period_fwhm > 1e-9:\n",
    "            sharpness_q = dominant_period / delta_period_fwhm\n",
    "\n",
    "    return pd.Series({\n",
    "        'dominant_period': dominant_period,\n",
    "        'dominant_power': dominant_power,\n",
    "        'sharpness_q': sharpness_q\n",
    "    })\n",
    "\n",
    "# ───────────────── Core Data Processing Function ─────────────────\n",
    "\n",
    "def _apply_metric_and_spectrum_from_lombscargle(group: pd.DataFrame, min_lag: int, max_lag: int,\n",
    "                                                metric_period_min: float, metric_period_max: float,\n",
    "                                                spectrum_plot_period_min: float, spectrum_plot_period_max: float,\n",
    "                                                spectrum_plot_period_step: float,\n",
    "                                                min_valid_points: int) -> Tuple[pd.Series, Optional[pd.DataFrame]]:\n",
    "    group = group.sort_values('lag')\n",
    "    original_data_in_range = group[\n",
    "        (group['lag'] >= min_lag) & (group['lag'] <= max_lag)\n",
    "    ]['autocorr'].dropna()\n",
    "\n",
    "    default_metrics = pd.Series({'dominant_period': np.nan, 'dominant_power': np.nan, 'sharpness_q': np.nan})\n",
    "    if len(original_data_in_range) < min_valid_points: return default_metrics, None\n",
    "\n",
    "    autocorr_series_for_fft = group.set_index('lag')['autocorr'].reindex(np.arange(min_lag, max_lag + 1))\n",
    "    autocorr_values = autocorr_series_for_fft.fillna(0).values\n",
    "    if np.all(autocorr_values == 0): return default_metrics, None\n",
    "\n",
    "    # 1. Generate the Lomb-Scargle spectrum over the broad plotting range\n",
    "    lombscargle_plot_spectrum_df = extract_power_spectrum_lombscargle(\n",
    "        autocorr_values, 1.0, # sampling_interval = 1.0 bp\n",
    "        spectrum_plot_period_min, spectrum_plot_period_max, spectrum_plot_period_step\n",
    "    )\n",
    "\n",
    "    # 2. Derive dominant peak metrics from this spectrum, focusing on the NUC_PERIOD range\n",
    "    if lombscargle_plot_spectrum_df is not None and not lombscargle_plot_spectrum_df.empty:\n",
    "        metrics_series = find_dominant_peak_from_lombscargle_spectrum(\n",
    "            lombscargle_plot_spectrum_df,\n",
    "            metric_period_min, metric_period_max,\n",
    "            period_step_in_spectrum=spectrum_plot_period_step # Pass the step used\n",
    "        )\n",
    "    else:\n",
    "        metrics_series = default_metrics\n",
    "        lombscargle_plot_spectrum_df = None # Ensure it's None if spectrum calculation failed\n",
    "\n",
    "    return metrics_series, lombscargle_plot_spectrum_df\n",
    "\n",
    "\n",
    "def _process_group_for_lombscargle(args):\n",
    "    \"\"\"\n",
    "    Worker wrapper: runs the metric + spectrum extraction and\n",
    "    captures any exception.\n",
    "    \"\"\"\n",
    "    name, group_df, grouping_cols, min_lag, max_lag, \\\n",
    "    metric_period_min, metric_period_max, \\\n",
    "    spectrum_plot_period_min, spectrum_plot_period_max, spectrum_plot_period_step, \\\n",
    "    min_valid_points = args\n",
    "\n",
    "    try:\n",
    "        metrics_s, spectrum_df = _apply_metric_and_spectrum_from_lombscargle(\n",
    "            group_df,\n",
    "            min_lag, max_lag,\n",
    "            metric_period_min, metric_period_max,\n",
    "            spectrum_plot_period_min, spectrum_plot_period_max,\n",
    "            spectrum_plot_period_step,\n",
    "            min_valid_points\n",
    "        )\n",
    "        return (name, metrics_s, spectrum_df, None)\n",
    "    except Exception as e:\n",
    "        return (name, None, None, e)\n",
    "\n",
    "def generate_all_lombscargle_data(\n",
    "    source_df: pd.DataFrame,\n",
    "    min_lag: int, max_lag: int,\n",
    "    metric_period_min: float, metric_period_max: float,\n",
    "    spectrum_plot_period_min: float, spectrum_plot_period_max: float,\n",
    "    spectrum_plot_period_step: float,\n",
    "    processing_option: int, min_valid_points: int,\n",
    "    n_workers: int = 50\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    grouping_cols = ['read_id', 'condition', 'type', 'chr_type']\n",
    "    if processing_option == 2:\n",
    "        grouping_cols.append('pos_neg')\n",
    "\n",
    "    print(f\"[INFO] Grouping by {grouping_cols} for Lomb-Scargle metrics and spectra extraction...\")\n",
    "    grouped = source_df.groupby(grouping_cols)\n",
    "    group_items = []\n",
    "    for name, group_df in grouped:\n",
    "        group_items.append((name, group_df.copy(), grouping_cols,\n",
    "                            min_lag, max_lag,\n",
    "                            metric_period_min, metric_period_max,\n",
    "                            spectrum_plot_period_min, spectrum_plot_period_max,\n",
    "                            spectrum_plot_period_step,\n",
    "                            min_valid_points))\n",
    "\n",
    "    # Parallel map with tqdm\n",
    "    with mp.Pool(processes=n_workers) as pool:\n",
    "        results = list(tqdm(\n",
    "            pool.imap_unordered(_process_group_for_lombscargle, group_items),\n",
    "            total=len(group_items),\n",
    "            desc=\"Processing Lomb-Scargle per group\"\n",
    "        ))\n",
    "\n",
    "    # Collect results, skipping any that errored\n",
    "    metrics_results_list = []\n",
    "    spectra_results_list: List[pd.DataFrame] = []\n",
    "\n",
    "    for name, metrics_s, spectrum_df, err in results:\n",
    "        if err is not None:\n",
    "            print(f\"[WARN] Error processing group {name}: {err}\")\n",
    "            continue\n",
    "\n",
    "        # build metadata dict\n",
    "        if isinstance(name, tuple):\n",
    "            meta_dict = dict(zip(grouping_cols, name))\n",
    "        else:\n",
    "            meta_dict = {grouping_cols[0]: name}\n",
    "\n",
    "        # append metrics\n",
    "        metrics_results_list.append({**meta_dict, **metrics_s})\n",
    "\n",
    "        # append raw spectra with metadata columns\n",
    "        if spectrum_df is not None and not spectrum_df.empty:\n",
    "            for col, val in meta_dict.items():\n",
    "                spectrum_df[col] = val\n",
    "            spectra_results_list.append(spectrum_df)\n",
    "\n",
    "    # finalize DataFrames\n",
    "    lombscargle_metrics_df = pd.DataFrame(metrics_results_list)\n",
    "    if not lombscargle_metrics_df.empty:\n",
    "        lombscargle_metrics_df.dropna(\n",
    "            subset=['dominant_period', 'dominant_power', 'sharpness_q'],\n",
    "            how='all', inplace=True)\n",
    "\n",
    "    all_spectra_raw_df = (\n",
    "        pd.concat(spectra_results_list, ignore_index=True)\n",
    "        if spectra_results_list else pd.DataFrame()\n",
    "    )\n",
    "\n",
    "    return lombscargle_metrics_df, all_spectra_raw_df\n",
    "\n",
    "# ───────────────── Perform Lomb-Scargle Analysis & Spectra Extraction ─────────────────\n",
    "if 'conditions' not in globals():\n",
    "    print(\"[WARN] 'conditions' list not found, inferring from per_read_ac['condition'].\")\n",
    "    conditions = sorted(list(per_read_ac['condition'].unique()))\n",
    "\n",
    "lombscargle_metrics_df, all_spectra_raw_df = generate_all_lombscargle_data(\n",
    "    per_read_ac,\n",
    "    MIN_LAG, MAX_LAG,\n",
    "    NUC_PERIOD_MIN, NUC_PERIOD_MAX,\n",
    "    SPECTRUM_PLOT_PERIOD_MIN, SPECTRUM_PLOT_PERIOD_MAX,\n",
    "    SPECTRUM_PLOT_PERIOD_STEP,\n",
    "    PROCESSING_OPTION, MIN_VALID_AUTOCORR_POINTS,\n",
    "    n_workers=50\n",
    ")\n",
    "\n",
    "print(f\"[INFO] Lomb-Scargle metrics calculation complete. Shape: {lombscargle_metrics_df.shape}\")\n",
    "if not lombscargle_metrics_df.empty: print(lombscargle_metrics_df.head())\n",
    "else: print(\"[WARN] Lomb-Scargle metrics DataFrame is empty.\")\n",
    "\n",
    "print(f\"[INFO] Raw spectra (Lomb-Scargle) extraction complete. Shape: {all_spectra_raw_df.shape}\")\n",
    "if not all_spectra_raw_df.empty: print(all_spectra_raw_df.head())\n",
    "else: print(\"[WARN] Raw spectra DataFrame is empty.\")\n",
    "\n",
    "# ───────────────── Aggregate Power Spectra ─────────────────\n",
    "aggregated_spectra_df = pd.DataFrame()\n",
    "if not all_spectra_raw_df.empty:\n",
    "    agg_group_cols = ['condition', 'period']\n",
    "    if PROCESSING_OPTION == 2 and 'pos_neg' in all_spectra_raw_df.columns:\n",
    "        agg_group_cols.insert(1, 'pos_neg')\n",
    "\n",
    "    if all(col in all_spectra_raw_df.columns for col in agg_group_cols):\n",
    "        with pd.option_context('mode.chained_assignment', None):\n",
    "            aggregated_spectra_df = all_spectra_raw_df.groupby(agg_group_cols)['power'].agg(['mean', 'sem']).reset_index()\n",
    "        aggregated_spectra_df.rename(columns={'mean': 'mean_power', 'sem': 'sem_power'}, inplace=True)\n",
    "        if 'sem_power' in aggregated_spectra_df.columns:\n",
    "             aggregated_spectra_df['sem_power'] = aggregated_spectra_df['sem_power'].fillna(0)\n",
    "        print(f\"[INFO] Aggregated spectra DataFrame created. Shape: {aggregated_spectra_df.shape}\")\n",
    "        if not aggregated_spectra_df.empty: print(aggregated_spectra_df.head())\n",
    "    else:\n",
    "        print(f\"[WARN] Could not aggregate spectra. Missing one or more grouping cols: {agg_group_cols}\")\n",
    "else:\n",
    "    print(\"[INFO] Skipping spectra aggregation as raw spectra DataFrame is empty.\")\n",
    "\n",
    "# ───────────────── Plotting Functions (definitions assumed from previous correct version) ─────────────────\n",
    "def plot_lombscargle_metric_distribution(df: pd.DataFrame, metric_col: str, title_suffix: str,\n",
    "                                         plot_conditions: list, color_map: dict, p_option: int,\n",
    "                                         y_axis_label: Optional[str] = None): # Renamed for clarity\n",
    "    if df.empty or metric_col not in df.columns or df[metric_col].isnull().all():\n",
    "        print(f\"[WARN] No data to plot for metric: {metric_col} {title_suffix}\")\n",
    "        return\n",
    "    base_title = f\"Distribution of {metric_col.replace('_', ' ').title()}\"\n",
    "    y_label = y_axis_label if y_axis_label else metric_col.replace('_', ' ').title()\n",
    "    plot_items: List[Tuple[str, pd.DataFrame]] = [('whole', df)] if p_option == 1 else \\\n",
    "        ([('neg', df[df['pos_neg'] == 'neg']), ('pos', df[df['pos_neg'] == 'pos'])] if 'pos_neg' in df.columns else [('combined', df)])\n",
    "\n",
    "    for side_label, plot_data in plot_items:\n",
    "        if plot_data.empty:\n",
    "            print(f\"[WARN] No data for side: {side_label} for metric: {metric_col}\")\n",
    "            continue\n",
    "        fig = go.Figure(layout=dict(template=\"plotly_white\"))\n",
    "        valid_data_for_plot = False\n",
    "        for cond in plot_conditions:\n",
    "            subset = plot_data[plot_data['condition'] == cond]\n",
    "            if not subset.empty and not subset[metric_col].isnull().all():\n",
    "                valid_data_for_plot = True\n",
    "                fig.add_trace(go.Violin(y=subset[metric_col], name=cond, box_visible=True, meanline_visible=True,\n",
    "                                      marker_color=color_map.get(cond, '#888888')))\n",
    "        if not valid_data_for_plot:\n",
    "            print(f\"[WARN] No valid data to plot for metric: {metric_col}, side: {side_label} {title_suffix}\")\n",
    "            continue\n",
    "        current_title = f\"{base_title} {title_suffix}\"\n",
    "        if p_option == 2 and side_label != 'combined': current_title = f\"{base_title} ({side_label} side) {title_suffix}\"\n",
    "        fig.update_layout(title=current_title, yaxis_title=y_label, xaxis_title=\"Condition\", showlegend=True,\n",
    "                          legend_title=\"Condition\", width=max(600, 150 * len(plot_conditions)), height=600)\n",
    "        fig.show()\n",
    "        print(f\"[INFO] Displayed violin plot: {current_title}\")\n",
    "\n",
    "def plot_average_power_spectrum(agg_spectra_df: pd.DataFrame, title_suffix: str,\n",
    "                                plot_conditions: list, color_map: dict, p_option: int,\n",
    "                                spectrum_plot_period_min_axis: float, spectrum_plot_period_max_axis: float):\n",
    "    if agg_spectra_df.empty:\n",
    "        print(f\"[WARN] No aggregated spectra data to plot {title_suffix}\")\n",
    "        return\n",
    "    base_title = \"Average Power Spectrum (Lomb-Scargle)\"\n",
    "    y_label, x_label = \"Mean Power (a.u.)\", \"Period (bp)\"\n",
    "    plot_items: List[Tuple[str, pd.DataFrame]] = [('whole', agg_spectra_df)] if p_option == 1 else \\\n",
    "        ([('neg', agg_spectra_df[agg_spectra_df['pos_neg'] == 'neg']), ('pos', agg_spectra_df[agg_spectra_df['pos_neg'] == 'pos'])] if 'pos_neg' in agg_spectra_df.columns else [('combined', agg_spectra_df)])\n",
    "\n",
    "    for side_label, plot_data_side in plot_items:\n",
    "        if plot_data_side.empty:\n",
    "            if p_option == 2 : print(f\"[WARN] No aggregated spectra data for side: {side_label} {title_suffix}\")\n",
    "            elif p_option == 1 : print(f\"[WARN] No aggregated spectra data for {title_suffix}\")\n",
    "            continue\n",
    "        fig = go.Figure(layout=dict(template=\"plotly_white\"))\n",
    "        valid_data_for_plot = False\n",
    "        for cond in plot_conditions:\n",
    "            subset = plot_data_side[plot_data_side['condition'] == cond].sort_values('period')\n",
    "            if not subset.empty and 'mean_power' in subset.columns and 'sem_power' in subset.columns:\n",
    "                valid_data_for_plot = True\n",
    "                subset_sem = subset['sem_power'].fillna(0)\n",
    "                fig.add_trace(go.Scatter(x=subset['period'], y=subset['mean_power'], mode='lines', name=cond,\n",
    "                                       line=dict(color=color_map.get(cond, '#888888'))))\n",
    "                fig.add_trace(go.Scatter(x=np.concatenate([subset['period'], subset['period'][::-1]]),\n",
    "                                       y=np.concatenate([subset['mean_power'] + subset_sem, (subset['mean_power'] - subset_sem)[::-1]]),\n",
    "                                       fill='toself', fillcolor=hex_to_rgba_str(color_map.get(cond, '#888888'), 0.2),\n",
    "                                       line=dict(color='rgba(255,255,255,0)'), hoverinfo=\"skip\", showlegend=False))\n",
    "        if not valid_data_for_plot:\n",
    "            print(f\"[WARN] No valid data to plot for power spectrum, side: {side_label} {title_suffix}\")\n",
    "            continue\n",
    "        current_title = f\"{base_title} {title_suffix}\"\n",
    "        if p_option == 2 and side_label != 'combined': current_title = f\"{base_title} ({side_label} side) {title_suffix}\"\n",
    "        fig.update_layout(title=current_title, yaxis_title=y_label, xaxis_title=x_label,\n",
    "                          xaxis=dict(range=[spectrum_plot_period_min_axis, spectrum_plot_period_max_axis]),\n",
    "                          showlegend=True, legend_title=\"Condition\", width=1000, height=600)\n",
    "        fig.show()\n",
    "        print(f\"[INFO] Displayed power spectrum plot: {current_title}\")\n",
    "\n",
    "# ───────────────── Generate and Display Plots ─────────────────\n",
    "if not lombscargle_metrics_df.empty:\n",
    "    plot_lombscargle_metric_distribution(lombscargle_metrics_df, 'dominant_period', # Changed DataFrame\n",
    "                                 f\"({NUC_PERIOD_MIN}-{NUC_PERIOD_MAX} bp range, Lomb-Scargle based)\", # Updated suffix\n",
    "                                 conditions, COND_COLORS, PROCESSING_OPTION,\n",
    "                                 y_axis_label=\"Dominant Period (bp)\")\n",
    "    plot_lombscargle_metric_distribution(lombscargle_metrics_df, 'dominant_power', # Changed DataFrame\n",
    "                                 f\"(from {NUC_PERIOD_MIN}-{NUC_PERIOD_MAX} bp period range, Lomb-Scargle based)\", # Updated suffix\n",
    "                                 conditions, COND_COLORS, PROCESSING_OPTION,\n",
    "                                 y_axis_label=\"Power of Dominant Frequency (a.u.)\")\n",
    "    plot_lombscargle_metric_distribution(lombscargle_metrics_df, 'sharpness_q', # Changed DataFrame\n",
    "                                 f\"(Q Factor, {NUC_PERIOD_MIN}-{NUC_PERIOD_MAX} bp period range, Lomb-Scargle based)\", # Updated suffix\n",
    "                                 conditions, COND_COLORS, PROCESSING_OPTION,\n",
    "                                 y_axis_label=\"Sharpness (Q Factor)\")\n",
    "else:\n",
    "    print(\"[INFO] Skipping Lomb-Scargle metric distribution plots as metrics DataFrame is empty.\")\n",
    "\n",
    "if not aggregated_spectra_df.empty:\n",
    "    plot_average_power_spectrum(aggregated_spectra_df,\n",
    "                                f\"({SPECTRUM_PLOT_PERIOD_MIN}-{SPECTRUM_PLOT_PERIOD_MAX} bp, {SPECTRUM_PLOT_PERIOD_STEP}bp res.)\",\n",
    "                                conditions, COND_COLORS, PROCESSING_OPTION,\n",
    "                                SPECTRUM_PLOT_PERIOD_MIN, SPECTRUM_PLOT_PERIOD_MAX)\n",
    "else:\n",
    "    print(\"[INFO] Skipping average power spectrum plots as aggregated spectra DataFrame is empty.\")\n",
    "\n",
    "print(f\"[INFO] Cell 3: Analysis and Plotting finished in {(time.time() - t_cell3_start):.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49fbc3411ee91bc",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# CELL 4 — window‑wise dampening metrics & box‑plots\n",
    "###############################################################################\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "from scipy.signal import hilbert\n",
    "\n",
    "\n",
    "# ────────────────── New high‑level toggle ────────────────── #\n",
    "DATAPOINT_UNIT = \"type\"   # \"type\" (current)  |  \"read\" (one point per read)\n",
    "\n",
    "# ─────────────── Metric configuration (unchanged) ───────────────\n",
    "METRIC_OPTION = 5          # 0=max‑min, 1=robust, 2=median, 3=RMS, 4=AUC, 5=hilbert RNS\n",
    "HIGH_PCTL     = 95\n",
    "LOW_PCTL      = 5\n",
    "METRIC_LABELS = {0:\"Max − Min\",1:\"P95 − P5\",2:\"Median Difference\",\n",
    "                 3:\"RMS Difference\",4:\"AUC Difference\", 5:\"Hilbert-RMS\"}\n",
    "\n",
    "# ─────────────── Unchanged user config ───────────────\n",
    "# choose half‑width for the centred window (175 bp total)\n",
    "WINDOW_CENTER_START = 135   # Center for \"n\"\n",
    "BIN_STEP            = 177   # Step size between window centers\n",
    "WINDOW_WIDTH        = 150   # Window width (bp)\n",
    "NUC_REPS            = 5     # Number of windows on each side (set as needed)\n",
    "\n",
    "WIN_HALF = round(BIN_STEP / 2)  # 88 bp\n",
    "\n",
    "# ─────────────── Build df_all at desired grain ───────────────\n",
    "if DATAPOINT_UNIT == \"type\":\n",
    "    # existing per‑type dataframe already prepared in Cell 2\n",
    "    df_all = plot_df_auto.copy()\n",
    "else:  # \"read\" → regenerate per‑read smoothed autocorr\n",
    "    df_all = per_read_ac.copy()\n",
    "    df_all['autocorr_smooth'] = (\n",
    "        df_all.groupby(['read_id', 'pos_neg'])['autocorr']\n",
    "              .transform(final_smooth)\n",
    "    )\n",
    "    # retain *type* for optional future use\n",
    "    # (no aggregation → one row per read/lag already)\n",
    "\n",
    "# common lag re‑mapping\n",
    "if PROCESSING_OPTION == 2:\n",
    "    df_all['plot_lag'] = np.where(df_all['pos_neg'] == 'neg',\n",
    "                                  -df_all['lag'],\n",
    "                                   df_all['lag'])\n",
    "else:\n",
    "    df_all['plot_lag'] = df_all['lag']\n",
    "df_all = df_all[df_all['plot_lag'].between(-MAX_LAG, MAX_LAG)].reset_index(drop=True)\n",
    "\n",
    "# ─────────────── Build sliding windows (unchanged) ───────────────\n",
    "window_defs = []\n",
    "\n",
    "if PROCESSING_OPTION == 1:\n",
    "    # Only the positive side: n+1 to n+NUC_REPS\n",
    "    for i in range(1, NUC_REPS + 1):\n",
    "        center = WINDOW_CENTER_START + (i - 1) * BIN_STEP\n",
    "        win_start = center - WINDOW_WIDTH // 2\n",
    "        win_end = center + WINDOW_WIDTH // 2\n",
    "        label = f\"n+{i}\"\n",
    "        window_defs.append({\n",
    "            'center': center,\n",
    "            'start': win_start,\n",
    "            'end': win_end,\n",
    "            'label': label\n",
    "        })\n",
    "else:\n",
    "    # Negative side: n-NUC_REPS to n-1 (left of zero)\n",
    "    for i in range(NUC_REPS, 0, -1):  # n-5, ..., n-1\n",
    "        center = -WINDOW_CENTER_START - (i - 1) * BIN_STEP\n",
    "        win_start = center - WINDOW_WIDTH // 2\n",
    "        win_end = center + WINDOW_WIDTH // 2\n",
    "        label = f\"n-{i}\"\n",
    "        window_defs.append({\n",
    "            'center': center,\n",
    "            'start': win_start,\n",
    "            'end': win_end,\n",
    "            'label': label\n",
    "        })\n",
    "    # Positive side: n+1 to n+NUC_REPS (right of zero)\n",
    "    for i in range(1, NUC_REPS + 1):  # n+1, ..., n+5\n",
    "        center = WINDOW_CENTER_START + (i - 1) * BIN_STEP\n",
    "        win_start = center - WINDOW_WIDTH // 2\n",
    "        win_end = center + WINDOW_WIDTH // 2\n",
    "        label = f\"n+{i}\"\n",
    "        window_defs.append({\n",
    "            'center': center,\n",
    "            'start': win_start,\n",
    "            'end': win_end,\n",
    "            'label': label\n",
    "        })\n",
    "\n",
    "# Sort by center position for plotting/order\n",
    "window_defs = sorted(window_defs, key=lambda w: w['center'])\n",
    "ordered_windows = [w['label'] for w in window_defs]\n",
    "\n",
    "\n",
    "\n",
    "# ─────────────── Compute metric per window  (vectorised) ───────────────\n",
    "records = []\n",
    "\n",
    "# 1.  Build a long dataframe that tags each row with its window + part\n",
    "# For each window, tag all points within the window\n",
    "parts = []\n",
    "key_cols = ['condition', 'type'] if DATAPOINT_UNIT == \"type\" else ['condition', 'read_id', 'type']\n",
    "\n",
    "for w in window_defs:\n",
    "    mask = df_all['plot_lag'].between(w['start'], w['end'])\n",
    "    if mask.any():\n",
    "        tmp = df_all.loc[mask, key_cols + ['autocorr_smooth','plot_lag']].copy()\n",
    "        tmp['window'] = w['label']\n",
    "        parts.append(tmp)\n",
    "\n",
    "long_df = pd.concat(parts, ignore_index=True)\n",
    "\n",
    "\n",
    "# 2.  Aggregate once → pivot so ‘peak’ and ‘trough’ are columns\n",
    "if METRIC_OPTION == 0:\n",
    "    wide = (\n",
    "        long_df\n",
    "        .groupby(key_cols + ['window'])['autocorr_smooth']\n",
    "        .agg(['max', 'min'])\n",
    "        .dropna(subset=['max', 'min'])\n",
    "    )\n",
    "    diff_vals = wide['max'] - wide['min']\n",
    "\n",
    "elif METRIC_OPTION == 1:\n",
    "    q = (\n",
    "        long_df\n",
    "        .groupby(key_cols + ['window'])['autocorr_smooth']\n",
    "        .quantile([LOW_PCTL/100, HIGH_PCTL/100])\n",
    "        .unstack(-1)\n",
    "        .rename(columns={LOW_PCTL/100: f'q{LOW_PCTL}', HIGH_PCTL/100: f'q{HIGH_PCTL}'})\n",
    "        .dropna(subset=[f'q{LOW_PCTL}', f'q{HIGH_PCTL}'])\n",
    "    )\n",
    "    diff_vals = q[f'q{HIGH_PCTL}'] - q[f'q{LOW_PCTL}']\n",
    "\n",
    "elif METRIC_OPTION == 2:\n",
    "    median = (\n",
    "        long_df\n",
    "        .groupby(key_cols + ['window'])['autocorr_smooth']\n",
    "        .median()\n",
    "    )\n",
    "    diff_vals = median  # Or: median - median, or just use as summary\n",
    "\n",
    "\n",
    "elif METRIC_OPTION == 3:\n",
    "    rms = (\n",
    "        long_df\n",
    "        .groupby(key_cols + ['window'])['autocorr_smooth']\n",
    "        .apply(lambda x: np.sqrt((x**2).mean()))\n",
    "    )\n",
    "    diff_vals = rms\n",
    "\n",
    "\n",
    "elif METRIC_OPTION == 4:\n",
    "    auc = (\n",
    "        long_df\n",
    "        .set_index('plot_lag', append=True)\n",
    "        .groupby(key_cols + ['window'])['autocorr_smooth']\n",
    "        .apply(lambda x: np.trapz(x.values, x.index.get_level_values('plot_lag')))\n",
    "    )\n",
    "    diff_vals = auc\n",
    "\n",
    "\n",
    "elif METRIC_OPTION == 5:\n",
    "    from scipy.signal import hilbert\n",
    "    hilbert_rms = (\n",
    "        long_df\n",
    "        .groupby(key_cols + ['window'])['autocorr_smooth']\n",
    "        .apply(lambda x: np.sqrt(np.mean(np.abs(hilbert(x.values))**2)))\n",
    "    )\n",
    "    diff_vals = hilbert_rms\n",
    "\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Unsupported option in fast path.\")\n",
    "\n",
    "# 3.  Re‑assemble diff_df from the differences\n",
    "diff_df = (\n",
    "    diff_vals\n",
    "    .rename('diff')\n",
    "    .reset_index()\n",
    "    .merge(\n",
    "        pd.DataFrame(window_defs),\n",
    "        left_on='window', right_on='label',\n",
    "        how='left'\n",
    "    )\n",
    ")\n",
    "\n",
    "diff_df['window'] = pd.Categorical(diff_df['window'], categories=ordered_windows, ordered=True)\n",
    "\n",
    "\n",
    "# ─────────────── Combined box‑plot across conditions ───────────────\n",
    "from plotly.subplots import make_subplots\n",
    "# 1. compute per‐window median and IQR\n",
    "stats = (\n",
    "    diff_df\n",
    "    .groupby(['window','condition'])['diff']\n",
    "    .agg(\n",
    "        q1=lambda x: np.percentile(x, 25),\n",
    "        median='median',\n",
    "        q3=lambda x: np.percentile(x, 75)\n",
    "    )\n",
    ")\n",
    "# pivot into wide form\n",
    "stats_wide = stats.unstack('condition')\n",
    "median_df = stats_wide['median']\n",
    "q1_df     = stats_wide['q1']\n",
    "q3_df     = stats_wide['q3']\n",
    "\n",
    "first_cond = conditions[0]\n",
    "# 2. compute relative median and IQR with respect to the first condition\n",
    "rel_median = median_df.subtract(median_df[first_cond], axis=0)\n",
    "rel_q1     = q1_df    .subtract(median_df[first_cond], axis=0)\n",
    "rel_q3     = q3_df    .subtract(median_df[first_cond], axis=0)\n",
    "\n",
    "# 3. rebuild the 2‐row subplot (top row unchanged)\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=1,\n",
    "    shared_xaxes=True,\n",
    "    vertical_spacing=0.02,\n",
    "    row_heights=[0.8, 0.2],\n",
    "    subplot_titles=[\n",
    "        f\"{METRIC_LABELS[METRIC_OPTION]} per 175 bp Window\",\n",
    "        f\"Median Δ vs. {first_cond} (with IQR)\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# --- top row: your existing box‐plots ---\n",
    "for cond in conditions:\n",
    "    sub = diff_df[diff_df['condition'] == cond]\n",
    "    jitter_on = len(sub) <= 10_000\n",
    "    fig.add_trace(\n",
    "        go.Box(\n",
    "            x=sub['window'], y=sub['diff'], name=cond,\n",
    "            marker_color=COND_COLORS[cond],\n",
    "            boxpoints='all' if jitter_on else 'outliers',\n",
    "            jitter=0.3 if jitter_on else 0,\n",
    "            marker_size=4 if jitter_on else 3\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "# --- bottom row: line + asymmetric IQR error bars ---\n",
    "# zero‐baseline line\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=rel_median.index, y=[0]*len(rel_median),\n",
    "        mode='lines', line=dict(color='black', dash='dash'),\n",
    "        showlegend=False\n",
    "    ),\n",
    "    row=2, col=1\n",
    ")\n",
    "for cond in conditions[1:]:\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=rel_median.index,\n",
    "            y=rel_median[cond],\n",
    "            mode='lines+markers',\n",
    "            name=f\"{cond} − {first_cond}\",\n",
    "            line=dict(color=COND_COLORS[cond]),\n",
    "            error_y=dict(\n",
    "                type='data',\n",
    "                symmetric=False,\n",
    "                array= rel_q3[cond] - rel_median[cond],      # upper bar\n",
    "                arrayminus= rel_median[cond] - rel_q1[cond]  # lower bar\n",
    "            )\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "\n",
    "# 4. axes & layout tweaks\n",
    "fig.update_xaxes(categoryorder='array', categoryarray=ordered_windows, row=1, col=1)\n",
    "fig.update_xaxes(categoryorder='array', categoryarray=ordered_windows, row=2, col=1)\n",
    "fig.update_yaxes(title_text=METRIC_LABELS[METRIC_OPTION], row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Δ Median (IQR)\", row=2, col=1)\n",
    "\n",
    "fig.update_layout(\n",
    "    template='plotly_white',\n",
    "    width=900, height=800,\n",
    "    boxmode='group',\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "#\n",
    "#\n",
    "# # ─────────────── Faceted box‑plots per condition ───────────────\n",
    "# n = len(conditions)\n",
    "# fig_sub = make_subplots(rows=1, cols=n, shared_yaxes=True,\n",
    "#                         subplot_titles=conditions, horizontal_spacing=0.03)\n",
    "# for i, cond in enumerate(conditions, start=1):\n",
    "#     sub = diff_df[diff_df['condition'] == cond]\n",
    "#     jitter_on = len(sub) <= 10_000\n",
    "#     fig_sub.add_trace(\n",
    "#         go.Box(x=sub['window'], y=sub['diff'],\n",
    "#                marker_color=COND_COLORS[cond],\n",
    "#                boxpoints='all' if jitter_on else 'outliers',\n",
    "#                jitter=0.3 if jitter_on else 0,\n",
    "#                marker_size=4 if jitter_on else 3,\n",
    "#                showlegend=False),\n",
    "#         row=1, col=i\n",
    "#     )\n",
    "#     fig_sub.update_xaxes(categoryorder='array',\n",
    "#                          categoryarray=ordered_windows, row=1, col=i)\n",
    "# fig_sub.update_layout(\n",
    "#     title=(f\"{METRIC_LABELS[METRIC_OPTION]} per Condition\"\n",
    "#            + (\" (auto‑flipped)\" if AUTO_FLIP and PROCESSING_OPTION != 1 else \"\")\n",
    "#            + (f\" – one point per {DATAPOINT_UNIT}\" if DATAPOINT_UNIT==\"read\" else \"\")),\n",
    "#     xaxis_title=\"Window span (bp)\",\n",
    "#     yaxis_title=METRIC_LABELS[METRIC_OPTION],\n",
    "#     width=300 * n, height=600,\n",
    "#     template='plotly_white'\n",
    "# )\n",
    "# fig_sub.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ae7fe4ea70e677",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL 5 — subtraction of baseline (condition1) vs others per sliding windows\n",
    "#            matching Cell 3’s binning\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# ─────────────── Bring in df_all from Cell 2’s plot_df_auto ───────────────\n",
    "if PROCESSING_OPTION == 2:\n",
    "    neg = plot_df_auto[plot_df_auto['pos_neg']=='neg'].copy()\n",
    "    neg['plot_lag'] = -neg['lag']\n",
    "    pos = plot_df_auto[plot_df_auto['pos_neg']=='pos'].copy()\n",
    "    pos['plot_lag'] = pos['lag']\n",
    "    df_all = pd.concat([neg, pos], ignore_index=True)\n",
    "else:\n",
    "    df_all = plot_df_auto.copy()\n",
    "    df_all['plot_lag'] = df_all['lag']\n",
    "\n",
    "# ─────────────── Configuration ───────────────\n",
    "BIN_STEP = 173    # same as in Cell 3\n",
    "# offsets for the two half‐windows\n",
    "MIN_OFF, MAX_OFF = (80, 119), (160, 199)\n",
    "\n",
    "# ─────────────── Build sliding windows ───────────────\n",
    "window_defs = []\n",
    "k = 0\n",
    "while True:\n",
    "    # positive‐side min/max\n",
    "    min_s = MIN_OFF[0] + BIN_STEP * k\n",
    "    min_e = MIN_OFF[1] + BIN_STEP * k\n",
    "    max_s = MAX_OFF[0] + BIN_STEP * k\n",
    "    max_e = MAX_OFF[1] + BIN_STEP * k\n",
    "    if max_e > MAX_LAG:\n",
    "        break\n",
    "\n",
    "    window_defs.append({\n",
    "        'side':      'pos',\n",
    "        'min_range': (min_s, min_e),\n",
    "        'max_range': (max_s, max_e),\n",
    "        'span':      (min_s, max_e)\n",
    "    })\n",
    "    if PROCESSING_OPTION != 1:\n",
    "        window_defs.append({\n",
    "            'side':      'neg',\n",
    "            'min_range': (-min_e, -min_s),\n",
    "            'max_range': (-max_e, -max_s),\n",
    "            'span':      (-max_e, -min_s)\n",
    "        })\n",
    "\n",
    "    k += 1\n",
    "\n",
    "# sort by numeric start of each span, and label\n",
    "window_defs = sorted(window_defs, key=lambda w: w['span'][0])\n",
    "for w in window_defs:\n",
    "    w['label'] = f\"{int(w['span'][0])} to {int(w['span'][1])}\"\n",
    "\n",
    "# ─────────────── Compute max−min per condition/type/window ───────────────\n",
    "records = []\n",
    "for w in window_defs:\n",
    "    sub_min = df_all[df_all['plot_lag'].between(*w['min_range'])]\n",
    "    sub_max = df_all[df_all['plot_lag'].between(*w['max_range'])]\n",
    "    if sub_min.empty or sub_max.empty:\n",
    "        continue\n",
    "\n",
    "    gm = sub_max.groupby(['condition','type'])['autocorr_smooth'].max()\n",
    "    gn = sub_min.groupby(['condition','type'])['autocorr_smooth'].min()\n",
    "    diff = (gm - gn).reset_index(name='range_val')\n",
    "    diff['window'] = w['label']\n",
    "    records.append(diff)\n",
    "\n",
    "range_df = pd.concat(records, ignore_index=True)\n",
    "\n",
    "# ─────────────── Prepare subtraction DataFrame ───────────────\n",
    "baseline    = conditions[0]\n",
    "other_conds = conditions[1:]\n",
    "\n",
    "pivot = range_df.pivot_table(\n",
    "    index=['type','window'],\n",
    "    columns='condition',\n",
    "    values='range_val'\n",
    ")\n",
    "\n",
    "sub_records = []\n",
    "for (typ, win), row in pivot.iterrows():\n",
    "    base_val = row.get(baseline, np.nan)\n",
    "    for cond in other_conds:\n",
    "        val = row.get(cond, np.nan)\n",
    "        if not np.isnan(base_val) and not np.isnan(val):\n",
    "            sub_records.append({\n",
    "                'type':        typ,\n",
    "                'window':      win,\n",
    "                'subtraction': f\"{baseline} − {cond}\",\n",
    "                'diff':        base_val - val\n",
    "            })\n",
    "\n",
    "sub_df = pd.DataFrame(sub_records)\n",
    "\n",
    "# ─────────────── Combined box‑plot of subtractions ───────────────\n",
    "fig_comb = go.Figure(layout=dict(template='plotly_white'))\n",
    "for cond in other_conds:\n",
    "    label = f\"{baseline} − {cond}\"\n",
    "    sub   = sub_df[sub_df['subtraction']==label]\n",
    "    fig_comb.add_trace(go.Box(\n",
    "        x=sub['window'], y=sub['diff'],\n",
    "        name=label,\n",
    "        marker_color=COND_COLORS[cond],\n",
    "        boxpoints='all', jitter=0.3\n",
    "    ))\n",
    "\n",
    "fig_comb.update_layout(\n",
    "    title=f\"Range Difference per Window: {baseline} minus Others\",\n",
    "    xaxis_title=\"Window span (bp)\",\n",
    "    yaxis_title=\"(Max–Min)₍baseline₎ − (Max–Min)₍cond₎\",\n",
    "    boxmode='group',\n",
    "    width=1200, height=600\n",
    ")\n",
    "fig_comb.update_xaxes(\n",
    "    categoryorder='array',\n",
    "    categoryarray=[w['label'] for w in window_defs]\n",
    ")\n",
    "fig_comb.show()\n",
    "\n",
    "# ─────────────── Subplots per subtraction pair ───────────────\n",
    "subs = [f\"{baseline} − {c}\" for c in other_conds]\n",
    "n    = len(subs)\n",
    "fig_sub = make_subplots(\n",
    "    rows=1, cols=n,\n",
    "    shared_yaxes=True,\n",
    "    subplot_titles=subs,\n",
    "    horizontal_spacing=0.03\n",
    ")\n",
    "\n",
    "for i, label in enumerate(subs, start=1):\n",
    "    sub  = sub_df[sub_df['subtraction']==label]\n",
    "    cond = label.split(' − ')[1]\n",
    "    fig_sub.add_trace(\n",
    "        go.Box(\n",
    "            x=sub['window'], y=sub['diff'],\n",
    "            name=label,\n",
    "            marker_color=COND_COLORS[cond],\n",
    "            boxpoints='all', jitter=0.3,\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=1, col=i\n",
    "    )\n",
    "    fig_sub.update_xaxes(\n",
    "        categoryorder='array',\n",
    "        categoryarray=[w['label'] for w in window_defs],\n",
    "        row=1, col=i\n",
    "    )\n",
    "\n",
    "fig_sub.update_layout(\n",
    "    title=f\"Per‑window Range Difference: {baseline} minus Each Condition\",\n",
    "    xaxis_title=\"Window span (bp)\",\n",
    "    yaxis_title=\"Difference\",\n",
    "    width=300 * n, height=600\n",
    ")\n",
    "fig_sub.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a692b607f5ddae5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# COLOR_LOOKUP        = {          # user palette\n",
    "#     CONDITIONS_TO_PLOT[0]: \"#1F78B4\", # blue\n",
    "#     CONDITIONS_TO_PLOT[1]: \"#E31A1C\", # red\n",
    "# }\n",
    "\n",
    "# COLOR_LOOKUP        = {          # user palette\n",
    "#     CONDITIONS_TO_PLOT[0]: \"#E31A1C\", # red\n",
    "#     CONDITIONS_TO_PLOT[1]: \"#51ab4d\", # green\n",
    "# }\n",
    "# \n",
    "COLOR_LOOKUP        = {          # user palette\n",
    "    CONDITIONS_TO_PLOT[0]: \"#1F78B4\", # blue\n",
    "    CONDITIONS_TO_PLOT[1]: \"#51ab4d\", # green\n",
    "}\n",
    "\n",
    "# ───────────────────────────── 4.  Plotly figure ────────────────────────────\n",
    "fig = go.Figure(layout=dict(template=\"plotly_white\"))\n",
    "for cond in CONDITIONS_TO_PLOT:\n",
    "    sub = plot_df_auto[plot_df_auto[\"condition\"] == cond]\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=sub[\"lag\"],\n",
    "            y=sub[\"autocorr_smooth\"],\n",
    "            mode=\"lines\",\n",
    "            name=cond,\n",
    "            line=dict(color=COLOR_LOOKUP.get(cond, None)),\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Read‑level Autocorrelation\",\n",
    "    xaxis_title=\"Lag (bp)\",\n",
    "    yaxis_title=\"Mean autocorrelation\",\n",
    "    legend_title=\"Condition\",\n",
    "    # set x axis range min to 20\n",
    "    xaxis=dict(range=[20, LAG_RANGE]),\n",
    "    yaxis=dict(range=[-0.025, 0.07]),\n",
    "    width=900,\n",
    "    height=600,\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# ───────────────────────── QUICK‑DIAGNOSTIC SNIPPET ─────────────────────────\n",
    "print(\"\\n─ BASIC SHAPES ─\")\n",
    "print(\"per_read_ac :\", per_read_ac.shape)\n",
    "print(\"plot_df_auto     :\", plot_df_auto.shape)\n",
    "\n",
    "print(\"\\n─ UNIQUE CONDITIONS IN per_read_ac ─\")\n",
    "print(per_read_ac[\"condition\"].value_counts(dropna=False).head(10))\n",
    "\n",
    "print(\"\\n─ UNIQUE CONDITIONS IN plot_df_auto ─\")\n",
    "print(plot_df_auto[\"condition\"].value_counts(dropna=False))\n",
    "\n",
    "print(\"\\n─ CONDITIONS_TO_PLOT VS plot_df_auto ─\")\n",
    "print(\"Missing in plot_df_auto:\",\n",
    "      set(CONDITIONS_TO_PLOT) - set(plot_df_auto[\"condition\"].unique()))\n",
    "\n",
    "print(\"\\n─ per_read_ac autocorr stats by condition (lag 0 only) ─\")\n",
    "print(\n",
    "    per_read_ac[per_read_ac[\"lag\"] == 0]\n",
    "      .groupby(\"condition\")[\"autocorr\"]\n",
    "      .agg([\"count\", \"min\", \"max\", \"mean\"])\n",
    "      .head(10)\n",
    ")\n",
    "\n",
    "print(\"\\n─ NaN fraction in per_read_ac['autocorr'] ─\")\n",
    "print(per_read_ac[\"autocorr\"].isna().mean().round(3))\n",
    "\n",
    "print(\"\\n─ First few rows of plot_df_auto ─\")\n",
    "print(plot_df_auto.head())\n",
    "\n",
    "print(\"\\n─ Check smoothed values ─\")\n",
    "print(plot_df_auto[\"autocorr_smooth\"].describe())\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cd473784e72800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reimport nanotools\n",
    "import importlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.signal import find_peaks\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Assuming 'nanotools' is a module you have and 'down_sampled_plot_df', 'down_sampled_group_df', etc. are defined\n",
    "# importlib.reload(nanotools)\n",
    "\n",
    "def create_plot(plot_df, group_df, condition, chr_type, data_type, plot_window, plot_motifs=True):\n",
    "    print(\"Creating dataframes...\")\n",
    "    plot_df_copy = plot_df.copy(deep=True)\n",
    "    plot_df_copy = plot_df_copy[(plot_df_copy['condition'] == condition) &\n",
    "                                (plot_df_copy['chr_type'] == chr_type) &\n",
    "                                (plot_df_copy['type'] == data_type) &\n",
    "                                (plot_df_copy['rel_pos'] > -plot_window) &\n",
    "                                (plot_df_copy['rel_pos'] < plot_window)]\n",
    "\n",
    "    # Drop rows where both smallest_positive_nuc_midpoint and greatest_negative_nuc_midpoint are NaN\n",
    "    plot_df_copy = plot_df_copy[~(plot_df_copy['smallest_positive_nuc_midpoint'].isna() & plot_df_copy['greatest_negative_nuc_midpoint'].isna())]\n",
    "    plot_df_copy = plot_df_copy.sort_values(by=['smallest_positive_nuc_midpoint', 'greatest_negative_nuc_midpoint'], ascending=[True, False])\n",
    "    plot_df_copy_nodups = plot_df_copy.drop_duplicates(subset=['read_id'])[['read_id', 'smallest_positive_nuc_midpoint', 'greatest_negative_nuc_midpoint']]\n",
    "    plot_df_copy.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    plot_df_copy_nodups.reset_index(inplace=True, drop=True)\n",
    "    # Use ngroup to create an incrementing column in ascending order\n",
    "    plot_df_copy_nodups['read_count'] = range(1, len(plot_df_copy_nodups) + 1)\n",
    "\n",
    "    # Merge the read_count column back into plot_df_copy\n",
    "    plot_df_copy = pd.merge(plot_df_copy, plot_df_copy_nodups[['read_id', 'read_count']], on='read_id', how='left')\n",
    "\n",
    "    # Drop rows from down_sampled_group_df_copy where read_id not in plot_df_copy read_ids\n",
    "    down_sampled_group_df_copy = group_df.copy(deep=True)\n",
    "    down_sampled_group_df_copy = down_sampled_group_df_copy[down_sampled_group_df_copy['read_id'].isin(plot_df_copy_nodups['read_id'])]\n",
    "    # Merge read_count column from plot_df_copy_no_dups with down_sampled_group_df_copy on read_id\n",
    "    down_sampled_group_df_copy = pd.merge(down_sampled_group_df_copy, plot_df_copy_nodups[['read_id', 'read_count']], on='read_id', how='left')\n",
    "    # Drop rows where nucs_list is NaN\n",
    "    down_sampled_group_df_copy.dropna(subset=['nucs_list'], inplace=True)\n",
    "    # nanotools.display_sample_rows(down_sampled_group_df_copy, 10)  # Uncomment if needed\n",
    "\n",
    "    # Initialize a numpy array with zeros for each base pair in the genome region\n",
    "    genome_size = 2 * plot_window\n",
    "\n",
    "    # Initialize read_counts for occupancy calculation\n",
    "    read_counts = np.zeros(genome_size)\n",
    "\n",
    "    # Calculate sum and count of mod_qual_bin at each rel_pos\n",
    "    agg_df = plot_df_copy.groupby('rel_pos')['mod_qual_bin'].agg(['sum', 'count']).reset_index()\n",
    "    agg_df['ratio'] = agg_df['sum'] / agg_df['count']\n",
    "    # Calculate the moving average of the ratio with a centered window\n",
    "    rolling_window_size = 50\n",
    "    agg_df['moving_avg'] = agg_df['ratio'].rolling(window=rolling_window_size, center=True).mean()\n",
    "    # Drop NaN values\n",
    "    agg_df.dropna(inplace=True)\n",
    "\n",
    "    print(\"Adding m6A line traces...\")\n",
    "    # Prepare data for read plot (Figure 1)\n",
    "    fig1 = make_subplots(rows=1, cols=1)\n",
    "    fig1.update_xaxes(range=[-plot_window, plot_window])\n",
    "\n",
    "    # For calculating occupancy\n",
    "    for read_id in plot_df_copy['read_count'].unique():\n",
    "        read_data = plot_df_copy[plot_df_copy['read_count'] == read_id]\n",
    "        min_rel_pos = read_data['rel_pos'].min()\n",
    "        max_rel_pos = read_data['rel_pos'].max()\n",
    "\n",
    "        for pos in range(int(min_rel_pos + plot_window), int(max_rel_pos + plot_window + 1)):\n",
    "            if 0 <= pos < genome_size:\n",
    "                read_counts[pos] += 1\n",
    "\n",
    "        fig1.add_trace(\n",
    "            go.Scatter(x=[min_rel_pos, max_rel_pos],\n",
    "                       y=[read_data['read_count'].iloc[0], read_data['read_count'].iloc[0]],\n",
    "                       mode='lines', line=dict(color='#000000', width=0.2), showlegend=False),\n",
    "            row=1, col=1\n",
    "        )\n",
    "\n",
    "    print(\"Plotting nucleosomes on read plot...\")\n",
    "    # Plot nucleosomes on read plot\n",
    "    midpoints_list = []\n",
    "    x_coords = []\n",
    "    y_coords = []\n",
    "\n",
    "    for read_id in down_sampled_group_df_copy['read_count']:\n",
    "        read_data = down_sampled_group_df_copy[down_sampled_group_df_copy['read_count'] == read_id]\n",
    "        read_height = read_data['read_count'].iloc[0]\n",
    "        # Drop nucs from nucs_list that are outside of plot_window\n",
    "        read_data['nucs_list'] = read_data['nucs_list'].apply(lambda x: [nuc for nuc in x if nuc >= -plot_window and nuc <= plot_window])\n",
    "\n",
    "        for nuc in read_data['nucs_list'].iloc[0]:\n",
    "            NUC_width = 147\n",
    "            midpoints_list.append(nuc)\n",
    "            min_rel_pos = nuc - NUC_width / 2\n",
    "            max_rel_pos = nuc + NUC_width / 2\n",
    "\n",
    "            x_coords.extend([min_rel_pos, max_rel_pos, None])  # Use None to separate individual line segments\n",
    "            y_coords.extend([read_height, read_height, None])\n",
    "\n",
    "    # Create a new column for marker colors based on 'mod_qual' values\n",
    "    plot_df_copy['marker_color'] = plot_df_copy['mod_qual'].apply(\n",
    "        lambda x: '#FF0000' if x >= 0.85 else 'rgba(0,0,0,0)'  # Red color for >=0.85, transparent otherwise\n",
    "    )\n",
    "\n",
    "    # Upper scatter plot\n",
    "    scatter_trace = go.Scatter(\n",
    "        x=plot_df_copy['rel_pos'],\n",
    "        y=plot_df_copy['read_count'],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=4,  # Increase the size of the dots\n",
    "            color='rgba(0,0,0,0)',  # No fill color\n",
    "            line=dict(\n",
    "                width=1,  # Border width\n",
    "                color=plot_df_copy['marker_color']  # Set border color to the same as 'marker_color'\n",
    "            ),\n",
    "        ),\n",
    "        name='Read Count'\n",
    "    )\n",
    "    fig1.add_trace(scatter_trace, row=1, col=1)\n",
    "\n",
    "    # Add a single trace for all nucleosome line segments\n",
    "    fig1.add_trace(\n",
    "        go.Scatter(\n",
    "            x=x_coords,\n",
    "            y=y_coords,\n",
    "            mode='lines',\n",
    "            line=dict(color='rgba(51,0,141,45)', width=2),\n",
    "            opacity=0.75,\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1\n",
    "    )\n",
    "\n",
    "    # # Add Rex Line to fig1\n",
    "    # fig1.add_shape(\n",
    "    #     go.layout.Shape(\n",
    "    #         type=\"line\",\n",
    "    #         x0=0,\n",
    "    #         x1=0,\n",
    "    #         y0=0,\n",
    "    #         y1=1,\n",
    "    #         yref=\"paper\",\n",
    "    #         line=dict(\n",
    "    #             color=\"grey\",\n",
    "    #             width=1,\n",
    "    #             dash=\"dash\",\n",
    "    #         )\n",
    "    #     )\n",
    "    # )\n",
    "\n",
    "    # Plot motifs if plot_motifs is True\n",
    "    # Update layout for fig1\n",
    "    fig1.update_layout(template=\"simple_white\",\n",
    "                       height=300,  # Adjusted to maintain relative dimensions\n",
    "                       width=1000,\n",
    "                       )\n",
    "    fig1.update_yaxes(title_text=\"Read_ID\", row=1, col=1)\n",
    "    fig1.update_xaxes(title_text=\"Genomic location (bp)\", row=1, col=1)\n",
    "\n",
    "    # Create Figure 2 for the other plots\n",
    "    print(\"Creating Figure 2 for the other plots...\")\n",
    "    fig2 = make_subplots(rows=2,\n",
    "                         cols=1,\n",
    "                         shared_xaxes=True,\n",
    "                         vertical_spacing=0.02,\n",
    "                         specs=[[{}], [{\"secondary_y\": True}]],\n",
    "                         row_heights=[0.25, 0.25])\n",
    "\n",
    "    fig2.update_xaxes(range=[-plot_window, plot_window])\n",
    "\n",
    "    # Lower line plot for moving average of the ratio\n",
    "    line_trace = go.Scatter(x=agg_df['rel_pos'], y=agg_df['moving_avg'], mode='lines',\n",
    "                            line=dict(color='#FF0000', width=2),\n",
    "                            line_shape='spline')\n",
    "    fig2.add_trace(line_trace, row=1, col=1)\n",
    "\n",
    "    print(\"Plotting histogram and smoothed density on Figure 2...\")\n",
    "    # Histogram of nucleosome midpoints\n",
    "    rolling_window_size_hist = 20\n",
    "    hist_bins = int(round(2 * plot_window / 10) + 1)\n",
    "    midpoint_histogram = go.Histogram(x=midpoints_list,\n",
    "                                      nbinsx=hist_bins,\n",
    "                                      marker=dict(color='rgba(51,0,141,45)', opacity=0.6)\n",
    "                                      )\n",
    "    fig2.add_trace(midpoint_histogram, row=2, col=1, secondary_y=False)\n",
    "\n",
    "    # Assume midpoints_list contains midpoints of nucleosomes for the current plot\n",
    "    nucleosome_array = np.zeros(genome_size)\n",
    "\n",
    "    # Populate the nucleosome_array based on the midpoints\n",
    "    for midpoint in midpoints_list:\n",
    "        position_index = int(midpoint + plot_window)\n",
    "        if 0 <= position_index < genome_size:\n",
    "            nucleosome_array[position_index] += 1\n",
    "\n",
    "    nucleosome_normalized = np.divide(nucleosome_array, read_counts, where=read_counts != 0)\n",
    "\n",
    "    # Apply Gaussian smoothing with a standard deviation of 20 base pairs\n",
    "    smoothed_nucleosome_array = gaussian_filter1d(nucleosome_normalized, 10)\n",
    "\n",
    "    # Generate x values for the smoothed density plot\n",
    "    x_values = np.arange(-plot_window, plot_window, 1)\n",
    "\n",
    "    # Add the smoothed nucleosome density as a line trace to the second subplot\n",
    "    smoothed_trace = go.Scatter(\n",
    "        x=x_values,\n",
    "        y=smoothed_nucleosome_array,\n",
    "        mode='lines',\n",
    "        name='Smoothed Nucleosome Density',\n",
    "        line=dict(color='rgba(51,0,141,45)', width=2),\n",
    "    )\n",
    "    fig2.add_trace(smoothed_trace, row=2, col=1, secondary_y=True)\n",
    "\n",
    "    # Adjust x-axis labels to appear only on the lower plot of fig2\n",
    "    fig2.update_xaxes(showticklabels=False, row=1, col=1)\n",
    "    fig2.update_xaxes(title_text=\"Genomic location (bp)\", row=2, col=1)\n",
    "\n",
    "    # Update y-axes titles for fig2\n",
    "    fig2.update_yaxes(title_text=\"% m6A\", row=1, col=1)\n",
    "    fig2.update_yaxes(title_text=\"Nucleosome Count\", row=2, col=1)\n",
    "    fig2.update_yaxes(title_text=\"Nucleosome Density\", row=2, col=1, secondary_y=True)\n",
    "\n",
    "    # Update layout for fig2\n",
    "    fig2.update_layout(template=\"simple_white\",\n",
    "                       height=300,  # Adjusted to maintain relative dimensions\n",
    "                       width=1000,\n",
    "                       )\n",
    "\n",
    "    # Plot motifs if plot_motifs is True\n",
    "    if plot_motifs:\n",
    "        print(\"Plotting motifs on both figures...\")\n",
    "        # Ensure 'motif_rel_start' and 'motif_id' are in plot_df_copy\n",
    "        if 'motif_rel_start' in plot_df_copy.columns and 'motif_id' in plot_df_copy.columns:\n",
    "            data_filtered = plot_df_copy[['motif_rel_start', 'motif_id']].dropna()\n",
    "\n",
    "            if not data_filtered.empty:\n",
    "                import ast\n",
    "\n",
    "                def ensure_list(x):\n",
    "                    if isinstance(x, str):\n",
    "                        return ast.literal_eval(x)\n",
    "                    elif isinstance(x, tuple):\n",
    "                        return list(x)\n",
    "                    elif isinstance(x, list):\n",
    "                        return x\n",
    "                    else:\n",
    "                        return [x]\n",
    "\n",
    "                # Ensure 'motif_rel_start' and 'motif_id' are lists\n",
    "                data_filtered['motif_rel_start'] = data_filtered['motif_rel_start'].apply(ensure_list)\n",
    "                data_filtered['motif_id'] = data_filtered['motif_id'].apply(ensure_list)\n",
    "\n",
    "                # Create a new column 'motif_pairs' which is a list of tuples\n",
    "                data_filtered['motif_pairs'] = data_filtered.apply(\n",
    "                    lambda row: list(zip(row['motif_rel_start'], row['motif_id'])),\n",
    "                    axis=1\n",
    "                )\n",
    "\n",
    "                # Explode 'motif_pairs' to get each (motif_rel_start, motif_id) pair in its own row\n",
    "                exploded_df = data_filtered.explode('motif_pairs')\n",
    "\n",
    "                # Split 'motif_pairs' into 'motif_rel_start' and 'motif_id'\n",
    "                exploded_df[['motif_rel_start', 'motif_id']] = pd.DataFrame(\n",
    "                    exploded_df['motif_pairs'].tolist(), index=exploded_df.index\n",
    "                )\n",
    "\n",
    "                # Remove duplicates if any\n",
    "                exploded_df = exploded_df[['motif_rel_start', 'motif_id']].drop_duplicates()\n",
    "\n",
    "                # Group by 'motif_rel_start' to handle overlapping motifs at the same position\n",
    "                grouped_motifs = exploded_df.groupby('motif_rel_start')['motif_id'].apply(list).reset_index()\n",
    "\n",
    "                # Function to add motifs to a given figure\n",
    "                def add_motifs_to_figure(fig):\n",
    "                    # Global counter for motif annotations\n",
    "                    annotation_idx = 0\n",
    "\n",
    "                    for _, row in grouped_motifs.iterrows():\n",
    "                        motif_rel_start = row['motif_rel_start']\n",
    "                        motif_ids = row['motif_id']\n",
    "                        num_motifs = len(motif_ids)\n",
    "\n",
    "                        # Plot the vertical line once per position\n",
    "                        fig.add_shape(\n",
    "                            type=\"line\",\n",
    "                            x0=motif_rel_start,\n",
    "                            x1=motif_rel_start,\n",
    "                            y0=0,\n",
    "                            y1=1,\n",
    "                            line=dict(color=\"grey\", width=1, dash=\"dash\"),\n",
    "                            xref=\"x\",\n",
    "                            yref=\"paper\",\n",
    "                        )\n",
    "\n",
    "                        # Adjust the vertical position of every motif annotation using a global index\n",
    "                        for motif_id in motif_ids:\n",
    "                            # Calculate y position using the global annotation index\n",
    "                            y = 1 + (annotation_idx % 2) * 0.1  # Alternate y-position between 1 and 0.8\n",
    "                            y_anchor = 'bottom'\n",
    "                            fig.add_annotation(\n",
    "                                x=motif_rel_start,\n",
    "                                y=y,\n",
    "                                yref=\"paper\",\n",
    "                                text=f\"{motif_id}\",\n",
    "                                showarrow=False,\n",
    "                                yanchor=y_anchor,\n",
    "                                xanchor=\"center\",\n",
    "                                font=dict(size=10),\n",
    "                                bgcolor=\"rgba(255,255,255,0.5)\",  # Optional: Background color for readability\n",
    "                            )\n",
    "\n",
    "                            # Increment the global annotation index for each motif\n",
    "                            annotation_idx += 1\n",
    "\n",
    "\n",
    "                # Add motifs to fig1\n",
    "                add_motifs_to_figure(fig1)\n",
    "\n",
    "                # Add motifs to fig2\n",
    "                add_motifs_to_figure(fig2)\n",
    "\n",
    "    # remove legends from both figures\n",
    "    fig1.update_layout(showlegend=False)\n",
    "    fig2.update_layout(showlegend=False)\n",
    "    \n",
    "    return fig1, fig2\n",
    "\n",
    "# Sample usage of the function\n",
    "# Replace the following variables with your actual data\n",
    "selec_cond = analysis_cond[0]\n",
    "selec_type = \"rex32\"\n",
    "selected_chr_type = \"X\"\n",
    "# bed_window = 1000  # Example value\n",
    "# n_read_ids = 100   # Example value\n",
    "\n",
    "# Assuming 'down_sampled_plot_df' and 'down_sampled_group_df' are your DataFrames\n",
    "fig1, fig2 = create_plot(down_sampled_plot_df, down_sampled_group_df, selec_cond, selected_chr_type, selec_type, int(round(bed_window, 0)))\n",
    "\n",
    "# Save and show the figures\n",
    "fig1.write_image(\"temp_files/\"+selec_cond+\"_\"+selec_type+\"_read_track.png\", width=1000, height=400)\n",
    "fig2.write_image(\"temp_files/\"+selec_cond+\"_\"+selec_type+\"_nuc_track.png\", width=1000, height=400)\n",
    "fig1.write_image(\"temp_files/\"+selec_cond+\"_\"+selec_type+\"_read_track.svg\", width=1000, height=400)\n",
    "fig2.write_image(\"temp_files/\"+selec_cond+\"_\"+selec_type+\"_nuc_track.svg\", width=1000, height=400)\n",
    "# save as svg\n",
    "\n",
    "fig1.show(renderer='plotly_mimetype+notebook')\n",
    "fig2.show(renderer='plotly_mimetype+notebook')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0bcae4eb8df761",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate bedgraph from bam files\n",
    "## Note: files are saved in same folders as original .bam files\n",
    "regenerate_bit = False # SEt to true to force regenerate, otherwise load if available.\n",
    "num_processors = 10\n",
    "\n",
    "# Generating the list of input bam folder paths from new_bam_files\n",
    "input_bam_paths = [os.path.dirname(bam) for bam in new_bam_files]\n",
    "\n",
    "# Function to run a single command\n",
    "def modkit_pileup_extract(args):\n",
    "    (each_bam, each_thresh, each_condition, each_index, each_bamfrac, each_expid, \n",
    "     modkit_path, output_stem, num_processors) = args\n",
    "\n",
    "    # if regenerate_bit is True delete all files ending in .bedgraph in output_stem\n",
    "    if regenerate_bit:\n",
    "        for file in os.listdir(output_stem):\n",
    "            if file.endswith(\".bedgraph\"):\n",
    "                print(\"Deleting file: \", os.path.join(output_stem, file))\n",
    "                os.remove(os.path.join(output_stem, file))\n",
    "                \n",
    "    # Check if the output file exists\n",
    "    if not regenerate_bit:\n",
    "        print(\"Checking if file exists: \", output_stem + \"/\"+each_expid + \"-\" + each_condition + \"_a_A0_m_GC1.bedgraph\")\n",
    "        if os.path.exists(output_stem + \"/\"+each_expid + \"-\" + each_condition + \"_a_A0_m_GC1.bedgraph\"):\n",
    "            print(f\"File already exists: {output_stem}/{each_expid}-{each_condition}_a_A0_m_GC1.bedgraph\")\n",
    "            # Read in output file and check if empty\n",
    "            return\n",
    "        else:\n",
    "            for file in os.listdir(output_stem):\n",
    "                # if file contains {each_expid}-{each_condition} and ends with .bedgraph, delete it\n",
    "                if each_expid in file and each_condition in file and file.endswith(\".bedgraph\"):\n",
    "                    print(\"Deleting file: \", os.path.join(output_stem, file))\n",
    "\n",
    "    \n",
    "    print(f\"Starting on bam file: \", each_bam)\n",
    "    command = [\n",
    "        modkit_path,\n",
    "        \"pileup\",\n",
    "        #\"--only-tabs\",\n",
    "        #\"--ignore\",\n",
    "        #\"m\",\n",
    "        \"--threads\",\n",
    "        f\"{num_processors}\",\n",
    "        \"--bedgraph\",\n",
    "        #\"--combine-strands\",\n",
    "        #\"--filter-threshold\",\n",
    "        #f\"A:{1-each_thresh}\",\n",
    "        #f\"A:{1-each_thresh}\",\n",
    "        \"--mod-thresholds\",\n",
    "        f\"a:{each_thresh}\",\n",
    "        \"--mod-thresholds\",\n",
    "        f\"m:{each_thresh}\",\n",
    "        \"--ref\",\n",
    "        \"/Data1/reference/c_elegans.WS235.genomic.fa\",\n",
    "        #\"--filter-threshold\",\n",
    "        #f\"A:{1-each_thresh}\",\n",
    "        #\"--filter-threshold\",\n",
    "        #f\"C:{1-each_thresh}\",\n",
    "        \"--motif\",\n",
    "        \"GC\",\n",
    "        \"1\",\n",
    "        #\"--motif\",\n",
    "        #\"CC\",\n",
    "        #\"0\",\n",
    "        \"--motif\",\n",
    "        \"A\",\n",
    "        \"0\",\n",
    "        \"--prefix\",\n",
    "        f\"{each_expid}-{each_condition}\",\n",
    "\n",
    "        #\"--include-bed\",\n",
    "        #modkit_bed_name,\n",
    "        each_bam,\n",
    "        output_stem\n",
    "    ]\n",
    "    subprocess.run(command, text=True)\n",
    "    \n",
    "    # delete any files in output_stem that contain any of the following strings: \"a_CG0\" or \"m_A0\"\n",
    "    for file in os.listdir(output_stem):\n",
    "        if \"a_GC1\" in file or \"m_A0\" in file or \"a_CC0\" in file:\n",
    "            print(\"Deleting file: \", os.path.join(output_stem, file))\n",
    "            os.remove(os.path.join(output_stem, file))\n",
    "\n",
    "    # if m_GC1_positive and m_GC1_negative files not exist, due to missing mods in bam file, create file with empty row\n",
    "    if not os.path.exists(f\"{output_stem}/{each_expid}-{each_condition}_m_GC1_positive.bedgraph\"):\n",
    "        with open(f\"{output_stem}/{each_expid}-{each_condition}_m_GC1_positive.bedgraph\", \"w\") as f:\n",
    "            f.write(\"\\n\")\n",
    "    if not os.path.exists(f\"{output_stem}/{each_expid}-{each_condition}_m_GC1_negative.bedgraph\"):\n",
    "        with open(f\"{output_stem}/{each_expid}-{each_condition}_m_GC1_negative.bedgraph\", \"w\") as f:\n",
    "            f.write(\"\\n\")\n",
    "    \n",
    "    # Merge A0_negative and A0_positive files by concatenating them, and then sorting by chromosome and start position in bash\n",
    "    # and saving the output to a new file, then deleting the old files\n",
    "    def merge_and_sort_bedgraph_files(output_stem, each_expid, each_condition, file_suffixes, num_processors=8):\n",
    "        for suffix_pair in file_suffixes:\n",
    "            negative_suffix, positive_suffix, output_suffix = suffix_pair\n",
    "    \n",
    "            negative_file = f\"{output_stem}/{each_expid}-{each_condition}_{negative_suffix}.bedgraph\"\n",
    "            positive_file = f\"{output_stem}/{each_expid}-{each_condition}_{positive_suffix}.bedgraph\"\n",
    "            merged_file = f\"{output_stem}/{each_expid}-{each_condition}_{output_suffix}.bedgraph\"\n",
    "    \n",
    "            command = f\"cat {negative_file} {positive_file} | sort -k1,1 -k2,2n --parallel={num_processors} > {merged_file}\"\n",
    "            subprocess.run(command, shell=True)\n",
    "            \n",
    "            \n",
    "            # if either suffix contains \"positive\" or \"negative, Delete the old files\n",
    "            if \"positive\" in negative_suffix or \"negative\" in negative_suffix or \"positive\" in positive_suffix or \"negative\" in positive_suffix:\n",
    "                os.remove(negative_file)\n",
    "                os.remove(positive_file)\n",
    "    \n",
    "    file_suffixes = [\n",
    "        (\"a_A0_negative\", \"a_A0_positive\", \"a_A0\"),\n",
    "        (\"m_GC1_negative\", \"m_GC1_positive\", \"m_GC1\"),\n",
    "        (\"a_A0\", \"m_GC1\", \"a_A0_m_GC1\"),\n",
    "    ]\n",
    "    \n",
    "    merge_and_sort_bedgraph_files(output_stem, each_expid, each_condition, file_suffixes, num_processors)\n",
    "\n",
    "    \n",
    "    \n",
    "# Now you need to adjust the task_args to include the index\n",
    "# Prepare the arguments for each task\n",
    "task_args = list(zip(\n",
    "    new_bam_files,\n",
    "    thresh_list,\n",
    "    conditions,\n",
    "    sample_indices,\n",
    "    bam_fracs,\n",
    "    exp_ids,\n",
    "    [modkit_path]*len(new_bam_files),\n",
    "    input_bam_paths,\n",
    "    [num_processors] * len(new_bam_files)\n",
    "))\n",
    "\n",
    "# Select task_args where new_bam_files contains \"AG1\"\n",
    "#task_args = [task for task in task_args if \"AG1\" in task[0]]\n",
    "\n",
    "# Print bam paths for debugging\n",
    "print(\"new_bam_files: \", input_bam_paths)\n",
    "\n",
    "# Execute commands in parallel w\n",
    "with Pool(processes=8) as pool:\n",
    "    pool.map(modkit_pileup_extract, task_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e764d991f8a5096",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # OPTIONAL\n",
    "### Fill and Smooth\n",
    "import os\n",
    "import subprocess\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "import importlib\n",
    "import nanotools\n",
    "\n",
    "def process_bedgraph(args):\n",
    "    each_bam, each_condition, each_expid, smoothing_window, imputation_window, bedgraphtobigwig_path, force_replace, raw_only = args\n",
    "\n",
    "    # Define raw output file names\n",
    "    bedgraph_fn = os.path.join(os.path.dirname(each_bam), f\"{each_expid}-{each_condition}_a_A0.bedgraph\")\n",
    "    raw_bw_fn = os.path.join(os.path.dirname(each_bam), f\"{each_expid}-{each_condition}_a_A0.bw\")\n",
    "\n",
    "    if not os.path.exists(raw_bw_fn) or force_replace:\n",
    "        print(\"Converting raw bedgraph directly to raw bigwig...\")\n",
    "\n",
    "        with tempfile.NamedTemporaryFile(mode='w+t', delete=False, suffix='.bedgraph') as temp_file:\n",
    "            temp_filename = temp_file.name\n",
    "            cut_command = f\"cut -f 1-4 {bedgraph_fn}\"\n",
    "\n",
    "            try:\n",
    "                subprocess.run(cut_command, shell=True, check=True, stdout=temp_file)\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                print(f\"An error occurred while cutting the bedgraph file: {e}\")\n",
    "                os.unlink(temp_filename)\n",
    "                raise\n",
    "\n",
    "        try:\n",
    "            bigwig_command = [\n",
    "                bedgraphtobigwig_path,\n",
    "                temp_filename,\n",
    "                chrom_sizes,\n",
    "                raw_bw_fn\n",
    "            ]\n",
    "            subprocess.run(bigwig_command, check=True)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"An error occurred during bedgraph to bigwig conversion: {e}\")\n",
    "            raise\n",
    "        finally:\n",
    "            print(\"Saved raw bigwig file: \", raw_bw_fn)\n",
    "            os.unlink(temp_filename)\n",
    "    else:\n",
    "        print(\"Raw bigwig file already exists, skipping conversion.\")\n",
    "\n",
    "    # If only raw bigwig files are to be created, skip the rest\n",
    "    if raw_only:\n",
    "        return raw_bw_fn\n",
    "\n",
    "    # Define output file names\n",
    "    filled_bedgraph_fn = os.path.join(os.path.dirname(each_bam),\n",
    "                                      f\"{each_expid}-{each_condition}_a_A0_raw_filled.bedgraph\")\n",
    "    filled_bw_fn = filled_bedgraph_fn.replace(\".bedgraph\", \".bw\")\n",
    "    nafilled_bedgraph_fn = os.path.join(os.path.dirname(each_bam),\n",
    "                                        f\"{each_expid}-{each_condition}_a_A0_nafilled.bedgraph\")\n",
    "    nafilled_bw_fn = nafilled_bedgraph_fn.replace(\".bedgraph\", \".bw\")\n",
    "    smoothed_bedgraph_fn = os.path.join(os.path.dirname(each_bam),\n",
    "                                        f\"{each_expid}-{each_condition}_a_A0_smoothed-{smoothing_window}-{imputation_window}.bedgraph\")\n",
    "    smoothed_bigwig_fn = smoothed_bedgraph_fn.replace(\".bedgraph\", \".bw\")\n",
    "\n",
    "    bedgraph_df = pd.DataFrame()\n",
    "\n",
    "    if not os.path.exists(filled_bedgraph_fn) or force_replace:\n",
    "        print(\"Starting to fill raw bedgraph...\")\n",
    "        print(\"Loading bedgraph file: \", bedgraph_fn)\n",
    "        if bedgraph_df.empty:\n",
    "            bedgraph_df = nanotools.load_bedgraph_file(bedgraph_fn)\n",
    "            bedgraph_df['score'] = bedgraph_df['score'].fillna(0)\n",
    "\n",
    "        bedgraph_df[['chromosome', 'start', 'end', 'score']].to_csv(\n",
    "            filled_bedgraph_fn,\n",
    "            sep=\"\\t\", header=False, index=False)\n",
    "    else:\n",
    "        print(f\"Raw filled bedgraph file already exists, skipping: {filled_bedgraph_fn}\")\n",
    "\n",
    "    if not os.path.exists(filled_bw_fn) or force_replace:\n",
    "        print(\"Converting filled bedgraph to bigwig...\")\n",
    "        try:\n",
    "            bigwig_command = [\n",
    "                bedgraphtobigwig_path,\n",
    "                filled_bedgraph_fn,\n",
    "                chrom_sizes,\n",
    "                filled_bw_fn\n",
    "            ]\n",
    "            subprocess.run(bigwig_command, check=True)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"An error occurred during bedgraph to bigwig conversion: {e}\")\n",
    "            raise\n",
    "    else:\n",
    "        print(f\"Raw filled bigwig file already exists, skipping: {filled_bw_fn}\")\n",
    "\n",
    "    if not os.path.exists(nafilled_bedgraph_fn) or force_replace:\n",
    "        print(\"Starting to fill raw bedgraph with NAs...\")\n",
    "        print(\"Loading bedgraph file: \", bedgraph_fn)\n",
    "        if bedgraph_df.empty:\n",
    "            bedgraph_df = nanotools.load_bedgraph_file(bedgraph_fn)\n",
    "\n",
    "        bedgraph_df[['chromosome', 'start', 'end', 'score']].to_csv(\n",
    "            nafilled_bedgraph_fn,\n",
    "            sep=\"\\t\", header=False, index=False)\n",
    "    else:\n",
    "        print(f\"Raw NA filled bedgraph file already exists, skipping: {nafilled_bedgraph_fn}\")\n",
    "\n",
    "    if not os.path.exists(smoothed_bedgraph_fn) or force_replace:\n",
    "        if bedgraph_df.empty:\n",
    "            print(\"Loading bedgraph file: \", bedgraph_fn)\n",
    "            bedgraph_df = nanotools.load_bedgraph_file(bedgraph_fn)\n",
    "            bedgraph_df['score'] = bedgraph_df['score'].fillna(0)\n",
    "\n",
    "        print(f\"Imputing and smoothing bedgraph file: {smoothed_bedgraph_fn}\")\n",
    "        bedgraph_df['imputed_score'], bedgraph_df['imputed_coverage'], bedgraph_df['smoothed_score'], bedgraph_df[\n",
    "            'smoothed_coverage'] = nanotools.parallel_impute_and_smooth(\n",
    "            bedgraph_df,\n",
    "            impute_window=imputation_window,\n",
    "            smooth_window=smoothing_window,\n",
    "            fill_value=0\n",
    "        )\n",
    "\n",
    "        print(f\"Saving smoothed bedgraph file: {smoothed_bedgraph_fn}\")\n",
    "        bedgraph_df[['chromosome', 'start', 'end', 'smoothed_score']].to_csv(\n",
    "            smoothed_bedgraph_fn,\n",
    "            sep=\"\\t\", header=False, index=False)\n",
    "    else:\n",
    "        print(f\"Imputed and smoothed bedgraph file already exists, skipping: {smoothed_bedgraph_fn}\")\n",
    "\n",
    "    if not os.path.exists(smoothed_bigwig_fn) or force_replace:\n",
    "        command = [\n",
    "            bedgraphtobigwig_path,\n",
    "            smoothed_bedgraph_fn,\n",
    "            chrom_sizes,\n",
    "            smoothed_bigwig_fn\n",
    "        ]\n",
    "        print(f\"Converting smoothed bedgraph to bigwig: {smoothed_bigwig_fn}\")\n",
    "        subprocess.run(command, text=True, capture_output=True)\n",
    "    else:\n",
    "        print(f\"Imputed and smoothed bigwig file already exists, skipping: {smoothed_bigwig_fn}\")\n",
    "\n",
    "    return raw_bw_fn\n",
    "\n",
    "# Configurable parameters\n",
    "smoothing_window = 20\n",
    "imputation_window = 0\n",
    "force_replace = False  # Set this to True if you want to force replacement of existing files\n",
    "raw_only = True  # Set this to True to create raw bw files only, skipping other operations\n",
    "\n",
    "# Prepare arguments for multiprocessing\n",
    "args_list = [\n",
    "    (each_bam, each_condition, each_expid, smoothing_window, imputation_window, bedgraphtobigwig_path, force_replace, raw_only)\n",
    "    for each_bam, each_condition, each_expid in zip(new_bam_files, conditions, exp_ids)\n",
    "]\n",
    "\n",
    "# Use multiprocessing to process bedgraph files in parallel and collect raw bigwig paths\n",
    "with multiprocessing.Pool(processes=15) as pool:\n",
    "    raw_bw_files = pool.map(process_bedgraph, args_list)\n",
    "\n",
    "print(\"All processing completed.\")\n",
    "print(\"Raw bigwig files:\", raw_bw_files)\n",
    "\n",
    "if not raw_only:\n",
    "    # If you still want to collect the filled bigwig files as before:\n",
    "    filled_bw_files = [os.path.join(os.path.dirname(each_bam), f\"{each_expid}-{each_condition}_a_A0_raw_filled.bw\")\n",
    "                       for each_bam, each_condition, each_expid in zip(new_bam_files, conditions, exp_ids)]\n",
    "    print(\"Filled bigwig files:\", filled_bw_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf7f39b312613ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot size of files\n",
    "import os\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "\n",
    "def get_file_info(bam_files, conditions, exp_ids):\n",
    "    file_info = []\n",
    "    filled_bigwig_files = []\n",
    "    smoothed_bigwig_files = []\n",
    "    for bam_file, condition, exp_id in zip(bam_files, conditions, exp_ids):\n",
    "        output_dir = os.path.dirname(bam_file)\n",
    "\n",
    "        patterns = [\n",
    "            f\"{exp_id}-{condition}_a_A0.bedgraph\",\n",
    "            f\"{exp_id}-{condition}_a_A0_raw.bw\",\n",
    "            f\"{exp_id}-{condition}_a_A0_raw_filled.bedgraph\",\n",
    "            f\"{exp_id}-{condition}_a_A0_raw_filled.bw\",\n",
    "            f\"{exp_id}-{condition}_a_A0_smoothed-{smoothing_window}-{imputation_window}.bedgraph\",\n",
    "            f\"{exp_id}-{condition}_a_A0_smoothed-{smoothing_window}-{imputation_window}.bw\"\n",
    "        ]\n",
    "\n",
    "        for pattern in patterns:\n",
    "            file_path = os.path.join(output_dir, pattern)\n",
    "            if os.path.exists(file_path):\n",
    "                file_size = os.path.getsize(file_path) / (1024 ** 3)  # Convert to GB\n",
    "\n",
    "                if file_path.endswith('.bedgraph'):\n",
    "                    file_type = 'Bedgraph'\n",
    "                elif file_path.endswith('.bw'):\n",
    "                    file_type = 'Bigwig'\n",
    "\n",
    "                if 'raw' in file_path and 'filled' not in file_path:\n",
    "                    processing = 'Raw'\n",
    "                elif 'filled' in file_path:\n",
    "                    processing = 'Filled'\n",
    "                    if file_type == 'Bigwig':\n",
    "                        filled_bigwig_files.append(file_path)\n",
    "                elif 'smoothed' in file_path:\n",
    "                    processing = 'Smoothed'\n",
    "                    if file_type == 'Bigwig':\n",
    "                        smoothed_bigwig_files.append(file_path)\n",
    "                else:\n",
    "                    processing = 'Original'\n",
    "\n",
    "                file_info.append({\n",
    "                    'File Name': os.path.basename(file_path),\n",
    "                    'File Path': file_path,\n",
    "                    'File Size (bytes)': file_size,\n",
    "                    'File Type': file_type,\n",
    "                    'Processing': processing,\n",
    "                    'Experiment ID': exp_id,\n",
    "                    'Condition': condition\n",
    "                })\n",
    "\n",
    "    return file_info, filled_bigwig_files, smoothed_bigwig_files\n",
    "\n",
    "\n",
    "# Use the variables from your original script\n",
    "new_bam_files = [each_bam for each_bam, each_condition, each_expid in zip(new_bam_files, conditions, exp_ids)]\n",
    "conditions = [each_condition for each_bam, each_condition, each_expid in zip(new_bam_files, conditions, exp_ids)]\n",
    "exp_ids = [each_expid for each_bam, each_condition, each_expid in zip(new_bam_files, conditions, exp_ids)]\n",
    "\n",
    "# Define smoothing_window and imputation_window as in your original script\n",
    "smoothing_window = 20\n",
    "imputation_window = 0\n",
    "\n",
    "# Get the file information\n",
    "file_info, filled_bigwig_files, smoothed_bigwig_files = get_file_info(new_bam_files, conditions, exp_ids)\n",
    "\n",
    "if file_info:\n",
    "    df_file_info = pd.DataFrame(file_info)\n",
    "    df_file_info = df_file_info.sort_values('File Size (bytes)', ascending=False)\n",
    "\n",
    "    # Create plots using Plotly (same as before)\n",
    "    file_types = df_file_info['File Type'].unique()\n",
    "    fig = make_subplots(rows=len(file_types), cols=1,\n",
    "                        subplot_titles=[f\"{ftype} File Sizes\" for ftype in file_types],\n",
    "                        vertical_spacing=0.1)\n",
    "\n",
    "    for i, file_type in enumerate(file_types, start=1):\n",
    "        df_subset = df_file_info[df_file_info['File Type'] == file_type]\n",
    "\n",
    "        trace = go.Bar(\n",
    "            x=df_subset['Experiment ID'],\n",
    "            y=df_subset['File Size (bytes)'],\n",
    "            name=file_type,\n",
    "            text=df_subset['Processing'],\n",
    "            hoverinfo='text+y',\n",
    "            hovertext=[f\"Exp ID: {exp}<br>Size: {size:.2f} GB<br>Processing: {proc}\"\n",
    "                       for exp, size, proc in zip(df_subset['Experiment ID'],\n",
    "                                                  df_subset['File Size (bytes)'],\n",
    "                                                  df_subset['Processing'])]\n",
    "        )\n",
    "\n",
    "        fig.add_trace(trace, row=i, col=1)\n",
    "\n",
    "        fig.update_xaxes(title_text=\"Experiment ID\", row=i, col=1)\n",
    "        fig.update_yaxes(title_text=\"File Size (GB)\", row=i, col=1)\n",
    "\n",
    "    fig.update_layout(\n",
    "        height=300 * len(file_types),\n",
    "        title_text=\"File Sizes by Experiment ID and File Type\",\n",
    "        showlegend=False,\n",
    "        template=\"plotly_white\"\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    # Print total number of files found\n",
    "    print(f\"\\nTotal number of files found: {len(file_info)}\")\n",
    "\n",
    "    # Print total size of all files\n",
    "    total_size = sum(file['File Size (bytes)'] for file in file_info)\n",
    "    print(f\"Total size of all files: {total_size:.2f} GB\")\n",
    "\n",
    "    # Print lists of filled and smoothed bigwig files\n",
    "    print(\"\\nFilled Bigwig Files:\")\n",
    "    for file in filled_bigwig_files:\n",
    "        print(file)\n",
    "\n",
    "    print(\"\\nSmoothed Bigwig Files:\")\n",
    "    for file in smoothed_bigwig_files:\n",
    "        print(file)\n",
    "\n",
    "else:\n",
    "    print(\"No matching files found in the specified directories.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48ad5bcb55a338e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_optimal_dtypes(chunk):\n",
    "    dtypes = {\n",
    "        'chromosome': 'category',\n",
    "        'start': 'uint32',\n",
    "    }\n",
    "    score_col = chunk.columns[-1]\n",
    "    if pd.api.types.is_float_dtype(chunk[score_col]):\n",
    "        if chunk[score_col].apply(lambda x: x.is_integer()).all():\n",
    "            dtypes[score_col] = 'float32'\n",
    "        else:\n",
    "            dtypes[score_col] = 'float32'\n",
    "    elif pd.api.types.is_integer_dtype(chunk[score_col]):\n",
    "        dtypes[score_col] = 'float32'\n",
    "    else:\n",
    "        dtypes[score_col] = 'float32'\n",
    "    return dtypes\n",
    "\n",
    "\n",
    "def process_chunk(args):\n",
    "    chunk, file_name = args\n",
    "    chunk.columns = ['chromosome', 'start', file_name]\n",
    "    dtypes = get_optimal_dtypes(chunk)\n",
    "    return chunk.astype(dtypes)\n",
    "\n",
    "\n",
    "def process_bedgraph_in_chunks(file_path, chunk_size=1000000, rows_to_process=None, suffix=None):\n",
    "    file_name = os.path.basename(file_path).replace(suffix, '')\n",
    "    chunks = pd.read_csv(file_path, sep='\\t', header=None, chunksize=chunk_size, usecols=[0, 1, 3],\n",
    "                         nrows=rows_to_process)\n",
    "\n",
    "    with Pool(processes=10) as pool:\n",
    "        processed_chunks = list(pool.imap(process_chunk, ((chunk, file_name) for chunk in chunks)))\n",
    "\n",
    "    return pd.concat(processed_chunks)\n",
    "\n",
    "\n",
    "def check_and_concat_dataframes(base_df, new_df):\n",
    "    if base_df is None:\n",
    "        return new_df\n",
    "\n",
    "    if not np.array_equal(base_df[['chromosome', 'start']].values, new_df[['chromosome', 'start']].values):\n",
    "        raise ValueError(\"chromosome, start, and end columns are not identical across all files.\")\n",
    "\n",
    "    base_df[new_df.columns[-1]] = new_df.iloc[:, -1]\n",
    "    return base_df\n",
    "\n",
    "\n",
    "def find_bedgraph_files(directory, suffix=None):\n",
    "    if suffix is None:\n",
    "        print(\"Requires Suffix to find files\")\n",
    "        return []\n",
    "    return [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith(suffix)]\n",
    "\n",
    "\n",
    "def process_bedgraph_files(new_bam_files, file_prefix, force_replace=False, bam_filter=None, bedgraph_filter=None,\n",
    "                           output_dir=None):\n",
    "    unique_directories = list(set(os.path.dirname(path) for path in new_bam_files))\n",
    "    print(f\"Found {len(unique_directories)} unique directories.\")\n",
    "    suffix = \"_nafilled.bedgraph\"\n",
    "    all_bedgraph_files = []\n",
    "    for directory in unique_directories:\n",
    "        all_bedgraph_files.extend(find_bedgraph_files(directory, suffix=suffix))\n",
    "\n",
    "    if bedgraph_filter:\n",
    "        all_bedgraph_files = [file for file in all_bedgraph_files if any(substr in file for substr in bedgraph_filter)]\n",
    "    else:\n",
    "        all_bedgraph_files = [file for file in all_bedgraph_files if\n",
    "                              \"BM_\" in file or \"BK_\" in file or \"BN_\" in file or \"AG-22\" in file or \"AH-\" in file or \"AM\" in file or \"H1\" in file]\n",
    "\n",
    "    print(f\"Found {len(all_bedgraph_files)} bedgraph files.\")\n",
    "    for file in all_bedgraph_files:\n",
    "        print(os.path.basename(file))\n",
    "\n",
    "    final_df = None\n",
    "    if output_dir is None:\n",
    "        output_dir = \"/Data1/git/meyer-nanopore/scripts/analysis/temp_files/\"\n",
    "    temp_folder = os.path.join(output_dir, file_prefix)\n",
    "    input_file = os.path.join(temp_folder, \"merged_bedgraph_test.csv\")\n",
    "\n",
    "    if not os.path.exists(input_file) or force_replace:\n",
    "        for file in tqdm(all_bedgraph_files, desc=\"Processing files\"):\n",
    "            try:\n",
    "                df = process_bedgraph_in_chunks(file, rows_to_process=None, suffix=suffix)\n",
    "                final_df = check_and_concat_dataframes(final_df, df)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        if final_df is not None:\n",
    "            print(\"Saving merged dataframe...\")\n",
    "            os.makedirs(os.path.dirname(input_file), exist_ok=True)\n",
    "            final_df.to_csv(input_file, index=False,force_replace=True)\n",
    "            print(\n",
    "                f\"Processed {len(all_bedgraph_files)} files from {len(unique_directories)} directories. Merged dataframe saved as '{input_file}'\")\n",
    "        else:\n",
    "            print(\"No data was processed successfully. Please check the errors above.\")\n",
    "    else:\n",
    "        print(\"Merged dataframe already exists, skipping. Use force_replace=True to overwrite.\")\n",
    "\n",
    "    return input_file\n",
    "\n",
    "\n",
    "# Filter BAM files based on original criteria\n",
    "filtered_bam_files = new_bam_files\n",
    "#[\n",
    "#    bam for bam in new_bam_files\n",
    "#    if \"BM_\" in bam or \"BK_\" in bam or \"BN_\" in bam or \"AG1_\" in bam or \"AH_\" in bam or \"AM\" in bam or \"H1\" in bam\n",
    "#]\n",
    "\n",
    "# Execute the function with pre-filled filters\n",
    "result = process_bedgraph_files(\n",
    "    filtered_bam_files,\n",
    "    file_prefix,\n",
    "    force_replace=False,  # Set to True if you want to overwrite existing files\n",
    "    bedgraph_filter=['Y9B-08_03_21_23', 'D1A-nb_12_22_22', 'AD1-nb_06_13_23', 'AB-05_04_10_23', 'AB-04_04_10_23', 'AH-07_08_19_23', 'AH-08_08_19_23', 'H1-nb_12_10_22', 'AB-10_04_10_23', 'AB-09_04_10_23', 'AH-09_08_19_23', 'AG-22_11_30_23', 'BN_05_24_24', 'BM_05_30_24'], #[\"BM_\", \"BK_\", \"BN_\", \"AG-22\", \"AH-\", \"AM\",\"H1\"],\n",
    "    output_dir=\"/Data1/git/meyer-nanopore/scripts/analysis/temp_files/\"\n",
    ")\n",
    "\n",
    "print(f\"Output file: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2eb4cbb902425",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Qnromalization\n",
    "import stat\n",
    "import numpy as np\n",
    "\n",
    "# https://pypi.org/project/qnorm/\n",
    "from qnorm import quantile_normalize\n",
    "\n",
    "# reimport nanotools\n",
    "importlib.reload(nanotools)\n",
    "import pyarrow.csv as pv\n",
    "\n",
    "\n",
    "def import_normalize_smooth_and_convert(bedgraphtobigwig_path, chrom_sizes, imputation_window, smoothing_window):\n",
    "    temp_folder = \"/Data1/git/meyer-nanopore/scripts/analysis/temp_files/\"\n",
    "    input_file = temp_folder + file_prefix + \"/merged_bedgraph_test.csv\"\n",
    "    qnormalized_bedgraph_files = []\n",
    "    qnormalized_bigwig_files = []\n",
    "\n",
    "    # Check if the input file exists\n",
    "    if not os.path.exists(input_file):\n",
    "        raise FileNotFoundError(f\"Input file not found: {input_file}\")\n",
    "\n",
    "    # Read the CSV file\n",
    "    print(\"Importing data...\")\n",
    "    # Read the CSV file using pyarrow with parallel processing\n",
    "    read_options = pv.ReadOptions(use_threads=True)\n",
    "    table = pv.read_csv(input_file, read_options=read_options)\n",
    "\n",
    "    # Convert the pyarrow Table to a pandas DataFrame\n",
    "    df = table.to_pandas()\n",
    "\n",
    "    print(f\"Imported data shape: {df.shape}\")\n",
    "\n",
    "    # Check the structure of the dataframe\n",
    "    if 'chromosome' not in df.columns or 'start' not in df.columns:\n",
    "        raise ValueError(\"Input file must have 'chromosome' and 'start' columns\")\n",
    "\n",
    "    if df.shape[1] < 3:\n",
    "        raise ValueError(\"Input file must have at least one data column besides 'chromosome' and 'start'\")\n",
    "\n",
    "    # Identify columns to normalize (all except 'chrom' and 'start')\n",
    "    columns_to_normalize = df.columns[2:]\n",
    "\n",
    "    # Convert 'na' to NaN and columns to float\n",
    "    df[columns_to_normalize] = df[columns_to_normalize].replace('na', np.nan).astype(float)\n",
    "\n",
    "    # Prepare data for normalization\n",
    "    data_to_normalize = df[columns_to_normalize].values\n",
    "\n",
    "    # Apply qnorm with parallel processing\n",
    "    print(\"Applying quantile normalization using 10 CPU cores...\")\n",
    "    normalized_data = quantile_normalize(data_to_normalize, ncpus=10)\n",
    "\n",
    "    # Replace original data with normalized data\n",
    "    df[columns_to_normalize] = normalized_data\n",
    "\n",
    "    print(\"Normalization complete.\")\n",
    "    print(f\"Final data shape: {df.shape}\")\n",
    "\n",
    "    # Process each column\n",
    "    for column in columns_to_normalize:\n",
    "        # Create a new dataframe for this column\n",
    "        bedgraph_df = df[['chromosome', 'start']].copy()\n",
    "        bedgraph_df['end'] = bedgraph_df['start'] + 1  # Assuming 1-base positions\n",
    "        bedgraph_df['score'] = df[column]\n",
    "        bedgraph_df['coverage'] = 1  # Placeholder for 'coverage' column\n",
    "\n",
    "        # Generate output filenames\n",
    "        original_filename = f\"{column}_filled.bedgraph\"\n",
    "        qnorm_bedgraph_filename = original_filename.replace(\"filled.bedgraph\", \"qnorm.bedgraph\")\n",
    "        qnorm_bigwig_filename = qnorm_bedgraph_filename.replace(\".bedgraph\", \".bw\")\n",
    "        smoothed_bedgraph_filename = original_filename.replace(\"filled.bedgraph\", \"qnorm_smoothed.bedgraph\")\n",
    "        smoothed_bigwig_filename = smoothed_bedgraph_filename.replace(\".bedgraph\", \".bw\")\n",
    "\n",
    "        qnorm_bedgraph_path = os.path.join(os.path.dirname(input_file), qnorm_bedgraph_filename)\n",
    "        qnorm_bigwig_path = os.path.join(os.path.dirname(input_file), qnorm_bigwig_filename)\n",
    "        smoothed_bedgraph_path = os.path.join(os.path.dirname(input_file), smoothed_bedgraph_filename)\n",
    "        smoothed_bigwig_path = os.path.join(os.path.dirname(input_file), smoothed_bigwig_filename)\n",
    "\n",
    "        # Save the qnorm bedgraph file from the first 4 columns of bedgraph_df\n",
    "        bedgraph_df[['chromosome', 'start', 'end', 'score']].to_csv(qnorm_bedgraph_path, sep='\\t', index=False,\n",
    "                                                                    header=False, na_rep='na')\n",
    "        print(f\"Saved qnorm bedgraph: {qnorm_bedgraph_path}\")\n",
    "        qnormalized_bedgraph_files.append(qnorm_bedgraph_path)\n",
    "\n",
    "        # Convert qnorm bedgraph to bigwig\n",
    "        if not os.path.exists(qnorm_bigwig_path):\n",
    "            print(f\"Converting {qnorm_bedgraph_filename} to bigwig...\")\n",
    "            try:\n",
    "                bigwig_command = [bedgraphtobigwig_path, qnorm_bedgraph_path, chrom_sizes, qnorm_bigwig_path]\n",
    "                subprocess.run(bigwig_command, check=True)\n",
    "                print(f\"Qnorm bigwig file created: {qnorm_bigwig_path}\")\n",
    "                qnormalized_bigwig_files.append(qnorm_bigwig_path)\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                print(f\"An error occurred during bedgraph to bigwig conversion: {e}\")\n",
    "                raise\n",
    "        else:\n",
    "            print(f\"Qnorm bigwig file already exists, skipping: {qnorm_bigwig_path}\")\n",
    "            qnormalized_bigwig_files.append(qnorm_bigwig_path)\n",
    "\n",
    "        # Apply smoothing\n",
    "        if not os.path.exists(smoothed_bedgraph_path):\n",
    "            print(f\"Imputing and smoothing bedgraph file: {smoothed_bedgraph_path}\")\n",
    "            bedgraph_df['score'] = bedgraph_df['score'].fillna(0)\n",
    "            bedgraph_df['imputed_score'], bedgraph_df['imputed_coverage'], bedgraph_df['smoothed_score'], bedgraph_df[\n",
    "                'smoothed_coverage'] = nanotools.parallel_impute_and_smooth(\n",
    "                bedgraph_df,\n",
    "                impute_window=imputation_window,\n",
    "                smooth_window=smoothing_window,\n",
    "                fill_value=0\n",
    "            )\n",
    "\n",
    "            print(f\"Saving smoothed bedgraph file: {smoothed_bedgraph_path}\")\n",
    "            bedgraph_df[['chromosome', 'start', 'end', 'smoothed_score']].to_csv(\n",
    "                smoothed_bedgraph_path,\n",
    "                sep=\"\\t\", header=False, index=False)\n",
    "        else:\n",
    "            print(f\"Imputed and smoothed bedgraph file already exists, skipping: {smoothed_bedgraph_path}\")\n",
    "\n",
    "        # Convert smoothed bedgraph to bigwig\n",
    "        if not os.path.exists(smoothed_bigwig_path):\n",
    "            print(f\"Converting smoothed bedgraph to bigwig: {smoothed_bigwig_path}\")\n",
    "            command = [bedgraphtobigwig_path, smoothed_bedgraph_path, chrom_sizes, smoothed_bigwig_path]\n",
    "            subprocess.run(command, text=True, capture_output=True)\n",
    "        else:\n",
    "            print(f\"Imputed and smoothed bigwig file already exists, skipping: {smoothed_bigwig_path}\")\n",
    "\n",
    "    print(\"All files have been processed, normalized, smoothed, and converted.\")\n",
    "    return qnormalized_bedgraph_files, qnormalized_bigwig_files\n",
    "\n",
    "\n",
    "# Usage\n",
    "imputation_window = 0  # Set this to your desired value\n",
    "smoothing_window = 50  # Set this to your desired value\n",
    "qnormalized_bedgraph_paths, qnormalized_bigwig_paths = import_normalize_smooth_and_convert(bedgraphtobigwig_path,\n",
    "                                                                                           chrom_sizes,\n",
    "                                                                                           imputation_window,\n",
    "                                                                                           smoothing_window)\n",
    "\n",
    "# Print the list of qnormalized bedgraph file paths\n",
    "print(\"Qnormalized bedgraph file paths:\")\n",
    "for path in qnormalized_bedgraph_paths:\n",
    "    print(path)\n",
    "\n",
    "# Print the list of qnormalized bigwig file paths\n",
    "print(\"\\nQnormalized bigwig file paths:\")\n",
    "for path in qnormalized_bigwig_paths:\n",
    "    print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b594e942ddb638a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import os\n",
    "\n",
    "def filter_and_plot(result_df, qvalue_cutoff=None, percentile_cutoff=None, num_categories=4):\n",
    "    # Filter rows based on qvalue cutoff if provided\n",
    "    if qvalue_cutoff is not None:\n",
    "        result_df = result_df[result_df['LOG10(qvalue)'] >= qvalue_cutoff]\n",
    "\n",
    "    # Split the type column by \"_\" and keep only the first element\n",
    "    result_df['type'] = result_df['type'].str.split(\"_\").str[0]\n",
    "    # Rename 'all' to 'rex'\n",
    "    result_df['type'] = result_df['type'].replace('all', 'rex')\n",
    "\n",
    "    # Apply percentile cutoff for each type if provided\n",
    "    if percentile_cutoff is not None:\n",
    "        def filter_by_percentile(group):\n",
    "            if group.name in ['SDC2', 'SDC3']:\n",
    "                norm_col = f'mNeon{group.name}_rep1_antimNeon'\n",
    "                threshold = group[norm_col].quantile(percentile_cutoff)\n",
    "                return group[group[norm_col] >= threshold]\n",
    "            else:\n",
    "                return group  # No filtering for 'rex' and 'intergenic'\n",
    "\n",
    "        result_df = result_df.groupby('type').apply(filter_by_percentile).reset_index(drop=True)\n",
    "\n",
    "    # Create normalized columns\n",
    "    result_df['sdc3_norm'] = result_df['mNeonSDC3_rep1_antimNeon'] / result_df['mNeonSDC3_rep1_IgG']\n",
    "    result_df['sdc2_norm'] = result_df['mNeonSDC2_rep1_antimNeon'] / result_df['mNeonSDC2_rep1_IgG']\n",
    "\n",
    "    # Create categories for each type\n",
    "    def categorize(group):\n",
    "        if group.name == 'SDC2':\n",
    "            return pd.qcut(group['sdc2_norm'], q=num_categories, labels=[f'D{i + 1}' for i in range(num_categories)])\n",
    "        elif group.name == 'SDC3':\n",
    "            return pd.qcut(group['sdc3_norm'], q=num_categories, labels=[f'D{i + 1}' for i in range(num_categories)])\n",
    "        else:\n",
    "            return pd.Series([group.name] * len(group), index=group.index)\n",
    "\n",
    "    result_df['chip_category'] = result_df.groupby('type').apply(categorize).reset_index(level=0, drop=True)\n",
    "\n",
    "    # Extract experiment names\n",
    "    experiment_names = analysis_cond  # Replace with your actual variable\n",
    "\n",
    "    # Create a custom RdBu colormap for SDC2 and SDC3 (blue to red)\n",
    "    colors_sdc = plt.cm.RdBu_r(np.linspace(0, 1, num_categories))\n",
    "    cmap_sdc = LinearSegmentedColormap.from_list(\"custom_RdBu\", colors_sdc)\n",
    "\n",
    "    # Colors for 'rex' and 'intergenic'\n",
    "    color_rex = 'green'\n",
    "    color_intergenic = 'orange'\n",
    "\n",
    "    def plot_boxplots(data, bw_column, ax1, ax2, title, palette_sdc, buffer=1):\n",
    "        #drop rows where type == SDC2\n",
    "\n",
    "        # Create a custom order for the chip_category\n",
    "        category_order = sorted(\n",
    "            [cat for cat in data['chip_category'].unique() if cat.startswith('D')],\n",
    "            key=lambda x: int(x[1:])\n",
    "        )\n",
    "\n",
    "        # Boxplot properties\n",
    "        boxprops = dict(facecolor='none')\n",
    "        medianprops = dict(color='red', linewidth=2)\n",
    "\n",
    "\n",
    "        # Plot SDC2 on the first subplot\n",
    "        sns.boxplot(\n",
    "            x='chip_category', y=bw_column, data=data[data['type'] == 'SDC3'],\n",
    "            ax=ax1, palette=palette_sdc, showfliers=False, order=category_order,\n",
    "            boxprops=boxprops, medianprops=medianprops, width=1\n",
    "        )\n",
    "\n",
    "        # Plot 'rex' and 'intergenic' on the second subplot\n",
    "        sns.boxplot(\n",
    "            x='chip_category', y=bw_column, data=data[data['type'].isin(['rex', 'intergenic'])],\n",
    "            ax=ax2, palette=[color_rex, color_intergenic], showfliers=False, order=['rex', 'intergenic'],\n",
    "            boxprops=boxprops, medianprops=medianprops, width=1\n",
    "        )\n",
    "\n",
    "        # Set titles and labels\n",
    "        ax1.set_title(f'Boxplot for {title}')\n",
    "        ax1.set_xlabel('ChIP Category')\n",
    "        ax1.set_ylabel('Average Methylation')\n",
    "        ax2.set_xlabel('ChIP Category')\n",
    "        ax2.set_ylabel('')\n",
    "\n",
    "        # Rotate x-axis labels and decrease font size\n",
    "        ax1.tick_params(axis='x', rotation=45, labelsize=8)\n",
    "        ax2.tick_params(axis='x', rotation=45, labelsize=8)\n",
    "\n",
    "        # Adjust y-axis label font size\n",
    "        ax1.yaxis.label.set_fontsize(12)\n",
    "\n",
    "        # Set title font size\n",
    "        ax1.title.set_fontsize(16)\n",
    "\n",
    "        # Remove background\n",
    "        ax1.set_facecolor('none')\n",
    "        ax2.set_facecolor('none')\n",
    "        ax1.grid(False)\n",
    "        ax2.grid(False)\n",
    "\n",
    "        # Remove legends from individual subplots\n",
    "        ax1.legend().remove()\n",
    "        ax2.legend().remove()\n",
    "\n",
    "        data = data[data['type'] != 'SDC2']\n",
    "        # Add (n=) for number of datapoints on the x-axis\n",
    "        for ax in [ax1, ax2]:\n",
    "            for i, label in enumerate(ax.get_xticklabels()):\n",
    "                category = label.get_text()\n",
    "                count = data[(data['chip_category'] == category) & (data[bw_column].notna())].shape[0]\n",
    "                ax.text(i, ax.get_ylim()[0], f'(n={count})', ha='center', va='top', fontsize=8)\n",
    "\n",
    "        # Set y-axis limits to 0 to 0.5\n",
    "        ax1.set_ylim(0, 0.5)\n",
    "        ax2.set_ylim(0, 0.5)\n",
    "\n",
    "        # Add buffer space by setting xlim for each subplot\n",
    "        ax1.set_xlim(-buffer, len(category_order) - 1 + buffer)  # Adjust limits to add buffer space\n",
    "        ax2.set_xlim(-buffer, 1 + buffer)  # Add buffer space for two categories: rex and intergenic\n",
    "\n",
    "\n",
    "\n",
    "    # Generate palette for SDC2 categories\n",
    "    palette_sdc = [cmap_sdc(i / (num_categories - 1)) for i in range(num_categories)]\n",
    "\n",
    "    # Set up the subplots\n",
    "    fig, axes = plt.subplots(\n",
    "        6, 4, figsize=(18, 28),\n",
    "        gridspec_kw={'width_ratios': [5, 1, 5, 1]}\n",
    "    )\n",
    "\n",
    "    # Plot for each average_bw column\n",
    "    for i, (col, exp_name) in enumerate(\n",
    "        zip([col for col in result_df.columns if col.startswith('average_')], experiment_names)\n",
    "    ):\n",
    "        plot_boxplots(\n",
    "            result_df, col,\n",
    "            axes[i // 2, 2 * (i % 2)],\n",
    "            axes[i // 2, 2 * (i % 2) + 1],\n",
    "            exp_name, palette_sdc\n",
    "        )\n",
    "\n",
    "    # Remove overall background\n",
    "    fig.patch.set_facecolor('none')\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Adjust subplots to add buffer space around the boxplots\n",
    "    plt.subplots_adjust(left=0.05, right=0.95)\n",
    "\n",
    "    # Save as PNG and SVG\n",
    "    save_path = '/Data1/git/meyer-nanopore/scripts/analysis/temp_files/'\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    png_path = os.path.join(save_path, 'SDC2_boxplot_figure.png')\n",
    "    svg_path = os.path.join(save_path, 'SDC2_boxplot_figure.svg')\n",
    "\n",
    "    plt.savefig(png_path, format='png', dpi=300, bbox_inches='tight', transparent=True)\n",
    "    plt.savefig(svg_path, format='svg', bbox_inches='tight', transparent=True)\n",
    "\n",
    "    print(f\"Figures saved as:\\n{png_path}\\n{svg_path}\")\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()\n",
    "\n",
    "    return result_df\n",
    "\n",
    "# Example usage:\n",
    "# Assuming result_df is your DataFrame and analysis_cond is defined\n",
    "result_df_cat = filter_and_plot(result_df, percentile_cutoff=None, num_categories=10)\n",
    "\n",
    "# Display sample rows and print columns\n",
    "nanotools.display_sample_rows(result_df_cat, 5)\n",
    "print(result_df_cat.columns)\n",
    "\n",
    "# Print count by type\n",
    "print(result_df_cat['type'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a8042e6debe140",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate dataframe for plotting correlation between chip and accessibility\n",
    "import pandas as pd\n",
    "import pyBigWig\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "qnormalized_bigwig_paths_smoothed = [\n",
    "    \"/Data1/git/meyer-nanopore/scripts/analysis/temp_files/082624/BM_05_30_24-N2_old_SMACseq_R10_a_A0_norm.bw\",\n",
    "    \"/Data1/git/meyer-nanopore/scripts/analysis/temp_files/082624/BK_05_30_24-N2_young_SMACseq_R10_a_A0_norm.bw\",\n",
    "    \"/Data1/git/meyer-nanopore/scripts/analysis/temp_files/082624/AG-22_11_30_23-N2_old_fiber_R10_a_A0_norm.bw\",\n",
    "    \"/Data1/git/meyer-nanopore/scripts/analysis/temp_files/082624/AM_10_08_22-N2_mixed_endogenous_R10_a_A0_norm.bw\",\n",
    "    \"/Data1/git/meyer-nanopore/scripts/analysis/temp_files/082624/BN_05_24_24-96_old_DPY27degron_SMACseq_R10_a_A0_norm.bw\"\n",
    "]\n",
    "\n",
    "\n",
    "def calculate_bigwig_scores(df, qnormalized_bigwig_paths):\n",
    "    \n",
    "    # Initialize new columns for each bigWig file\n",
    "    stats = ['average', 'median', 'sum', 'max']\n",
    "    for i, bw_path in enumerate(qnormalized_bigwig_paths):\n",
    "        for stat in stats:\n",
    "            df[f'{stat}_bw_{i + 1}'] = np.nan\n",
    "\n",
    "    # Process each bigWig file\n",
    "    for i, bw_path in enumerate(qnormalized_bigwig_paths):\n",
    "        print(f\"Processing bigWig file {i + 1}/{len(qnormalized_bigwig_paths)}: {bw_path}\")\n",
    "        with pyBigWig.open(bw_path) as bw:\n",
    "            for index, row in df.iterrows():\n",
    "                chrom = row['chr']\n",
    "                start = row['start']\n",
    "                end = row['end']\n",
    "\n",
    "                try:\n",
    "                    values = bw.values(chrom, start, end)\n",
    "\n",
    "                    values = [v for v in values if v is not None]  # Remove any None values\n",
    "                    values = [v for v in values if v is not None and not np.isnan(v)]\n",
    "\n",
    "                    if values:\n",
    "                        df.at[index, f'average_bw_{i + 1}'] = np.mean(values)\n",
    "                        df.at[index, f'median_bw_{i + 1}'] = np.median(values)\n",
    "                        df.at[index, f'sum_bw_{i + 1}'] = np.sum(values)\n",
    "                        df.at[index, f'max_bw_{i + 1}'] = np.max(values)\n",
    "                except RuntimeError:\n",
    "                    # This can happen if the region is not in the bigWig file\n",
    "                    pass\n",
    "\n",
    "        print(f\"Finished processing bigWig file {i + 1}\")\n",
    "\n",
    "    # Calculate normalized average columns\n",
    "    for i in range(1, len(qnormalized_bigwig_paths) + 1):\n",
    "        df[f'norm_avg_bw_{i}'] = df[f'average_bw_{i}'] / df['average_bw_1'] - 1\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "bed_file_path = \"/Data1/ext_data/qiming_2024/SDC2_SDC3_20_peaks_500_2000_RPKM.csv\" # SDC2_SDC3_20_peaks_500_2000_RPKM.csv or SDC2_SDC3_gt20_rpkm.csv for whole regions\n",
    "\n",
    "center_length = 500 # 500,  1000, or 2000 \n",
    "\n",
    "# Read the bed file\n",
    "chip_bed = pd.read_csv(bed_file_path)\n",
    "\n",
    "# drop rows where length not equal to center_length\n",
    "chip_bed = chip_bed[chip_bed['length'] == center_length]\n",
    "\n",
    "# recalculate start and end based on abs_summit and 1/2 of length\n",
    "chip_bed['start'] = chip_bed['abs_summit'] - chip_bed['length'] // 2\n",
    "chip_bed['end'] = chip_bed['abs_summit'] + chip_bed['length'] // 2\n",
    "\n",
    "nanotools.display_sample_rows(chip_bed, 3)\n",
    "#print columms\n",
    "print(chip_bed.columns)\n",
    "\n",
    "### Add bed regions of interest (e.g. control regions\n",
    "# Create a new dataframe with the same columns as chip_bed\n",
    "new_rows = pd.DataFrame(columns=chip_bed.columns)\n",
    "\n",
    "# Map the columns from combined_bed_df_ext to chip_bed\n",
    "new_rows['type'] = combined_bed_df_ext['type']\n",
    "new_rows['chr'] = combined_bed_df_ext['chrom']\n",
    "new_rows['start'] = combined_bed_df_ext['bed_start']\n",
    "new_rows['end'] = combined_bed_df_ext['bed_end']\n",
    "\n",
    "# Calculate length and abs_summit\n",
    "new_rows['length'] = new_rows['end'] - new_rows['start']\n",
    "new_rows['abs_summit'] = ((new_rows['start'] + new_rows['end']) / 2).astype(int)\n",
    "\n",
    "# Append the new rows to chip_bed\n",
    "chip_bed = pd.concat([chip_bed, new_rows], ignore_index=True)\n",
    "\n",
    "# Sort the resulting dataframe by chr and start position\n",
    "#chip_bed = chip_bed.sort_values(['chr', 'start']).reset_index(drop=True)\n",
    "\n",
    "result_df = calculate_bigwig_scores(chip_bed, raw_bw_files)\n",
    "\n",
    "# Display the first few rows of the resulting dataframe\n",
    "nanotools.display_sample_rows(result_df, 10)\n",
    "# print column names\n",
    "print(result_df.columns)\n",
    "\n",
    "# display number of rows by type\n",
    "result_df['type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a840bba87a813042",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import os\n",
    "\n",
    "def filter_and_plot(result_df, type_to_plot='SDC3', qvalue_cutoff=None, percentile_cutoff=None, num_categories=4):\n",
    "    \"\"\"\n",
    "    Filters and plots the ChIP-seq data for visualization.\n",
    "    Parameters:\n",
    "        - type_to_plot (str): The specific type ('SDC2' or 'SDC3') to be plotted against 'intergenic'.\n",
    "        - qvalue_cutoff (float): Cutoff for filtering by q-value.\n",
    "        - percentile_cutoff (float): Percentile cutoff for filtering specific types.\n",
    "        - num_categories (int): Number of categories to use for categorization.\n",
    "    \"\"\"\n",
    "    # Filter rows based on qvalue cutoff if provided\n",
    "    if qvalue_cutoff is not None:\n",
    "        result_df = result_df[result_df['LOG10(qvalue)'] >= qvalue_cutoff]\n",
    "\n",
    "    # Split the type column by \"_\" and keep only the first element\n",
    "    result_df['type'] = result_df['type'].str.split(\"_\").str[0]\n",
    "    # Rename 'all' to 'rex'\n",
    "    result_df['type'] = result_df['type'].replace('all', 'rex')\n",
    "\n",
    "    # Apply percentile cutoff for each type if provided\n",
    "    if percentile_cutoff is not None:\n",
    "        def filter_by_percentile(group):\n",
    "            if group.name in ['SDC2', 'SDC3']:\n",
    "                norm_col = f'mNeon{group.name}_rep1_antimNeon'\n",
    "                threshold = group[norm_col].quantile(percentile_cutoff)\n",
    "                return group[group[norm_col] >= threshold]\n",
    "            else:\n",
    "                return group  # No filtering for 'rex' and 'intergenic'\n",
    "\n",
    "        result_df = result_df.groupby('type').apply(filter_by_percentile).reset_index(drop=True)\n",
    "\n",
    "    # Create normalized columns\n",
    "    result_df['sdc3_norm'] = result_df['mNeonSDC3_rep1_antimNeon'] / result_df['mNeonSDC3_rep1_IgG']\n",
    "    result_df['sdc2_norm'] = result_df['mNeonSDC2_rep1_antimNeon'] / result_df['mNeonSDC2_rep1_IgG']\n",
    "\n",
    "    # Extract experiment names\n",
    "    experiment_names = analysis_cond  # Replace with your actual variable\n",
    "\n",
    "    # Colors for 'rex', 'SDC2/SDC3', and 'intergenic'\n",
    "    color_sdc = '#1F78B4'  # Blue\n",
    "    color_intergenic = '#E31A1C'  # Red\n",
    "\n",
    "    def plot_boxplots(data, bw_column, ax, title, type_to_plot):\n",
    "        # Filter data to include only the selected type and 'intergenic'\n",
    "        data = data[data['type'].isin([type_to_plot, 'intergenic'])]\n",
    "\n",
    "        # Boxplot properties\n",
    "        boxprops = dict(facecolor='none')\n",
    "        medianprops = dict(color='red', linewidth=2)\n",
    "\n",
    "        # Plot the selected type and 'intergenic'\n",
    "        sns.boxplot(\n",
    "            x='type', y=bw_column, data=data,\n",
    "            ax=ax, palette=[color_sdc, color_intergenic], showfliers=False, order=[type_to_plot, 'intergenic'],\n",
    "            boxprops=boxprops, medianprops=medianprops, width=0.6\n",
    "        )\n",
    "\n",
    "        # Set titles and labels\n",
    "        ax.set_title(f'Boxplot for {title} ({type_to_plot} vs. intergenic)')\n",
    "        ax.set_xlabel('Type')\n",
    "        ax.set_ylabel('Average Methylation')\n",
    "\n",
    "        # Rotate x-axis labels and decrease font size\n",
    "        ax.tick_params(axis='x', rotation=45, labelsize=8)\n",
    "\n",
    "        # Adjust y-axis label font size\n",
    "        ax.yaxis.label.set_fontsize(12)\n",
    "\n",
    "        # Set title font size\n",
    "        ax.title.set_fontsize(16)\n",
    "\n",
    "        # Remove background\n",
    "        ax.set_facecolor('none')\n",
    "        ax.grid(False)\n",
    "\n",
    "        # Add (n=) for number of datapoints on the x-axis\n",
    "        for i, label in enumerate(ax.get_xticklabels()):\n",
    "            category = label.get_text()\n",
    "            count = data[(data['type'] == category) & (data[bw_column].notna())].shape[0]\n",
    "            ax.text(i, ax.get_ylim()[0], f'(n={count})', ha='center', va='top', fontsize=8)\n",
    "\n",
    "        # Set y-axis limits to 0 to 0.5\n",
    "        ax.set_ylim(0, 0.4)\n",
    "\n",
    "    # Set width and height for the entire plot\n",
    "    # Adjust the number of subplots dynamically based on the number of conditions\n",
    "    num_conditions = len(experiment_names)\n",
    "\n",
    "    # Set width and height for the entire plot\n",
    "    fig_width = 2  # Fixed width for one-column layout\n",
    "    fig_height = 4 * num_conditions  # Adjust height based on number of rows\n",
    "\n",
    "    # Create a figure with a single column and the right number of rows\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=num_conditions, ncols=1,\n",
    "        figsize=(fig_width, fig_height)  # Adjust height dynamically\n",
    "    )\n",
    "\n",
    "    # If only one axis, convert to list for consistency\n",
    "    if num_conditions == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    # Update indexing in the loop to use a single axis per plot\n",
    "    for i, (col, exp_name) in enumerate(\n",
    "        zip([col for col in result_df.columns if col.startswith('average_')], experiment_names)\n",
    "    ):\n",
    "        plot_boxplots(result_df, col, axes[i], exp_name, type_to_plot)\n",
    "\n",
    "    # Remove overall background\n",
    "    fig.patch.set_facecolor('none')\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Adjust subplots to add buffer space around the boxplots\n",
    "    plt.subplots_adjust(left=0.05, right=0.95)\n",
    "\n",
    "    # Save as PNG and SVG\n",
    "    save_path = '/Data1/git/meyer-nanopore/scripts/analysis/temp_files/'\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    png_path = os.path.join(save_path, f'{type_to_plot}_ChIP_boxplot_figure.png')\n",
    "    svg_path = os.path.join(save_path, f'{type_to_plot}_ChIP_boxplot_figure.svg')\n",
    "\n",
    "    plt.savefig(png_path, format='png', dpi=300, bbox_inches='tight', transparent=True)\n",
    "    plt.savefig(svg_path, format='svg', bbox_inches='tight', transparent=True)\n",
    "\n",
    "    print(f\"Figures saved as:\\n{png_path}\\n{svg_path}\")\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()\n",
    "\n",
    "    return result_df\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# Assuming result_df is your DataFrame and analysis_cond is defined\n",
    "result_df_cat = filter_and_plot(result_df, type_to_plot='SDC2', percentile_cutoff=None, num_categories=10)\n",
    "\n",
    "# Display sample rows and print columns\n",
    "nanotools.display_sample_rows(result_df_cat, 5)\n",
    "print(result_df_cat.columns)\n",
    "\n",
    "# Print count by type\n",
    "print(result_df_cat['type'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418b4d8901a68b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import os\n",
    "import matplotlib.ticker as ticker\n",
    "from scipy import stats\n",
    "\n",
    "def filter_and_plot(result_df, qvalue_cutoff=None, percentile_cutoff=None, num_categories=4):\n",
    "    # Filter rows based on qvalue cutoff if provided\n",
    "    if qvalue_cutoff is not None:\n",
    "        result_df = result_df[result_df['LOG10(qvalue)'] >= qvalue_cutoff]\n",
    "\n",
    "    # Create the new normalized columns\n",
    "    result_df['sdc3_norm'] = result_df['mNeonSDC3_rep1_antimNeon'] / result_df['mNeonSDC3_rep1_IgG']\n",
    "    result_df['sdc2_norm'] = result_df['mNeonSDC2_rep1_antimNeon'] / result_df['mNeonSDC2_rep1_IgG']\n",
    "\n",
    "    # Split the type column by \"_\" and keep only the first element\n",
    "    result_df['type'] = result_df['type'].str.split(\"_\").str[0]\n",
    "    result_df['type'] = result_df['type'].replace('all', 'rex')\n",
    "\n",
    "    # Filter to keep only SDC2 and SDC3 types\n",
    "    sdc_df = result_df[result_df['type'].isin(['SDC2', 'SDC3'])].copy()\n",
    "\n",
    "    # Get 'rex' type rows\n",
    "    rex_df = result_df[result_df['type'] == 'rex']\n",
    "\n",
    "    # Function to check if a row overlaps with any 'rex' row\n",
    "    def overlaps_with_rex(row):\n",
    "        return any((row['chr'] == rex_row['chr']) and\n",
    "                   (row['start'] < rex_row['end']) and\n",
    "                   (row['end'] > rex_row['start'])\n",
    "                   for _, rex_row in rex_df.iterrows())\n",
    "\n",
    "    # Mark overlapping rows\n",
    "    sdc_df['overlaps_rex'] = sdc_df.apply(overlaps_with_rex, axis=1)\n",
    "\n",
    "    # Apply percentile cutoff for each type if provided\n",
    "    if percentile_cutoff is not None:\n",
    "        def filter_by_percentile(group):\n",
    "            norm_col = f'{group.name.lower()}_norm'\n",
    "            threshold = group[norm_col].quantile(percentile_cutoff)\n",
    "            return group[group[norm_col] >= threshold]\n",
    "\n",
    "        sdc_df = sdc_df.groupby('type').apply(filter_by_percentile).reset_index(drop=True)\n",
    "\n",
    "    # Create categories for each type using the appropriate normalized column\n",
    "    def categorize(group):\n",
    "        norm_col = f'{group.name.lower()}_norm'\n",
    "        return pd.qcut(group[norm_col], q=num_categories, labels=[f'D{i + 1}' for i in range(num_categories)])\n",
    "\n",
    "    sdc_df['chip_category'] = sdc_df.groupby('type').apply(categorize).reset_index(level=0, drop=True)\n",
    "\n",
    "    # Extract experiment names\n",
    "    experiment_names = [\n",
    "        'H1_021_SDC2-AIDpAux',\n",
    "        'AH-09_08_19_23-SDC2_degron',\n",
    "        'AG-22_11_30_23-N2_old',\n",
    "        'BN_05_24_24-96_old_DPY27degron',\n",
    "        'BK_05_30_24-N2_young',\n",
    "        'BM_05_30_24-N2_old',\n",
    "    ]\n",
    "\n",
    "    # Create a custom RdBu (reversed) colormap\n",
    "    colors = plt.cm.RdBu_r(np.linspace(0, 1, num_categories))\n",
    "    cmap = LinearSegmentedColormap.from_list(\"custom_RdBu_r\", colors)\n",
    "\n",
    "    # Color for overlapping points\n",
    "    overlap_color = 'green'\n",
    "\n",
    "    # Create a function to plot scatter for each type and average_bw\n",
    "    def plot_scatter(data, bw_column, ax, title, type_name):\n",
    "        # Use the colormap to generate colors for each category\n",
    "        palette = [cmap(i / (num_categories - 1)) for i in range(num_categories)]\n",
    "\n",
    "        # Determine which normalized column to use\n",
    "        norm_column = f'{type_name.lower()}_norm'\n",
    "\n",
    "        # Plot non-overlapping points\n",
    "        sns.scatterplot(x=norm_column, y=bw_column, hue='chip_category',\n",
    "                        data=data[~data['overlaps_rex']], ax=ax,\n",
    "                        palette=palette, alpha=0.6, legend=False)\n",
    "\n",
    "        # Plot overlapping points\n",
    "        ax.scatter(data[data['overlaps_rex']][norm_column],\n",
    "                   data[data['overlaps_rex']][bw_column],\n",
    "                   c=overlap_color, alpha=0.6, label='Overlaps with \"rex\"')\n",
    "\n",
    "        ax.set_title(f'{title} - {type_name}')\n",
    "        ax.set_xlabel('Fold change (mNeon/IgG RPKM, log scale)')\n",
    "        ax.set_ylabel('Average m6A Methylation')\n",
    "\n",
    "        # Calculate R-squared value\n",
    "        x = data[norm_column].astype(float)\n",
    "        y = data[bw_column].astype(float)\n",
    "        mask = ~np.isnan(x) & ~np.isnan(y)\n",
    "        if mask.sum() > 1:  # Ensure we have at least two valid points\n",
    "            slope, intercept, r_value, p_value, std_err = stats.linregress(x[mask], y[mask])\n",
    "            r_squared = r_value**2\n",
    "        else:\n",
    "            r_squared = np.nan\n",
    "\n",
    "        # Add number of datapoints and R-squared value\n",
    "        count = data[data[bw_column].notna()].shape[0]\n",
    "        overlap_count = data[data['overlaps_rex']].shape[0]\n",
    "        ax.text(0.95, 0.15, f'n={count}\\nOverlaps={overlap_count}\\nR²={r_squared:.3f}',\n",
    "                ha='right', va='bottom', transform=ax.transAxes)\n",
    "\n",
    "\n",
    "\n",
    "        # Set axes to log scale\n",
    "        ax.set_xscale('log')\n",
    "\n",
    "        # set max x range to log 10\n",
    "        ax.set_xlim(1, 100)\n",
    "\n",
    "        # Add minor ticks\n",
    "        ax.xaxis.set_minor_locator(ticker.LogLocator(subs=np.arange(2, 10)))\n",
    "        ax.xaxis.set_minor_formatter(ticker.LogFormatter(minor_thresholds=(2, 0.4)))\n",
    "\n",
    "        # Set major ticks format to non-scientific notation\n",
    "        ax.xaxis.set_major_formatter(ticker.ScalarFormatter(useOffset=False))\n",
    "        ax.xaxis.get_major_formatter().set_scientific(False)\n",
    "\n",
    "        # Remove background\n",
    "        ax.set_facecolor('none')\n",
    "        ax.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n",
    "        # increase font sizes\n",
    "        ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "        ax.tick_params(axis='x', which='minor', labelsize=8, labelbottom=True)\n",
    "        ax.tick_params(axis='x', which='major', labelsize=8, labelbottom=True)\n",
    "        ax.title.set_fontsize(16)\n",
    "        ax.xaxis.label.set_fontsize(14)\n",
    "        ax.yaxis.label.set_fontsize(14)\n",
    "\n",
    "    # Set up the subplots\n",
    "    n_experiments = len(experiment_names)\n",
    "    n_types = 2  # Only SDC2 and SDC3\n",
    "    fig, axes = plt.subplots(n_experiments, n_types, figsize=(10, 5*n_experiments), squeeze=False)\n",
    "\n",
    "    # Plot for each average_bw column and type\n",
    "    for i, (col, exp_name) in enumerate(zip([col for col in sdc_df.columns if col.startswith('average_')], experiment_names)):\n",
    "        for j, type_name in enumerate(['SDC2', 'SDC3']):\n",
    "            ax = axes[i, j]\n",
    "            data_subset = sdc_df[sdc_df['type'] == type_name]\n",
    "            plot_scatter(data_subset, col, ax, exp_name, type_name)\n",
    "\n",
    "    # Create a custom legend\n",
    "    legend_elements = [plt.Line2D([0], [0], marker='o', color='w', label=f'D{i+1}',\n",
    "                                  markerfacecolor=cmap(i / (num_categories - 1)), markersize=10)\n",
    "                       for i in range(num_categories)]\n",
    "    legend_elements.append(plt.Line2D([0], [0], marker='o', color='w', label='Overlaps with \"rex\"',\n",
    "                                      markerfacecolor=overlap_color, markersize=10))\n",
    "\n",
    "    # Add the legend to the figure\n",
    "    fig.legend(handles=legend_elements, loc='center left', bbox_to_anchor=(1, 0.5), title='ChIP decile')\n",
    "\n",
    "    # Remove overall background\n",
    "    fig.patch.set_facecolor('none')\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save as PNG and SVG\n",
    "    save_path = '/Data1/git/meyer-nanopore/scripts/analysis/temp_files/'\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    png_path = os.path.join(save_path, 'scatterplot_figure_0to10.png')\n",
    "    svg_path = os.path.join(save_path, 'scatterplot_figure_0to10.svg')\n",
    "\n",
    "    plt.savefig(png_path, format='png', dpi=300, bbox_inches='tight', transparent=True)\n",
    "    plt.savefig(svg_path, format='svg', bbox_inches='tight', transparent=True)\n",
    "\n",
    "    print(f\"Figures saved as:\\n{png_path}\\n{svg_path}\")\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "filter_and_plot(result_df, percentile_cutoff=0, num_categories=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195346e8eacd184f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate bam summary statistics\n",
    "importlib.reload(nanotools)\n",
    "# import px\n",
    "import plotly.express as px\n",
    "force_replace = False\n",
    "sampling_frac = 0.25 # fraction of bam to sample for summary statistics\n",
    "\n",
    "summary_bam_df = pd.DataFrame()\n",
    "\n",
    "### Define filename for summary table based on selected conditions\n",
    "# We'll start by defining a function to encapsulate the task you want parallelized\n",
    "def process_bam(args):\n",
    "    each_bam, each_condition, each_thresh, each_exp_id = args\n",
    "    print(\"starting on bam:\", each_bam,\" | condition:\", each_condition,\"| each_exp_id:\",each_exp_id, \"with thresh:\", each_thresh)\n",
    "    return nanotools.get_summary_from_bam(sampling_frac, each_thresh, modkit_path, each_bam, each_condition,each_exp_id, thread_ct = 100)\n",
    "\n",
    "# Define filename for summary table based on selected conditions\n",
    "summary_table_name = \"temp_files/\" + \"_\" + conditions[0] + conditions[-1] + \"_\" + str(sampling_frac) + \"_thresh\" + str(thresh_list[0]) + \"_summary_table.csv\"\n",
    "\n",
    "# Check if summary table exists\n",
    "if not force_replace and os.path.exists(summary_table_name):\n",
    "    print(\"Summary table exists, importing...\")\n",
    "    summary_bam_df = pd.read_csv(summary_table_name, sep=\"\\t\", header=0)\n",
    "else:\n",
    "    print(\"Summary table does not exist, creating...\")\n",
    "    #\n",
    "    # Create a pool of worker processes\n",
    "    pool = multiprocessing.Pool(1)\n",
    "\n",
    "    # Map the function to the arguments\n",
    "    results = pool.map(process_bam, zip(new_bam_files, conditions, thresh_list, exp_ids))\n",
    "\n",
    "    # Close the pool\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    # Append the results to the summary dataframe\n",
    "    summary_bam_df = pd.concat(results, ignore_index=True)\n",
    "    # Reset the index\n",
    "    summary_bam_df = summary_bam_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "    # Save the dataframe to a CSV file\n",
    "    summary_bam_df.to_csv(summary_table_name, sep=\"\\t\", header=True, index=False)\n",
    "\n",
    "### Create coverage_df file name (similar to summary_table_name)\n",
    "coverage_df_name =  \"temp_files/\"+\"_\"+conditions[0]+conditions[-1] + \"_\"+str(sampling_frac)+\"_thresh\"+str(thresh_list[0])+\"_coverage_df.csv\"\n",
    "# if coverage_df exists, import it otherwise create it\n",
    "if not force_replace and os.path.exists(coverage_df_name):\n",
    "    print(\"Coverage table exists, importing...\")\n",
    "    coverage_df = pd.read_csv(coverage_df_name, sep=\"\\t\", header=0)\n",
    "else:\n",
    "    # Calculate total_m6a and total_A\n",
    "    nanotools.display_sample_rows(summary_bam_df,2)\n",
    "    # Call the function to create and export the coverage DataFrame\n",
    "    total_m6a = summary_bam_df.loc[summary_bam_df['code'] == 'a'].groupby(['exp_id', 'condition'])['pass_count'].sum().reset_index()\n",
    "    total_m6a.rename(columns={'pass_count': 'total_m6a'}, inplace=True)\n",
    "    total_5mc = summary_bam_df.loc[summary_bam_df['code'] == 'm'].groupby(['exp_id', 'condition'])['pass_count'].sum().reset_index()\n",
    "    total_5mc.rename(columns={'pass_count': 'total_5mc'}, inplace=True)\n",
    "\n",
    "    total_A = summary_bam_df.loc[(summary_bam_df['base'] == 'A') & (summary_bam_df['code'] == '-')].groupby(['exp_id', 'condition'])['pass_count'].sum().reset_index()\n",
    "    total_A.rename(columns={'pass_count': 'total_A'}, inplace=True)\n",
    "\n",
    "    total_C = summary_bam_df.loc[(summary_bam_df['base'] == 'C') & (summary_bam_df['code'] == '-')].groupby(['exp_id', 'condition'])['pass_count'].sum().reset_index()\n",
    "    total_C.rename(columns={'pass_count': 'total_C'}, inplace=True)\n",
    "\n",
    "    # Merge total_m6a and total_A DataFrames\n",
    "    coverage_df_A = pd.merge(total_m6a, total_A,on=['exp_id', 'condition'], how='outer').fillna(0)\n",
    "    coverage_df_C = pd.merge(total_5mc, total_C,on=['exp_id', 'condition'], how='outer').fillna(0)\n",
    "    coverage_df = pd.merge(coverage_df_A, coverage_df_C,on=['exp_id', 'condition'], how='outer').fillna(0)\n",
    "\n",
    "    # Calculate coverage (ce genome size = 100,272,763)\n",
    "    coverage_df['coverage'] = ((coverage_df['total_A'] + coverage_df['total_m6a']) * (1/sampling_frac)) / 100000000 * 4 # * 4 since As are 1/4 of genome\n",
    "\n",
    "    coverage_df['total_A_m6a'] = coverage_df['total_A'] + coverage_df['total_m6a']\n",
    "    coverage_df['total_C_5mc'] = coverage_df['total_C'] + coverage_df['total_5mc']\n",
    "\n",
    "    # Calculate m6A_frac\n",
    "    coverage_df['m6A_frac'] = coverage_df['total_m6a'] / (coverage_df['total_A_m6a'])\n",
    "    coverage_df['5mC_frac'] = coverage_df['total_5mc'] / (coverage_df['total_C_5mc'])\n",
    "\n",
    "    # Drop rows where exp_id == AD1-nb_06_13_23\n",
    "    #coverage_df = coverage_df[coverage_df.exp_id != 'AD1-nb_06_13_23']\n",
    "\n",
    "    nanotools.display_sample_rows(coverage_df, 5)\n",
    "    #Save coverage df\n",
    "    coverage_df.to_csv(coverage_df_name, sep=\"\\t\", header=True, index=False)\n",
    "\n",
    "nanotools.display_sample_rows(coverage_df, 5)\n",
    "\n",
    "### Plot\n",
    "def create_plots(coverage_df):\n",
    "    # Define the desired order of conditions\n",
    "    #condition_order = ['N2_mixed_endogenous_R10', 'N2_old_fiber_R10', 'N2_old_SMACseq_R10', 'N2_mixed_fiber_R10', 'N2_young_SMACseq_R10', '96_old_DPY27degron_SMACseq_R10','51_old_dpy21null_fiber_R10']\n",
    "\n",
    "    # Filter and sort the dataframe\n",
    "    #coverage_df = coverage_df[coverage_df['condition'].isin(condition_order)]\n",
    "    #coverage_df['condition'] = pd.Categorical(coverage_df['condition'], categories=condition_order, ordered=True)\n",
    "    #coverage_df = coverage_df.sort_values('condition')\n",
    "\n",
    "    # Select a color scale\n",
    "    color_scale = px.colors.qualitative.Vivid\n",
    "\n",
    "    # Assign a color from the color scale to each condition\n",
    "    color_dict = {condition: color_scale[i] for i, condition in enumerate(conditions)}\n",
    "\n",
    "    # Create subplots\n",
    "    fig = make_subplots(rows=1, cols=3, subplot_titles=(\"%m6A by Condition\", \"%5mC by Condition\", \"Coverage by Condition\"))\n",
    "\n",
    "    # Add bars for %m6A\n",
    "    m6a_trace = go.Bar(\n",
    "        x=coverage_df['condition'],\n",
    "        y=coverage_df['m6A_frac'] * 100,\n",
    "        name='%m6A',\n",
    "        marker_color=[color_dict[condition] for condition in coverage_df['condition']],\n",
    "        text=(coverage_df['m6A_frac']*100).round(2)\n",
    "    )\n",
    "    fig.add_trace(m6a_trace, row=1, col=1)\n",
    "\n",
    "    # Add bars for %5mC\n",
    "    mc5_trace = go.Bar(\n",
    "        x=coverage_df['condition'],\n",
    "        y=coverage_df['5mC_frac'] * 100,\n",
    "        name='%5mC',\n",
    "        marker_color=[color_dict[condition] for condition in coverage_df['condition']],\n",
    "        text=(coverage_df['5mC_frac']*100).round(2)\n",
    "    )\n",
    "    fig.add_trace(mc5_trace, row=1, col=2)\n",
    "\n",
    "    # Add bars for coverage\n",
    "    coverage_trace = go.Bar(\n",
    "        x=coverage_df['condition'],\n",
    "        y=coverage_df['coverage'],\n",
    "        name='Coverage',\n",
    "        marker_color=[color_dict[condition] for condition in coverage_df['condition']],\n",
    "        # label bars\n",
    "        text=coverage_df['coverage'].round(2)\n",
    "    )\n",
    "    fig.add_trace(coverage_trace, row=1, col=3)\n",
    "\n",
    "    # Update layout using plotly_white theme\n",
    "    fig.update_layout(\n",
    "        template=\"plotly_white\",\n",
    "        title_text='%m6A, %5mC, and Coverage by Condition',\n",
    "        showlegend=True,\n",
    "        height=500,\n",
    "        width=1500,  # Increased width to accommodate the third subplot\n",
    "    )\n",
    "\n",
    "    # Update axes\n",
    "    for i in range(1, 4):\n",
    "        fig.update_xaxes(title_text='Condition', tickangle=45, row=1, col=i)\n",
    "\n",
    "    fig.update_yaxes(title_text='Percentage (%)', row=1, col=1)\n",
    "    fig.update_yaxes(title_text='Percentage (%)', row=1, col=2)\n",
    "    fig.update_yaxes(title_text='Coverage', row=1, col=3)\n",
    "\n",
    "    # Create individual figures for each plot\n",
    "    m6a_fig = go.Figure(m6a_trace)\n",
    "    m6a_fig.update_layout(\n",
    "        template=\"plotly_white\",\n",
    "        title_text='%m6A by Condition',\n",
    "        xaxis_title='Condition',\n",
    "        yaxis_title='Percentage (%)',\n",
    "        xaxis_tickangle=45,\n",
    "        width = 600\n",
    "    )\n",
    "\n",
    "    mc5_fig = go.Figure(mc5_trace)\n",
    "    mc5_fig.update_layout(\n",
    "        template=\"plotly_white\",\n",
    "        title_text='%5mC by Condition',\n",
    "        xaxis_title='Condition',\n",
    "        yaxis_title='Percentage (%)',\n",
    "        xaxis_tickangle=45,\n",
    "        width = 600\n",
    "    )\n",
    "\n",
    "    coverage_fig = go.Figure(coverage_trace)\n",
    "    coverage_fig.update_layout(\n",
    "        template=\"plotly_white\",\n",
    "        title_text='Coverage by Condition',\n",
    "        xaxis_title='Condition',\n",
    "        yaxis_title='Coverage',\n",
    "        xaxis_tickangle=45,\n",
    "        width = 600\n",
    "    )\n",
    "\n",
    "    return fig, m6a_fig, mc5_fig, coverage_fig\n",
    "\n",
    "# Function call to create and display the plots\n",
    "combined_plot, m6a_plot, mc5_plot, coverage_plot = create_plots(coverage_df)\n",
    "\n",
    "# Display the plots\n",
    "m6a_plot.show()\n",
    "mc5_plot.show()\n",
    "coverage_plot.show()\n",
    "\n",
    "\"\"\"\n",
    "fig.write_image(\"images_11_14_23/bulk_m6Afrac_n2_sdc2degron_0p1sample.svg\")\n",
    "fig.write_image(\"images_11_14_23/bulk_m6Afrac_n2_sdc2degron_0p1sample.png\")\n",
    "fig2.write_image(\"images_11_14_23/coverage_n2_sdc2degron_0p1sample.svg\")\n",
    "fig2.write_image(\"images_11_14_23/coverage_n2_sdc2degron_0p1sample.png\")\n",
    "\n",
    "# Function call example\n",
    "### Calculate N50s SKIP, NOT NECESSARY FOR ANY FOLLOWING STEPS\n",
    "n50_fig = nanotools.calculate_and_plot_n50(new_bam_files, conditions, exp_ids)\n",
    "n50_fig.show(renderer='plotly_mimetype+notebook')\n",
    "n50_fig.write_image(\"images_11_14_23/n50_fig_n2_sdc2degron_0p1sample.svg\")\n",
    "n50_fig.write_image(\"images_11_14_23/n50_fig_n2_sdc2degron_0p1sample.png\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31f4696dac4255f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def create_combined_bed_df(result_df, type_selected, num_categories):\n",
    "    # Normalize the columns\n",
    "    result_df['sdc2_norm'] = result_df['mNeonSDC2_rep1_antimNeon'] / result_df['mNeonSDC2_rep1_IgG']\n",
    "    result_df['sdc3_norm'] = result_df['mNeonSDC3_rep1_antimNeon'] / result_df['mNeonSDC3_rep1_IgG']\n",
    "\n",
    "    # Filter the dataframe for the specified type\n",
    "    # if type_selected is None, use the entire dataframe\n",
    "    if type_selected is None:\n",
    "        filtered_df = result_df\n",
    "    else:\n",
    "        # keep all rows where type in type_selected\n",
    "        filtered_df = result_df[result_df['type'].isin(type_selected)]\n",
    "        #filtered_df = result_df[result_df['type'] == type_selected]\n",
    "\n",
    "    # Create categories for each type using the appropriate normalized column\n",
    "    def categorize(group):\n",
    "        if group.name == 'SDC2':\n",
    "            print(\"grouping \", group.name)\n",
    "            # print average value for each num_categories\n",
    "            print(group['sdc2_norm'].quantile(np.linspace(0, 1, num_categories + 1)))\n",
    "\n",
    "            return pd.qcut(group['sdc2_norm'], q=num_categories, labels=[f'SDC2_D{i + 1}' for i in range(num_categories)])\n",
    "        elif group.name == 'SDC3':\n",
    "            print(\"grouping \", group.name)\n",
    "            print(group['sdc3_norm'].quantile(np.linspace(0, 1, num_categories + 1)))\n",
    "            return pd.qcut(group['sdc3_norm'], q=num_categories, labels=[f'SDC3_D{i + 1}' for i in range(num_categories)])\n",
    "        else:\n",
    "            print(\"grouping \", group.name)\n",
    "            return pd.Series([group.name] * len(group), index=group.index)\n",
    "\n",
    "    filtered_df['type'] = filtered_df.groupby('type').apply(categorize).reset_index(level=0, drop=True)\n",
    "\n",
    "    # Create the combined_bed_df\n",
    "    combined_bed_df = pd.DataFrame({\n",
    "        'chrom': filtered_df['chr'],\n",
    "        'bed_start': filtered_df['start'],\n",
    "        'bed_end': filtered_df['end'],\n",
    "        'bed_strand': '+',  # Assuming all strands are positive\n",
    "        'type': filtered_df['type'],\n",
    "        'chr_type': np.where(filtered_df['chr'] == 'CHROMOSOME_X', 'X', 'Autosome')\n",
    "    })\n",
    "\n",
    "    # Reset the index\n",
    "    combined_bed_df = combined_bed_df.reset_index(drop=True)\n",
    "\n",
    "    return combined_bed_df\n",
    "\n",
    "# Example usage:\n",
    "# Assuming result_df is already defined\n",
    "#type_selected = ['SDC2'] #,'SDC3','intergenic_control','all']\n",
    "#type_selected = None\n",
    "num_categories = 10\n",
    "\n",
    "combined_bed_df = create_combined_bed_df(result_df, type_selected, num_categories)\n",
    "print(combined_bed_df)\n",
    "\n",
    "# print count by type\n",
    "print(combined_bed_df['type'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce923479e3971e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to use mex combined bed dfs: combined_bed_df_mex_clust or combined_bed_df_mex_cat\n",
    "#combined_bed_df = combined_bed_df_mex_cat.copy()\n",
    "\n",
    "# keep only type that contain MEX_D10\n",
    "#combined_bed_df = combined_bed_df[combined_bed_df['type'].str.contains('MEX_D10')]\n",
    "\n",
    "# Test to keep only strand == \"+\"\n",
    "#combined_bed_df = combined_bed_df[combined_bed_df['bed_strand'] == '+']\n",
    "\n",
    "import pandas as pd\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "\n",
    "def create_modkit_bed_df(filtered_df):\n",
    "    # Create the modkit_bed_df\n",
    "    modkit_bed_df = pd.DataFrame({\n",
    "        0: filtered_df['chrom'],\n",
    "        1: filtered_df['bed_start'],\n",
    "        2: filtered_df['bed_end'],\n",
    "        3: '.',\n",
    "        4: '.',\n",
    "        5: filtered_df['bed_strand']\n",
    "        #'+' for mex combined bed dfs use filtered_df['bed_strand'] otherwise set strand to '+'\n",
    "    })\n",
    "\n",
    "\n",
    "    ## Duplicate each row with '-' strand\n",
    "    modkit_bed_df_minus = modkit_bed_df.copy()\n",
    "    ##  if 6th colomn is + set to - otherwise set to +\n",
    "    modkit_bed_df_minus[5] = np.where(modkit_bed_df_minus[5] == '+', '-', '+')\n",
    "    ## Combine the '+' and '-' strand dataframes\n",
    "    modkit_bed_df = pd.concat([modkit_bed_df, modkit_bed_df_minus]).sort_index().reset_index(drop=True)\n",
    "\n",
    "    # Remove the header\n",
    "    modkit_bed_df.columns = range(modkit_bed_df.shape[1])\n",
    "    # convert bed_start and bed_end to ints\n",
    "    modkit_bed_df[1] = modkit_bed_df[1].astype(int)\n",
    "    modkit_bed_df[2] = modkit_bed_df[2].astype(int)\n",
    "\n",
    "    return modkit_bed_df\n",
    "\n",
    "def save_modkit_bed_to_temp(modkit_bed_df, filename):\n",
    "    # Create a temporary directory\n",
    "    temp_dir = \"/Data1/git/meyer-nanopore/scripts/analysis/temp_files/\"\n",
    "    \n",
    "    # Create the full path for the temporary file\n",
    "    temp_file_path = os.path.join(temp_dir, filename)\n",
    "    \n",
    "    # Save the dataframe to the temporary file\n",
    "    modkit_bed_df.to_csv(temp_file_path, sep='\\t', header=False, index=False)\n",
    "    \n",
    "    print(f\"Modkit BED file saved to: {temp_file_path}\")\n",
    "    return temp_file_path\n",
    "\n",
    "# Create modkit_bed_df\n",
    "modkit_bed_df = create_modkit_bed_df(combined_bed_df)\n",
    "\n",
    "# Save modkit_bed_df to a temporary file\n",
    "modkit_bed_name = \"modkit_temp.bed\"\n",
    "temp_file_path = save_modkit_bed_to_temp(modkit_bed_df, modkit_bed_name)\n",
    "\n",
    "print(f\"Modkit BED file saved to: {temp_file_path}\")\n",
    "\n",
    "# drop rows with duplicate values\n",
    "modkit_bed_df = modkit_bed_df.drop_duplicates()\n",
    "\n",
    "print(modkit_bed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2577ba9f6146d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_stem = \"/Data1/git/meyer-nanopore/scripts/analysis/temp_files/082624/modkit/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9e443605cbc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set type_selected to unique type in combined_bed_df\n",
    "type_selected = combined_bed_df[\"type\"].unique()\n",
    "print(type_selected)\n",
    "\n",
    "regenerate_bit = True # SEt to true to force regenerate, otherwise load if available.\n",
    "\n",
    "### Generate modkit pileup file, used for plotting m6A/A in a given region.\n",
    "# Generating the list of output_file_names based on the given structure\n",
    "out_file_names = [output_stem + \"modkit-pileup-\" + each_condition +\"_\"+ str(round(each_thresh,2))+\"_\"+str(each_index)+\"_\"+str(each_bamfrac)+ \"_\".join([each_type[-5:] for each_type in [\"SDC2\"]]) + \"_\".join([each_type[0:5] for each_type in [\"SDC2\"]]) + str(bed_window)+\".bed\" for each_condition,each_thresh,each_index, each_bamfrac in zip(conditions,thresh_list,sample_indices,bam_fracs)]\n",
    "\n",
    "# Function to run a single command\n",
    "def modkit_pileup_extract(args, index):\n",
    "    each_bam, each_thresh, each_condition, each_index, each_bamfrac, each_type,modkit_path, output_stem, modkit_bed_name = args\n",
    "\n",
    "    # Use the index to get the correct file name from out_file_names\n",
    "    each_output = out_file_names[index]\n",
    "\n",
    "    # Check if the output file exists\n",
    "    if not regenerate_bit:\n",
    "        if os.path.exists(each_output):\n",
    "            print(f\"File already exists: {each_output}\")\n",
    "            # Read in output file and check if empty\n",
    "            modkit_qc = pd.DataFrame()\n",
    "            try:\n",
    "                modkit_qc = pd.read_csv(each_output, sep=\"\\t\", header=None, nrows=10)\n",
    "            except:\n",
    "                if modkit_qc.empty:\n",
    "                    print(f\"File is empty: {each_output}\")\n",
    "                    return\n",
    "            return\n",
    "    print(f\"Starting on: {each_output}\", \"with bam file: \", each_bam,\"and bedfile:\", modkit_bed_name)\n",
    "    command = [\n",
    "        modkit_path,\n",
    "        \"pileup\",\n",
    "        \"--only-tabs\",\n",
    "        #\"--ignore\",\n",
    "        #\"m\",\n",
    "        \"--threads\",\n",
    "        \"15\",\n",
    "        #\"--filter-threshold\",\n",
    "        #f\"A:{1-each_thresh}\",\n",
    "        #f\"A:{1-each_thresh}\",\n",
    "        \"--mod-thresholds\",\n",
    "        f\"a:{each_thresh}\",\n",
    "        \"--mod-thresholds\",\n",
    "        f\"m:{each_thresh}\",\n",
    "        \"--ref\",\n",
    "        \"/Data1/reference/c_elegans.WS235.genomic.fa\",\n",
    "        \"--filter-threshold\",\n",
    "        f\"A:{1-each_thresh}\",\n",
    "        \"--filter-threshold\",\n",
    "        f\"C:{1-each_thresh}\",\n",
    "        \"--motif\",\n",
    "        \"GC\",\n",
    "        \"1\",\n",
    "        \"--motif\",\n",
    "        \"A\",\n",
    "        \"0\",\n",
    "        \"--log-filepath\",\n",
    "        output_stem + each_condition + str(each_index) + \"_modkit-pileup.log\",\n",
    "        \"--include-bed\",\n",
    "        modkit_bed_name,\n",
    "        each_bam,\n",
    "        each_output\n",
    "    ]\n",
    "    subprocess.run(command, text=True)\n",
    "\n",
    "# Now you need to adjust the task_args to include the index\n",
    "# Instead of directly zipping, enumerate one of the lists to get the index\n",
    "task_args_with_index = [(args, index) for index, args in enumerate(zip(\n",
    "    new_bam_files,\n",
    "    thresh_list,\n",
    "    conditions,\n",
    "    sample_indices,\n",
    "    bam_fracs,\n",
    "    [type_selected]*len(new_bam_files),\n",
    "    [modkit_path]*len(new_bam_files),\n",
    "    [output_stem]*len(new_bam_files),\n",
    "    [temp_file_path]*len(new_bam_files),\n",
    "))]\n",
    "\n",
    "# Execute commands in parallel, unpacking the arguments and index within the map call\n",
    "with Pool(\n",
    "    processes=10\n",
    ") as pool:\n",
    "    pool.starmap(modkit_pileup_extract, task_args_with_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4715c3b5fd1a1fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Add bed and condition details to modkit output for plotting\n",
    "# Using DataFrame merging to achieve the task without explicit loops\n",
    "## Looks up the bed_start and bed_end values for each row in bedmethyl_df\n",
    "\n",
    "def add_bed_columns_no_loops(bedmethyl_df_loc, combined_bed_df):\n",
    "    # Calculate midpoint in combined_bed_df\n",
    "    combined_bed_df['midpoint'] = (combined_bed_df['bed_start'] + combined_bed_df['bed_end']) / 2\n",
    "    # Convert midpoint to the same type as start_position (int, in this case)\n",
    "    combined_bed_df['midpoint'] = combined_bed_df['midpoint'].astype(int)\n",
    "\n",
    "    combined_bed_df = combined_bed_df.sort_values(by='midpoint')\n",
    "\n",
    "    # Ensure that start_position is of type int (if it's not already)\n",
    "    bedmethyl_df_loc['start_position'] = bedmethyl_df_loc['start_position'].astype(int)\n",
    "\n",
    "    # Merge bedmethyl_df with combined_bed_df based on the nearest midpoint\n",
    "    merged_df = pd.merge_asof(bedmethyl_df_loc.sort_values('start_position'),\n",
    "                              combined_bed_df,\n",
    "                              by='chrom',\n",
    "                              left_on='start_position',\n",
    "                              right_on='midpoint',\n",
    "                              direction='nearest')\n",
    "\n",
    "    # Filter out rows where the start_position is not within the bed_start and bed_end range\n",
    "    merged_df = merged_df.loc[(merged_df['start_position'] >= merged_df['bed_start']) &\n",
    "                                (merged_df['start_position'] <= merged_df['bed_end'])]\n",
    "\n",
    "    #reset index\n",
    "    merged_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    # Create the final DataFrame by merging the merged DataFrame back to the original bedmethyl_df\n",
    "    final_df = pd.merge(bedmethyl_df_loc,\n",
    "                        merged_df[['chrom', 'start_position', 'bed_start', 'bed_end', 'bed_strand', 'type', 'chr_type']],\n",
    "                        on=['chrom', 'start_position'],\n",
    "                        how='left')\n",
    "\n",
    "    # Drop all final_df rows where type == NaN\n",
    "    final_df = final_df[final_df['type'].notna()]\n",
    "\n",
    "    return final_df\n",
    "\n",
    "def add_bed_columns_no_loops_small(bedmethyl_df_loc, combined_bed_df):\n",
    "    # Ensure 'start_position' is of type int\n",
    "    bedmethyl_df_loc['start_position'] = bedmethyl_df_loc['start_position'].astype(int)\n",
    "    \n",
    "    # Merge 'bedmethyl_df_loc' with 'combined_bed_df' on 'chrom' using an inner join\n",
    "    merged_df = pd.merge(bedmethyl_df_loc, combined_bed_df, on='chrom', how='inner')\n",
    "\n",
    "    # Filter rows where 'start_position' is within 'bed_start' and 'bed_end'\n",
    "    merged_df = merged_df[\n",
    "        (merged_df['start_position'] >= merged_df['bed_start']) &\n",
    "        (merged_df['start_position'] <= merged_df['bed_end'])\n",
    "    ]\n",
    "\n",
    "    # Reset index\n",
    "    merged_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Drop rows where 'type' is NaN, if necessary\n",
    "    merged_df = merged_df[merged_df['type'].notna()]\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "\n",
    "# combined_bed_df = nanotools.create_lookup_bed(new_bed_files)\n",
    "\n",
    "# Initialize comb_bedmethyl_plot_df\n",
    "comb_bedmethyl_df = pd.DataFrame()\n",
    "\n",
    "# Create combined plotting dataframe\n",
    "for each_output,each_condition,each_exp_id in zip(out_file_names,conditions,exp_ids):\n",
    "    #print(\"Starting on:\",each_output)\n",
    "    # Define bed methyl columns and import bedmethyl file\n",
    "    bedmethyl_df = pd.DataFrame()\n",
    "    bedmethyl_cols = ['chrom','start_position','end_position','modified_base_code','score','strand','start_position_compat','end_position_compat','color','Nvalid_cov','fraction_modified','Nmod','Ncanonical','Nother_mod','Ndelete','Nfail','Ndiff','Nnocall']\n",
    "\n",
    "    bedmethyl_df=pd.read_csv(each_output, sep=\"\\t\", header=None, names=bedmethyl_cols)\n",
    "\n",
    "    # if bedmethyl_df is empty\n",
    "    # drop all rows where modified_base_code is not equal to \"a,A,0\" or \"m,GC,1\"\n",
    "    bedmethyl_df = bedmethyl_df[bedmethyl_df['modified_base_code'].isin(['a,A,0'])]#,'m,GC,1'])]\n",
    "    if bedmethyl_df.empty:\n",
    "        print(\"!Read in empty csv!!\")\n",
    "        print(\"Tried to select:\",each_output,\" \",each_condition,\" \",each_exp_id, \"and failed...\")\n",
    "        continue\n",
    "\n",
    "\n",
    "    # sort bedmethyl_df by chrom and start_position\n",
    "    bedmethyl_df = bedmethyl_df.sort_values(['start_position'], ascending=[True])\n",
    "    # drop any rows with a nan\n",
    "    bedmethyl_df = bedmethyl_df.dropna()\n",
    "    bedmethyl_df.drop_duplicates(inplace=True)\n",
    "    bedmethyl_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    bedmethyl_df = add_bed_columns_no_loops(bedmethyl_df, combined_bed_df)\n",
    "    # Add rel_start and rel_end columns equal to start-bed_start and end-bed_start\n",
    "\n",
    "    # if type_selected contains 'gene', map to metagene bins\n",
    "    if 'gene' in type_selected[0] or 'damID' in type_selected[0]:\n",
    "        print(\"Mapping to metagene bins...\")\n",
    "        # Define a function to process each group\n",
    "        def process_group(group_tuple, num_bins, edge_window_size, sum_columns):\n",
    "            _, group = group_tuple\n",
    "            min_pos, max_pos = group['start_position'].min(), group['start_position'].max()\n",
    "            bed_start, bed_end = group['bed_start'].iloc[0], group['bed_end'].iloc[0]\n",
    "\n",
    "            group['rel_start'] = np.where(group['start_position'] < bed_start + edge_window_size, # if start position is less than or equal to bed_start + edge_window_size then\n",
    "                                          group['start_position'] - bed_start - edge_window_size, # shift rel_pos by bed_start and window size. Otherwise\n",
    "                                          np.where(group['start_position'] > bed_end - edge_window_size, #if start position is greater than bed_end - edge_window_size then\n",
    "                                                   num_bins + edge_window_size - (bed_end - group['start_position']), #assign to bin otherwise\n",
    "                                                   100000)) # assign to nan\n",
    "\n",
    "            # delete any points outside of window\n",
    "            if (max_pos - min_pos) > (num_bins + 2 * edge_window_size):\n",
    "                binning_mask = (group['start_position'] >= bed_start + edge_window_size) & (group['start_position'] <= bed_end - edge_window_size)\n",
    "                bin_edges = np.linspace(bed_start + edge_window_size, bed_end - edge_window_size, num_bins + 1)\n",
    "                group.loc[binning_mask, 'rel_start'] = np.digitize(group.loc[binning_mask, 'start_position'], bins=bin_edges, right=True)\n",
    "\n",
    "            return group\n",
    "\n",
    "        def map_to_metagene_bins_and_sum(df, num_bins=1000, edge_window_size=500):\n",
    "            # Columns for summing within bins\n",
    "            sum_columns = ['Nmod', 'Ncanonical', 'Nother_mod', 'Ndelete', 'Nfail', 'Ndiff', 'Nnocall','Nvalid_cov']\n",
    "            # Columns to retain in the final DataFrame\n",
    "            retain_columns = ['bed_strand', 'chr_type', 'strand', 'bed_end','type']\n",
    "            # Adjust group columns based on the updated request\n",
    "            group_columns = ['bed_start', 'chrom' ,'modified_base_code']\n",
    "\n",
    "            # Splitting the DataFrame into groups\n",
    "            groups = list(df.groupby(group_columns))\n",
    "\n",
    "            # Using multiprocessing to process groups in parallel\n",
    "            with Pool(500) as pool:\n",
    "                processed_groups = pool.starmap(process_group, [(group, num_bins, edge_window_size, sum_columns) for group in groups])\n",
    "\n",
    "            # Combine the processed groups into a single DataFrame\n",
    "            result_df = pd.concat(processed_groups, ignore_index=True)\n",
    "\n",
    "            # Summing within bins and merging\n",
    "            sum_group_columns = group_columns + ['rel_start']\n",
    "            summed_df = result_df.groupby(sum_group_columns)[sum_columns].sum()\n",
    "            merged_df = pd.merge(result_df[sum_group_columns + retain_columns].drop_duplicates(), summed_df, on=sum_group_columns, how='left')\n",
    "\n",
    "            return merged_df\n",
    "\n",
    "        bedmethyl_df = map_to_metagene_bins_and_sum(bedmethyl_df, num_bins=num_bins, edge_window_size=bed_window)\n",
    "\n",
    "    else:\n",
    "        bedmethyl_df['rel_start'] = bedmethyl_df['start_position'] - bedmethyl_df['bed_start'] - bed_window + 2\n",
    "\n",
    "        # if \"MEX_\" in \"type\" then multiple rel_start by -1\n",
    "        if \"MEX_\" in type_selected[0]:\n",
    "            bedmethyl_df['rel_start'] = bedmethyl_df['rel_start']  #-1 # *-1\n",
    "\n",
    "    # set rel_start to int\n",
    "    bedmethyl_df['rel_start'] = bedmethyl_df['rel_start'].astype(int)\n",
    "\n",
    "    #print(\"2. bedmethyl_df\")\n",
    "    #display(bedmethyl_df)\n",
    "    bedmethyl_df['condition'] = each_condition\n",
    "    bedmethyl_df['exp_id'] = each_exp_id\n",
    "    # eliminate levels in dataframe\n",
    "\n",
    "    # if bedmethyl_df is empty\n",
    "    if bedmethyl_df.empty:\n",
    "        print(\"!Bedmethyl_df is empty!\")\n",
    "        print(\"Tried to select:\",each_output,\" \",each_condition,\" \",each_exp_id, \"and failed...\")\n",
    "        continue\n",
    "\n",
    "    # if comb_bedmethyl_plot_df is null, set it equal to bedmethyl_plot\n",
    "    if comb_bedmethyl_df.empty:\n",
    "        print(\"comb_bedmethyl_plot_df is empty, setting it equal to bedmethyl_plot...\")\n",
    "        comb_bedmethyl_df = bedmethyl_df\n",
    "        #print(\"comb_bedmethyl_plot_df:\",comb_bedmethyl_plot_df)\n",
    "    # else append bedmethyl_plot to comb_bedmethyl_plot_df\n",
    "    else:\n",
    "        print(\"comb_bedmethyl_plot_df is not empty, appending bedmethyl_plot...\")\n",
    "        comb_bedmethyl_df = comb_bedmethyl_df.append(bedmethyl_df)\n",
    "        #print(\"comb_bedmethyl_plot_df:\",comb_bedmethyl_plot_df)\n",
    "\n",
    "comb_bedmethyl_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "#print(\"head\")\n",
    "#display(comb_bedmethyl_df.head(100))\n",
    "print(\"sample\")\n",
    "display(nanotools.display_sample_rows(comb_bedmethyl_df,10))\n",
    "#print(\"tail\")\n",
    "#display(comb_bedmethyl_df.tail(100))\n",
    "\n",
    "#print head of first 15 cols\n",
    "print(\"head\")\n",
    "display(comb_bedmethyl_df.iloc[:,15:30].head(10))\n",
    "\n",
    "# print unique type counts\n",
    "print(\"unique type counts\")\n",
    "print(comb_bedmethyl_df['type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419b15de8c63b144",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyranges as pr\n",
    "\n",
    "# Load FIMO files\n",
    "fimo_files = [\n",
    "    \"/Data1/ext_data/motifs/fimo_MEX_0.01.tsv\",\n",
    "    \"/Data1/ext_data/motifs/fimo_MEXII_0.01.tsv\",\n",
    "    \"/Data1/ext_data/motifs/fimo_motifc_0.01.tsv\"\n",
    "]\n",
    "\n",
    "dfs = []\n",
    "for file in fimo_files:\n",
    "    print(f\"Loading {file}\")\n",
    "    df = pd.read_csv(file, sep='\\t')\n",
    "    print(f\"Loaded {len(df)} rows from {file}\")\n",
    "    dfs.append(df)\n",
    "\n",
    "# Combine dataframes\n",
    "fimo_df = pd.concat(dfs, ignore_index=True)\n",
    "print(f\"Combined dataframe has {len(fimo_df)} rows\")\n",
    "\n",
    "# keep 200 random rows\n",
    "#fimo_df = fimo_df.sample(n=200, random_state=1)\n",
    "\n",
    "# Convert 'chrI' to 'CHROMOSOME_I' in 'sequence_name'\n",
    "print(\"Converting 'sequence_name' to 'CHROMOSOME_*' format\")\n",
    "fimo_df['sequence_name'] = fimo_df['sequence_name'].str.replace('chr', 'CHROMOSOME_')\n",
    "\n",
    "# Keep only rows where ln(pv-alue) <= -6.5\n",
    "fimo_df['ln_p_value'] = round(np.log(fimo_df['p-value']), 1)\n",
    "fimo_df = fimo_df[fimo_df['ln_p_value'] <= -8]  # Adjust as needed\n",
    "print(f\"Filtered dataframe has {len(fimo_df)} rows\")\n",
    "\n",
    "# Select relevant columns\n",
    "print(\"Selecting relevant columns\")\n",
    "fimo_df = fimo_df[['sequence_name', 'start', 'stop', 'strand', 'score', 'ln_p_value', 'motif_id']]\n",
    "fimo_df = fimo_df.rename(columns={'sequence_name': 'chr'})\n",
    "\n",
    "# Assign motif priorities\n",
    "print(\"Assigning motif priorities\")\n",
    "motif_priority = {'MEXII': 1, 'MEX': 2, 'motifC': 3}\n",
    "fimo_df['motif_priority'] = fimo_df['motif_id'].map(motif_priority)\n",
    "\n",
    "# Sort by chromosome, start, and motif priority\n",
    "print(\"Sorting dataframe\")\n",
    "fimo_df = fimo_df.sort_values(by=['chr', 'start', 'motif_priority'])\n",
    "\n",
    "# Deduplicate overlapping intervals\n",
    "# Deduplicate overlapping intervals using pyranges\n",
    "print(\"Deduplicating overlapping intervals using pyranges\")\n",
    "\n",
    "# Rename columns to match PyRanges requirements\n",
    "fimo_df = fimo_df.rename(columns={'chr': 'Chromosome', 'start': 'Start', 'stop': 'End'})\n",
    "\n",
    "# Ensure 'Start' and 'End' are integers\n",
    "fimo_df['Start'] = fimo_df['Start'].astype(int)\n",
    "fimo_df['End'] = fimo_df['End'].astype(int)\n",
    "\n",
    "# Convert DataFrame to PyRanges object\n",
    "pr_df = pr.PyRanges(fimo_df)\n",
    "\n",
    "# Cluster overlapping intervals\n",
    "clusters = pr_df.cluster()\n",
    "\n",
    "# Convert back to DataFrame\n",
    "clusters_df = clusters.df\n",
    "\n",
    "# Sort by Cluster and motif_priority\n",
    "clusters_df = clusters_df.sort_values(['Cluster', 'motif_priority'])\n",
    "\n",
    "# Drop duplicates, keeping the interval with the lowest motif_priority in each cluster\n",
    "fimo_dedup_df = clusters_df.drop_duplicates(subset=['Cluster'], keep='first')\n",
    "\n",
    "# Optionally, rename columns back to original names\n",
    "fimo_dedup_df = fimo_dedup_df.rename(columns={'Chromosome': 'chr', 'Start': 'start', 'End': 'stop'})\n",
    "\n",
    "print(f\"Deduplicated dataframe has {len(fimo_dedup_df)} rows\")\n",
    "\n",
    "# Rename columns for PyRanges compatibility\n",
    "print(\"Renaming columns for PyRanges compatibility\")\n",
    "fimo_df_renamed = fimo_dedup_df.rename(columns={'chr': 'Chromosome', 'start': 'Start', 'stop': 'End'})\n",
    "comb_bedmethyl_df_renamed = comb_bedmethyl_df.rename(columns={'chrom': 'Chromosome', 'bed_start': 'Start', 'bed_end': 'End'})\n",
    "\n",
    "# Convert DataFrames to PyRanges objects\n",
    "print(\"Converting DataFrames to PyRanges objects\")\n",
    "fimo_pr = pr.PyRanges(fimo_df_renamed)\n",
    "comb_bedmethyl_df_pr = pr.PyRanges(comb_bedmethyl_df_renamed)\n",
    "\n",
    "# Perform the join to find overlapping intervals\n",
    "print(\"Performing join to find overlapping intervals\")\n",
    "overlap_result = comb_bedmethyl_df_pr.join(fimo_pr, suffix=\"_fimo\")\n",
    "\n",
    "# Extract motif_start and motif_id values and aggregate as pairs\n",
    "print(\"Aggregating overlapping intervals\")\n",
    "overlap_df = overlap_result.df\n",
    "agg_df = overlap_df.groupby(['Chromosome', 'Start', 'End']).apply(\n",
    "    lambda x: list(zip(x['Start_fimo'], x['motif_id'], x['ln_p_value']))\n",
    ").reset_index(name='motif_info')\n",
    "\n",
    "# Remove duplicate ('motif_start', 'motif_id') pairs if needed\n",
    "print(\"Removing duplicate motif info\")\n",
    "agg_df['motif_info'] = agg_df['motif_info'].apply(lambda x: list(set(x)))\n",
    "\n",
    "# Separate 'motif_info' into 'motif_start' and 'motif_id' lists\n",
    "print(\"Separating 'motif_info' into 'motif_start' and 'motif_id'\")\n",
    "agg_df['motif_start'] = agg_df['motif_info'].apply(lambda x: [item[0] for item in x])\n",
    "agg_df['motif_id'] = agg_df['motif_info'].apply(lambda x: [item[1] for item in x])\n",
    "agg_df['motif_score'] = agg_df['motif_info'].apply(lambda x: [item[2] for item in x])\n",
    "\n",
    "# Compute 'motif_rel_start'\n",
    "print(\"Computing 'motif_rel_start'\")\n",
    "agg_df['motif_rel_start'] = agg_df.apply(\n",
    "    lambda row: [start - row['Start'] - bed_window + 2 for start in row['motif_start']], axis=1\n",
    ")\n",
    "\n",
    "# Rename columns back to original names in comb_bedmethyl_df\n",
    "print(\"Renaming columns back to original names in comb_bedmethyl_df\")\n",
    "comb_bedmethyl_df = comb_bedmethyl_df.rename(columns={\n",
    "    'Chromosome': 'chrom',\n",
    "    'Start': 'bed_start',\n",
    "    'End': 'bed_end',\n",
    "})\n",
    "\n",
    "# Rename columns back to the original names\n",
    "print(\"Renaming columns back to original names\")\n",
    "agg_df = agg_df.rename(columns={\n",
    "    'Chromosome': 'chrom',\n",
    "    'Start': 'bed_start',\n",
    "    'End': 'bed_end',\n",
    "})\n",
    "\n",
    "# Merge the aggregated data back into the original DataFrame\n",
    "print(\"Merging aggregated data back into comb_bedmethyl_df\")\n",
    "comb_bedmethyl_df = comb_bedmethyl_df.merge(\n",
    "    agg_df[['chrom', 'bed_start', 'bed_end', 'motif_rel_start', 'motif_id','motif_score']],\n",
    "    how='left',\n",
    "    on=['chrom', 'bed_start', 'bed_end']\n",
    ")\n",
    "\n",
    "print(comb_bedmethyl_df.columns)\n",
    "\n",
    "# Convert lists to tuples\n",
    "print(\"Converting lists to tuples\")\n",
    "comb_bedmethyl_df['motif_rel_start'] = comb_bedmethyl_df['motif_rel_start'].apply(\n",
    "    lambda x: tuple(x) if isinstance(x, list) else x\n",
    ")\n",
    "comb_bedmethyl_df['motif_id'] = comb_bedmethyl_df['motif_id'].apply(\n",
    "    lambda x: tuple(x) if isinstance(x, list) else x\n",
    ")\n",
    "comb_bedmethyl_df['motif_score'] = comb_bedmethyl_df['motif_score'].apply(\n",
    "    lambda x: tuple(x) if isinstance(x, list) else x\n",
    ")\n",
    "\n",
    "# Replace NaN with empty tuples\n",
    "print(\"Replacing NaN with empty tuples\")\n",
    "comb_bedmethyl_df['motif_rel_start'] = comb_bedmethyl_df['motif_rel_start'].apply(\n",
    "    lambda x: tuple() if pd.isna(x) else x\n",
    ")\n",
    "\n",
    "comb_bedmethyl_df['motif_id'] = comb_bedmethyl_df['motif_id'].apply(\n",
    "    lambda x: tuple() if pd.isna(x) else x\n",
    ")\n",
    "\n",
    "# Convert all tuple elements in motif_rel_start to int\n",
    "comb_bedmethyl_df['motif_rel_start'] = comb_bedmethyl_df['motif_rel_start'].apply(\n",
    "    lambda x: tuple(map(int, x)) if isinstance(x, tuple) else x\n",
    ")\n",
    "\n",
    "comb_bedmethyl_df['motif_score'] = comb_bedmethyl_df['motif_score'].apply(\n",
    "    lambda x: tuple(map(float, x)) if isinstance(x, tuple) else x\n",
    ")\n",
    "\n",
    "# Debugging statements\n",
    "print(\"Final DataFrame:\")\n",
    "print(comb_bedmethyl_df.head())\n",
    "\n",
    "print(\"Unique type counts:\")\n",
    "print(comb_bedmethyl_df['type'].value_counts())\n",
    "\n",
    "print(\"Rows containing 'rex' in 'type':\")\n",
    "print(comb_bedmethyl_df[comb_bedmethyl_df['type'].str.contains('rex')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6675ba70cb1ca60",
   "metadata": {},
   "outputs": [],
   "source": [
    "### SHIFT AND TRANSFORM (OPTIONAL)\n",
    "align_zero_bool = False\n",
    "flip_bool = False\n",
    "plot_motifs = True\n",
    "\n",
    "# Set normalization_type parameter here\n",
    "normalization_type = 'global'  # or 'local'\n",
    "\n",
    "def compute_lag_for_maximum_alignment(series1, bed_start1):\n",
    "    flip = 0\n",
    "    pos_max_series1 = np.argmax(series1)\n",
    "    lag = (round(len(series1)/2))-pos_max_series1\n",
    "    return (lag, flip)\n",
    "\n",
    "def get_continuous_series(df_subset):\n",
    "    # Create a Series with rel_start as the index and use 'weighted_norm_mod_frac' as the value\n",
    "    series_filled = df_subset.set_index('rel_start')['weighted_norm_mod_frac']\n",
    "    series_filled = series_filled.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "    try:\n",
    "        series_filled = series_filled.reindex(range(int(series_filled.index.min()), int(series_filled.index.max()) + 1), fill_value=0)\n",
    "    except:\n",
    "        print(\"Failed series_filled:\", series_filled)\n",
    "        print(\"Duplicate indexes:\",series_filled.index[series_filled.index.duplicated()])\n",
    "\n",
    "    return series_filled.values\n",
    "\n",
    "def align_profiles(df):\n",
    "    df = df.sort_values(['bed_start', 'rel_start']).copy()\n",
    "    bed_starts = df['bed_start'].unique()\n",
    "\n",
    "    # Determine the reference bed_start\n",
    "    summed_Nvalid_cov = df.groupby('bed_start')['Nvalid_cov'].sum()\n",
    "    reference_bed_start = summed_Nvalid_cov.idxmax()\n",
    "    series_reference = get_continuous_series(df[df['bed_start'] == reference_bed_start])\n",
    "\n",
    "    # Calculate the number of positions to shift\n",
    "    shift_positions = int(round(len(series_reference)/2)) - np.argmax(series_reference)\n",
    "\n",
    "    # Shift the entire series_reference by shift_positions\n",
    "    if shift_positions > 0:  # shift to the left\n",
    "        series_reference = np.concatenate(([0]*shift_positions, series_reference))\n",
    "    else:\n",
    "        series_reference = np.concatenate((series_reference,[0]*abs(shift_positions)))\n",
    "\n",
    "    df[\"flipped\"] = 0\n",
    "\n",
    "    for other_bed_start in bed_starts:\n",
    "        series_to_shift = get_continuous_series(df[df['bed_start'] == other_bed_start])\n",
    "        lag, flip = compute_lag_for_maximum_alignment(series_to_shift, other_bed_start)\n",
    "\n",
    "        df.loc[df['bed_start'] == other_bed_start, 'shift'] = lag\n",
    "        df.loc[df['bed_start'] == other_bed_start, 'flipped'] = 1 if flip else 0\n",
    "\n",
    "    total_flipped = df[df['flipped'] == 1]['bed_start'].nunique()\n",
    "    lag_distribution = df['shift'].describe()\n",
    "    print(f\"Total bed_starts flipped: {total_flipped} out of {len(bed_starts) - 1}\")\n",
    "    print(\"Lag Distribution:\")\n",
    "    print(lag_distribution)\n",
    "\n",
    "    return df\n",
    "\n",
    "print(\"Copying and dropping rows...\")\n",
    "comb_bedmethyl_plot_df = comb_bedmethyl_df.copy()\n",
    "\n",
    "nanotools.display_sample_rows(comb_bedmethyl_plot_df,5)\n",
    "\n",
    "def debug_print(message, df=None, group_cols=None):\n",
    "    print(f\"DEBUG: {message}\")\n",
    "    if df is not None:\n",
    "        print(f\"Shape: {df.shape}\")\n",
    "        print(f\"Columns: {df.columns.tolist()}\")\n",
    "        if group_cols:\n",
    "            print(df.groupby(group_cols).size())\n",
    "        print(df.head())\n",
    "    print(\"\\n\")\n",
    "\n",
    "for each_type in type_selected:\n",
    "    if any(x in each_type for x in [\"TSS\", \"TES\", \"MEX\", \"MEXII\", \"gene\"]):\n",
    "        print(f\"Strand orientation sensitive {each_type} type selected, multiplying rel_start by -1 for '-' strand genes...\")\n",
    "        if 'gene' in each_type:\n",
    "            comb_bedmethyl_plot_df['rel_start'] -= num_bins/2\n",
    "\n",
    "        mask = (comb_bedmethyl_plot_df['type'] == each_type) & (comb_bedmethyl_plot_df['bed_strand'] == '-')\n",
    "        comb_bedmethyl_plot_df.loc[mask, 'rel_start'] *= -1\n",
    "        comb_bedmethyl_plot_df.loc[mask, 'rel_start'] += 13\n",
    "\n",
    "        if plot_motifs:\n",
    "            print(\"adjusting motif start...\")\n",
    "            comb_bedmethyl_plot_df.loc[\n",
    "                comb_bedmethyl_plot_df.index.isin(mask.index) & mask.fillna(False), 'motif_rel_start'\n",
    "            ] = comb_bedmethyl_plot_df.loc[\n",
    "                comb_bedmethyl_plot_df.index.isin(mask.index) & mask.fillna(False), 'motif_rel_start'\n",
    "            ].apply(lambda motif_tuple: tuple(int(x) * -1 + 13 for x in motif_tuple))\n",
    "\n",
    "        debug_print(\"After gene strand adjustment\", comb_bedmethyl_plot_df, ['type'])\n",
    "\n",
    "        if 'gene' in each_type:\n",
    "            print(\"Adjusting rel_start for gene type...\")\n",
    "            comb_bedmethyl_plot_df['rel_start'] += num_bins/2\n",
    "            debug_print(\"After gene bin adjustment\", comb_bedmethyl_plot_df, ['type'])\n",
    "\n",
    "if plot_motifs:\n",
    "    print(\"Grouping by chrom, rel_start, exp_id, modified_base_code, condition, type, chr_type, strand, motif_rel_start, motif_id...\")\n",
    "    grouped_df = comb_bedmethyl_plot_df.groupby([\n",
    "        'chrom', 'rel_start', 'motif_rel_start', 'motif_id','motif_score',\n",
    "        'exp_id', 'modified_base_code', 'condition',\n",
    "        'type', 'chr_type', 'strand','bed_start','bed_strand'\n",
    "    ]).agg({\n",
    "        'Nvalid_cov': 'sum',\n",
    "        'Nmod': 'sum',\n",
    "        'Ncanonical': 'sum',\n",
    "        'Nother_mod': 'sum'\n",
    "    }).reset_index()\n",
    "else:\n",
    "    print(\"Grouping by chrom, rel_start, exp_id, modified_base_code, condition, type, chr_type...\")\n",
    "    grouped_df = comb_bedmethyl_plot_df.groupby([\n",
    "        'chrom', 'rel_start', 'exp_id', 'modified_base_code', 'condition',\n",
    "        'type', 'chr_type', 'strand'\n",
    "    ]).agg({\n",
    "        'Nvalid_cov': 'sum',\n",
    "        'Nmod': 'sum',\n",
    "        'Ncanonical': 'sum',\n",
    "        'Nother_mod': 'sum'\n",
    "    }).reset_index()\n",
    "\n",
    "print(\"Calculating normalized m6A...\")\n",
    "grouped_df['raw_mod_frac'] = grouped_df['Nmod'] / (grouped_df['Nmod'] + grouped_df['Ncanonical'])\n",
    "\n",
    "coverage_df['exp_id'] = coverage_df['exp_id'].str.strip()\n",
    "grouped_df['exp_id'] = grouped_df['exp_id'].str.strip()\n",
    "\n",
    "# Merge with coverage_df to get exp_id_m6A_frac\n",
    "merged_df = pd.merge(grouped_df, coverage_df[['exp_id', 'm6A_frac']], on=['exp_id'], how='left')\n",
    "merged_df.rename(columns={'m6A_frac': 'exp_id_m6A_frac'}, inplace=True)\n",
    "\n",
    "if normalization_type == 'global':\n",
    "    # Global normalization\n",
    "    merged_df['norm_mod_frac_init'] = merged_df['exp_id_m6A_frac']\n",
    "elif normalization_type == 'local':\n",
    "    # Local normalization using intergenic mod_frac\n",
    "    intergenic_df = grouped_df[grouped_df['type'].str.contains(\"intergenic\", case=False)]\n",
    "    local_mod_frac = intergenic_df.groupby('exp_id').agg({\n",
    "        'Nmod': 'sum',\n",
    "        'Ncanonical': 'sum'\n",
    "    }).reset_index()\n",
    "    local_mod_frac['mod_frac_local'] = local_mod_frac['Nmod'] / (local_mod_frac['Nmod'] + local_mod_frac['Ncanonical'])\n",
    "\n",
    "    merged_df = pd.merge(merged_df, local_mod_frac[['exp_id', 'mod_frac_local']], on='exp_id', how='left')\n",
    "    merged_df['mod_frac_local'] = merged_df['mod_frac_local'].fillna(1)\n",
    "    merged_df['norm_mod_frac_init'] = merged_df['mod_frac_local']\n",
    "else:\n",
    "    raise ValueError(\"Invalid normalization_type. Choose 'global' or 'local'.\")\n",
    "\n",
    "plot_df = merged_df[grouped_df.columns.tolist() + ['norm_mod_frac_init']]\n",
    "\n",
    "if plot_motifs:\n",
    "    plot_df = plot_df.groupby([\n",
    "        'rel_start', 'modified_base_code', 'motif_rel_start', 'motif_id','motif_score', 'condition', 'type', 'chr_type','strand','norm_mod_frac_init'\n",
    "    ])[['Nvalid_cov', 'Ncanonical', 'Nmod']].sum().reset_index()\n",
    "else:\n",
    "    plot_df = plot_df.groupby([\n",
    "        'rel_start', 'modified_base_code', 'condition', 'type', 'chr_type','strand','norm_mod_frac_init'\n",
    "    ])[['Nvalid_cov', 'Ncanonical', 'Nmod']].sum().reset_index()\n",
    "\n",
    "# Compute raw_mod_frac and weighted_norm_mod_frac\n",
    "plot_df['raw_mod_frac'] = plot_df['Nmod']/(plot_df['Nmod']+plot_df['Ncanonical'])\n",
    "plot_df['weighted_norm_mod_frac'] = plot_df['raw_mod_frac']/plot_df['norm_mod_frac_init']\n",
    "\n",
    "plot_df.sort_values(['rel_start'], inplace=True)\n",
    "plot_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "print(\"Count of rows by type in merged_df:\")\n",
    "print(merged_df.groupby(['type']).size())\n",
    "print(\"Count of rows by type in plot_df:\")\n",
    "print(plot_df.groupby(['type']).size())\n",
    "\n",
    "print(\"unique type counts\")\n",
    "print(plot_df['type'].value_counts())\n",
    "\n",
    "print(\"plot_df:\")\n",
    "nanotools.display_sample_rows(plot_df,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b8c62caf7a0573",
   "metadata": {},
   "outputs": [],
   "source": [
    "force_replace = True\n",
    "# save final_df to /temp folder as csv, with all configurations in file name if it does not exist. If it does exist, import it.\n",
    "final_fn = \"temp_files/\" + \"final_df_\" + \"_\".join([each_type for each_type in type_selected[:5]]) + str(round(thresh_list[0],2)) + \"_\"+str(bam_fracs[0])+str(bed_window)+\".csv\"\n",
    "final_fn_chip = \"temp_files/\" + \"final_df_chip\" + \"_\".join([each_type for each_type in type_selected[:5]]) + str(round(thresh_list[0],2)) + \"_\"+str(bam_fracs[0])+str(bed_window)+\".csv\"\n",
    "\n",
    "if not force_replace and os.path.exists(final_fn):\n",
    "    print(\"final_df already exists, importing it...\")\n",
    "    plot_df = pd.read_csv(final_fn)\n",
    "    nanotools.display_sample_rows(plot_df,5)\n",
    "else:\n",
    "    print(\"final_df does not exist, saving it...\")\n",
    "    plot_df.to_csv(final_fn, index=False)\n",
    "\n",
    "# if plot_comb_bigwig_df dataframe does not exist:\n",
    "try:\n",
    "    if not force_replace and os.path.exists(final_fn_chip):\n",
    "        print(\"final_df_chip already exists, importing it...\")\n",
    "        plot_comb_bigwig_df = pd.read_csv(final_fn_chip)\n",
    "        nanotools.display_sample_rows(plot_comb_bigwig_df,5)\n",
    "    else:\n",
    "        print(\"final_df_chip does not exist, saving it...\")\n",
    "        plot_comb_bigwig_df.to_csv(final_fn_chip, index=False)\n",
    "except:\n",
    "    print(\"plot_comb_bigwig_df does not exist, skipping...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59f90bfc708ad2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(nanotools)\n",
    "from scipy.signal import gaussian\n",
    "import scipy.ndimage\n",
    "\n",
    "import importlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import gaussian\n",
    "import scipy.ndimage\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "def plot_bedmethyl(comb_bedmethyl_df, conditions_input, chr_types=None, types=None, strands=[\"all\"], window_size=50, metagene_bins=1000, smoothing_type=\"weighted\", selection_indices=None, bed_window=[-500,500], mod_types=['5mC','m6A'], ignore_selec=[], bigwig_df=None, bw_selections=None, plot_motifs=False, plot_type=\"raw\"):\n",
    "    global analysis_cond\n",
    "    fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "    cov_fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "    y_min = float('inf')\n",
    "    y_max = float('-inf')\n",
    "    line_counter = -1\n",
    "\n",
    "    # Drop all rows with rel_start outside of bed_window\n",
    "    comb_bedmethyl_df = comb_bedmethyl_df[(comb_bedmethyl_df['rel_start'] >= bed_window[0]) & (comb_bedmethyl_df['rel_start'] <= bed_window[1])]\n",
    "\n",
    "    motifs_plotted = False\n",
    "\n",
    "    if selection_indices is not None:\n",
    "        conditions = [conditions_input[i] for i in selection_indices]\n",
    "    else:\n",
    "        conditions = conditions_input\n",
    "\n",
    "    for selected_condition in conditions:\n",
    "        for selected_modification in (mod_types or [\"all\"]):\n",
    "            skip_line = False\n",
    "            if selected_modification == '5mC':\n",
    "                selected_mod = 'm,GC,1'\n",
    "            elif selected_modification == 'm6A':\n",
    "                selected_mod = 'a,A,0'\n",
    "            else:\n",
    "                selected_mod = 'all'\n",
    "\n",
    "            for each in ignore_selec:\n",
    "                if selected_condition == conditions_input[each[0]] and selected_modification == each[1]:\n",
    "                    skip_line = True\n",
    "                    break\n",
    "\n",
    "            if skip_line:\n",
    "                continue\n",
    "\n",
    "            # Handle chr_types == [\"all\"] by combining 'Autosome' and 'X' types if needed\n",
    "            if chr_types == [\"all\"]:\n",
    "                selected_chr_types = [\"all_combined\"]\n",
    "                comb_df = comb_bedmethyl_df.copy()\n",
    "                comb_df['chr_type'] = 'all_combined'\n",
    "                if plot_motifs:\n",
    "                    group_fields = ['rel_start', 'motif_rel_start','motif_id','motif_score','condition', 'type', 'strand', 'modified_base_code']\n",
    "                else:\n",
    "                    group_fields = ['rel_start','condition', 'type', 'strand', 'modified_base_code']\n",
    "                sum_fields = ['raw_mod_frac', 'weighted_norm_mod_frac', 'Nvalid_cov','Nmod']\n",
    "                comb_df = comb_df.groupby(group_fields, as_index=False)[sum_fields].sum()\n",
    "            else:\n",
    "                selected_chr_types = chr_types or [\"all\"]\n",
    "                comb_df = comb_bedmethyl_df\n",
    "\n",
    "            for selected_chr_type in selected_chr_types:\n",
    "                for selected_type in (types or [\"all\"]):\n",
    "                    for selected_strand in (strands or [\"all\"]):\n",
    "                        if selected_strand == \"all\":\n",
    "                            selected_strand_types = [\"all_combined\"]\n",
    "                            temp_df = comb_df.copy()\n",
    "                            temp_df['strand'] = \"all_combined\"\n",
    "                            if plot_motifs:\n",
    "                                group_fields = ['rel_start', 'motif_rel_start','motif_id','motif_score','condition', 'type','chr_type', 'modified_base_code']\n",
    "                            else:\n",
    "                                group_fields = ['rel_start','condition', 'type','chr_type', 'modified_base_code']\n",
    "                            sum_fields = ['raw_mod_frac', 'weighted_norm_mod_frac', 'Nvalid_cov','Nmod']\n",
    "                            temp_df = temp_df.groupby(group_fields, as_index=False)[sum_fields].sum()\n",
    "                            df_to_use = temp_df\n",
    "                        else:\n",
    "                            selected_strand_types = strands or [\"all\"]\n",
    "                            df_to_use = comb_df\n",
    "\n",
    "                        # Apply filters\n",
    "                        filters = []\n",
    "                        if selected_condition:\n",
    "                            filters.append(df_to_use['condition'] == selected_condition)\n",
    "                        if selected_chr_type != \"all\":\n",
    "                            filters.append(df_to_use['chr_type'] == selected_chr_type)\n",
    "                        if selected_type != \"all\":\n",
    "                            filters.append(df_to_use['type'] == selected_type)\n",
    "                        if selected_strand != \"all\":\n",
    "                            filters.append(df_to_use['strand'] == selected_strand)\n",
    "                        if selected_mod != \"all\":\n",
    "                            filters.append(df_to_use['modified_base_code'] == selected_mod)\n",
    "                        if not filters:\n",
    "                            base_filter = np.full(len(df_to_use), True)\n",
    "                        else:\n",
    "                            base_filter = np.logical_and.reduce(filters)\n",
    "\n",
    "                        data_filtered = df_to_use.loc[base_filter][['raw_mod_frac', 'weighted_norm_mod_frac', 'rel_start', 'motif_rel_start', 'motif_id', 'motif_score', 'Nvalid_cov', 'Nmod']].copy()\n",
    "                        if data_filtered.empty:\n",
    "                            continue\n",
    "                        data_filtered_nonan = data_filtered.dropna(subset=['motif_rel_start', 'motif_id', 'motif_score'])\n",
    "                        if not data_filtered_nonan.empty:\n",
    "                            import ast\n",
    "\n",
    "                            def ensure_list(x):\n",
    "                                if isinstance(x, str):\n",
    "                                    return ast.literal_eval(x)\n",
    "                                elif isinstance(x, tuple):\n",
    "                                    return list(x)\n",
    "                                elif isinstance(x, list):\n",
    "                                    return x\n",
    "                                else:\n",
    "                                    return [x]\n",
    "                            if not motifs_plotted:\n",
    "                                data_filtered_nonan['motif_rel_start'] = data_filtered_nonan['motif_rel_start'].apply(ensure_list)\n",
    "                                data_filtered_nonan['motif_id'] = data_filtered_nonan['motif_id'].apply(ensure_list)\n",
    "                                data_filtered_nonan['motif_score'] = data_filtered_nonan['motif_score'].apply(ensure_list)\n",
    "                                data_filtered_nonan['motif_info'] = data_filtered_nonan.apply(\n",
    "                                    lambda row: list(zip(row['motif_rel_start'], row['motif_id'], row['motif_score'])),\n",
    "                                    axis=1\n",
    "                                )\n",
    "                                exploded_df = data_filtered_nonan.explode('motif_info')\n",
    "                                exploded_df[['motif_rel_start', 'motif_id', 'motif_score']] = pd.DataFrame(\n",
    "                                    exploded_df['motif_info'].tolist(), index=exploded_df.index\n",
    "                                )\n",
    "                                exploded_df = exploded_df[['motif_rel_start', 'motif_id', 'motif_score']].drop_duplicates()\n",
    "    \n",
    "                                # Group by position and collect all motifs and scores\n",
    "                                grouped_motifs = exploded_df.groupby('motif_rel_start').agg({\n",
    "                                    'motif_id': list,\n",
    "                                    'motif_score': list\n",
    "                                }).reset_index()\n",
    "    \n",
    "                                # Function to calculate vertical position for staggering\n",
    "                                def get_y_position(idx):\n",
    "                                    # Base step size\n",
    "                                    step = -0.025\n",
    "                                    # Stagger every other label\n",
    "                                    return 1 + (idx % 3) * step\n",
    "                                \n",
    "                                id_rep = 0\n",
    "                                # Add vertical dashed lines and staggered labels\n",
    "                                for _, row in grouped_motifs.iterrows():\n",
    "                                    motif_rel_start = row['motif_rel_start']\n",
    "                                    motif_ids = row['motif_id']\n",
    "                                    motif_scores = row['motif_score']\n",
    "    \n",
    "                                    # Add vertical dashed line\n",
    "                                    fig.add_shape(\n",
    "                                        type=\"line\",\n",
    "                                        x0=motif_rel_start, x1=motif_rel_start,\n",
    "                                        y0=0, y1=1,\n",
    "                                        line=dict(color=\"grey\", width=1, dash=\"dash\"),\n",
    "                                        xref=\"x\", yref=\"paper\"\n",
    "                                    )\n",
    "    \n",
    "                                    # Add labels with scores\n",
    "                                    for idx, (motif_id, score) in enumerate(zip(motif_ids, motif_scores)):\n",
    "                                        y_pos = get_y_position(id_rep)\n",
    "                                        label_text = f\"{motif_id} ln({score:.1f})\"\n",
    "                                        \n",
    "                                        #print(\"idx:\",idx,\"y_pos:\",y_pos,\"label_text:\",label_text)\n",
    "                                        \n",
    "                                        fig.add_annotation(\n",
    "                                            x=motif_rel_start,\n",
    "                                            y=y_pos,\n",
    "                                            yref=\"paper\",\n",
    "                                            text=label_text,\n",
    "                                            showarrow=False,\n",
    "                                            #yanchor=\"top\",\n",
    "                                            #xanchor=\"center\",\n",
    "                                            font=dict(size=12, color=\"white\"),\n",
    "                                            bgcolor=\"rgba(0,0,0,0)\"\n",
    "                                        )\n",
    "                                        id_rep += 1\n",
    "                        motifs_plotted = True\n",
    "                        \n",
    "                        if data_filtered.empty or pd.isna(data_filtered['rel_start'].min()) or pd.isna(data_filtered['rel_start'].max()):\n",
    "                            continue\n",
    "\n",
    "                        full_range_df = pd.DataFrame({'rel_start': range(int(data_filtered['rel_start'].min()), int(data_filtered['rel_start'].max() + 1))})\n",
    "                        merged_df = pd.merge(full_range_df, data_filtered, on='rel_start', how='left')\n",
    "\n",
    "                        # Update raw_mod_frac to be equal to Nmod / Nvalid_cov\n",
    "                        merged_df['raw_mod_frac'] = merged_df['Nmod'] / merged_df['Nvalid_cov']\n",
    "\n",
    "                        if smoothing_type != \"weighted\":\n",
    "                            merged_df.fillna({'weighted_norm_mod_frac': 0, 'Nvalid_cov': 0,'Nmod':0,'raw_mod_frac':0}, inplace=True)\n",
    "                        else:\n",
    "                            merged_df.dropna(subset=['weighted_norm_mod_frac', 'Nvalid_cov','Nmod','raw_mod_frac'], inplace=True)\n",
    "                            merged_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "                        # Allow selecting raw or normalized data\n",
    "                        if plot_type == \"raw\":\n",
    "                            m6A_data = merged_df['raw_mod_frac']\n",
    "                        else:\n",
    "                            m6A_data = merged_df['weighted_norm_mod_frac']\n",
    "\n",
    "                        m6A_data_xaxis = merged_df['rel_start']\n",
    "                        Nvalid_cov_data = merged_df['Nvalid_cov']\n",
    "\n",
    "                        # Smooth coverage data\n",
    "                        if smoothing_type == \"none\":\n",
    "                            smoothed_cov_data = Nvalid_cov_data\n",
    "                        else:\n",
    "                            smoothed_cov_data = Nvalid_cov_data.rolling(window=window_size, center=True).mean()\n",
    "\n",
    "                        # Apply smoothing to m6A_data\n",
    "                        if smoothing_type == \"weighted\":\n",
    "                            def weighted_rolling_average(values, weights, window_size):\n",
    "                                def calculate_weighted_avg(window):\n",
    "                                    return (window * weights[window.index]).sum() / weights[window.index].sum()\n",
    "                                return values.rolling(window=window_size, center=True).apply(calculate_weighted_avg, raw=False)\n",
    "                            smoothed_data = weighted_rolling_average(m6A_data, Nvalid_cov_data, window_size)\n",
    "\n",
    "                        elif smoothing_type == \"gaussian\":\n",
    "                            smoothed_data_array = scipy.ndimage.gaussian_filter1d(m6A_data, sigma=window_size)\n",
    "                            smoothed_data = pd.Series(smoothed_data_array)\n",
    "\n",
    "                        elif smoothing_type == \"exponential\":\n",
    "                            def exponential_decay_smoothing(x, alpha=0.1):\n",
    "                                sm = np.zeros_like(x)\n",
    "                                sm[0] = x[0]\n",
    "                                for t in range(1, len(x)):\n",
    "                                    sm[t] = alpha * x[t] + (1 - alpha) * sm[t-1]\n",
    "                                return sm\n",
    "\n",
    "                            def symmetrical_exponential_smoothing(x, alpha=0.1):\n",
    "                                forward_smoothed = exponential_decay_smoothing(x, alpha)\n",
    "                                backward_smoothed = exponential_decay_smoothing(x[::-1], alpha)[::-1]\n",
    "                                return (forward_smoothed + backward_smoothed) / 2\n",
    "\n",
    "                            alpha = 0.05\n",
    "                            smoothed_data_array = symmetrical_exponential_smoothing(m6A_data, alpha=alpha)\n",
    "                            smoothed_data = pd.Series(smoothed_data_array)\n",
    "\n",
    "                        elif smoothing_type == \"lowess\":\n",
    "                            from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "                            smoothed = lowess(m6A_data, m6A_data_xaxis, frac=0.05, it=0)\n",
    "                            m6A_data_xaxis_array, smoothed_data_array = smoothed[:, 0], smoothed[:, 1]\n",
    "                            smoothed_data = pd.Series(smoothed_data_array, index=m6A_data_xaxis_array)\n",
    "                            m6A_data_xaxis = pd.Series(m6A_data_xaxis_array, index=m6A_data_xaxis_array)\n",
    "\n",
    "                        elif smoothing_type == \"none\":\n",
    "                            smoothed_data = m6A_data\n",
    "                        else:\n",
    "                            smoothed_data = m6A_data.rolling(window=window_size, center=True).mean()\n",
    "\n",
    "                        y_min = min(y_min, smoothed_data.min())\n",
    "                        y_max = max(y_max, smoothed_data.max())\n",
    "\n",
    "                        label = f\"{selected_condition}_{selected_chr_type}_{selected_type}_{selected_strand}_{selected_modification}\"\n",
    "\n",
    "                        color = nanotools.get_colors(selected_condition)\n",
    "                        fig.add_trace(\n",
    "                            go.Scatter(\n",
    "                                x=m6A_data_xaxis.values,\n",
    "                                y=smoothed_data.values,\n",
    "                                mode='lines',\n",
    "                                name=label,\n",
    "                                opacity=0.9,\n",
    "                                line=dict(width=3, color=color)\n",
    "                            ),\n",
    "                            secondary_y=False\n",
    "                        )\n",
    "\n",
    "                        color = nanotools.get_colors(selected_condition)\n",
    "                        cov_fig.add_trace(\n",
    "                            go.Scatter(\n",
    "                                x=m6A_data_xaxis.values,\n",
    "                                y=smoothed_cov_data.values,\n",
    "                                mode='lines',\n",
    "                                name=label + \"_Nvalid_cov\",\n",
    "                                opacity=0.9,\n",
    "                                line=dict(width=3, color=color)\n",
    "                            ),\n",
    "                            secondary_y=False\n",
    "                        )\n",
    "\n",
    "    if bigwig_df is not None:\n",
    "        bigwig_df = bigwig_df[(bigwig_df['rel_start'] >= bed_window[0]) & (bigwig_df['rel_start'] <= bed_window[1])]\n",
    "        if bw_selections is not None:\n",
    "            for bw_selection in bw_selections:\n",
    "                for selected_chr_type in (chr_types or [\"all\"]):\n",
    "                    for selected_type in (types or [\"all\"]):\n",
    "                        for selected_strand in (strands or [\"all\"]):\n",
    "                            filters = []\n",
    "                            if bw_selection:\n",
    "                                filters.append(bigwig_df['condition'] == bw_selection)\n",
    "                            if selected_chr_type != \"all\":\n",
    "                                filters.append(bigwig_df['chr_type'] == selected_chr_type)\n",
    "                            if selected_type != \"all\":\n",
    "                                filters.append(bigwig_df['type'] == selected_type)\n",
    "                            if selected_strand != \"all\":\n",
    "                                filters.append(bigwig_df['strand'] == selected_strand)\n",
    "\n",
    "                            if filters:\n",
    "                                base_filter = np.logical_and.reduce(filters)\n",
    "                            else:\n",
    "                                base_filter = np.full(len(bigwig_df), True)\n",
    "\n",
    "                            value_data = bigwig_df.loc[base_filter]['value']\n",
    "                            value_data_xaxis = bigwig_df.loc[base_filter]['rel_start']\n",
    "\n",
    "                            if smoothing_type == \"none\":\n",
    "                                smoothed_data = value_data\n",
    "                            else:\n",
    "                                smoothed_data = value_data.rolling(window=window_size, center=True).mean()\n",
    "\n",
    "                            y_min = min(y_min, smoothed_data.min())\n",
    "                            y_max = max(y_max, smoothed_data.max())\n",
    "\n",
    "                            label = f\"{bw_selection}_{selected_chr_type}_{selected_type}_{selected_strand}\"\n",
    "\n",
    "                            fig.add_trace(\n",
    "                                go.Scatter(\n",
    "                                    x=value_data_xaxis.values,\n",
    "                                    y=smoothed_data.values,\n",
    "                                    mode='lines',\n",
    "                                    name=label,\n",
    "                                    opacity=0.9,\n",
    "                                    line=dict(width=3)\n",
    "                                ),\n",
    "                                secondary_y=True,\n",
    "                            )\n",
    "\n",
    "    # Add a white border shape\n",
    "    border_shape = dict(\n",
    "        type=\"rect\",\n",
    "        x0=0, y0=0, x1=0.95, y1=1,\n",
    "        xref=\"paper\", yref=\"paper\",\n",
    "        line=dict(color=\"white\", width=2, dash='solid'),\n",
    "        fillcolor='rgba(0,0,0,0)',\n",
    "    )\n",
    "\n",
    "    # Title\n",
    "    plot_title = \"_\".join([each_type for each_type in (types or [])])\n",
    "\n",
    "    # Update main figure layout\n",
    "    fig.update_xaxes(showgrid=False,showline=False)\n",
    "             \n",
    "    fig.update_yaxes(showgrid=False,showline=False)\n",
    "    # Add border after all motif lines and annotations are added\n",
    "    fig.add_shape(border_shape)\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=plot_title,\n",
    "        xaxis_title='Genomic Position',\n",
    "        template=\"plotly_white\",\n",
    "        title_font=dict(size=24),\n",
    "        width=900,\n",
    "        height=900,\n",
    "        paper_bgcolor='rgba(0,0,0,0)',\n",
    "        plot_bgcolor='rgba(0,0,0,0)',\n",
    "        font=dict(color='white')\n",
    "    )\n",
    "    fig.update_yaxes(title_text=\"modBase/Base\", secondary_y=False)\n",
    "    #fig.update_yaxes(title_text=\"ChIP enrichment\", secondary_y=True)\n",
    "    fig.update_yaxes(tickformat=\".0%\")\n",
    "    fig.update_layout(legend=dict(\n",
    "        traceorder=\"normal\",\n",
    "        y=-0.2,\n",
    "        x=0.25,\n",
    "        yanchor=\"top\",\n",
    "        orientation='h',\n",
    "        font=dict(size=14),\n",
    "    ))\n",
    "    fig.update_xaxes(range=[bed_window[0], bed_window[1]])\n",
    "\n",
    "    # Adjust tick fonts and axes lines\n",
    "    fig.update_xaxes(\n",
    "        tickfont=dict(size=24),\n",
    "        ticks='inside',\n",
    "        ticklen=10,\n",
    "        tickwidth=2,\n",
    "        tickcolor='white',\n",
    "        showline=False,\n",
    "        zeroline=False\n",
    "        #linecolor='white',\n",
    "    )\n",
    "\n",
    "    fig.update_yaxes(\n",
    "        tickfont=dict(size=24),\n",
    "        ticks='inside',\n",
    "        ticklen=10,\n",
    "        tickwidth=2,\n",
    "        tickcolor='white',\n",
    "        showline=False,\n",
    "        zeroline=False\n",
    "        #linecolor='white',\n",
    "    )\n",
    "\n",
    "    # If plot_type is norm, update y-axis label and format\n",
    "    if plot_type == \"norm\":\n",
    "        fig.update_yaxes(title_text=\"Norm modBase/Base\", secondary_y=False,\n",
    "                         # increase size\n",
    "                         title_font=dict(size=24))\n",
    "        fig.update_yaxes(tickformat=\".2\")\n",
    "\n",
    "    # Update coverage figure layout\n",
    "    cov_fig.update_xaxes(showgrid=False)\n",
    "    cov_fig.update_yaxes(showgrid=False)\n",
    "    cov_plot_title = \"Motif Count\" + \"_\".join([each_type for each_type in (types or [])])\n",
    "    cov_fig.update_layout(\n",
    "        title=cov_plot_title,\n",
    "        xaxis_title='Genomic Position',\n",
    "        title_font=dict(size=24),\n",
    "        template=\"plotly_white\",\n",
    "        width=900,\n",
    "        height=900,\n",
    "        paper_bgcolor='rgba(0,0,0,0)',\n",
    "        plot_bgcolor='rgba(0,0,0,0)',\n",
    "        shapes=[border_shape],\n",
    "        font=dict(color='white')\n",
    "    )\n",
    "    cov_fig.update_yaxes(title_text=\"Nvalid_cov\", secondary_y=False)\n",
    "    cov_fig.update_layout(legend=dict(\n",
    "        traceorder=\"normal\",\n",
    "        y=-1,\n",
    "        x=0.25,\n",
    "        yanchor=\"top\",\n",
    "        orientation='h',\n",
    "        font=dict(size=14),\n",
    "    ))\n",
    "    cov_fig.update_xaxes(range=[bed_window[0], bed_window[1]])\n",
    "\n",
    "    # Add TSS/TES lines if applicable\n",
    "    if any('TSS' in (types or []) for _ in (types or [])) or any('gene' in (types or []) for _ in (types or [])) or any('damID' in (types or []) for _ in (types or [])):\n",
    "        fig.add_shape(type=\"line\", x0=0, y0=0, x1=0, y1=1, line=dict(color=\"grey\", width=1.5), xref=\"x\", yref=\"paper\")\n",
    "        fig.add_annotation(x=0, y=1, yref=\"paper\", text=\"TSS\", showarrow=False, yanchor=\"bottom\", xanchor=\"center\")\n",
    "        cov_fig.add_shape(type=\"line\", x0=0, y0=0, x1=0, y1=1, line=dict(color=\"grey\", width=1.5), xref=\"x\", yref=\"paper\")\n",
    "\n",
    "    if any('TES' in (types or []) for _ in (types or [])) or any('gene' in (types or []) for _ in (types or [])) or any('damID' in (types or []) for _ in (types or [])):\n",
    "        fig.add_shape(type=\"line\", x0=metagene_bins, y0=0, x1=metagene_bins, y1=1, line=dict(color=\"grey\", width=1.5), xref=\"x\", yref=\"paper\")\n",
    "        fig.add_annotation(x=metagene_bins, y=1, yref=\"paper\", text=\"TES\", showarrow=False, yanchor=\"bottom\", xanchor=\"center\")\n",
    "        cov_fig.add_shape(type=\"line\", x0=metagene_bins, y0=0, x1=metagene_bins, y1=1, line=dict(color=\"grey\", width=1.5), xref=\"x\", yref=\"paper\")\n",
    "\n",
    "    return fig, label\n",
    "\n",
    "\n",
    "#Display random 100 rows from comb_bedmethyl_plot_df\n",
    "print(\"type_selected:\",type_selected)\n",
    "#print head of plot_df where type is intergenic_control\n",
    "print(\"analysis_cond:\",analysis_cond)\n",
    "window_s = 50\n",
    "smoothing_type = \"weighted\" #gaussian\n",
    "bed_w = 1000\n",
    "num_bins = 1500\n",
    "plot_type = \"norm\"\n",
    "# Example usage:\n",
    "# Note: final_df and conditions should be defined in your code\n",
    "\n",
    "#list(plot_df['type'].unique()[15:16])\n",
    "if True:\n",
    "    for each_type in plot_df['type'].unique():\n",
    "        print(\"Plotting:\", each_type)\n",
    "        region_fig = plot_bedmethyl(plot_df, analysis_cond, chr_types=[\"X\"], types=[each_type], strands=[\"all\"], window_size=window_s, metagene_bins=num_bins, smoothing_type=smoothing_type,selection_indices=[2,3], bed_window=[-bed_w,bed_w], mod_types=['m6A'],ignore_selec=[],plot_motifs = True,plot_type = plot_type)\n",
    "        \n",
    "        # create folder if it does not exist\n",
    "        if not os.path.exists(\"temp_files/single_weak_mex/N2_DPY27old/\"):\n",
    "            os.makedirs(\"temp_files/single_weak_mex/N2_DPY27old/\")\n",
    "                    \n",
    "        region_fig[0].write_image(\"temp_files/single_weak_mex/N2_DPY27old/\"+smoothing_type+\"_\"+each_type+str(bed_w)+\"bp_withfiber\"+\"_smooth\"+str(window_s)+\".svg\")\n",
    "        region_fig[0].write_image(\"temp_files/single_weak_mex/N2_DPY27old/\"+smoothing_type+\"_\"+each_type+str(bed_w)+\"bp_withfiber\"+\"_smooth\"+str(window_s)+\".png\")\n",
    "        \n",
    "if False:\n",
    "    # just show plot\n",
    "    region_fig = plot_bedmethyl(plot_df, analysis_cond, chr_types=[\"X\"], types=[\"MEX_motif_weak_dcc_2\"], strands=[\"all\"], window_size=window_s, metagene_bins=num_bins, smoothing_type=smoothing_type,selection_indices=[0,1,2,3,4,5], bed_window=[-bed_w,bed_w], mod_types=['m6A'],ignore_selec=[],plot_motifs = True,plot_type = plot_type)\n",
    "    \n",
    "    region_fig[0].show()\n",
    "\n",
    "\n",
    "    #region_fig[0].write_image(\"temp_files/\"+smoothing_type+\"_\"+region_fig[1]+type_selected[0]+str(bed_w)+\"bp_withfiber\"+\"_smooth\"+str(window_s)+\".svg\")\n",
    "    #region_fig[0].write_image(\"temp_files/\"+smoothing_type+\"_\"+region_fig[1]+type_selected[0]+str(bed_w)+\"bp_withfiber\"+\"_smooth\"+str(window_s)+\".png\")\n",
    "\n",
    "#\"MEX_none\",\"MEX_D1to5\",\"MEX_D6to9\",\"MEX_D10\"\n",
    "\n",
    "#,bw_selections=[\"sdc2_chip_albritton\",\"sdc3_chip_albritton\",\"sdc3_chip_anderson\",\"dpy27_chip_anderson\"],bigwig_df=plot_comb_bigwig_df)#[1,'5mC'],[3,'5mC']])# #\n",
    "# smoothing types: \"gaussian\", \"weighted\", \"rolling\"\n",
    "\n",
    "#analysis_cond = [\"N2_mixed_DPY27_dimelo_pAHia5_R10\",\"50_mixed_dpy27-3xGNB_GFP-Hia5_mcvipi_R10\",\"66_old_sdc2_3xGNB_GFPHia5_mChMCVIPI\",\"N2_mixed_endogenous_R10\",\"54_mixed_sdc2_3xmCNB_mChMCVIPI_GFPHia5\"]\n",
    "\n",
    "# print unique count bed_start values for combination of chr_type, type and condition in each comb_bedmethyl_df\n",
    "#print(\"Unique count of bed_start values for each combination of chr_type, type and condition in comb_bedmethyl_df:\")\n",
    "#print(plot_df.groupby(['chr_type','type','condition'])['bed_start'].nunique())\n",
    "\n",
    "# # #rand_suffix = nanotools.random_alpha_numeric(8)\n",
    "\n",
    "#\"center_DPY27_chip_albretton_ONLY\",\"center_DPY27_chip_albretton;gene_ol2000;TSS_ol2000\",\"strong_rex;DPY27_ol2000;SDC_ol2000\",\"center_DPY27_chip_albretton;SDC_ol2000\"\n",
    "#\"center_DPY27_chip_albretton\",\"intergenic_control\",\"strong_rex\",\"weak_rex\",\"TSS_q4\",\"TSS_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddb7c65c6504815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reset index of plot_df\n",
    "# plot_df.reset_index(drop=True, inplace=True)\n",
    "# # drop type == MEX_D10_3040109_minus\n",
    "# plot_df = plot_df[plot_df['type'] != 'MEX_D10_9697989_plus']\n",
    "# plot_df = plot_df[plot_df['type'] != 'MEX_D10_3040109_minus']\n",
    "\n",
    "for each_type in plot_df['type'].unique():\n",
    "    print(\"Plotting:\", each_type)\n",
    "    region_fig = plot_bedmethyl(plot_df, analysis_cond, chr_types=[\"X\"], types=[each_type], strands=[\"all\"], window_size=window_s*2, metagene_bins=num_bins, smoothing_type=smoothing_type,selection_indices=[3,4], bed_window=[-1000,1000], mod_types=['m6A'],ignore_selec=[],plot_motifs = True)\n",
    "\n",
    "    region_fig[0].write_image(\"temp_files/single_D10_MEX/\"+smoothing_type+\"_\"+each_type+str(bed_w)+\"bp_withfiber\"+\"_smooth\"+str(window_s)+\".svg\")\n",
    "    region_fig[0].write_image(\"temp_files/single_D10_MEX/\"+smoothing_type+\"_\"+each_type+str(bed_w)+\"bp_withfiber\"+\"_smooth\"+str(window_s)+\".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28b3d4137df1402",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Configuration\n",
    "down_var = True  # Set to True to enable downsampling\n",
    "\n",
    "# Define the folder paths\n",
    "folders = [\n",
    "    '/Data1/ext_data/motifs/MEX_none',\n",
    "    '/Data1/ext_data/motifs/MEX_D1to5',\n",
    "    '/Data1/ext_data/motifs/MEX_D10'\n",
    "]\n",
    "# folders = [\n",
    "#     '/Data1/ext_data/motifs/rohslab_D1_5_1000bp',\n",
    "#     '/Data1/ext_data/motifs/rohslab_D6_9_1000bp',\n",
    "#     '/Data1/ext_data/motifs/rohslab_D10_1000bp'\n",
    "# ]\n",
    "\n",
    "# Define the file extensions\n",
    "extensions = ['HelT', 'MGW', 'ProT', 'Roll']\n",
    "\n",
    "def get_type_from_folder(folder):\n",
    "    if 'D1to5' in folder:\n",
    "        return 'MEX_D1to5'\n",
    "    elif 'none' in folder:\n",
    "        return 'MEX_none'\n",
    "    elif 'D10' in folder:\n",
    "        return 'MEX_D10'\n",
    "    return 'Unknown'\n",
    "\n",
    "# Function to process a single file\n",
    "def process_file(file_path, data_type):\n",
    "    print(f\"Processing file: {file_path}\")\n",
    "    data = []\n",
    "    current_chr = current_start = current_end = None\n",
    "    position = 0\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # if (+) in line, replace with nothing\n",
    "            line = line.replace(\"(+)\",\"\")\n",
    "            if line.startswith('>'):\n",
    "                # Reset position for new sequence\n",
    "                position = 0\n",
    "                # Parse the header\n",
    "                chr_info = line.strip()[1:].split(':')\n",
    "                current_chr = chr_info[0]\n",
    "                current_start, current_end = map(int, chr_info[1].split('-'))\n",
    "            else:\n",
    "                # Parse the data line\n",
    "                values = line.strip().split(',')\n",
    "                for v in values:\n",
    "                    data.append({\n",
    "                        'chr': current_chr,\n",
    "                        'start': current_start,\n",
    "                        'end': current_end,\n",
    "                        'position': position,\n",
    "                        'value': float(v) if v != 'NA' else np.nan,\n",
    "                        'type': data_type\n",
    "                    })\n",
    "                    position += 1\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    print(f\"Created DataFrame with shape: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "# Function to downsample dataframe\n",
    "def downsample_df(df):\n",
    "    # Create ID column\n",
    "    df['id'] = df['chr'] + ':' + df['start'].astype(str) + '-' + df['end'].astype(str)\n",
    "    \n",
    "    # Count unique IDs for each type\n",
    "    id_counts = df.groupby('type')['id'].nunique()\n",
    "    min_count = id_counts.min()\n",
    "    \n",
    "    # Downsample each type to the minimum count\n",
    "    downsampled_dfs = []\n",
    "    for type_name in df['type'].unique():\n",
    "        type_df = df[df['type'] == type_name]\n",
    "        unique_ids = type_df['id'].unique()\n",
    "        if len(unique_ids) > min_count:\n",
    "            sampled_ids = resample(unique_ids, n_samples=min_count, random_state=42)\n",
    "            downsampled_dfs.append(type_df[type_df['id'].isin(sampled_ids)])\n",
    "        else:\n",
    "            downsampled_dfs.append(type_df)\n",
    "    \n",
    "    return pd.concat(downsampled_dfs, ignore_index=True)\n",
    "\n",
    "# Process all files and create dataframes\n",
    "dfs = {}\n",
    "for folder in folders:\n",
    "    print(f\"Processing folder: {folder}\")\n",
    "    data_type = get_type_from_folder(folder)\n",
    "    print(f\"Data type: {data_type}\")\n",
    "    \n",
    "    for ext in extensions:\n",
    "        file_name = [f for f in os.listdir(folder) if f.endswith(ext)][0]\n",
    "        file_path = os.path.join(folder, file_name)\n",
    "        df_name = f\"{ext.lower()}_df\"\n",
    "        \n",
    "        if df_name not in dfs:\n",
    "            dfs[df_name] = process_file(file_path, data_type)\n",
    "        else:\n",
    "            dfs[df_name] = pd.concat([dfs[df_name], process_file(file_path, data_type)], ignore_index=True)\n",
    "\n",
    "# Downsample if enabled\n",
    "if down_var:\n",
    "    for df_name in dfs:\n",
    "        print(f\"Downsampling {df_name}...\")\n",
    "        dfs[df_name] = downsample_df(dfs[df_name])\n",
    "\n",
    "# Assign dataframes to variables\n",
    "helt_df = dfs['helt_df']\n",
    "mgw_df = dfs['mgw_df']\n",
    "prot_df = dfs['prot_df']\n",
    "roll_df = dfs['roll_df']\n",
    "\n",
    "# Print debug information\n",
    "for name, df in dfs.items():\n",
    "    print(f\"\\nDataFrame: {name}\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Columns: {df.columns}\")\n",
    "    print(f\"Sample data:\\n{df.head()}\")\n",
    "    print(f\"NaN values: {df['value'].isna().sum()}\")\n",
    "    print(f\"Unique types: {df['type'].unique()}\")\n",
    "    print(f\"Position range: {df['position'].min()} to {df['position'].max()}\")\n",
    "    print(f\"Unique IDs per type:\")\n",
    "    print(df.groupby('type')['id'].nunique())\n",
    "\n",
    "print(\"\\nData processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bba97bb85634133",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "\n",
    "# Configuration\n",
    "WINDOW_SIZE = 50  # Rolling average window size\n",
    "\n",
    "# Function to process data\n",
    "def process_data(df):\n",
    "    # Create ID column\n",
    "    df['id'] = df['chr'] + ':' + df['start'].astype(str) + '-' + df['end'].astype(str)\n",
    "    \n",
    "    # Sort by ID and position\n",
    "    df = df.sort_values(['id', 'position'])\n",
    "    \n",
    "    # Apply rolling average\n",
    "    df['smoothed_value'] = df.groupby('id')['value'].transform(lambda x: x.rolling(window=WINDOW_SIZE, center=True, min_periods=1).mean())\n",
    "    \n",
    "    # Calculate average by position and type\n",
    "    avg_df = df.groupby(['position', 'type'])['smoothed_value'].mean().reset_index()\n",
    "    \n",
    "    # Center the x-axis\n",
    "    avg_df['centered_position'] = avg_df['position'] - avg_df['position'].mean()\n",
    "    \n",
    "    return avg_df\n",
    "\n",
    "# Function to create 2x2 subplots\n",
    "def create_subplots(dfs, titles, y_labels):\n",
    "    fig = make_subplots(rows=2, cols=2, subplot_titles=titles)\n",
    "    \n",
    "    # Get the default Plotly color sequence\n",
    "    colors = px.colors.qualitative.Plotly\n",
    "\n",
    "    # Create a mapping of types to colors\n",
    "    unique_types = sorted(set(type for df in dfs.values() for type in df['type'].unique()))\n",
    "    color_map = {type: colors[i % len(colors)] for i, type in enumerate(unique_types)}\n",
    "    \n",
    "    for i, (df_name, df) in enumerate(dfs.items()):\n",
    "        row = i // 2 + 1\n",
    "        col = i % 2 + 1\n",
    "        \n",
    "        avg_df = process_data(df)\n",
    "        \n",
    "        for data_type in avg_df['type'].unique():\n",
    "            type_data = avg_df[avg_df['type'] == data_type]\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=type_data['centered_position'],\n",
    "                    y=type_data['smoothed_value'],\n",
    "                    mode='lines',\n",
    "                    name=data_type,\n",
    "                    legendgroup=data_type,\n",
    "                    showlegend=(i == 0),  # Show legend only for the first subplot\n",
    "                    line=dict(color=color_map[data_type])\n",
    "                ),\n",
    "                row=row, col=col\n",
    "            )\n",
    "        \n",
    "        fig.update_xaxes(title_text='Distance from MEX pos 1', row=row, col=col)\n",
    "        fig.update_yaxes(title_text=y_labels[i], row=row, col=col)\n",
    "    \n",
    "    fig.update_layout(\n",
    "        height=800,\n",
    "        width=1000,\n",
    "        title_text=\"DNA Shape Analysis\",\n",
    "        template='plotly_white',\n",
    "        legend_title_text='Type'\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Prepare data for plotting\n",
    "dfs = {\n",
    "    'HelT': helt_df,\n",
    "    'MGW': mgw_df,\n",
    "    'ProT': prot_df,\n",
    "    'Roll': roll_df\n",
    "}\n",
    "\n",
    "titles = [\n",
    "    'HelT - DNA Shape Analysis',\n",
    "    'MGW - DNA Shape Analysis',\n",
    "    'ProT - DNA Shape Analysis',\n",
    "    'Roll - DNA Shape Analysis'\n",
    "]\n",
    "\n",
    "y_labels = [\n",
    "    'Helical twist (°)',\n",
    "    'Major groove width (Å)',\n",
    "    'Propeller twist (°)',\n",
    "    'Roll (°)'\n",
    "]\n",
    "\n",
    "# Create and display the subplot figure\n",
    "fig = create_subplots(dfs, titles, y_labels)\n",
    "fig.show()\n",
    "\n",
    "# Print some statistics\n",
    "for name, df in dfs.items():\n",
    "    print(f\"\\n{name} DataFrame:\")\n",
    "    print(f\"Total rows: {len(df)}\")\n",
    "    print(f\"Unique IDs: {df['id'].nunique()}\")\n",
    "    print(f\"Position range: {df['position'].min()} to {df['position'].max()}\")\n",
    "    print(f\"Unique types: {df['type'].unique()}\")\n",
    "    print(f\"Average smoothed values by type:\")\n",
    "    print(df.groupby('type')['value'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c64d45b4a4a472",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Aligning on motif:\n",
    "\n",
    "# Convert this pseudo code into python code to be executed in a jupyter notebook cell\n",
    "# import tsv from: \"/Data1/ext_data/motifs/fimo_D10_100bp_mex.tsv\" into dataframe called fimo_df\n",
    "# format looks like:\n",
    "# motif_id\tmotif_alt_id\tsequence_name\tstart\tstop\tstrand\tscore\tp-value\tq-value\tmatched_sequence\n",
    "# 1\tTNYCCCTKCSCHWWT-MEME-1\tchrV\t20651347\t20651361\t+\t21.7701\t3.08E-09\t0.381\tTCCCCCTTCGCCATT\n",
    "# 1\tTNYCCCTKCSCHWWT-MEME-1\tchrV\t1743236\t1743250\t+\t21.3448\t7.40E-09\t0.381\tTCCCCCTGCCCATTT\n",
    "\n",
    "# keep only sequence_name, start, stop, strand, p-value columns, and rename them to chr, start, stop, strand, p_value\n",
    "\n",
    "# in chr column, replace all \"chr\" with \"CHROMOSOME_\"\n",
    "\n",
    "# use IntervalTree to add column corresponding to the 'chip_category' in the 'result_df_cat' dataframe, if the start, end in fimo_df overlaps the start, end in result_df_cat and the 'chr's match.\n",
    "# otherwise set to \"none\"\n",
    "# result_df_cat is the dataframe with following columns:\n",
    "# Index(['type', 'chr', 'start', 'end', 'length', 'abs_summit', 'pileup',\n",
    "#        'LOG10(pvalue)', 'fold_enrichment', ... , 'chip_category'],\n",
    "#       dtype='object')\n",
    "\n",
    "# Add a 'cluster_count' column corresponding to the number of rows in fimo_df that are within a configurable window (e.g. 100bp) of the current row's start or end positions and on the same 'chr'\n",
    "\n",
    "# plot a boxplot of 'cluster_count' for each 'chip_category' in result_df_cat using seaborn, showing scatter points\n",
    "\n",
    "# add a configurable bed_window e.g. 500bp to either side of each region?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6013ea034b4e3216",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from intervaltree import Interval, IntervalTree\n",
    "from collections import defaultdict\n",
    "\n",
    "# Configurable parameters\n",
    "window_size = 200  # For cluster count\n",
    "bed_window = 1000  # For expanding regions in result_df_cat\n",
    "\n",
    "# Step 1: Import the TSV file into a dataframe called fimo_df\n",
    "fimo_df = pd.read_csv(\"/Data1/ext_data/motifs/fimo_JANS_rex.csv\", sep=',')\n",
    "# print head of fimo_df\n",
    "print(fimo_df.head())\n",
    "# /Data1/ext_data/motifs/fimo_D10_100bp_mex.tsv # \\t\n",
    "\n",
    "# Step 2: Keep only the specified columns and rename them\n",
    "fimo_df = fimo_df[['sequence_name', 'start', 'stop', 'strand', 'score','p-value']]\n",
    "fimo_df = fimo_df.rename(columns={'sequence_name': 'chr', 'p-value': 'p_value'})\n",
    "# drop rows where score < 15\n",
    "fimo_df = fimo_df[fimo_df['score'] >= 12]\n",
    "# drop score column\n",
    "fimo_df.drop(columns=['score'], inplace=True)\n",
    "\n",
    "# Step 3: Replace \"chr\" with \"CHROMOSOME_\" in the 'chr' column\n",
    "fimo_df['chr'] = fimo_df['chr'].str.replace('chr', 'CHROMOSOME_')\n",
    "\n",
    "# Assume result_df_cat is already loaded into the environment\n",
    "# Ensure 'chr' column matches the format in fimo_df\n",
    "result_df_cat['chr'] = result_df_cat['chr'].str.replace('chr', 'CHROMOSOME_')\n",
    "\n",
    "# Step 5: Build IntervalTrees for each chromosome in result_df_cat\n",
    "interval_trees = defaultdict(IntervalTree)\n",
    "for idx, row in result_df_cat.iterrows():\n",
    "    chrom = row['chr']\n",
    "    start = row['start']\n",
    "    end = row['end']\n",
    "    category = row['chip_category']\n",
    "    interval_trees[chrom][start:end] = category\n",
    "\n",
    "# Add an 'id' column to fimo_df to keep track of original rows\n",
    "fimo_df['id'] = fimo_df.index\n",
    "\n",
    "# Step 6: Expand fimo_df to account for multiple overlapping chip_categories\n",
    "expanded_rows = []\n",
    "\n",
    "for idx, row in fimo_df.iterrows():\n",
    "    chrom = row['chr']\n",
    "    start = row['start']\n",
    "    end = row['stop']\n",
    "    overlaps = interval_trees[chrom][start:end] if chrom in interval_trees else []\n",
    "    if overlaps:\n",
    "        for interval in overlaps:\n",
    "            new_row = row.copy()\n",
    "            new_row['chip_category'] = interval.data\n",
    "            expanded_rows.append(new_row)\n",
    "    else:\n",
    "        # No overlaps, chip_category is 'none'\n",
    "        new_row = row.copy()\n",
    "        new_row['chip_category'] = 'none'\n",
    "        expanded_rows.append(new_row)\n",
    "\n",
    "# Create the expanded DataFrame\n",
    "fimo_expanded_df = pd.DataFrame(expanded_rows)\n",
    "\n",
    "# Step 7: Build IntervalTrees for fimo_df for cluster counting (using original fimo_df)\n",
    "fimo_trees = defaultdict(IntervalTree)\n",
    "for idx, row in fimo_df.iterrows():\n",
    "    chrom = row['chr']\n",
    "    start = row['start']\n",
    "    end = row['stop']\n",
    "    fimo_trees[chrom][start:end] = row['id']  # Use 'id' to uniquely identify intervals\n",
    "\n",
    "\n",
    "# Step 8: Calculate 'cluster_count' for each row in fimo_df\n",
    "def get_cluster_count(row):\n",
    "    chrom = row['chr']\n",
    "    start = row['start']\n",
    "    end = row['stop']\n",
    "    intervals = fimo_trees[chrom][start - window_size:end + window_size] if chrom in fimo_trees else []\n",
    "    count = len(intervals)\n",
    "    return max(count, 0)\n",
    "\n",
    "\n",
    "fimo_df['cluster_count'] = fimo_df.apply(get_cluster_count, axis=1)\n",
    "\n",
    "# Merge 'cluster_count' back into the expanded DataFrame\n",
    "fimo_expanded_df = fimo_expanded_df.merge(fimo_df[['id', 'cluster_count']], on='id', how='left')\n",
    "\n",
    "# Step 9: Compute mean cluster_count and sample sizes per chip_category\n",
    "category_stats = fimo_expanded_df.groupby('chip_category')['cluster_count'].agg(['mean', 'count']).reset_index()\n",
    "\n",
    "# Sort by mean cluster_count\n",
    "category_stats = category_stats.sort_values(by='mean', ascending=False)\n",
    "\n",
    "# Create x-axis labels with 'n='\n",
    "category_stats['label'] = category_stats.apply(lambda x: f\"{x['chip_category']}\\n(n={int(x['count'])})\", axis=1)\n",
    "\n",
    "# Plot the bar plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='label', y='mean', data=category_stats, palette='viridis')\n",
    "plt.title('Average Cluster Count by Chip Category')\n",
    "plt.xlabel('Chip Category')\n",
    "plt.ylabel('Average Cluster Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "nanotools.display_sample_rows(fimo_df, 5)\n",
    "\n",
    "# Add bed_window to either side of each region in result_df_cat\n",
    "fimo_expanded_df['start'] = fimo_expanded_df['start'] - bed_window\n",
    "fimo_expanded_df['start'] = fimo_expanded_df['start'].apply(lambda x: max(0, x))\n",
    "fimo_expanded_df['stop'] = fimo_expanded_df['stop'] + bed_window\n",
    "\n",
    "# drop nan rows in chrom column\n",
    "fimo_expanded_df.dropna(subset=['chr'], inplace=True)\n",
    "\n",
    "# add \"MEX_\" to the start of each 'chip_category' in fimo_expanded_df\n",
    "fimo_expanded_df['chip_category'] = 'MEX_' + fimo_expanded_df['chip_category']\n",
    "# convert start and stop to int\n",
    "fimo_expanded_df['start'] = fimo_expanded_df['start'].astype(int)\n",
    "fimo_expanded_df['stop'] = fimo_expanded_df['stop'].astype(int)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'fimo_expanded_df' is available from previous code\n",
    "\n",
    "# Create 'combined_bed_df_mex_cat' with specified columns\n",
    "combined_bed_df_mex_cat = fimo_expanded_df[['chr', 'start', 'stop', 'strand', 'chip_category']].copy()\n",
    "combined_bed_df_mex_cat = combined_bed_df_mex_cat.rename(columns={\n",
    "    'chr': 'chrom',\n",
    "    'start': 'bed_start',\n",
    "    'stop': 'bed_end',\n",
    "    'strand': 'bed_strand',\n",
    "    'chip_category': 'type'\n",
    "})\n",
    "\n",
    "# Add 'chr_type' column\n",
    "combined_bed_df_mex_cat['chr_type'] = combined_bed_df_mex_cat['chrom'].apply(\n",
    "    lambda x: 'X' if x == 'CHROMOSOME_X' else 'Autosome')\n",
    "\n",
    "# Display the first few rows\n",
    "print(\"combined_bed_df_mex_cat:\")\n",
    "nanotools.display_sample_rows(combined_bed_df_mex_cat, 5)\n",
    "\n",
    "# Create 'combined_bed_df_mex_clust' with 'type' column as \"clust_\" + cluster_count\n",
    "combined_bed_df_mex_clust = fimo_expanded_df[['chr', 'start', 'stop', 'strand', 'cluster_count']].copy()\n",
    "combined_bed_df_mex_clust = combined_bed_df_mex_clust.rename(columns={\n",
    "    'chr': 'chrom',\n",
    "    'start': 'bed_start',\n",
    "    'stop': 'bed_end',\n",
    "    'strand': 'bed_strand'\n",
    "})\n",
    "\n",
    "# 'type' column is 'clust_' + cluster_count\n",
    "combined_bed_df_mex_clust['type'] = 'clust_' + combined_bed_df_mex_clust['cluster_count'].astype(str)\n",
    "\n",
    "# Add 'chr_type' column\n",
    "combined_bed_df_mex_clust['chr_type'] = combined_bed_df_mex_clust['chrom'].apply(\n",
    "    lambda x: 'X' if x == 'CHROMOSOME_X' else 'Autosome')\n",
    "\n",
    "# drop cluster_count column\n",
    "combined_bed_df_mex_clust.drop(columns='cluster_count', inplace=True)\n",
    "\n",
    "# Display the first few rows\n",
    "print(\"\\ncombined_bed_df_mex_clust:\")\n",
    "nanotools.display_sample_rows(combined_bed_df_mex_clust, 5)\n",
    "\n",
    "# print columns\n",
    "print(\"\\ncombined_bed_df_mex_cat columns:\")\n",
    "print(combined_bed_df_mex_cat.columns)\n",
    "\n",
    "# convert MEX_D1, MEX_D2, MEX_D3, MEX_D4, MEX_D5 to MEX_D1to5\n",
    "combined_bed_df_mex_cat['type'] = combined_bed_df_mex_cat['type'].replace({\n",
    "    'MEX_D1': 'MEX_D1to5',\n",
    "    'MEX_D2': 'MEX_D1to5',\n",
    "    'MEX_D3': 'MEX_D1to5',\n",
    "    'MEX_D4': 'MEX_D1to5',\n",
    "    'MEX_D5': 'MEX_D1to5'\n",
    "})\n",
    "\n",
    "# calculate end - start and return unique values\n",
    "combined_bed_df_mex_cat['length'] = combined_bed_df_mex_cat['bed_end'] - combined_bed_df_mex_cat['bed_start']\n",
    "print(\"\\nUnique lengths:\")\n",
    "print(combined_bed_df_mex_cat['length'].unique())\n",
    "\n",
    "# convert MEX_D6, MEX_D7, MEX_D8, MEX_D9 to MEX_D6to9\n",
    "combined_bed_df_mex_cat['type'] = combined_bed_df_mex_cat['type'].replace({\n",
    "    'MEX_D6': 'MEX_D6to9',\n",
    "    'MEX_D7': 'MEX_D6to9',\n",
    "    'MEX_D8': 'MEX_D6to9',\n",
    "    'MEX_D9': 'MEX_D6to9'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e42cbeb4f40aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from intervaltree import Interval, IntervalTree\n",
    "from collections import defaultdict\n",
    "\n",
    "# Configurable parameters\n",
    "window_size = 200  # For cluster count\n",
    "bed_window = 100  # For expanding regions in result_df_cat\n",
    "\n",
    "# Step 1: Import the TSV files into dataframes\n",
    "fimo_files = [\n",
    "    \"/Data1/ext_data/motifs/fimo_MEX_0.01.tsv\"#,\n",
    "    #\"/Data1/ext_data/motifs/fimo_MEXII_0.01.tsv\",\n",
    "    #\"/Data1/ext_data/motifs/fimo_motifc_0.01.tsv\"\n",
    "]\n",
    "\n",
    "fimo_dfs = []\n",
    "for file in fimo_files:\n",
    "    # Read the TSV file\n",
    "    df = pd.read_csv(file, sep='\\t')\n",
    "    fimo_dfs.append(df)\n",
    "\n",
    "# Combine the dataframes into one\n",
    "fimo_df = pd.concat(fimo_dfs, ignore_index=True)\n",
    "\n",
    "# Step 2: Pre-filter by p-value < 0.0001\n",
    "fimo_df = fimo_df[fimo_df['p-value'] < 0.00001] # 0.000001 #0.00001\n",
    "\n",
    "# Step 3: Deduplicate overlapping rows on the same sequence_name (chromosome)\n",
    "# Define the priority order\n",
    "motif_priority = {'MEXII': 1, 'MEX': 2, 'motifC': 3}\n",
    "\n",
    "# Map motif priorities\n",
    "fimo_df['motif_priority'] = fimo_df['motif_id'].map(motif_priority)\n",
    "\n",
    "# Drop any rows where motif_id is not in the priority list\n",
    "fimo_df = fimo_df.dropna(subset=['motif_priority'])\n",
    "fimo_df['motif_priority'] = fimo_df['motif_priority'].astype(int)\n",
    "\n",
    "# Sort by sequence_name, start, and motif_priority\n",
    "fimo_df = fimo_df.sort_values(by=['sequence_name', 'start', 'motif_priority'])\n",
    "\n",
    "# Initialize a dictionary to store IntervalTrees for each chromosome\n",
    "chrom_trees = defaultdict(IntervalTree)\n",
    "deduped_indices = set()\n",
    "\n",
    "# Iterate over the dataframe to deduplicate\n",
    "for idx, row in fimo_df.iterrows():\n",
    "    chrom = row['sequence_name']\n",
    "    start = row['start']\n",
    "    end = row['stop']\n",
    "    priority = row['motif_priority']\n",
    "\n",
    "    # Check for overlaps\n",
    "    overlaps = chrom_trees[chrom][start:end]\n",
    "    if not overlaps:\n",
    "        # No overlap, add interval\n",
    "        chrom_trees[chrom][start:end] = (priority, idx)\n",
    "        deduped_indices.add(idx)\n",
    "    else:\n",
    "        # There is an overlap; check priority\n",
    "        replace = False\n",
    "        for interval in overlaps:\n",
    "            existing_priority, existing_idx = interval.data\n",
    "            if start < interval.end and end > interval.begin:\n",
    "                if priority < existing_priority:\n",
    "                    # Current row has higher priority\n",
    "                    chrom_trees[chrom].remove(interval)\n",
    "                    chrom_trees[chrom][start:end] = (priority, idx)\n",
    "                    deduped_indices.discard(existing_idx)\n",
    "                    deduped_indices.add(idx)\n",
    "                    replace = True\n",
    "                else:\n",
    "                    replace = False\n",
    "        if not replace:\n",
    "            continue\n",
    "\n",
    "# Keep only deduplicated rows\n",
    "fimo_df = fimo_df.loc[deduped_indices]\n",
    "\n",
    "# Step 4: Adjust columns\n",
    "fimo_df = fimo_df[['sequence_name', 'start', 'stop', 'strand', 'score', 'p-value', 'motif_id']]\n",
    "fimo_df = fimo_df.rename(columns={'sequence_name': 'chr', 'p-value': 'p_value'})\n",
    "\n",
    "# Step 5: Replace \"chr\" with \"CHROMOSOME_\" in the 'chr' column\n",
    "fimo_df['chr'] = fimo_df['chr'].str.replace('chr', 'CHROMOSOME_')\n",
    "\n",
    "# Ensure 'chr' column in result_df_cat matches the format in fimo_df\n",
    "result_df_cat['chr'] = result_df_cat['chr'].str.replace('chr', 'CHROMOSOME_')\n",
    "\n",
    "# Step 6: Build IntervalTrees for each chromosome in result_df_cat\n",
    "interval_trees = defaultdict(IntervalTree)\n",
    "for idx, row in result_df_cat.iterrows():\n",
    "    chrom = row['chr']\n",
    "    start = row['start']\n",
    "    end = row['end']\n",
    "    category = row['chip_category']\n",
    "    interval_trees[chrom][start:end] = category\n",
    "\n",
    "# Add an 'id' column to fimo_df to keep track of original rows\n",
    "fimo_df.reset_index(drop=True, inplace=True)\n",
    "fimo_df['id'] = fimo_df.index\n",
    "\n",
    "# Step 7: Expand fimo_df to account for multiple overlapping chip_categories\n",
    "expanded_rows = []\n",
    "\n",
    "for idx, row in fimo_df.iterrows():\n",
    "    chrom = row['chr']\n",
    "    start = row['start']\n",
    "    end = row['stop']\n",
    "    overlaps = interval_trees[chrom][start:end] if chrom in interval_trees else []\n",
    "    if overlaps:\n",
    "        for interval in overlaps:\n",
    "            new_row = row.copy()\n",
    "            new_row['chip_category'] = interval.data\n",
    "            expanded_rows.append(new_row)\n",
    "    else:\n",
    "        # No overlaps, chip_category is 'none'\n",
    "        new_row = row.copy()\n",
    "        new_row['chip_category'] = 'none'\n",
    "        expanded_rows.append(new_row)\n",
    "\n",
    "# Create the expanded DataFrame\n",
    "fimo_expanded_df = pd.DataFrame(expanded_rows)\n",
    "\n",
    "# Step 8: Build IntervalTrees for fimo_df for cluster counting\n",
    "fimo_trees = defaultdict(IntervalTree)\n",
    "for idx, row in fimo_df.iterrows():\n",
    "    chrom = row['chr']\n",
    "    start = row['start']\n",
    "    end = row['stop']\n",
    "    fimo_trees[chrom][start:end] = row['id']  # Use 'id' to uniquely identify intervals\n",
    "\n",
    "# Step 9: Calculate 'cluster_count' for each row in fimo_df\n",
    "def get_cluster_count(row):\n",
    "    chrom = row['chr']\n",
    "    start = row['start']\n",
    "    end = row['stop']\n",
    "    intervals = fimo_trees[chrom][start - window_size:end + window_size] if chrom in fimo_trees else []\n",
    "    count = len(intervals)\n",
    "    return max(count, 0)\n",
    "\n",
    "fimo_df['cluster_count'] = fimo_df.apply(get_cluster_count, axis=1)\n",
    "\n",
    "# Merge 'cluster_count' back into the expanded DataFrame\n",
    "fimo_expanded_df = fimo_expanded_df.merge(fimo_df[['id', 'cluster_count']], on='id', how='left')\n",
    "\n",
    "# Step 10: Compute mean cluster_count and sample sizes per chip_category\n",
    "category_stats = fimo_expanded_df.groupby('chip_category')['cluster_count'].agg(['mean', 'count']).reset_index()\n",
    "\n",
    "# Sort by mean cluster_count\n",
    "category_stats = category_stats.sort_values(by='mean', ascending=False)\n",
    "\n",
    "# Create x-axis labels with 'n='\n",
    "category_stats['label'] = category_stats.apply(lambda x: f\"{x['chip_category']}\\n(n={int(x['count'])})\", axis=1)\n",
    "\n",
    "# Plot the bar plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='label', y='mean', data=category_stats, palette='viridis')\n",
    "plt.title('Average Cluster Count by Chip Category')\n",
    "plt.xlabel('Chip Category')\n",
    "plt.ylabel('Average Cluster Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Add bed_window to either side of each region in fimo_expanded_df\n",
    "fimo_expanded_df['start'] = fimo_expanded_df['start'] - bed_window\n",
    "fimo_expanded_df['start'] = fimo_expanded_df['start'].apply(lambda x: max(0, x))\n",
    "fimo_expanded_df['stop'] = fimo_expanded_df['stop'] + bed_window\n",
    "\n",
    "# Drop NaN rows in 'chr' column\n",
    "fimo_expanded_df.dropna(subset=['chr'], inplace=True)\n",
    "\n",
    "# Add \"MEX_\" to the start of each 'chip_category' in fimo_expanded_df\n",
    "fimo_expanded_df['chip_category'] = 'MEX_' + fimo_expanded_df['chip_category']\n",
    "\n",
    "# Convert 'start' and 'stop' to integers\n",
    "fimo_expanded_df['start'] = fimo_expanded_df['start'].astype(int)\n",
    "fimo_expanded_df['stop'] = fimo_expanded_df['stop'].astype(int)\n",
    "\n",
    "# Create 'combined_bed_df_mex_cat' with specified columns\n",
    "combined_bed_df_mex_cat = fimo_expanded_df[['chr', 'start', 'stop', 'strand', 'chip_category']].copy()\n",
    "combined_bed_df_mex_cat = combined_bed_df_mex_cat.rename(columns={\n",
    "    'chr': 'chrom',\n",
    "    'start': 'bed_start',\n",
    "    'stop': 'bed_end',\n",
    "    'strand': 'bed_strand',\n",
    "    'chip_category': 'type'\n",
    "})\n",
    "\n",
    "# Add 'chr_type' column\n",
    "combined_bed_df_mex_cat['chr_type'] = combined_bed_df_mex_cat['chrom'].apply(\n",
    "    lambda x: 'X' if x == 'CHROMOSOME_X' else 'Autosome')\n",
    "\n",
    "# Create 'combined_bed_df_mex_clust' with 'type' column as \"clust_\" + cluster_count\n",
    "combined_bed_df_mex_clust = fimo_expanded_df[['chr', 'start', 'stop', 'strand', 'cluster_count']].copy()\n",
    "combined_bed_df_mex_clust = combined_bed_df_mex_clust.rename(columns={\n",
    "    'chr': 'chrom',\n",
    "    'start': 'bed_start',\n",
    "    'stop': 'bed_end',\n",
    "    'strand': 'bed_strand'\n",
    "})\n",
    "\n",
    "# 'type' column is 'clust_' + cluster_count\n",
    "combined_bed_df_mex_clust['type'] = 'clust_' + combined_bed_df_mex_clust['cluster_count'].astype(str)\n",
    "\n",
    "# Add 'chr_type' column\n",
    "combined_bed_df_mex_clust['chr_type'] = combined_bed_df_mex_clust['chrom'].apply(\n",
    "    lambda x: 'X' if x == 'CHROMOSOME_X' else 'Autosome')\n",
    "\n",
    "# Drop 'cluster_count' column\n",
    "combined_bed_df_mex_clust.drop(columns='cluster_count', inplace=True)\n",
    "\n",
    "# Update 'type' in 'combined_bed_df_mex_cat'\n",
    "combined_bed_df_mex_cat['type'] = combined_bed_df_mex_cat['type'].replace({\n",
    "    'MEX_D1': 'MEX_D1to5',\n",
    "    'MEX_D2': 'MEX_D1to5',\n",
    "    'MEX_D3': 'MEX_D1to5',\n",
    "    'MEX_D4': 'MEX_D1to5',\n",
    "    'MEX_D5': 'MEX_D1to5',\n",
    "    'MEX_D6': 'MEX_D6to9',\n",
    "    'MEX_D7': 'MEX_D6to9',\n",
    "    'MEX_D8': 'MEX_D6to9',\n",
    "    'MEX_D9': 'MEX_D6to9'\n",
    "})\n",
    "\n",
    "# Display the first few rows (optional)\n",
    "print(\"combined_bed_df_mex_cat:\")\n",
    "nanotools.display_sample_rows(combined_bed_df_mex_cat, 5)\n",
    "\n",
    "print(\"\\ncombined_bed_df_mex_clust:\")\n",
    "nanotools.display_sample_rows(combined_bed_df_mex_clust, 5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a8c6d662881fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot boxplot centered on MEX motifs:\n",
    "### Generate dataframe for plotting correlation between chip and accessibility\n",
    "import pandas as pd\n",
    "import pyBigWig\n",
    "import numpy as np\n",
    "\n",
    "def calculate_bigwig_scores(df, qnormalized_bigwig_paths):\n",
    "    \n",
    "    # Initialize new columns for each bigWig file\n",
    "    stats = ['average', 'median', 'sum', 'max']\n",
    "    for i, bw_path in enumerate(qnormalized_bigwig_paths):\n",
    "        for stat in stats:\n",
    "            df[f'{stat}_bw_{i + 1}'] = np.nan\n",
    "\n",
    "    # Process each bigWig file\n",
    "    for i, bw_path in enumerate(qnormalized_bigwig_paths):\n",
    "        print(f\"Processing bigWig file {i + 1}/{len(qnormalized_bigwig_paths)}: {bw_path}\")\n",
    "        with pyBigWig.open(bw_path) as bw:\n",
    "            for index, row in df.iterrows():\n",
    "                chrom = row['chr']\n",
    "                start = row['start']\n",
    "                end = row['end']\n",
    "\n",
    "                try:\n",
    "                    values = bw.values(chrom, start, end)\n",
    "\n",
    "                    values = [v for v in values if v is not None]  # Remove any None values\n",
    "                    values = [v for v in values if v is not None and not np.isnan(v)]\n",
    "\n",
    "                    if values:\n",
    "                        df.at[index, f'average_bw_{i + 1}'] = np.mean(values)\n",
    "                        df.at[index, f'median_bw_{i + 1}'] = np.median(values)\n",
    "                        df.at[index, f'sum_bw_{i + 1}'] = np.sum(values)\n",
    "                        df.at[index, f'max_bw_{i + 1}'] = np.max(values)\n",
    "                except RuntimeError:\n",
    "                    # This can happen if the region is not in the bigWig file\n",
    "                    pass\n",
    "\n",
    "        print(f\"Finished processing bigWig file {i + 1}\")\n",
    "\n",
    "    # Calculate normalized average columns\n",
    "    for i in range(1, len(qnormalized_bigwig_paths) + 1):\n",
    "        df[f'norm_avg_bw_{i}'] = df[f'average_bw_{i}'] / df['average_bw_1'] - 1\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "bed_file_path = \"/Data1/ext_data/qiming_2024/SDC2_SDC3_20_peaks_500_2000_RPKM.csv\" # SDC2_SDC3_20_peaks_500_2000_RPKM.csv or SDC2_SDC3_gt20_rpkm.csv for whole regions\n",
    "\n",
    "center_length = bed_window * 2\n",
    "\n",
    "# Read the bed file\n",
    "chip_bed = pd.read_csv(bed_file_path)\n",
    "\n",
    "# drop rows where length not equal to center_length\n",
    "chip_bed = chip_bed[chip_bed['length'] == center_length]\n",
    "\n",
    "# recalculate start and end based on abs_summit and 1/2 of length\n",
    "chip_bed['start'] = chip_bed['abs_summit'] - chip_bed['length'] // 2\n",
    "chip_bed['end'] = chip_bed['abs_summit'] + chip_bed['length'] // 2\n",
    "\n",
    "nanotools.display_sample_rows(chip_bed, 3)\n",
    "#print columms\n",
    "print(chip_bed.columns)\n",
    "\n",
    "### Add bed regions of interest (e.g. control regions\n",
    "# Create a new dataframe with the same columns as chip_bed\n",
    "new_rows = pd.DataFrame(columns=chip_bed.columns)\n",
    "\n",
    "# Map the columns from combined_bed_df_ext to chip_bed\n",
    "new_rows['type'] = combined_bed_df_mex_cat['type']\n",
    "new_rows['chr'] = combined_bed_df_mex_cat['chrom']\n",
    "new_rows['start'] = combined_bed_df_mex_cat['bed_start']\n",
    "new_rows['end'] = combined_bed_df_mex_cat['bed_end']\n",
    "\n",
    "# Calculate length and abs_summit\n",
    "new_rows['length'] = new_rows['end'] - new_rows['start']\n",
    "new_rows['abs_summit'] = ((new_rows['start'] + new_rows['end']) / 2).astype(int)\n",
    "\n",
    "# Append the new rows to chip_bed\n",
    "chip_bed = pd.concat([chip_bed, new_rows], ignore_index=True)\n",
    "\n",
    "# Sort the resulting dataframe by chr and start position\n",
    "#chip_bed = chip_bed.sort_values(['chr', 'start']).reset_index(drop=True)\n",
    "\n",
    "result_df = calculate_bigwig_scores(chip_bed, raw_bw_files)\n",
    "\n",
    "# Display the first few rows of the resulting dataframe\n",
    "nanotools.display_sample_rows(result_df, 10)\n",
    "# print column names\n",
    "print(result_df.columns)\n",
    "\n",
    "# display number of rows by type\n",
    "result_df['type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78b78fb31799868",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import os\n",
    "\n",
    "\n",
    "def filter_and_plot(result_df, qvalue_cutoff=None, percentile_cutoff=None, num_categories=4):\n",
    "    # Filter rows based on qvalue cutoff if provided\n",
    "    if qvalue_cutoff is not None:\n",
    "        result_df = result_df[result_df['LOG10(qvalue)'] >= qvalue_cutoff]\n",
    "\n",
    "    # drop rows with SDC in type\n",
    "    result_df = result_df[~result_df['type'].str.contains('SDC')]\n",
    "    \n",
    "    # rename type == \"MEX_internenic\" to \"MEX_none\"\n",
    "    result_df['type'] = result_df['type'].replace('MEX_intergenic', 'MEX_none')\n",
    "\n",
    "    # create a column called chip_category that is equal to type\n",
    "    result_df['chip_category'] = result_df['type']\n",
    "    # set type to the first element of chip_cetegory when split by _\n",
    "    result_df['type'] = result_df['chip_category'].apply(lambda x: x.split('_')[0])\n",
    "\n",
    "    # Extract experiment names\n",
    "    experiment_names = analysis_cond  # Replace with your actual variable\n",
    "\n",
    "    # Create a custom RdBu colormap for SDC2 and SDC3 (blue to red)\n",
    "    colors_sdc = plt.cm.RdBu_r(np.linspace(0, 1, num_categories))\n",
    "    cmap_sdc = LinearSegmentedColormap.from_list(\"custom_RdBu\", colors_sdc)\n",
    "\n",
    "    # Colors for 'rex' and 'intergenic'\n",
    "    color_rex = 'green'\n",
    "    color_intergenic = 'orange'\n",
    "\n",
    "    def plot_boxplots(data, bw_column, ax1, ax2, title, palette_sdc, buffer=1):\n",
    "        # Create a custom order for the chip_category\n",
    "        def get_order_key(cat):\n",
    "            if '_D' in cat:\n",
    "                return cat.split('_D')[1] #int(cat.split('_D')[1])\n",
    "            else:\n",
    "                return float('inf')  # Place categories without 'D' at the end\n",
    "\n",
    "        category_order = sorted(\n",
    "            data['chip_category'].unique()\n",
    "        )\n",
    "\n",
    "        print(\"Adjusted category_order:\", category_order)\n",
    "\n",
    "        # Boxplot properties\n",
    "        boxprops = dict(facecolor='none')\n",
    "        medianprops = dict(color='red', linewidth=2)\n",
    "\n",
    "        print(\"data[data['type'] == 'MEX']\")\n",
    "        print(data[data['type'] == 'MEX'])\n",
    "\n",
    "        # Plot SDC2 on the first subplot\n",
    "        sns.boxplot(\n",
    "            x='chip_category', y=bw_column, data=data[data['type'] == 'MEX'],\n",
    "            ax=ax1, palette=palette_sdc, showfliers=False, order=category_order,\n",
    "            boxprops=boxprops, medianprops=medianprops, width=1\n",
    "        )\n",
    "\n",
    "        # Plot 'rex' and 'intergenic' on the second subplot\n",
    "        sns.boxplot(\n",
    "            x='chip_category', y=bw_column, data=data[data['type'].isin(['rex', 'intergenic'])],\n",
    "            ax=ax2, palette=[color_rex, color_intergenic], showfliers=False, order=['rex', 'intergenic'],\n",
    "            boxprops=boxprops, medianprops=medianprops, width=1\n",
    "        )\n",
    "\n",
    "        # Set titles and labels\n",
    "        ax1.set_title(f'Boxplot for {title}')\n",
    "        ax1.set_xlabel('ChIP Category')\n",
    "        ax1.set_ylabel('Average Methylation')\n",
    "        ax2.set_xlabel('ChIP Category')\n",
    "        ax2.set_ylabel('')\n",
    "\n",
    "        # Rotate x-axis labels and decrease font size\n",
    "        ax1.tick_params(axis='x', rotation=45, labelsize=8)\n",
    "        ax2.tick_params(axis='x', rotation=45, labelsize=8)\n",
    "\n",
    "        # Adjust y-axis label font size\n",
    "        ax1.yaxis.label.set_fontsize(12)\n",
    "\n",
    "        # Set title font size\n",
    "        ax1.title.set_fontsize(16)\n",
    "\n",
    "        # Remove background\n",
    "        ax1.set_facecolor('none')\n",
    "        ax2.set_facecolor('none')\n",
    "        ax1.grid(False)\n",
    "        ax2.grid(False)\n",
    "\n",
    "        # Remove legends from individual subplots\n",
    "        ax1.legend().remove()\n",
    "        ax2.legend().remove()\n",
    "\n",
    "        data = data[data['type'] != 'SDC3']\n",
    "        # Add (n=) for number of datapoints on the x-axis\n",
    "        for ax in [ax1, ax2]:\n",
    "            for i, label in enumerate(ax.get_xticklabels()):\n",
    "                category = label.get_text()\n",
    "                count = data[(data['chip_category'] == category) & (data[bw_column].notna())].shape[0]\n",
    "                ax.text(i, ax.get_ylim()[0], f'(n={count})', ha='center', va='top', fontsize=8)\n",
    "\n",
    "        # Set y-axis limits to 0 to 0.5\n",
    "        ax1.set_ylim(0, 0.5)\n",
    "        ax2.set_ylim(0, 0.5)\n",
    "\n",
    "        # Add buffer space by setting xlim for each subplot\n",
    "        ax1.set_xlim(-buffer, len(category_order) - 1 + buffer)  # Adjust limits to add buffer space\n",
    "        ax2.set_xlim(-buffer, 1 + buffer)  # Add buffer space for two categories: rex and intergenic\n",
    "\n",
    "    # Generate palette for SDC2 categories\n",
    "    palette_sdc = [cmap_sdc(i / (num_categories - 1)) for i in range(num_categories)]\n",
    "\n",
    "    # Set up the subplots\n",
    "    fig, axes = plt.subplots(\n",
    "        6, 4, figsize=(18, 28),\n",
    "        gridspec_kw={'width_ratios': [5, 1, 5, 1]}\n",
    "    )\n",
    "\n",
    "    # Plot for each average_bw column\n",
    "    for i, (col, exp_name) in enumerate(\n",
    "            zip([col for col in result_df.columns if col.startswith('average_')], experiment_names)\n",
    "    ):\n",
    "        plot_boxplots(\n",
    "            result_df, col,\n",
    "            axes[i // 2, 2 * (i % 2)],\n",
    "            axes[i // 2, 2 * (i % 2) + 1],\n",
    "            exp_name, palette_sdc\n",
    "        )\n",
    "\n",
    "    # Remove overall background\n",
    "    fig.patch.set_facecolor('none')\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Adjust subplots to add buffer space around the boxplots\n",
    "    plt.subplots_adjust(left=0.05, right=0.95)\n",
    "\n",
    "    # Save as PNG and SVG\n",
    "    save_path = '/Data1/git/meyer-nanopore/scripts/analysis/temp_files/'\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    png_path = os.path.join(save_path, 'SDC3_boxplot_figure.png')\n",
    "    svg_path = os.path.join(save_path, 'SDC3_boxplot_figure.svg')\n",
    "\n",
    "    plt.savefig(png_path, format='png', dpi=300, bbox_inches='tight', transparent=True)\n",
    "    plt.savefig(svg_path, format='svg', bbox_inches='tight', transparent=True)\n",
    "\n",
    "    print(f\"Figures saved as:\\n{png_path}\\n{svg_path}\")\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()\n",
    "\n",
    "    return result_df\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# Assuming result_df is your DataFrame and analysis_cond is defined\n",
    "result_df_cat = filter_and_plot(result_df, percentile_cutoff=None, num_categories=10)\n",
    "\n",
    "# Display sample rows and print columns\n",
    "nanotools.display_sample_rows(result_df_cat, 5)\n",
    "print(result_df_cat.columns)\n",
    "\n",
    "# Print count by type\n",
    "print(result_df_cat['type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac53ec00c6d1b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "\n",
    "# Read the fasta file into a dictionary\n",
    "chrom_dict = SeqIO.to_dict(SeqIO.parse(\"/Data1/reference/c_elegans.WS235.genomic.fa\", \"fasta\"))\n",
    "\n",
    "# Drop all rows where chr_type == Autosome\n",
    "combined_bed_df_mex_cat = combined_bed_df_mex_cat[combined_bed_df_mex_cat['chr_type'] != 'Autosome']\n",
    "\n",
    "# drop rows where type equal to \"MEX_intergenic\" or \"MEX_none\"\n",
    "combined_bed_df_mex_cat = combined_bed_df_mex_cat[~combined_bed_df_mex_cat['type'].isin(['MEX_rex'])]\n",
    "\n",
    "# rename 'MEX_intergenic', to 'MEX_none'\n",
    "combined_bed_df_mex_cat['type'] = combined_bed_df_mex_cat['type'].replace({'MEX_intergenic': 'MEX_none'})\n",
    "\n",
    "# Define a function to get the sequence for each row\n",
    "def get_sequence(row):\n",
    "    chrom = row['chrom']\n",
    "    bed_start = int(row['bed_start'])\n",
    "    bed_end = int(row['bed_end'])\n",
    "    bed_strand = row['bed_strand']\n",
    "    # Get the sequence from the chromosome\n",
    "    seq = chrom_dict[chrom].seq[bed_start:bed_end]\n",
    "    # If the strand is negative, get the reverse complement\n",
    "    if bed_strand == '-':\n",
    "        seq = seq.reverse_complement()\n",
    "    return str(seq)\n",
    "\n",
    "# Apply the function to the dataframe to create the 'sequence' column\n",
    "combined_bed_df_mex_cat['sequence'] = combined_bed_df_mex_cat.apply(get_sequence, axis=1)\n",
    "\n",
    "# if length of sequence is an odd number, drop the last character from each sequence\n",
    "combined_bed_df_mex_cat['sequence'] = combined_bed_df_mex_cat['sequence'].apply(lambda x: x[:-1] if len(x) % 2 != 0 else x)\n",
    "\n",
    "# Keep only rows where bed_strand == '+'\n",
    "# combined_bed_df_mex_cat = combined_bed_df_mex_cat[combined_bed_df_mex_cat['bed_strand'] == '+']\n",
    "\n",
    "# Display the first few rows\n",
    "print(\"\\ncombined_bed_df_mex_cat with sequence:\")\n",
    "print(combined_bed_df_mex_cat.head())\n",
    "\n",
    "# **Updated Code to Output the Number of Rows Included for Each Combination of 'type' and 'chr_type'**\n",
    "# Count the number of sequences per 'type' and 'chr_type'\n",
    "type_chr_counts = combined_bed_df_mex_cat.groupby(['type', 'chr_type']).size().reset_index(name='count')\n",
    "\n",
    "# Display the table\n",
    "print(\"\\nNumber of sequences included for each combination of 'type' and 'chr_type':\")\n",
    "print(type_chr_counts)\n",
    "\n",
    "# **New Code to Create the Vertical Bar Plot with Data Labels**\n",
    "# Plotting the counts with 'type' on x-axis, colored by 'chr_type'\n",
    "# Set the desired order for the 'type' column\n",
    "type_order = ['MEX_none', 'MEX_D1to5', 'MEX_D6to9', 'MEX_D10']\n",
    "\n",
    "# Aggregate counts by both 'type' and 'chr_type'\n",
    "type_chr_counts_agg = type_chr_counts.groupby(['type', 'chr_type'], as_index=False).agg({'count': 'sum'})\n",
    "\n",
    "# Ensure 'type' column is a categorical with the specified order\n",
    "type_chr_counts_agg['type'] = pd.Categorical(type_chr_counts_agg['type'], categories=type_order, ordered=True)\n",
    "\n",
    "# Create a MultiIndex for all combinations of 'type' and 'chr_type'\n",
    "chr_types = type_chr_counts_agg['chr_type'].unique()\n",
    "all_combinations = pd.MultiIndex.from_product([type_order, chr_types], names=['type', 'chr_type'])\n",
    "\n",
    "# Convert the DataFrame to use this MultiIndex\n",
    "type_chr_counts_agg = type_chr_counts_agg.set_index(['type', 'chr_type'])\n",
    "\n",
    "# Reindex to include all combinations, filling missing values with 0\n",
    "type_chr_counts_agg = type_chr_counts_agg.reindex(all_combinations, fill_value=0).reset_index()\n",
    "\n",
    "# Plotting the counts with 'type' on x-axis, colored by 'chr_type'\n",
    "fig_bar = px.bar(\n",
    "    type_chr_counts_agg,\n",
    "    x='type',\n",
    "    y='count',\n",
    "    color='chr_type',  # Color by 'chr_type'\n",
    "    barmode='group',\n",
    "    text='count',  # Add data labels\n",
    "    labels={\n",
    "        'type': 'Type',\n",
    "        'count': 'Number of Sequences',\n",
    "        'chr_type': 'Chromosome Type'\n",
    "    },\n",
    "    title='Number of Sequences per Type and Chromosome Type'\n",
    ")\n",
    "\n",
    "# Update the position of the text labels to appear on top of the bars\n",
    "fig_bar.update_traces(textposition='outside')\n",
    "\n",
    "# Adjust the layout and size\n",
    "fig_bar.update_layout(template='plotly_white', width=900, height=500)\n",
    "\n",
    "# Update y-axis range\n",
    "fig_bar.update_yaxes(range=[0, fig_bar.data[0].y.max() + 300])\n",
    "\n",
    "# Display the bar plot\n",
    "fig_bar.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Bar plot by chr_type:\n",
    "# Count the number of sequences per 'chr_type' only\n",
    "chr_counts = combined_bed_df_mex_cat['chr_type'].value_counts().reset_index()\n",
    "chr_counts.columns = ['chr_type', 'count']\n",
    "\n",
    "# Adjust counts based on chr_type\n",
    "chr_counts['adjusted_count'] = chr_counts.apply(\n",
    "    lambda row: row['count'] / 170 if row['chr_type'] == 'X' else (row['count'] / 830 if row['chr_type'] == 'Autosome' else row['count']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Round the adjusted count to the nearest decimal\n",
    "chr_counts['adjusted_count'] = chr_counts['adjusted_count'].round(1)\n",
    "\n",
    "# Create the bar plot by 'chr_type', coloring by 'chr_type'\n",
    "fig_bar_chr = px.bar(\n",
    "    chr_counts,\n",
    "    x='chr_type',\n",
    "    y='adjusted_count',  # Use the adjusted counts\n",
    "    color='chr_type',  # Color bars by 'chr_type'\n",
    "    text='adjusted_count',  # Add data labels\n",
    "    labels={\n",
    "        'chr_type': 'Chromosome Type',\n",
    "        'adjusted_count': 'Motifs / 100kb'  # Update y-axis label\n",
    "    },\n",
    "    title='Adjusted Number of Sequences per Chromosome Type'\n",
    ")\n",
    "\n",
    "# Update the position of the text labels to appear on top of the bars\n",
    "fig_bar_chr.update_traces(textposition='outside')\n",
    "\n",
    "# Adjust the layout to match the existing style\n",
    "fig_bar_chr.update_layout(template='plotly_white', width=450, height=500)\n",
    "\n",
    "# Display the bar plot\n",
    "fig_bar_chr.show()\n",
    "\n",
    "\n",
    "# Configurable smoothing parameter\n",
    "window_size = 25 # Change this value to adjust the smoothing window size\n",
    "\n",
    "# Initialize a list to store data for each sequence\n",
    "sequence_data = []\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for idx, row in combined_bed_df_mex_cat.iterrows():\n",
    "    seq = row['sequence'].upper()  # Convert sequence to uppercase\n",
    "    seq_type = row['type']\n",
    "    chr_type = row['chr_type']\n",
    "    seq_length = len(seq)\n",
    "\n",
    "    # Create positions centered at 0\n",
    "    #positions = np.linspace(- (seq_length - 1) / 2, (seq_length - 1) / 2, seq_length)\n",
    "    # invert positions\n",
    "    positions = np.arange(-(seq_length // 2), seq_length // 2)\n",
    "    #positions = positions[::-1]\n",
    "    # Add 8 to the flipped positions\n",
    "    positions = positions + 7\n",
    "\n",
    "    # Create a DataFrame for each sequence\n",
    "    df_seq = pd.DataFrame({\n",
    "        'type': seq_type,\n",
    "        'chr_type': chr_type,\n",
    "        'position': positions,\n",
    "        'base': list(seq)\n",
    "    })\n",
    "\n",
    "    # Mark bases that are 'C' or 'G'\n",
    "    df_seq['is_cg'] = df_seq['base'].apply(lambda x: 1 if x in ['C', 'G'] else 0)\n",
    "\n",
    "    # Round positions to integer bins for grouping\n",
    "    df_seq['bin_position'] = df_seq['position'].round().astype(int)\n",
    "\n",
    "    # Append to the list\n",
    "    sequence_data.append(df_seq)\n",
    "\n",
    "# Concatenate all sequence DataFrames\n",
    "df_all = pd.concat(sequence_data, ignore_index=True)\n",
    "\n",
    "# Group by 'type', 'chr_type', and 'bin_position' to calculate average CG%\n",
    "df_avg_cg = df_all.groupby(['type', 'chr_type', 'bin_position'])['is_cg'].mean().reset_index()\n",
    "df_avg_cg.rename(columns={'bin_position': 'position', 'is_cg': 'average_cg'}, inplace=True)\n",
    "\n",
    "# Apply smoothing using a rolling mean\n",
    "def smooth_group(group):\n",
    "    group = group.sort_values('position')  # Ensure positions are in order\n",
    "    if(window_size == 0):\n",
    "        group['average_cg_smooth'] = group['average_cg']\n",
    "        return group\n",
    "    else:\n",
    "        group['average_cg_smooth'] = group['average_cg'].rolling(\n",
    "            window=window_size, center=True, min_periods=1\n",
    "        ).mean()\n",
    "        return group\n",
    "\n",
    "# Apply the smoothing function to each combination of 'type' and 'chr_type'\n",
    "df_avg_cg_smooth = df_avg_cg.groupby(['type', 'chr_type'], group_keys=False).apply(smooth_group)\n",
    "\n",
    "# Set the desired order for the 'type' column\n",
    "type_order = ['MEX_none', 'MEX_D1to5', 'MEX_D10']\n",
    "df_avg_cg_smooth['type'] = pd.Categorical(df_avg_cg_smooth['type'], categories=type_order, ordered=True)\n",
    "\n",
    "# Plot using Plotly\n",
    "fig = px.line(\n",
    "    df_avg_cg_smooth,\n",
    "    x='position',\n",
    "    y='average_cg_smooth',\n",
    "    color='type',\n",
    "    line_dash='chr_type',\n",
    "    labels={\n",
    "        'position': 'Position (centered at 0)',\n",
    "        'average_cg_smooth': 'Smoothed Average CG%',\n",
    "        'type': 'Type',\n",
    "        'chr_type': 'Chromosome Type'\n",
    "    },\n",
    "    title='Smoothed Average CG% at Each Position (Centered) by Type and Chromosome Type',\n",
    "    category_orders={'type': type_order}  # Explicitly set the order\n",
    ")\n",
    "\n",
    "# Set x-axis range\n",
    "fig.update_xaxes(range=[-400, 400])\n",
    "# set y range to 0.25 to 0.63\n",
    "fig.update_yaxes(range=[0.27, 0.63])\n",
    "\n",
    "fig.update_layout(template='plotly_white')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a6f11e2d1e4abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save Mex file for structure determination:\n",
    "# copy result_df\n",
    "# save bed file to \"/Data1/git/meyer-nanopore/scripts/analysis/temp_files/082624/result_df_D10.bed\"\n",
    "# where only regions in D10 are included, keeping only bed columns such as chr, start, end\n",
    "# but first replace \"CHROMOSOME_\" with \"chr\" in chr column\n",
    "# and do not save header\n",
    "result_df_copy = combined_bed_df_mex_cat.copy()\n",
    "# print unique types\n",
    "print(result_df_copy['type'].unique())\n",
    "# drop rows where chrom != \"CHROMOSOME_X\"\n",
    "result_df_copy = result_df_copy[result_df_copy['chrom'] == \"CHROMOSOME_X\"]\n",
    "\n",
    "#result_df_copy = result_df.copy()\n",
    "result_df_copy['chrom'] = result_df_copy['chrom'].str.replace(\"CHROMOSOME_\", \"chr\")\n",
    "print(result_df_copy['chrom'].unique())\n",
    "# result_df_copy is equal to result_df_copy[result_df_copy['type'] where type is in  ['SDC2_D1','SDC2_D2','SDC2_D3','SDC2_D4']\n",
    "#result_df_copy = result_df_copy[result_df_copy['type'].isin(['SDC2_D10'])]\n",
    "result_df_copy = result_df_copy[result_df_copy['type'].isin(['MEX_D1to5'])]\n",
    "#['SDC2_D1','SDC2_D2','SDC2_D3','SDC2_D4','SDC2_D5']\n",
    "#['SDC2_D6','SDC2_D7','SDC2_D8','SDC2_D9']\n",
    "#['SDC2_D10']\n",
    "result_df_copy[['chrom', 'bed_start', 'bed_end']].to_csv(\"/Data1/git/meyer-nanopore/scripts/analysis/temp_files/082624/result_df_MEX_D1to5_250bp.bed\", sep=\"\\t\", header=False, index=False)\n",
    "\n",
    "# print\n",
    "print(result_df_copy.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
