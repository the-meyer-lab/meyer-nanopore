{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608d7544",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T06:06:00.997265Z",
     "start_time": "2024-02-22T06:06:00.969266900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Purpose of this script is to take in a number of .bam files and a .bed file and plot both aggregate and read level methylation data.\n",
    "\n",
    "__author__ = \"Yuri Malina\"\n",
    "__contact__ = \"ymalina@berkeley.edu\"\n",
    "__copyright__ = \"The Meyer Lab, UC Berkeley\"\n",
    "__credits__ = [\"\"]\n",
    "__date__ = \"8/30/2023\"\n",
    "__deprecated__ = False\n",
    "__status__ = \"In development\"\n",
    "__version__ = \"0.0.1\"\n",
    "\n",
    "### TO DOS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cd4207",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T06:06:02.320685500Z",
     "start_time": "2024-02-22T06:06:00.998266Z"
    }
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import nanotools\n",
    "importlib.reload(nanotools) # reload nanotools module\n",
    "#import random\n",
    "#random.seed(10)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px # Used for plotting\n",
    "import plotly.graph_objects as go # Used for plotting\n",
    "from plotly.subplots import make_subplots # Used for plotting\n",
    "from sklearn import cluster # Used for clustering\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool, cpu_count # used for parallel processing\n",
    "from io import StringIO\n",
    "import subprocess\n",
    "import os\n",
    "import plotly.io as pio\n",
    "import plotly\n",
    "import matplotlib.pyplot as plt # Use for plotting m6A frac and coverage plot\n",
    "from matplotlib import cm # Use for plotting m6A frac and coverage plot\n",
    "import sqlite3\n",
    "import tqdm\n",
    "import pysam\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "#import dask.dataframe as dd\n",
    "\n",
    "# set renderer to vscode\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "pio.renderers.default = 'plotly_mimetype+notebook'\n",
    "pd.options.display.max_rows = None\n",
    "pd.set_option('display.max_columns', None)\n",
    "# display count_df with no limits on rows\n",
    "pd.set_option('display.max_rows', None)\n",
    "# left align print\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896d9ca7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T06:19:54.679251100Z",
     "start_time": "2024-02-22T06:19:54.565248200Z"
    }
   },
   "outputs": [],
   "source": [
    "### Bed file configurations:\n",
    "sample_source = \"chr_type\" # \"chr_type\" or \"type\" or \"chromosome\"\n",
    "sampleName = [\"X\"] # \"TES_q1\" \"strong_rex\" \"weak_rex\" \"type\", \"X\", \"Autosome\"; Must be same number of unique values in selected bed rows.\n",
    "chr_type_selected = [\"X\"] # 'X' or \"Autosome\"\n",
    "type_selected = [\"strong_rex\",\"weak_rex\"] #\",\"TSS_q4\",\"TSS_q3\",\"TSS_q2\",\"TSS_q1\",\"intergenic_control\"\n",
    "#,\"her-1_TSS\",\"fem-1_TSS\",\"fem-2_TSS\",\"fem-3_TSS\",\"sex-1_TSS\"\n",
    "# her-1_TSS/TES/FULL | TES_q1-4 | #TSS_q1-4 | strong/weak rex | whole_chr | 200kb_region | 50kb_region | center_DPY27_chip_albretton | gene_q1-q4 | MEX_motif | center_SDC2_chip_albretton | center_SDC3_chip_albretton |\n",
    "max_regions = 0 # max regions to consider; 0 = full set;\n",
    "chromosome_selected = [\"CHROMOSOME_I\", \"CHROMOSOME_II\", \"CHROMOSOME_III\", \"CHROMOSOME_IV\",\"CHROMOSOME_V\",\"CHROMOSOME_X\"]\n",
    "strand_selected = [\"+\",\"-\"] #+ and/or -\n",
    "select_opp_strand = True #If you want to select both + and - strands for all regions set to True\n",
    "down_sample_autosome = True # If you want to downsample autosome genes to match number of X genes set to True\n",
    "if chr_type_selected == [\"X\"]:\n",
    "    down_sample_autosome = False\n",
    "bed_file = \"/Data1/reference/tss_tes_rex_combined_v13_WS235.bed\"\n",
    "bed_window = 5000 # +/- around bed elements.\n",
    "intergenic_window = 2000 # +/- around intergenic regions\n",
    "mods = \"a\" # {A,CG,A+CG}\n",
    "if sample_source == \"chr_type\":\n",
    "    selection = chr_type_selected\n",
    "if sample_source == \"type\":\n",
    "    selection = type_selected\n",
    "if sample_source == \"chromosome\":\n",
    "    selection = chromosome_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28adf1e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T06:10:21.165445800Z",
     "start_time": "2024-02-22T06:10:21.030446400Z"
    }
   },
   "outputs": [],
   "source": [
    "### BAM Configurations\n",
    "R9_m6A_thresh_percent = 0.85\n",
    "R10_m6A_thresh_percent = 0.9\n",
    "R10_5mC_thresh_percent = 0.9\n",
    "R9_m6A_thresh = int(round(R9_m6A_thresh_percent*258,0)) #default is 129 = 50%; 181=70%; 194=75%; 207 = 80%; 232 = 90%\n",
    "m6A_thresh = int(round(R10_m6A_thresh_percent*258,0))\n",
    "mC_thresh = int(round(R10_5mC_thresh_percent*258,0))\n",
    "\n",
    "# modkit is used for aggregating methylation data from .bam files\n",
    "# https://nanoporetech.github.io/modkit/quick_start.html\n",
    "modkit_path = \"/Data1/software/modkit/modkit\"\n",
    "\n",
    "### IMPORT BAM FILES AND METADATA FROM CSV FILE\n",
    "input_metadata = pd.read_csv(\"/Data1/git/meyer-nanopore/scripts/bam_input_metadata_2_21_2024.txt\", sep=\"\\t\", header=0)\n",
    "# Options: N2_fiber; SDC2_degron_fiber; SDC2_degron_bg; N2_bg; N2-DPY27_dimelo_pAHia5\n",
    "analysis_cond = [\"N2_old_fiber\",\"51_dpy21null_fiber\",\"52_dpy21jmjc_fiber\"]#,\"N2_young\"]\n",
    "# Set bam_files equal to list of items in column bam_files where conditions == N2_fiber\n",
    "bam_files = input_metadata[input_metadata[\"conditions\"].isin(analysis_cond)][\"bam_files\"].tolist()\n",
    "conditions = input_metadata[input_metadata[\"conditions\"].isin(analysis_cond)][\"conditions\"].tolist()\n",
    "exp_ids = input_metadata[input_metadata[\"conditions\"].isin(analysis_cond)][\"exp_id_date\"].tolist()\n",
    "flowcells = input_metadata[input_metadata[\"conditions\"].isin(analysis_cond)][\"flowcell\"].tolist()\n",
    "bam_fracs = len(bam_files)*[1] # For full .bam set to = 1\n",
    "sample_indices = list(range(len(bam_files)))\n",
    "output_stem = \"/Data1/seq_data/combined_fiber_dimelo_8_30_23/\"\n",
    "thresh_list=len(bam_files)*[m6A_thresh/258] # For R10 flow cells use 0.5; for R9 flow cells use 0.9\n",
    "for i in range(len(flowcells)):\n",
    "    if \"R9\" in flowcells[i]:\n",
    "        thresh_list[i] = R9_m6A_thresh/258\n",
    "\n",
    "file_prefix = \"aug_27_\"\n",
    "\"\"\"\n",
    "if sample_source == \"chr_type\":\n",
    "    selection = chr_type_selected\n",
    "if sample_source == \"type\":\n",
    "    selection = type_selected\n",
    "if sample_source == \"chromosome\":\n",
    "    selection = chromosome_selected\n",
    "if sample_source == \"condition\":\n",
    "    selection = analysis_cond\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc71daf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T06:19:14.431215500Z",
     "start_time": "2024-02-22T06:19:14.313231900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Filter input bed_file based on input parameters (e.g. chromosome, type, strand, etc.)\n",
    "# Function saves a new filtered bed file to the same folder as the original bed file\n",
    "# called temp_do_not_use_\"type\".bed\n",
    "importlib.reload(nanotools)\n",
    "new_bed_files=nanotools.filter_bed_file(\n",
    "    bed_file,\n",
    "    sample_source,\n",
    "    selection,\n",
    "    chromosome_selected,\n",
    "    chr_type_selected,\n",
    "    type_selected,\n",
    "    strand_selected,\n",
    "    max_regions,\n",
    "    bed_window,\n",
    "    intergenic_window\n",
    ")\n",
    "\n",
    "modkit_bed_name = \"modkit_temp.bed\"\n",
    "modkit_bed_df = nanotools.generate_modkit_bed(new_bed_files, down_sample_autosome, select_opp_strand,modkit_bed_name)\n",
    "nanotools.display_sample_rows(modkit_bed_df, 5)\n",
    "\n",
    "# Subsample bam based on bam_frac, used to accelerate testing\n",
    "# if bam_frac = 1 will use original bam files, otherwise will save new subsampled bam files to output_stem.\n",
    "args_list = [(bam_file, condition, bam_frac, sample_index, output_stem) for bam_file, condition, bam_frac, sample_index in zip(bam_files,conditions,bam_fracs,sample_indices)]\n",
    "new_bam_files=[]\n",
    "new_bam_files = nanotools.parallel_subsample_bam(bam_files, conditions, bam_fracs, sample_indices, output_stem)\n",
    "\n",
    "print(\"Program finished!\")\n",
    "print(\"new_bam_files: \", new_bam_files)\n",
    "print(\"exp_ids: \", exp_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0cb661",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T06:08:58.385904Z",
     "start_time": "2023-11-15T06:08:58.171616800Z"
    }
   },
   "outputs": [],
   "source": [
    "### Plot by m6A/A by chromosome\n",
    "### Calculate bam summary statistics\n",
    "importlib.reload(nanotools)\n",
    "\n",
    "sampling_frac = 0.1 # fraction of bam to sample for summary statistics\n",
    "\n",
    "# Summarize regions only?\n",
    "regions_only = True\n",
    "summary_bam_df = pd.DataFrame()\n",
    "\n",
    "def process_region(args):\n",
    "    chromosome, start, end, _, _, strand, each_bam, each_condition, each_thresh, each_exp_id = args\n",
    "    temp_df = nanotools.get_summary_from_bam(sampling_frac, each_thresh, \"/Data1/software/modkit/modkit\", each_bam, each_condition, each_exp_id, chromosome, start, end)\n",
    "    temp_df[\"chromosome\"] = chromosome\n",
    "    temp_df[\"start\"] = start\n",
    "    temp_df[\"end\"] = end\n",
    "    return temp_df\n",
    "\n",
    "### IF SUMMARIZING BY REGION\n",
    "if regions_only:\n",
    "    ### Define filename for summary table based on selected conditions\n",
    "    summary_table_name =  \"temp_files/\"+\"_\"+conditions[0]+conditions[-1] + \"_\"+str(sampling_frac)+\"_thresh\"+str(thresh_list[0])+type_selected[0]+\"_summary_table.csv\"\n",
    "\n",
    "    modkit_bed_df_summary = modkit_bed_df.drop_duplicates(subset=[0,1,2])\n",
    "\n",
    "    # if summary table exists, import it otherwise create it\n",
    "    if os.path.exists(summary_table_name):\n",
    "        print(\"Summary table exists, importing...\")\n",
    "        summary_bam_df = pd.read_csv(summary_table_name, sep=\"\\t\", header=0)\n",
    "    # drop duplicate chromosome and start and end rows from modkit_bed_df\n",
    "    else:\n",
    "        # Prepare the arguments for multiprocessing\n",
    "        args_list = [(row[0], row[1], row[2], row[3],row[4],row[5], each_bam, each_condition, each_thresh, each_exp_id)\n",
    "                     for _, row in modkit_bed_df_summary.iterrows()\n",
    "                     for each_bam, each_condition, each_thresh, each_exp_id in zip(new_bam_files, conditions, thresh_list, exp_ids)]\n",
    "        # Use multiprocessing to process regions in parallel\n",
    "        with multiprocessing.Pool(multiprocessing.cpu_count()) as pool:\n",
    "            results = pool.map(process_region, tqdm.tqdm(args_list,total=len(args_list)))\n",
    "            #results = pool.map(process_region, args_list)\n",
    "\n",
    "        # Concatenate the results and append to summary_bam_df\n",
    "        summary_bam_df = pd.concat(results, ignore_index=True)\n",
    "        nanotools.display_sample_rows(summary_bam_df)\n",
    "\n",
    "    summary_bam_df.to_csv(summary_table_name, sep=\"\\t\", header=True, index=False)\n",
    "\n",
    "### Create coverage_df file name (similar to summary_table_name)\n",
    "coverage_df_name =  \"temp_files/\"+\"_\"+conditions[0]+conditions[-1] + \"_\"+str(sampling_frac)+\"_thresh\"+str(thresh_list[0])+type_selected[0]+\"_coverage_df.csv\"\n",
    "# if coverage_df exists, import it otherwise create it\n",
    "if os.path.exists(coverage_df_name):\n",
    "    print(\"Coverage table exists, importing...\")\n",
    "    coverage_df = pd.read_csv(coverage_df_name, sep=\"\\t\", header=0)\n",
    "else:\n",
    "    # drop rows where base == C from summary_bam_df\n",
    "    summary_bam_df = summary_bam_df[summary_bam_df.base != \"C\"]\n",
    "    # Calculate total_m6a and total_A\n",
    "    total_m6a = summary_bam_df.loc[summary_bam_df['code'] == 'a'].groupby(['condition','chromosome','start'])['pass_count'].sum().reset_index()\n",
    "    total_m6a.rename(columns={'pass_count': 'total_m6a'}, inplace=True)\n",
    "\n",
    "    total_A = summary_bam_df.loc[(summary_bam_df['base'] == 'A') & (summary_bam_df['code'] == '-')].groupby(['condition','chromosome','start'])['pass_count'].sum().reset_index()\n",
    "    total_A.rename(columns={'pass_count': 'total_A'}, inplace=True)\n",
    "\n",
    "    # Merge total_m6a and total_A DataFrames\n",
    "    coverage_df = pd.merge(total_m6a, total_A, on=['condition','chromosome','start'], how='outer').fillna(0)\n",
    "\n",
    "    # Calculate coverage (ce genome size = 100,272,763)\n",
    "    coverage_df['coverage'] = ((coverage_df['total_A'] + coverage_df['total_m6a']) * (1/sampling_frac)) / 100000000\n",
    "\n",
    "    coverage_df['total_A_m6A'] = coverage_df['total_A'] + coverage_df['total_m6a']\n",
    "\n",
    "    # Calculate m6A_frac\n",
    "    coverage_df['m6A_frac'] = coverage_df['total_m6a'] / (coverage_df['total_A_m6A'])\n",
    "\n",
    "    #Save coverage df\n",
    "    coverage_df.to_csv(coverage_df_name, sep=\"\\t\", header=True, index=False)\n",
    "\n",
    "nanotools.display_sample_rows(coverage_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5e60a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(nanotools)\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Group by condition and chromosome and aggregate the data\n",
    "grouped_df = coverage_df.groupby(['condition', 'chromosome','start']).agg({'total_m6a': 'sum', 'total_A_m6A': 'sum'}).reset_index()\n",
    "\n",
    "# Compute the m6A/A ratio for the aggregated data\n",
    "grouped_df['m6A_frac'] = grouped_df['total_m6a'] / grouped_df['total_A_m6A']\n",
    "\n",
    "# Create the \"CHROMOSOME_X\" and \"Others\" categories\n",
    "grouped_df['chromosome_group'] = grouped_df['chromosome'].apply(lambda x: \"X\" if x == 'CHROMOSOME_X' else 'Autosome')\n",
    "\n",
    "# Compute the overall mean m6A_frac for each condition\n",
    "condition_means = grouped_df.groupby('condition')['total_m6a','total_A_m6A'].sum().reset_index()\n",
    "condition_means['m6A_frac'] = condition_means['total_m6a'] / condition_means['total_A_m6A']\n",
    "condition_means = condition_means[['condition', 'm6A_frac']]  # Select only the necessary columns\n",
    "condition_means = condition_means.rename(columns={'m6A_frac': 'condition_mean_m6A_frac'})\n",
    "\n",
    "# Merge this overall mean back to the grouped_df using the condition column\n",
    "grouped_df = grouped_df.merge(condition_means, on='condition', how='left')\n",
    "\n",
    "# Compute the normalized m6A_frac\n",
    "grouped_df['normalized_m6A_frac'] = grouped_df['m6A_frac'] / grouped_df['condition_mean_m6A_frac']\n",
    "\n",
    "# Display the first 3 rows of the dataframe\n",
    "display(grouped_df.head(3))\n",
    "\n",
    "# Use the default Plotly color scheme\n",
    "colors = plotly.colors.qualitative.Plotly\n",
    "\n",
    "# Group by chromosome or chromosome_group\n",
    "group_by_var = 'chromosome'\n",
    "\n",
    "# Map the unique chromosomes to the colors\n",
    "color_mapping = {chrom: colors[i % len(colors)] for i, chrom in enumerate(grouped_df[group_by_var].unique())}\n",
    "print(color_mapping)\n",
    "\n",
    "n2_bg_df = grouped_df[grouped_df['condition'] == 'N2_bg']\n",
    "\n",
    "fig_n2_bg = make_subplots(rows=2, cols=1, vertical_spacing=0.1, subplot_titles=(\"N2 Background\",\"\"))\n",
    "# add title to subplot 1\n",
    "\n",
    "# For N2_bg\n",
    "for chrom_group in n2_bg_df[group_by_var].unique():\n",
    "    subset = n2_bg_df[n2_bg_df[group_by_var] == chrom_group]\n",
    "\n",
    "    # Add raw m6A_frac trace to the first row\n",
    "    if not subset.empty:\n",
    "        fig_n2_bg.add_trace(go.Box(\n",
    "            y=subset['m6A_frac'],\n",
    "            name=f'{chrom_group}',\n",
    "            boxpoints='all',\n",
    "            pointpos=0,\n",
    "            marker_size=2,\n",
    "            marker_opacity=0.5,\n",
    "            fillcolor='rgba(0,0,0,0)',  # Transparent fill\n",
    "            jitter=0.4,\n",
    "            width=0.5,\n",
    "            marker_color=color_mapping[chrom_group],\n",
    "            line=dict(color=color_mapping[chrom_group])\n",
    "        ), row=1, col=1)\n",
    "\n",
    "    # Add normalized m6A_frac trace to the second row\n",
    "    if not subset.empty:\n",
    "        fig_n2_bg.add_trace(go.Box(\n",
    "            y=subset['normalized_m6A_frac'],\n",
    "            name=f'{chrom_group}',\n",
    "            boxpoints='all',\n",
    "            pointpos=0,\n",
    "            marker_size=2,\n",
    "            marker_opacity=0.5,\n",
    "            fillcolor='rgba(0,0,0,0)',  # Transparent fill\n",
    "            jitter=0.4,\n",
    "            width=0.5,\n",
    "            marker_color=color_mapping[chrom_group],\n",
    "            line=dict(color=color_mapping[chrom_group])\n",
    "        ), row=2, col=1)\n",
    "\n",
    "other_conditions_df = grouped_df[grouped_df['condition'] != 'N2_bg']\n",
    "num_other_conditions = other_conditions_df['condition'].nunique()\n",
    "list_unique_conditions = other_conditions_df['condition'].unique()\n",
    "\n",
    "fig_others = make_subplots(rows=2, cols=num_other_conditions, vertical_spacing=0.1, shared_yaxes=True,subplot_titles=(\n",
    "    #firt element in unique list of conditions\n",
    "    list_unique_conditions[0],list_unique_conditions[1],\"\",\"\"))\n",
    "\n",
    "col = 1\n",
    "for condition in other_conditions_df['condition'].unique():\n",
    "    for chrom_group in other_conditions_df[group_by_var].unique():\n",
    "        subset = other_conditions_df[(other_conditions_df['condition'] == condition) &\n",
    "                                     (other_conditions_df[group_by_var] == chrom_group)]\n",
    "\n",
    "        # Add raw m6A_frac trace to the first row\n",
    "        if not subset.empty:\n",
    "            fig_others.add_trace(go.Box(\n",
    "                y=subset['m6A_frac'],\n",
    "                name=f'{chrom_group}',\n",
    "                boxpoints='all',\n",
    "                pointpos=0,\n",
    "                marker_size=2,\n",
    "                marker_opacity=0.5,\n",
    "                fillcolor='rgba(0,0,0,0)',  # Transparent fill\n",
    "                jitter=0.4,\n",
    "                width=0.5,\n",
    "                marker_color=color_mapping[chrom_group],\n",
    "                line=dict(color=color_mapping[chrom_group])\n",
    "            ), row=1, col=col)\n",
    "\n",
    "        # Add normalized m6A_frac trace to the second row\n",
    "        if not subset.empty:\n",
    "            fig_others.add_trace(go.Box(\n",
    "                y=subset['normalized_m6A_frac'],\n",
    "                name=f'{chrom_group}',\n",
    "                boxpoints='all',\n",
    "                pointpos=0,\n",
    "                marker_size=2,\n",
    "                marker_opacity=0.5,\n",
    "                fillcolor='rgba(0,0,0,0)',  # Transparent fill\n",
    "                jitter=0.4,\n",
    "                width=0.5,\n",
    "                marker_color=color_mapping[chrom_group],\n",
    "                line=dict(color=color_mapping[chrom_group])\n",
    "            ), row=2, col=col)\n",
    "\n",
    "    col += 1\n",
    "\n",
    "# Update the layout\n",
    "fig_n2_bg.update_layout(title=\"m6A/A ratio and Normalized m6A/A ratio by Chromosome Group and Condition\",\n",
    "                        yaxis_title=\"m6A/A\",\n",
    "                        template=\"plotly_white\",\n",
    "                        boxmode='group',\n",
    "                        yaxis_tickformat='.0%',\n",
    "                        width=400\n",
    "                        )\n",
    "fig_others.update_layout(title=\"m6A/A ratio and Normalized m6A/A ratio by Chromosome Group and Condition\",\n",
    "                        yaxis_title=\"m6A/A\",\n",
    "                        template=\"plotly_white\",\n",
    "                        boxmode='group',\n",
    "                        yaxis_tickformat='.0%',\n",
    "                        width=1200\n",
    "                        )\n",
    "\n",
    "fig_others.update_yaxes(tickformat='.0%', row=1, col=1)\n",
    "fig_others.update_yaxes(tickformat='.0%', row=1, col=2)\n",
    "# update y axis in row1 col2 to \"Norm m6A/A\"\n",
    "fig_others.update_yaxes(title_text=\"Norm m6A/A\", row=2, col=1)\n",
    "fig_n2_bg.update_yaxes(title_text=\"Norm m6A/A\", row=2, col=1)\n",
    "\"\"\"fig.update_yaxes(tickformat='.0%', row=2, col=1)\n",
    "fig.update_yaxes(tickformat='.0%', row=2, col=2)\n",
    "fig.update_yaxes(tickformat='.0%', row=2, col=3)\n",
    "fig.update_yaxes(tickformat='.0%', row=2, col=4)\"\"\"\n",
    "\n",
    "\"\"\"fig_n2_bg = nanotools.add_p_value_annotation(fig_n2_bg, [#[0,1],[0,2],[0,3],[0,4],[0,5],\n",
    "                                             [0,1]],2)\n",
    "fig_others = nanotools.add_p_value_annotation(fig_others, [#[0,1],[0,2],[0,3],[0,4],[0,5],\n",
    "                                             [0,1]],3)\n",
    "fig_others = nanotools.add_p_value_annotation(fig_others, [#[0,1],[0,2],[0,3],[0,4],[0,5],\n",
    "                                             [0,1]],4)\"\"\"\n",
    "\n",
    "fig_others.show(renderer='plotly_mimetype+notebook')\n",
    "fig_n2_bg.show(renderer='plotly_mimetype+notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb170f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.subplots as sp\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Read the bed file into a DataFrame\n",
    "bed_df_for_plotting = pd.read_csv(\"/Data1/reference/tss_tes_rex_combined_v11_WS235.bed\", sep='\\t')\n",
    "\n",
    "# Filter rows where type == \"all_rex\"\n",
    "filtered_bed_df = bed_df_for_plotting[bed_df_for_plotting['type'] == 'all_rex']\n",
    "\n",
    "# Filter your grouped_df for the required conditions and chromosomes (modify as needed)\n",
    "conditions_to_plot = ['N2_fiber','SDC2_degron_fiber']\n",
    "chromosomes_to_plot = ['CHROMOSOME_X']\n",
    "filtered_grouped_df = grouped_df[grouped_df['condition'].isin(conditions_to_plot) & grouped_df['chromosome'].isin(chromosomes_to_plot)]\n",
    "\n",
    "# Create subplots with shared x-axis\n",
    "fig = sp.make_subplots(rows=2, cols=1, shared_xaxes=True, vertical_spacing=0.1)\n",
    "\n",
    "# Create variables to store subtraction data\n",
    "m6A_frac_diff = None\n",
    "starts = None\n",
    "\n",
    "# Add traces for filtered data, calculate subtraction data, and compute rolling average\n",
    "for condition in conditions_to_plot:\n",
    "    for chromosome in chromosomes_to_plot:\n",
    "        subset_df = filtered_grouped_df[(filtered_grouped_df['condition'] == condition) & (filtered_grouped_df['chromosome'] == chromosome)]\n",
    "\n",
    "        rolling_avg = subset_df['m6A_frac'].rolling(window=5, center=True).mean()\n",
    "\n",
    "        if m6A_frac_diff is None:\n",
    "            m6A_frac_diff = subset_df['m6A_frac'].reset_index(drop=True)\n",
    "            starts = subset_df['start'].reset_index(drop=True)  # Storing starts to use later\n",
    "        else:\n",
    "            m6A_frac_diff = -(m6A_frac_diff - subset_df['m6A_frac'].reset_index(drop=True))\n",
    "\n",
    "        fig.add_trace(go.Scatter(x=subset_df['start'],\n",
    "                                 y=subset_df['m6A_frac'],\n",
    "                                 mode='lines',\n",
    "                                 name=f'{condition}_{chromosome}'\n",
    "                                ),row=1,col=1)\n",
    "        fig.add_trace(go.Scatter(x=subset_df['start'],\n",
    "                                 y=rolling_avg,\n",
    "                                 mode='lines',\n",
    "                                 line=dict(dash='dot'),\n",
    "                                 name=f'{condition}_{chromosome} (Rolling Avg)'\n",
    "                                ),row=1,col=1)\n",
    "\n",
    "# Plot the subtraction data and its rolling average in the second subplot\n",
    "rolling_avg_diff = pd.Series(m6A_frac_diff).rolling(window=5, center=True).mean()\n",
    "fig.add_trace(go.Scatter(x=starts, y=m6A_frac_diff, mode='lines', name='Difference'), row=2, col=1)\n",
    "fig.add_trace(go.Scatter(x=starts, y=rolling_avg_diff, mode='lines', line=dict(dash='dot'), name='Difference (Rolling Avg)'), row=2, col=1)\n",
    "\n",
    "# Add vertical lines for each start position in the filtered_bed_df\n",
    "for start in filtered_bed_df['start']:\n",
    "    fig.add_shape(go.layout.Shape(type='line',\n",
    "                                  x0=start, x1=start,\n",
    "                                  y0=0, y1=1,\n",
    "                                  yref='paper',\n",
    "                                  line=dict(color='black', width=1)))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(title_text=\"m6A_frac and Difference between conditions with Rolling Average\")\n",
    "fig.update_yaxes(title_text=\"m6A_frac\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Difference\", row=2, col=1)\n",
    "#set theme to plotly_white\n",
    "fig.update_layout(template=\"plotly_white\")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2326adbc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T19:31:36.566832400Z",
     "start_time": "2023-12-12T19:28:33.278058Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "### Calculate bam summary statistics\n",
    "importlib.reload(nanotools)\n",
    "\n",
    "sampling_frac = 0.1 # fraction of bam to sample for summary statistics\n",
    "\n",
    "summary_bam_df = pd.DataFrame()\n",
    "\n",
    "### Define filename for summary table based on selected conditions\n",
    "# We'll start by defining a function to encapsulate the task you want parallelized\n",
    "def process_bam(args):\n",
    "    each_bam, each_condition, each_thresh, each_exp_id = args\n",
    "    print(\"starting on:\", each_bam, each_condition, \"with thresh:\", each_thresh)\n",
    "    return nanotools.get_summary_from_bam(sampling_frac, each_thresh, \"/Data1/software/modkit/modkit\", each_bam, each_condition, each_exp_id)\n",
    "\n",
    "# Define filename for summary table based on selected conditions\n",
    "summary_table_name = \"temp_files/\" + \"_\" + conditions[0] + conditions[-1] + \"_\" + str(sampling_frac) + \"_thresh\" + str(thresh_list[0]) + \"_summary_table.csv\"\n",
    "\n",
    "# Check if summary table exists\n",
    "if os.path.exists(summary_table_name):\n",
    "    print(\"Summary table exists, importing...\")\n",
    "    summary_bam_df = pd.read_csv(summary_table_name, sep=\"\\t\", header=0)\n",
    "else:\n",
    "    print(\"Summary table does not exist, creating...\")\n",
    "    #\n",
    "    # Create a pool of worker processes\n",
    "    pool = multiprocessing.Pool(2)\n",
    "\n",
    "    # Map the function to the arguments\n",
    "    results = pool.map(process_bam, zip(new_bam_files, conditions, thresh_list, exp_ids))\n",
    "\n",
    "    # Close the pool\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    # Append the results to the summary dataframe\n",
    "    summary_bam_df = pd.concat(results, ignore_index=True)\n",
    "    # Reset the index\n",
    "    summary_bam_df = summary_bam_df.reset_index(drop=True)\n",
    "\n",
    "    # Use display as per the instructions\n",
    "    display(summary_bam_df.head(3))\n",
    "\n",
    "    # Save the dataframe to a CSV file\n",
    "    summary_bam_df.to_csv(summary_table_name, sep=\"\\t\", header=True, index=False)\n",
    "\n",
    "### Plot m6A frac and coverage by condition\n",
    "\n",
    "### Create coverage_df file name (similar to summary_table_name)\n",
    "coverage_df_name =  \"temp_files/\"+\"_\"+conditions[0]+conditions[-1] + \"_\"+str(sampling_frac)+\"_thresh\"+str(thresh_list[0])+\"_coverage_df.csv\"\n",
    "# if coverage_df exists, import it otherwise create it\n",
    "if os.path.exists(coverage_df_name):\n",
    "    print(\"Coverage table exists, importing...\")\n",
    "    coverage_df = pd.read_csv(coverage_df_name, sep=\"\\t\", header=0)\n",
    "else:\n",
    "    # Calculate total_m6a and total_A\n",
    "    total_m6a = summary_bam_df.loc[summary_bam_df['code'] == 'a'].groupby(['exp_id', 'condition'])['pass_count'].sum().reset_index()\n",
    "    total_m6a.rename(columns={'pass_count': 'total_m6a'}, inplace=True)\n",
    "\n",
    "    total_A = summary_bam_df.loc[(summary_bam_df['base'] == 'A') & (summary_bam_df['code'] == '-')].groupby(['exp_id', 'condition'])['pass_count'].sum().reset_index()\n",
    "    total_A.rename(columns={'pass_count': 'total_A'}, inplace=True)\n",
    "\n",
    "    # Merge total_m6a and total_A DataFrames\n",
    "    coverage_df = pd.merge(total_m6a, total_A, on=['exp_id', 'condition'], how='outer').fillna(0)\n",
    "\n",
    "    # Calculate coverage (ce genome size = 100,272,763)\n",
    "    coverage_df['coverage'] = ((coverage_df['total_A'] + coverage_df['total_m6a']) * (1/sampling_frac)) / 100000000 * 4 # * 4 since As are 1/4 of genome\n",
    "\n",
    "    coverage_df['total_A_m6A'] = coverage_df['total_A'] + coverage_df['total_m6a']\n",
    "\n",
    "    # Calculate m6A_frac\n",
    "    coverage_df['m6A_frac'] = coverage_df['total_m6a'] / (coverage_df['total_A_m6A'])\n",
    "\n",
    "    # Drop rows where exp_id == AD1-nb_06_13_23\n",
    "    #coverage_df = coverage_df[coverage_df.exp_id != 'AD1-nb_06_13_23']\n",
    "\n",
    "    print(coverage_df)\n",
    "    #Save coverage df\n",
    "    coverage_df.to_csv(coverage_df_name, sep=\"\\t\", header=True, index=False)\n",
    "\n",
    "# Create the bar plot\n",
    "fig = go.Figure()\n",
    "\n",
    "# Function to generate color map for n unique items\n",
    "def generate_color_map(n, cmap):\n",
    "    norm = plt.Normalize(-1, n)\n",
    "    return [cm.colors.to_hex(cmap(norm(i))) for i in range(n)]\n",
    "\n",
    "# Find unique conditions\n",
    "unique_conditions = coverage_df['condition'].unique()\n",
    "\n",
    "# Different color maps for each condition\n",
    "condition_colormaps = {\n",
    "    'N2_fiber': cm.Blues,\n",
    "    'SDC2_degron_fiber': cm.Greens,\n",
    "    'N2_bg': cm.Reds\n",
    "    # Add more conditions and their corresponding colormaps here\n",
    "}\n",
    "\n",
    "# Generate color families for each condition\n",
    "color_families = {}\n",
    "for condition in unique_conditions:\n",
    "    n_exp_ids = len(coverage_df[coverage_df['condition'] == condition]['exp_id'].unique())\n",
    "    color_families[condition] = generate_color_map(n_exp_ids, condition_colormaps.get(condition, cm.viridis))\n",
    "\n",
    "# Map each exp_id to its color\n",
    "coverage_df['custom_color'] = coverage_df.groupby('condition')['exp_id'].transform(lambda x: x.astype('category').cat.codes)\n",
    "coverage_df['custom_color'] = coverage_df.apply(lambda row: color_families[row['condition']][row['custom_color']], axis=1)\n",
    "coverage_df['exp_id'] = coverage_df['exp_id'].str.strip()\n",
    "\n",
    "display(coverage_df)\n",
    "# Create the bar plot using Plotly Graph Objects for more customization\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add bars for each condition and exp_id\n",
    "for condition, condition_df in coverage_df.groupby('condition'):\n",
    "    for exp_id, exp_df in condition_df.groupby('exp_id'):\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=[condition],\n",
    "                y=[exp_df['coverage'].iloc[0]],  # Assuming only one row per exp_id per condition\n",
    "                name=exp_id,\n",
    "                marker=dict(color=exp_df['custom_color'].iloc[0]),\n",
    "                legendgroup=exp_id\n",
    "            )\n",
    "        )\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Stacked Bar Plot of Coverage by Condition',\n",
    "    xaxis_title='Condition',\n",
    "    yaxis_title='Coverage',\n",
    "    barmode='stack',\n",
    "    legend_title=\"exp_id\"\n",
    ")\n",
    "\n",
    "# auto set height\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=600,\n",
    "    height=700,\n",
    "    template=\"simple_white\"\n",
    ")\n",
    "\n",
    "# Group data by 'condition'\n",
    "grouped = coverage_df.groupby('condition')\n",
    "\n",
    "\n",
    "# Initialize the figure\n",
    "fig2 = go.Figure()\n",
    "\n",
    "# Add weighted box plot and points for each condition\n",
    "for name, group in grouped:\n",
    "    weighted_points = np.repeat(group['m6A_frac'], np.ceil(group['coverage'].astype(int)))\n",
    "    weighted_avg = np.average(group['m6A_frac'], weights=(group['coverage'].astype(int)*100))# * 10))\n",
    "    # calculate median\n",
    "    weighted_median= np.median(weighted_points)\n",
    "    fig2.add_trace(go.Box(\n",
    "        y=weighted_points,\n",
    "        name=name,\n",
    "        boxmean=True,\n",
    "        boxpoints=\"all\",  # No points on the box plot itself\n",
    "        jitter=0.3,       # Add some jitter for visibility\n",
    "        pointpos=0,     # Position of points relative to box\n",
    "        marker_size=2,\n",
    "        marker_opacity=0.5,\n",
    "        fillcolor='rgba(0,0,0,0)'  # Transparent fill\n",
    "    ))\n",
    "        # Add annotation for weighted average\n",
    "    fig2.add_annotation(\n",
    "        x=name,\n",
    "        y=weighted_median,#weighted_avg,\n",
    "        text=f\"Median: {weighted_median:.4f}\",\n",
    "        arrowhead=1,\n",
    "        ax=0,\n",
    "        ay=-10\n",
    "    )\n",
    "\n",
    "# Customize the layout\n",
    "fig2.update_layout(\n",
    "    title='Distribution of m6A by Condition',\n",
    "    xaxis_title='Condition',\n",
    "    yaxis_title='m6A frac',\n",
    "    template=\"simple_white\",\n",
    "    width=600,\n",
    "    # set y axis range\n",
    "    yaxis=dict(\n",
    "        range=[0, 0.4]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Set y axis to %\n",
    "fig2.update_yaxes(tickformat='.0%')\n",
    "\n",
    "fig2.show(renderer='plotly_mimetype+notebook')\n",
    "fig.show(renderer='plotly_mimetype+notebook')\n",
    "n50_fig = nanotools.calculate_and_plot_n50(new_bam_files, conditions, exp_ids)\n",
    "n50_fig.show(renderer='plotly_mimetype+notebook')\n",
    "\"\"\"\n",
    "fig.write_image(\"images_11_14_23/bulk_m6Afrac_n2_sdc2degron_0p1sample.svg\")\n",
    "fig.write_image(\"images_11_14_23/bulk_m6Afrac_n2_sdc2degron_0p1sample.png\")\n",
    "fig2.write_image(\"images_11_14_23/coverage_n2_sdc2degron_0p1sample.svg\")\n",
    "fig2.write_image(\"images_11_14_23/coverage_n2_sdc2degron_0p1sample.png\")\n",
    "\n",
    "# Function call example\n",
    "### Calculate N50s SKIP, NOT NECESSARY FOR ANY FOLLOWING STEPS\n",
    "n50_fig.write_image(\"images_11_14_23/n50_fig_n2_sdc2degron_0p1sample.svg\")\n",
    "n50_fig.write_image(\"images_11_14_23/n50_fig_n2_sdc2degron_0p1sample.png\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e17694c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T19:31:38.097049200Z",
     "start_time": "2023-12-12T19:31:36.566832400Z"
    }
   },
   "outputs": [],
   "source": [
    "### Generate modkit pileup file, used for plotting m6A/A in a given region.\n",
    "# Generating the list of output_file_names based on the given structure\n",
    "out_file_names = [output_stem + \"modkit-pileup-\" + each_condition +\"_\"+ str(round(each_thresh,2))+\"_\"+str(each_index)+\"_\"+str(each_bamfrac)+ \"_\".join([each_type[-7:] for each_type in type_selected]) + str(bed_window)+\".bed\" for each_condition,each_thresh,each_index, each_bamfrac in zip(conditions,thresh_list,sample_indices,bam_fracs)]\n",
    "\n",
    "# Function to run a single command\n",
    "def modkit_pileup_extract(args, index):\n",
    "    each_bam, each_thresh, each_condition, each_index, each_bamfrac, each_type,modkit_path, output_stem, modkit_bed_name = args\n",
    "\n",
    "    # Use the index to get the correct file name from out_file_names\n",
    "    each_output = out_file_names[index]\n",
    "\n",
    "    # Check if the output file exists\n",
    "    if os.path.exists(each_output):\n",
    "        print(f\"File already exists: {each_output}\")\n",
    "        # Read in output file and check if empty\n",
    "        modkit_qc = pd.DataFrame()\n",
    "        try:\n",
    "            modkit_qc = pd.read_csv(each_output, sep=\"\\t\", header=None, nrows=10)\n",
    "        except:\n",
    "            if modkit_qc.empty:\n",
    "                print(f\"File is empty: {each_output}\")\n",
    "                return\n",
    "        return\n",
    "    print(f\"Starting on: {each_output}\")\n",
    "    command = [\n",
    "        modkit_path,\n",
    "        \"pileup\",\n",
    "        \"--only-tabs\",\n",
    "        \"--ignore\",\n",
    "        \"m\",\n",
    "        \"--threads\",\n",
    "        \"20\",\n",
    "        \"--filter-threshold\",\n",
    "        #f\"A:{1-each_thresh}\",\n",
    "        f\"A:{1-each_thresh}\",\n",
    "        \"--mod-thresholds\",\n",
    "        f\"a:{each_thresh}\",\n",
    "        \"--log-filepath\",\n",
    "        output_stem + each_condition + str(each_index) + \"_modkit-pileup.log\",\n",
    "        \"--include-bed\",\n",
    "        modkit_bed_name,\n",
    "        each_bam,\n",
    "        each_output\n",
    "    ]\n",
    "    subprocess.run(command, text=True)\n",
    "\n",
    "# Now you need to adjust the task_args to include the index\n",
    "# Instead of directly zipping, enumerate one of the lists to get the index\n",
    "task_args_with_index = [(args, index) for index, args in enumerate(zip(\n",
    "    new_bam_files,\n",
    "    thresh_list,\n",
    "    conditions,\n",
    "    sample_indices,\n",
    "    bam_fracs,\n",
    "    [type_selected]*len(new_bam_files),\n",
    "    [modkit_path]*len(new_bam_files),\n",
    "    [output_stem]*len(new_bam_files),\n",
    "    [modkit_bed_name]*len(new_bam_files),\n",
    "))]\n",
    "\n",
    "# Execute commands in parallel, unpacking the arguments and index within the map call\n",
    "with Pool() as pool:\n",
    "    pool.starmap(modkit_pileup_extract, task_args_with_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b595bd7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T19:31:51.251288400Z",
     "start_time": "2023-12-12T19:31:38.096089800Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "### Add bed and condition details to modkit output for plotting\n",
    "# Using DataFrame merging to achieve the task without explicit loops\n",
    "## Looks up the bed_start and bed_end values for each row in bedmethyl_df\n",
    "def add_bed_columns_no_loops(bedmethyl_df_loc, combined_bed_df):\n",
    "    # Calculate midpoint in combined_bed_df\n",
    "    combined_bed_df['midpoint'] = (combined_bed_df['bed_start'] + combined_bed_df['bed_end']) / 2\n",
    "    # Convert midpoint to the same type as start_position (int, in this case)\n",
    "    combined_bed_df['midpoint'] = combined_bed_df['midpoint'].astype(int)\n",
    "\n",
    "\n",
    "    # Ensure that start_position is of type int (if it's not already)\n",
    "    bedmethyl_df_loc['start_position'] = bedmethyl_df_loc['start_position'].astype(int)\n",
    "\n",
    "    # Merge bedmethyl_df with combined_bed_df based on the nearest midpoint\n",
    "    merged_df = pd.merge_asof(bedmethyl_df_loc.sort_values('start_position'),\n",
    "                              combined_bed_df,\n",
    "                              by='chrom',\n",
    "                              left_on='start_position',\n",
    "                              right_on='midpoint',\n",
    "                              direction='nearest')\n",
    "\n",
    "    # Filter out rows where the start_position is not within the bed_start and bed_end range\n",
    "    merged_df = merged_df.loc[(merged_df['start_position'] >= merged_df['bed_start']) &\n",
    "                                (merged_df['start_position'] <= merged_df['bed_end'])]\n",
    "\n",
    "    #reset index\n",
    "    merged_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    # Create the final DataFrame by merging the merged DataFrame back to the original bedmethyl_df\n",
    "    final_df = pd.merge(bedmethyl_df_loc,\n",
    "                        merged_df[['chrom', 'start_position', 'bed_start', 'bed_end', 'bed_strand', 'type', 'chr_type']],\n",
    "                        on=['chrom', 'start_position'],\n",
    "                        how='left')\n",
    "\n",
    "    # Drop all final_df rows where type == NaN\n",
    "    final_df = final_df[final_df['type'].notna()]\n",
    "\n",
    "    return final_df\n",
    "\n",
    "# Build combined bed df\n",
    "combined_bed_df=pd.DataFrame()\n",
    "for each_bed in new_bed_files:\n",
    "    bed_path = each_bed[:-3] # remove .gz\n",
    "    # read bedpath and append to bed_df\n",
    "    combined_bed_df = combined_bed_df.append(pd.read_csv(bed_path, sep=\"\\t\", header=None))\n",
    "combined_bed_df.columns = ['chrom','bed_start','bed_end','bed_strand','type','chr_type']\n",
    "# Sort by chrom and bed_start then reset index\n",
    "combined_bed_df['chrom'] = combined_bed_df['chrom'].astype(str)\n",
    "combined_bed_df['bed_start'] = combined_bed_df['bed_start'].astype(int)\n",
    "combined_bed_df = combined_bed_df.sort_values(['bed_start'],ascending=[True])\n",
    "#drop any rows with an nan,\n",
    "combined_bed_df = combined_bed_df.dropna()\n",
    "# drop duplicate rows\n",
    "combined_bed_df.drop_duplicates(inplace=True)\n",
    "combined_bed_df.reset_index(inplace=True, drop=True)\n",
    "nanotools.display_sample_rows(combined_bed_df, 5)\n",
    "\n",
    "\n",
    "# Initialize comb_bedmethyl_plot_df\n",
    "comb_bedmethyl_df = pd.DataFrame()\n",
    "\n",
    "# Create combined plotting dataframe\n",
    "for each_output,each_condition,each_exp_id in zip(out_file_names,conditions,exp_ids):\n",
    "    #print(\"Starting on:\",each_output)\n",
    "    # Define bed methyl columns and import bedmethyl file\n",
    "    bedmethyl_df = pd.DataFrame()\n",
    "    bedmethyl_cols = ['chrom','start_position','end_position','modified_base_code','score','strand','start_position_compat','end_position_compat','color','Nvalid_cov','fraction_modified','Nmod','Ncanonical','Nother_mod','Ndelete','Nfail','Ndiff','Nnocall']\n",
    "    bedmethyl_df=pd.read_csv(each_output, sep=\"\\t\", header=None, names=bedmethyl_cols)\n",
    "    # if bedmethyl_df is empty\n",
    "    if bedmethyl_df.empty:\n",
    "        print(\"!Read in empty csv!!\")\n",
    "        print(\"Tried to select:\",each_output,\" \",each_condition,\" \",each_exp_id, \"and failed...\")\n",
    "        continue\n",
    "\n",
    "    # sort bedmethyl_df by chrom and start_position\n",
    "    bedmethyl_df = bedmethyl_df.sort_values(['start_position'], ascending=[True])\n",
    "    # drop any rows with a nan\n",
    "    bedmethyl_df = bedmethyl_df.dropna()\n",
    "    bedmethyl_df.drop_duplicates(inplace=True)\n",
    "    bedmethyl_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    bedmethyl_df = add_bed_columns_no_loops(bedmethyl_df, combined_bed_df)\n",
    "    # Add rel_start and rel_end columns equal to start-bed_start and end-bed_start\n",
    "    bedmethyl_df['rel_start'] = bedmethyl_df['start_position'] - bedmethyl_df['bed_start'] - bed_window +1\n",
    "    #print(\"2. bedmethyl_df\")\n",
    "    #display(bedmethyl_df)\n",
    "    bedmethyl_df['condition'] = each_condition\n",
    "    bedmethyl_df['exp_id'] = each_exp_id\n",
    "    # eliminate levels in dataframe\n",
    "\n",
    "    # if bedmethyl_df is empty\n",
    "    if bedmethyl_df.empty:\n",
    "        print(\"!Bedmethyl_df is empty!\")\n",
    "        print(\"Tried to select:\",each_output,\" \",each_condition,\" \",each_exp_id, \"and failed...\")\n",
    "        continue\n",
    "\n",
    "    # if comb_bedmethyl_plot_df is null, set it equal to bedmethyl_plot\n",
    "    if comb_bedmethyl_df.empty:\n",
    "        print(\"comb_bedmethyl_plot_df is empty, setting it equal to bedmethyl_plot...\")\n",
    "        comb_bedmethyl_df = bedmethyl_df\n",
    "        #print(\"comb_bedmethyl_plot_df:\",comb_bedmethyl_plot_df)\n",
    "    # else append bedmethyl_plot to comb_bedmethyl_plot_df\n",
    "    else:\n",
    "        print(\"comb_bedmethyl_plot_df is not empty, appending bedmethyl_plot...\")\n",
    "        comb_bedmethyl_df = comb_bedmethyl_df.append(bedmethyl_df)\n",
    "        #print(\"comb_bedmethyl_plot_df:\",comb_bedmethyl_plot_df)\n",
    "\n",
    "comb_bedmethyl_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "#print(\"head\")\n",
    "#display(comb_bedmethyl_df.head(100))\n",
    "print(\"sample\")\n",
    "display(comb_bedmethyl_df.sample(n=100, random_state=1))\n",
    "#print(\"tail\")\n",
    "#display(comb_bedmethyl_df.tail(100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6e1c33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T19:31:51.252292600Z",
     "start_time": "2023-12-12T19:31:44.066546900Z"
    }
   },
   "outputs": [],
   "source": [
    "### SHIFT AND TRANSFORM (OPTIONAL)\n",
    "align_zero_bool = False\n",
    "flip_bool = False\n",
    "def compute_lag_for_maximum_alignment(series1, bed_start1): #,series2, bed_start2):\n",
    "    \"\"\"\n",
    "    Decides flipping based on maximum cross-correlation, and then computes the lag\n",
    "    required to align the maximum values of two series. Returns both the lag and the decision to flip.\n",
    "    \"\"\"\n",
    "    \"\"\" if flip_bool:\n",
    "         # Calculate the correlations without any shift\n",
    "         correlation_original = np.correlate(series1, series2, mode='valid')\n",
    "         correlation_flipped = np.correlate(series1, series2[::-1], mode='valid')\n",
    "\n",
    "         # Decide the flip based on the correlation values\n",
    "         original_max_correlation = correlation_original.max()\n",
    "         flipped_max_correlation = correlation_flipped.max()\n",
    "         # set flip to 1 if flipped_max_correlation > original_max_correlation otherwise set to 0\n",
    "         flip = 1 if flipped_max_correlation > original_max_correlation else 0\n",
    "     else:\n",
    "         flip = 0\n",
    "\n",
    "     # Depending on the flip decision, align based on max values in the series\n",
    "     if flip == 1:\n",
    "         pos_max_series1 = -np.argmax(series1)\n",
    "     else:\"\"\"\n",
    "    flip=0\n",
    "    # print max value in the series\n",
    "    # set all values > 9 to 0\n",
    "    #series1[series1 > 9] = 0\n",
    "    pos_max_series1 = np.argmax(series1)\n",
    "    #print(\"len(series1):\",len(series1))\n",
    "    #print(\"max(series1):\",max(series1))\n",
    "    #print(\"argmax:\",pos_max_series1)\n",
    "    # print value at argmax\n",
    "    #print(\"series1[pos_max_series1]:\",series1[pos_max_series1])\n",
    "\n",
    "    lag = (round(len(series1)/2))-pos_max_series1\n",
    "    #print(\"lag:\",lag)\n",
    "\n",
    "    return (lag, flip)\n",
    "\n",
    "def get_continuous_series(df_subset):\n",
    "    # Create a Series with rel_start as the index and norm_mod_frac_weighted as the values\n",
    "    series_filled = df_subset.set_index('rel_start')['weighted_norm_mod_frac']\n",
    "\n",
    "    #print(\"series before filling:\",series)\n",
    "    # Fill NaNs using a rolling average\n",
    "    #series_filled = series.rolling(50, min_periods=1,center=True).mean()\n",
    "    # Fill any remaining NaNs at the start or end of the series using ffill or bfill\n",
    "    series_filled = series_filled.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "    # Ensure it's a continuous series by filling any gaps in rel_start\n",
    "    try:\n",
    "        series_filled = series_filled.reindex(range(int(series_filled.index.min()), int(series_filled.index.max()) + 1), fill_value=0)\n",
    "    except:\n",
    "        print(\"Failed series_filled:\",series_filled)\n",
    "        print(\"Duplicate indexes:\",series_filled.index[series_filled.index.duplicated()])\n",
    "    # For the newly introduced NaNs due to reindexing, we fill them again using a rolling average\n",
    "    #rolling_avg_reindexed = series_filled.rolling(window=50, center=True, min_periods=1).mean()\n",
    "    #series_filled = series_filled.fillna(rolling_avg_reindexed)\n",
    "    #print(\"series after filling:\",series_filled)\n",
    "    return series_filled.values\n",
    "\n",
    "def align_profiles(df):\n",
    "    df = df.sort_values(['bed_start', 'rel_start']).copy()\n",
    "    bed_starts = df['bed_start'].unique()\n",
    "\n",
    "    # Determine the reference bed_start\n",
    "    #summed_Nvalid_cov = df.groupby('bed_start')['Nvalid_cov'].sum()\n",
    "    #reference_bed_start = summed_Nvalid_cov.idxmax()\n",
    "    #series_reference = get_continuous_series(df[df['bed_start'] == reference_bed_start])\n",
    "\n",
    "    # Calculate the number of positions to shift\n",
    "    #shift_positions = int(round(len(series_reference)/2)) - np.argmax(series_reference)\n",
    "\n",
    "    # Shift the entire series_reference by shift_positions to the left or right depending on the sign\n",
    "    #if shift_positions > 0:  # shift to the left\n",
    "    #    series_reference = np.concatenate(([0]*shift_positions, series_reference))\n",
    "    #else:\n",
    "    #    series_reference = np.concatenate((series_reference,[0]*shift_positions))\n",
    "\n",
    "    df[\"shift\"] = 0\n",
    "    df[\"flipped\"] = 0\n",
    "\n",
    "    for other_bed_start in bed_starts:\n",
    "        #if other_bed_start == reference_bed_start:\n",
    "        #    continue\n",
    "\n",
    "        series_to_shift = get_continuous_series(df[df['bed_start'] == other_bed_start])\n",
    "        # print every item in series_to_shift\n",
    "        #for item in series_to_shift:\n",
    "        lag, flip = compute_lag_for_maximum_alignment(series_to_shift, other_bed_start)#,series_reference, reference_bed_start)\n",
    "\n",
    "        df.loc[df['bed_start'] == other_bed_start, 'shift'] = lag\n",
    "        df.loc[df['bed_start'] == other_bed_start, 'flipped'] = 1 if flip else 0\n",
    "\n",
    "        #print(f\"Decision for bed_start {other_bed_start}: flipped={flip}, shift={lag}\\n\")\n",
    "\n",
    "    # Calculate statistics using the 'flipped' and 'shift' columns\n",
    "    total_flipped = df[df['flipped'] == 1]['bed_start'].nunique()\n",
    "    lag_distribution = df['shift'].describe()\n",
    "\n",
    "    print(f\"Total bed_starts flipped: {total_flipped} out of {len(bed_starts) - 1}\")\n",
    "    print(\"Lag Distribution:\")\n",
    "    print(lag_distribution)\n",
    "\n",
    "    return df\n",
    "\n",
    "comb_bedmethyl_plot_df = comb_bedmethyl_df.copy()\n",
    "\n",
    "# If aligning to 0\n",
    "if align_zero_bool:\n",
    "    final_df = comb_bedmethyl_plot_df.copy()\n",
    "\n",
    "    final_df = comb_bedmethyl_plot_df.groupby(\n",
    "    ['chrom', 'chr_type', 'rel_start', 'exp_id', 'condition', 'type', 'bed_start']).agg({\n",
    "    'Nvalid_cov': 'sum',\n",
    "    'Nmod': 'sum',\n",
    "    'Ncanonical': 'sum',\n",
    "    'Nother_mod': 'sum'\n",
    "    }).reset_index()\n",
    "\n",
    "    # Calculate normalized m6A\n",
    "    final_df['raw_mod_frac'] = final_df['Nmod'] / (final_df['Nmod'] + final_df['Ncanonical'])\n",
    "\n",
    "    # Merge operation\n",
    "    final_df = pd.merge(\n",
    "        final_df,\n",
    "        coverage_df[['exp_id', 'm6A_frac']],\n",
    "        on=['exp_id'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # rename m6A_frac column to exp_id_m6A_frac\n",
    "    final_df.rename(columns={'m6A_frac': 'exp_id_m6A_frac'}, inplace=True)\n",
    "\n",
    "    # Calculate norm_mod_frac\n",
    "    final_df['norm_mod_frac_init'] = final_df['raw_mod_frac'] / final_df['exp_id_m6A_frac']\n",
    "\n",
    "    # 2. Reuse DataFrame\n",
    "    final_df['norm_mod_frac_weighted'] = final_df['norm_mod_frac_init'] * (final_df['Nmod'] + final_df['Ncanonical'])\n",
    "\n",
    "    # set bed_start to bed_start as a string + chrom as a string\n",
    "    final_df['bed_start'] = final_df['bed_start'].astype(str) + \"_\" + final_df['chrom'].astype(str)\n",
    "\n",
    "    # Group by and aggregation\n",
    "    final_df = final_df.groupby(\n",
    "        ['rel_start', 'condition', 'type', 'chr_type','bed_start']\n",
    "    )[['Nvalid_cov', 'Ncanonical', 'Nmod', 'norm_mod_frac_weighted']].sum().reset_index()\n",
    "\n",
    "\n",
    "    # Additional calculations\n",
    "    final_df['weighted_norm_mod_frac'] = final_df['norm_mod_frac_weighted'] / (final_df['Nmod'] + final_df['Ncanonical'])\n",
    "    final_df['raw_mod_frac'] = final_df['Nmod'] / (final_df['Nmod'] + final_df['Ncanonical'])\n",
    "\n",
    "    # drop rows where Nvalid_cov is lower than lower quartile\n",
    "    final_df = final_df[final_df['Nvalid_cov'] > final_df['Nvalid_cov'].quantile(0.1)]\n",
    "\n",
    "    # Sorting and re-indexing\n",
    "    final_df.sort_values(['bed_start', 'rel_start'], inplace=True)\n",
    "    final_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    # print rows with duplicate rel_pos, bed_start, condition and type\n",
    "    print(\"Duplicate rows:\")\n",
    "    display(final_df[final_df.duplicated(['rel_start','bed_start','condition','type'])].head(10))\n",
    "\n",
    "    # Displaying the first 100 rows\n",
    "    display(final_df.head(100))\n",
    "    center_iter = 0\n",
    "    for each_condition in final_df['condition'].unique():\n",
    "        for each_type in final_df['type'].unique():\n",
    "            # Filtering the data\n",
    "            print(\"Starting on:\",each_condition,each_type)\n",
    "            final_df_cluster = final_df[(final_df['condition'] == each_condition) & (final_df['type'] == each_type)]\n",
    "            #print(\"final_df_cluster:\")\n",
    "            #display(final_df_cluster.head(100))\n",
    "\n",
    "            aligned_df = align_profiles(final_df_cluster)\n",
    "            aligned_df.drop_duplicates(subset=['bed_start'], inplace=True)\n",
    "\n",
    "            final_df_cluster = comb_bedmethyl_df[(comb_bedmethyl_df['condition'] == each_condition) & (comb_bedmethyl_df['type'] == each_type)].groupby(\n",
    "                ['chrom', 'rel_start', 'exp_id', 'modified_base_code', 'condition', 'type', 'chr_type', 'bed_start']\n",
    "            ).agg({\n",
    "                'Nvalid_cov': 'sum',\n",
    "                'Nmod': 'sum',\n",
    "                'Ncanonical': 'sum',\n",
    "                'Nother_mod': 'sum'\n",
    "            }).reset_index()\n",
    "\n",
    "            # set bed_start to bed_start as a string + chrom as a string\n",
    "            final_df_cluster['bed_start'] = final_df_cluster['bed_start'].astype(str) + \"_\" + final_df_cluster['chrom'].astype(str)\n",
    "\n",
    "            # Calculate normalized m6A\n",
    "            final_df_cluster['raw_mod_frac'] = final_df_cluster['Nmod'] / (final_df_cluster['Nmod'] + final_df_cluster['Ncanonical'])\n",
    "\n",
    "            # Merge operation\n",
    "            final_df_cluster = pd.merge(\n",
    "                final_df_cluster,\n",
    "                coverage_df[['exp_id', 'm6A_frac']],\n",
    "                on=['exp_id'],\n",
    "                how='left'\n",
    "            )\n",
    "\n",
    "            # rename m6A_frac column to exp_id_m6A_frac\n",
    "            final_df_cluster.rename(columns={'m6A_frac': 'exp_id_m6A_frac'}, inplace=True)\n",
    "\n",
    "            # Calculate norm_mod_frac\n",
    "            final_df_cluster['norm_mod_frac_init'] = final_df_cluster['raw_mod_frac'] / final_df_cluster['exp_id_m6A_frac']\n",
    "\n",
    "            # 2. Reuse DataFrame\n",
    "            final_df_cluster['norm_mod_frac_weighted'] = final_df_cluster['norm_mod_frac_init'] * (final_df_cluster['Nmod'] + final_df_cluster['Ncanonical'])\n",
    "\n",
    "            ### Since multiple samples have same condition:\n",
    "            #merge final_df with aligned_df on bed_start adding shift and flipped columns\n",
    "            print(\"Merging final_df_cluster with aligned_df...\")\n",
    "            #display(final_df_cluster.head(10))\n",
    "            #display(aligned_df.head(10))\n",
    "\n",
    "            final_df_cluster = pd.merge(final_df_cluster, aligned_df[['bed_start','shift','flipped']], on=['bed_start'], how='left')\n",
    "            final_df_cluster.loc[final_df_cluster['flipped'] == 1, 'rel_start'] *= -1\n",
    "            # add shift to rel_pos\n",
    "            final_df_cluster['rel_start'] += final_df_cluster['shift']\n",
    "            final_df_cluster = final_df_cluster.groupby(['rel_start','modified_base_code','condition','type','chr_type'])[['Nvalid_cov','Ncanonical','Nmod','norm_mod_frac_weighted']].sum() #,'strand'\n",
    "            final_df_cluster.reset_index(inplace=True)\n",
    "\n",
    "            # set norm_mod_frac to norm_mod_frac_weighted / Nvalid_cov\n",
    "            final_df_cluster['weighted_norm_mod_frac'] = final_df_cluster['norm_mod_frac_weighted']/(final_df_cluster['Nmod']+final_df_cluster['Ncanonical'])\n",
    "            final_df_cluster['raw_mod_frac'] = final_df_cluster['Nmod']/(final_df_cluster['Nmod']+final_df_cluster['Ncanonical'])\n",
    "            #sort by rel_start\n",
    "            final_df_cluster.sort_values(['rel_start'], inplace=True)\n",
    "            final_df_cluster.reset_index(inplace=True, drop=True)\n",
    "\n",
    "            if center_iter == 0:\n",
    "                plot_df = final_df_cluster.copy()\n",
    "            else:\n",
    "                plot_df = plot_df.append(final_df_cluster)\n",
    "            center_iter += 1\n",
    "\n",
    "else:\n",
    "    # FOR GENES If bed_strand is -, multiply rel_start by -1, and sort by rel_start resetting index afterwards\n",
    "    for each_type in type_selected:\n",
    "        # if each_type contains substring \"TSS\", \"TES\", \"MEX\", then flip only those genes\n",
    "        if any(x in each_type for x in [\"TSS\", \"TES\", \"MEX\"]):\n",
    "            print(f\"Strand orientation sensitive {each_type} type selected, multiplying rel_start by -1 for '-' strand genes...\")\n",
    "            # Mask comb_bedmethyl_plot_df by type == each_type and strand == '-'\n",
    "            mask = (comb_bedmethyl_plot_df['type'] == each_type) & (comb_bedmethyl_plot_df['bed_strand'] == '-')\n",
    "            # Multiply rel_start by -1  for all rows where mask is true\n",
    "            comb_bedmethyl_plot_df.loc[mask, 'rel_start'] *= -1\n",
    "\n",
    "    # Group comb_bedmethyl_plot_df and sum specific columns\n",
    "    grouped_df = comb_bedmethyl_plot_df.groupby(['chrom', 'rel_start','start_position','exp_id','modified_base_code','condition','type','chr_type']).agg({ #,'strand'\n",
    "        'Nvalid_cov': 'sum',\n",
    "        'Nmod': 'sum',\n",
    "        'Ncanonical': 'sum',\n",
    "        'Nother_mod': 'sum'\n",
    "    }).reset_index()\n",
    "\n",
    "    ### Calculate normalized m6A\n",
    "    #grouped_df['mod_frac'] = grouped_df['Nmod'] / grouped_df['Nvalid_cov']\n",
    "    grouped_df['raw_mod_frac'] = grouped_df['Nmod'] / (grouped_df['Nmod'] + grouped_df['Ncanonical'])\n",
    "    # Merge the two dataframes based on 'exp_id' and 'condition'\n",
    "\n",
    "    coverage_df['exp_id'] = coverage_df['exp_id'].str.strip()\n",
    "    grouped_df['exp_id'] = grouped_df['exp_id'].str.strip()\n",
    "    nanotools.display_sample_rows(coverage_df,10)\n",
    "    nanotools.display_sample_rows(grouped_df,10)\n",
    "    merged_df = pd.merge(grouped_df, coverage_df[['exp_id', 'm6A_frac']],\n",
    "                         on=['exp_id'], how='left')\n",
    "    nanotools.display_sample_rows(merged_df,10)\n",
    "    # rename m6A_frac column to exp_id_m6A_frac\n",
    "    merged_df.rename(columns={'m6A_frac': 'exp_id_m6A_frac'}, inplace=True)\n",
    "    # Calculate norm_mod_frac\n",
    "    merged_df['norm_mod_frac_init'] = merged_df['raw_mod_frac'] / merged_df['exp_id_m6A_frac']\n",
    "    # If you want to keep only the original columns plus the new 'norm_mod_frac'\n",
    "    plot_df = merged_df[grouped_df.columns.tolist() + ['norm_mod_frac_init']]\n",
    "    # Calculate norm_mod_frac_weighted\n",
    "    plot_df['norm_mod_weighted'] = plot_df['norm_mod_frac_init'] * (grouped_df['Nmod'] + grouped_df['Ncanonical'])\n",
    "\n",
    "    ### Since multiple samples have same condition:\n",
    "    plot_df = plot_df.groupby(['rel_start','modified_base_code','condition','type','chr_type'])[['Nvalid_cov','Ncanonical','Nmod','norm_mod_weighted']].sum() #,'strand'\n",
    "    plot_df.reset_index(inplace=True)\n",
    "\n",
    "    # set norm_mod_frac to norm_mod_frac_weighted / Nvalid_cov\n",
    "    plot_df['weighted_norm_mod_frac'] = plot_df['norm_mod_weighted']/(plot_df['Nmod']+plot_df['Ncanonical'])\n",
    "    plot_df['raw_mod_frac'] = plot_df['Nmod']/(plot_df['Nmod']+plot_df['Ncanonical'])\n",
    "    #sort by rel_start\n",
    "    plot_df.sort_values(['rel_start'], inplace=True)\n",
    "    plot_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "print(\"plot_df:\")\n",
    "# display random 100 rows\n",
    "nanotools.display_sample_rows(plot_df,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037e6481",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot correlation plot between replicates\n",
    "merged_df_correlation = merged_df.copy()\n",
    "merged_df_correlation[\"exp_condition_id\"] = merged_df_correlation[\"exp_id\"] + \"_\" + merged_df_correlation[\"condition\"]\n",
    "# Define a binning function to bin every 10 bases\n",
    "def binning_func(x):\n",
    "    return np.floor(x / 50) * 50\n",
    "\n",
    "# Apply the binning function to the start_position to create binned_start_position\n",
    "merged_df_correlation['binned_start_position'] = merged_df_correlation['start_position'].apply(binning_func)\n",
    "nanotools.display_sample_rows(merged_df_correlation)\n",
    "# Group by 'chrom', 'binned_start_position', and 'exp_condition_id', and then sum up 'Nvalid_cov' and 'Nmod'\n",
    "binned_df = merged_df_correlation.groupby(['chrom', 'binned_start_position', 'exp_condition_id','exp_id_m6A_frac']).agg({\n",
    "    'Nvalid_cov': 'sum',\n",
    "    'Nmod': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Calculate the new 'm6A_frac' as the ratio of 'Nmod' to 'Nvalid_cov'\n",
    "binned_df['m6A_frac'] = binned_df['Nmod'] / binned_df['Nvalid_cov']\n",
    "\n",
    "# add norm_mod_frac column\n",
    "binned_df['norm_mod_frac'] = binned_df['m6A_frac'] / binned_df['exp_id_m6A_frac']\n",
    "nanotools.display_sample_rows(binned_df)\n",
    "binned_df['transformed_mod_frac'] = np.arcsin(np.sqrt(binned_df['m6A_frac']))\n",
    "\n",
    "# Pivot the DataFrame with the new binned positions and calculated 'm6A_frac'\n",
    "pivoted_df = binned_df.pivot_table(\n",
    "    index=['chrom', 'binned_start_position'],\n",
    "    columns='exp_condition_id',\n",
    "    values='transformed_mod_frac'\n",
    ")\n",
    "\n",
    "nanotools.display_sample_rows(pivoted_df)\n",
    "\n",
    "# Step 3: Calculate the Pearson correlation coefficient matrix\n",
    "correlation_matrix = pivoted_df.corr(method='pearson')\n",
    "\n",
    "nanotools.display_sample_rows(correlation_matrix)\n",
    "\n",
    "# Step 4: Square the correlation coefficients to obtain r² values\n",
    "r_squared_matrix = correlation_matrix ** 2\n",
    "\n",
    "nanotools.display_sample_rows(r_squared_matrix)\n",
    "\n",
    "# Create a heatmap using plotly.graph_objects\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    z=r_squared_matrix.values,\n",
    "    x=r_squared_matrix.columns,\n",
    "    y=r_squared_matrix.index,\n",
    "    colorscale='Oranges'))\n",
    "\n",
    "# Update the layout to use the plotly_white template and adjust the title\n",
    "fig.update_layout(\n",
    "    template='plotly_white',\n",
    "    title='Pearson r² Values Heatmap'\n",
    ")\n",
    "\n",
    "# Show the figure in a Jupyter environment or it can be saved to an HTML file using fig.write_html('heatmap.html')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65392c6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T19:31:51.263294600Z",
     "start_time": "2023-12-12T19:31:51.095291Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# save final_df to /temp folder as csv, with all configurations in file name if it does not exist. If it does exist, import it.\n",
    "final_fn = \"temp_files/\" + \"final_df_\" + \"_\".join([each_type for each_type in type_selected]) + str(round(thresh_list[0],2)) + \"_\"+str(bam_fracs[0])+str(bed_window)+\".csv\"\n",
    "plot_df.to_csv(final_fn, index=False)\n",
    "if os.path.exists(final_fn):\n",
    "    print(\"final_df already exists, importing it...\")\n",
    "    plot_df = pd.read_csv(final_fn)\n",
    "else:\n",
    "    print(\"final_df does not exist, saving it...\")\n",
    "    plot_df.to_csv(final_fn, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03d8839",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T19:36:14.822762700Z",
     "start_time": "2023-12-12T19:36:05.956989700Z"
    }
   },
   "outputs": [],
   "source": [
    "importlib.reload(nanotools)\n",
    "def plot_bedmethyl(comb_bedmethyl_df, conditions, chr_types=None, types=None, strands=[\"all\"], window_size=25, selection_indices=None, bed_window=1000):\n",
    "    global analysis_cond\n",
    "    fig = go.Figure()\n",
    "    y_min = float('inf')\n",
    "    y_max = float('-inf')\n",
    "\n",
    "    #drop all rows with rel_pos > bed_window or < -bed_window\n",
    "    comb_bedmethyl_df = comb_bedmethyl_df[(comb_bedmethyl_df['rel_start'] <= bed_window) & (comb_bedmethyl_df['rel_start'] >= -bed_window)]\n",
    "\n",
    "    if selection_indices is not None:\n",
    "        conditions = [conditions[i] for i in selection_indices]\n",
    "\n",
    "    for selected_condition in conditions:\n",
    "        print(\"Starting on condition:\", selected_condition)\n",
    "\n",
    "        for selected_chr_type in (chr_types or [\"all\"]):\n",
    "            for selected_type in (types or [\"all\"]):\n",
    "                for selected_strand in (strands or [\"all\"]):  # Add this line for strands\n",
    "\n",
    "                    # Apply filters appropriately\n",
    "                    filters = []\n",
    "                    if selected_condition:\n",
    "                        filters.append(comb_bedmethyl_df['condition'] == selected_condition)\n",
    "                    if selected_chr_type != \"all\":\n",
    "                        filters.append(comb_bedmethyl_df['chr_type'] == selected_chr_type)\n",
    "                    if selected_type != \"all\":\n",
    "                        filters.append(comb_bedmethyl_df['type'] == selected_type)\n",
    "                    if selected_strand != \"all\":  # Add this line for strands\n",
    "                        filters.append(comb_bedmethyl_df['strand'] == selected_strand)\n",
    "\n",
    "                    base_filter = np.logical_and.reduce(filters)\n",
    "\n",
    "                    m6A_data = comb_bedmethyl_df.loc[base_filter]['weighted_norm_mod_frac']\n",
    "                    m6A_data_xaxis = comb_bedmethyl_df.loc[base_filter]['rel_start']\n",
    "\n",
    "                    smoothed_data = m6A_data.rolling(window=window_size, center=True).mean()\n",
    "                    y_min = min(y_min, smoothed_data.min())\n",
    "                    y_max = max(y_max, smoothed_data.max())\n",
    "\n",
    "                    label = f\"{selected_condition}_{selected_chr_type}_{selected_type}_{selected_strand}\"  # Add selected_strand here\n",
    "\n",
    "                    fig.add_trace(go.Scatter(\n",
    "                        x=m6A_data_xaxis.values,\n",
    "                        y=smoothed_data.values,\n",
    "                        mode='lines',\n",
    "                        name=label,\n",
    "                        opacity=0.9\n",
    "                    ))\n",
    "\n",
    "    # define plot_title as \"m6A Fraction\" + selected_types\n",
    "    plot_title = \"m6A Fraction\" + \"_\".join([each_type for each_type in type_selected])\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=plot_title,\n",
    "        xaxis_title='Genomic Position',\n",
    "        yaxis_title='normalized m6A/A',\n",
    "        template=\"plotly_white\",\n",
    "        width=800,\n",
    "        height=650\n",
    "    )\n",
    "    # Update to place legend at the bottom\n",
    "    fig.update_layout(legend=dict(\n",
    "        traceorder=\"normal\",\n",
    "        y=-0.2,\n",
    "        x=0.25,\n",
    "        # set absolute value of y position\n",
    "        yanchor=\"top\",\n",
    "        orientation='h',\n",
    "        # increase font size\n",
    "        font=dict(\n",
    "            size=14\n",
    "        ),\n",
    "    ))\n",
    "\n",
    "    fig.update_xaxes(range=[-bed_window, +bed_window])\n",
    "    #fig.update_yaxes(range=[0.8, 2.2])\n",
    "    #Shift y axis labels left\n",
    "    # Skip the first tick label\n",
    "    #tickvals = list([(x + 10)/10 for x in range(0,11,2)])\n",
    "    #ticktext = [str(x) for x in tickvals]  # Empty string for the first tick label\n",
    "    #fig.update_yaxes(tickvals=tickvals, ticktext=ticktext)\n",
    "\n",
    "    # Add vertical dashed line at x=0\n",
    "    fig.add_shape(\n",
    "        type=\"line\", line=dict(dash=\"dash\"),\n",
    "        x0=0, x1=0, y0=y_min, y1=y_max, line_color=\"Grey\"\n",
    "    )\n",
    "\n",
    "    fig.show(renderer='plotly_mimetype+notebook')\n",
    "    return fig, label\n",
    "\n",
    "#Display random 100 rows from comb_bedmethyl_plot_df\n",
    "print(\"type_selected:\",type_selected)\n",
    "print(\"plot_df:\")\n",
    "nanotools.display_sample_rows(plot_df)\n",
    "\n",
    "# Example usage:\n",
    "# Note: final_df and conditions should be defined in your code\n",
    "region_fig = plot_bedmethyl(plot_df, analysis_cond, chr_types=[\"X\",\"Autosome\"], types=[\"all\"], strands=[\"all\"], window_size=25, selection_indices=[0,1,2], bed_window=500)\n",
    "\n",
    "# print unique count bed_start values for combination of chr_type, type and condition in each comb_bedmethyl_df\n",
    "print(\"Unique count of bed_start values for each combination of chr_type, type and condition in comb_bedmethyl_df:\")\n",
    "print(comb_bedmethyl_df.groupby(['chr_type','type','condition'])['bed_start'].nunique())\n",
    "\n",
    "rand_suffix = nanotools.random_alpha_numeric(8)\n",
    "region_fig[0].write_image(\"images_11_14_23/\"+region_fig[1]+type_selected[0]+str(bed_window)+\"bp.svg\")\n",
    "region_fig[0].write_image(\"images_11_14_23/\"+region_fig[1]+type_selected[0]+str(bed_window)+\"bp.png\")\n",
    "\n",
    "#\"center_DPY27_chip_albretton_ONLY\",\"center_DPY27_chip_albretton;gene_ol2000;TSS_ol2000\",\"strong_rex;DPY27_ol2000;SDC_ol2000\",\"center_DPY27_chip_albretton;SDC_ol2000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfa4d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def filter_data(df, condition, chr_type, type_,strand):\n",
    "    filters = []\n",
    "    if condition:\n",
    "        filters.append(df['condition'] == condition)\n",
    "    if chr_type != \"all\":\n",
    "        filters.append(df['chr_type'] == chr_type)\n",
    "    if type_ != \"all\":\n",
    "        filters.append(df['type'] == type_)\n",
    "    if strand != \"all\":\n",
    "        filters.append(df['strand'] == strand)\n",
    "\n",
    "    base_filter = pd.concat(filters).groupby(level=0).all()\n",
    "    return df.loc[base_filter]\n",
    "\n",
    "def plot_bedmethyl_diff(bed_window,final_df, conditions, window_size=25, *args):\n",
    "    fig = go.Figure()\n",
    "    fig.update_layout(\n",
    "        title='m6A Fraction Difference vs Genomic Position N2 SDC3 - N2 intergenic control',\n",
    "        xaxis_title='Genomic Position',\n",
    "        yaxis_title='% change in norm m6A/A',\n",
    "        template=\"plotly_white\",\n",
    "        width=1000,\n",
    "        height=600,\n",
    "        #increase font size\n",
    "        font=dict(\n",
    "            size=14\n",
    "        ),\n",
    "        # set y axis to % with rounded to nearest int\n",
    "        yaxis_tickformat = '.0%'\n",
    "\n",
    "    )\n",
    "    # Update to place legend at the bottom\n",
    "    fig.update_layout(legend=dict(\n",
    "        y=-0.4,\n",
    "        x=0.25\n",
    "    ))\n",
    "\n",
    "    #Shift y axis labels left\n",
    "    # Skip the first tick label\n",
    "    \"\"\"x_min = -20\n",
    "    x_max = 20\n",
    "    tickvals = list([(x)/100 for x in range(x_min,x_max,5)])\n",
    "    ticktext = [(str(round(x*100))+\"%\") for x in tickvals]  # Empty string for the first tick label\n",
    "    fig.update_yaxes(tickvals=tickvals, ticktext=ticktext)\n",
    "    # set y axis min and max\n",
    "    fig.update_yaxes(range=[min(tickvals)-0.025,max(tickvals)+0.025])\n",
    "\n",
    "    # Add vertical dashed line at x=0\n",
    "    fig.add_shape(\n",
    "        type=\"line\", line=dict(dash=\"dash\"),\n",
    "        x0=0, x1=0, y0=min(tickvals)-0.025, y1=max(tickvals), line_color=\"Grey\"\n",
    "    )\"\"\"\n",
    "\n",
    "\n",
    "    fig.update_xaxes(range=[-bed_window, bed_window])\n",
    "    #fig.update_yaxes(range=[-0.1,0.4])\n",
    "\n",
    "    for (selection_index1, chr_type1, type1,strand, selection_index2, chr_type2, type2,strand) in args:\n",
    "        condition1 = conditions[selection_index1]\n",
    "        condition2 = conditions[selection_index2]\n",
    "\n",
    "        df1 = filter_data(final_df, condition1, chr_type1, type1,strand)\n",
    "        df2 = filter_data(final_df, condition2, chr_type2, type2,strand)\n",
    "\n",
    "        df1.reset_index(drop=True, inplace=True)\n",
    "        df2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        \"\"\"#Drop outlier weighted_norm_mod_frac datapoints from df1 and df2 more than 3 standard deviations away from the mean\n",
    "        # Calculate mean and standard deviation for the column 'weighted_norm_mod_frac' in df1\n",
    "        mean1 = df1['weighted_norm_mod_frac'].mean()\n",
    "        std1 = df1['weighted_norm_mod_frac'].std()\n",
    "\n",
    "        # Drop outliers in df1\n",
    "        df1 = df1[(df1['weighted_norm_mod_frac'] >= mean1 - 6 * std1) &\n",
    "                  (df1['weighted_norm_mod_frac'] <= mean1 + 6 * std1)]\n",
    "\n",
    "        # Calculate mean and standard deviation for the column 'weighted_norm_mod_frac' in df2\n",
    "        mean2 = df2['weighted_norm_mod_frac'].mean()\n",
    "        std2 = df2['weighted_norm_mod_frac'].std()\n",
    "\n",
    "        # Drop outliers in df2\n",
    "        df2 = df2[(df2['weighted_norm_mod_frac'] >= mean2 - 6 * std2) &\n",
    "                  (df2['weighted_norm_mod_frac'] <= mean2 + 6 * std2)]\"\"\"\n",
    "\n",
    "        def weighted_average(sub_df):\n",
    "            weights = sub_df['Nvalid_cov']\n",
    "            values = sub_df['weighted_norm_mod_frac']\n",
    "            if weights.sum() == 0:\n",
    "                return np.nan\n",
    "            return np.average(values, weights=weights)\n",
    "        # smooth df1 weighted_norm_mod_frac using a rolling average centered, weighted on Nvalid_cov column\n",
    "        df1['weighted_norm_mod_frac_smooth'] = df1.apply(lambda row: weighted_average(df1.loc[row.name - window_size // 2 : row.name + window_size // 2]), axis=1)\n",
    "\n",
    "        df2['weighted_norm_mod_frac_smooth'] = df2.apply(lambda row: weighted_average(df2.loc[row.name - window_size // 2 : row.name + window_size // 2]), axis=1)\n",
    "        diff_data = (df1['weighted_norm_mod_frac_smooth'] - df2['weighted_norm_mod_frac_smooth'])/df1['weighted_norm_mod_frac_smooth']\n",
    "        diff_data_xaxis = df1['rel_start']\n",
    "\n",
    "        # combine diff_data and diff_data_xaxis into a dataframe and display\n",
    "        diff_df = pd.concat([diff_data_xaxis, diff_data], axis=1)\n",
    "        diff_df.columns = ['rel_start', 'diff_data']\n",
    "        # display diff_df between -100 and 100\n",
    "        #display(diff_df[(diff_df['rel_start'] >= -50) & (diff_df['rel_start'] <= 50)])\n",
    "\n",
    "        #smoothed_data = diff_data.rolling(window=window_size, center=True).mean()\n",
    "\n",
    "        label = f\"Diff_{condition1}_{chr_type1}_{type1} - {condition2}_{chr_type2}_{type2}\"\n",
    "        print(label)\n",
    "\n",
    "        fig.add_trace(go.Scatter(\n",
    "            name=label,\n",
    "            x=diff_data_xaxis.values,\n",
    "            y=diff_data.values,\n",
    "            mode='lines',\n",
    "            # make color of lines shades of grey\n",
    "            #line=dict(color='grey', width=2)\n",
    "        ))\n",
    "        # set x axis min and max using bed_window\n",
    "        fig.update_xaxes(range=[-bed_window, bed_window])\n",
    "\n",
    "    fig.show(renderer='plotly_mimetype+notebook')\n",
    "    return fig,label\n",
    "\n",
    "# capture fig and label\n",
    "\n",
    "diff_fig = plot_bedmethyl_diff(1000, plot_df, conditions, 200,\n",
    "                               (8,\"X\",type_selected[0],\"all\",1,\"X\",type_selected[0],\"all\"))\n",
    "                               #(8,\"X\",type_selected[0],\"all\",8,\"X\",type_selected[1],\"all\"),\n",
    "                               #(8,\"X\",type_selected[0],\"all\",1,\"X\",type_selected[0],\"all\"),\n",
    "                               #(8,\"X\",type_selected[1],\"all\",1,\"X\",type_selected[1],\"all\"))\n",
    "                               #(8,\"X\",type_selected[1],1,\"X\",type_selected[1]))\n",
    "                    #(8,\"X\",\"all\",1,\"X\",\"all\"))\n",
    "                    #(8,\"X\",\"strong_rex\",1,\"X\",\"strong_rex\"),\n",
    "                    #(8,\"X\",\"weak_rex\",1,\"X\",\"weak_rex\"))\n",
    "                    #(8,\"X\",\"center_SDC3_chip_albretton\",1,\"X\",\"center_SDC3_chip_albretton\"),\n",
    "                    #(8,\"Autosome\",\"center_SDC3_chip_albretton\",1,\"Autosome\",\"center_SDC3_chip_albretton\"))\n",
    "                    #(1,\"Autosome\",\"center_SDC3_chip_albretton\",1,\"X\",\"center_SDC3_chip_albretton\"),\n",
    "                    #(8,\"Autosome\",\"center_SDC3_chip_albretton\",8,\"X\",\"center_SDC3_chip_albretton\"))\n",
    "                    #(1, \"X\", \"TSS_q4\", 1, \"Autosome\", \"TSS_q4\"),\n",
    "                    #(8, \"X\", \"TSS_q4\", 8, \"Autosome\", \"TSS_q4\"))\n",
    "                    #(8, \"X\", \"TSS_q4\", 1, \"X\", \"TSS_q4\"),\n",
    "                    #(8, \"Autosome\", \"TSS_q4\", 1, \"Autosome\", \"TSS_q4\"))\n",
    "                    #(8, \"X\", \"TSS_q3\", 1, \"X\", \"TSS_q3\"),\n",
    "                    #(8, \"X\", \"TSS_q2\", 1, \"X\", \"TSS_q2\"),\n",
    "                    #(8, \"X\", \"TSS_q1\", 1, \"X\", \"TSS_q1\"))\n",
    "\n",
    "diff_fig[0].write_image(\"images_11_14_23/\"+region_fig[1]+\"sdc2degron_minus_N2_strong_rex_1000_centered.svg\")\n",
    "diff_fig[0].write_image(\"images_11_14_23/\"+region_fig[1]+\"sdc2degron_minus_N2_strong_rex_1000_centered.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c5ea4d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T22:43:13.117965100Z",
     "start_time": "2023-12-12T22:08:12.553507800Z"
    }
   },
   "outputs": [],
   "source": [
    "### Extracting per read modifications\n",
    "out_file_names = [output_stem + \"modkit-extract-\" + each_condition +\"_\"+ str(round(each_thresh,2))+\"_\"+str(each_index)+ \"_\"+str(each_bamfrac)+\"_\"+str(bed_window)+\n",
    "                  # convert the first 3 characters of each element in \"type_selected\" into a single string separated by \"-\"\n",
    "                  \"-\".join([str(x)[0:7] for x in type_selected])+\"_\"+\n",
    "                  # convert first character and the last 3 characters of each element in \"choromosome_selected\" into a single string separated by \"-\"\n",
    "                  \"-\".join([str(x)[0]+str(x)[-3:] for x in chromosome_selected])+\"_\"+\n",
    "                  \".bed\"\n",
    "                  for each_condition,each_thresh,each_index, each_bamfrac in zip(conditions,thresh_list,sample_indices,bam_fracs)]\n",
    "\n",
    "modkit_bed_df = pd.read_csv(modkit_bed_name,sep='\\t',header=None)\n",
    "### Define bed file for modkit\n",
    "\n",
    "# Function to run a single extract command\n",
    "def modkit_extract(args):\n",
    "    each_bam, each_thresh, each_condition, each_index, each_bamfrac,modkit_path, output_stem, modkit_bed_name, bed_window = args\n",
    "\n",
    "    each_output = output_stem + \"modkit-extract-\" + each_condition +\"_\"+ str(round(each_thresh,2))+\"_\"+str(each_index)+ \"_\"+str(each_bamfrac)+\"_\"+str(bed_window)+ \"-\".join([str(x)[0:7] for x in type_selected])+\"_\"+ \"-\".join([str(x)[0]+str(x)[-3:] for x in chromosome_selected])+\"_\"+ \".bed\"\n",
    "\n",
    "    ### NOTE: Name of pileup file is not based on configurations\n",
    "    ### TODO: Name of output file should be based on configs so that we aren't recomputing pileups withidentical conditions.\n",
    "\n",
    "    # If each_output exsits, skip\n",
    "    if os.path.exists(each_output):\n",
    "        print(f\"Skipping: {each_output}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Starting on: {each_bam}\")\n",
    "    command = [\n",
    "        modkit_path,\n",
    "        \"extract\",\n",
    "        \"--threads\",\n",
    "        \"10\",\n",
    "        \"--force\",\n",
    "        \"--mapped\",\n",
    "        \"--ignore\",\n",
    "        \"m\",\n",
    "        \"--include-bed\",\n",
    "        modkit_bed_name,\n",
    "        \"--log-filepath\",\n",
    "        each_output + each_condition + \"_modkit-extract.log\",\n",
    "        each_bam,\n",
    "        each_output\n",
    "    ]\n",
    "    subprocess.run(command, text=True)\n",
    "\n",
    "    # Create a list of arguments for each task\n",
    "task_args = list(zip(\n",
    "    new_bam_files,\n",
    "    thresh_list,\n",
    "    conditions,\n",
    "    sample_indices,\n",
    "    bam_fracs,\n",
    "    [modkit_path]*len(new_bam_files),\n",
    "    [output_stem]*len(new_bam_files),\n",
    "    [modkit_bed_name]*len(new_bam_files),\n",
    "    [bed_window]*len(new_bam_files)\n",
    "))\n",
    "\n",
    "# Execute commands in parallel\n",
    "with Pool() as pool:\n",
    "    pool.map(modkit_extract, task_args)\n",
    "\n",
    "print(\"finished with:\")\n",
    "print(out_file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5bb718",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T09:44:12.242148900Z",
     "start_time": "2023-11-15T09:43:06.791907300Z"
    }
   },
   "outputs": [],
   "source": [
    "### Add bed and condition details to modkit output for plotting\n",
    "# Using DataFrame merging to achieve the task without explicit loops\n",
    "## Looks up the bed_start and bed_end values for each row in bedmethyl_df\n",
    "def add_bed_columns_no_loops(bedmethyl_df, bed_df):\n",
    "\n",
    "    # Initialize an empty DataFrame to store the merged data\n",
    "    merged_data = pd.DataFrame()\n",
    "    filtered_df = pd.DataFrame()\n",
    "\n",
    "    # drop all rows from bedmethly_df where mod_strand != '+' or '-'\n",
    "    bedmethyl_df = bedmethyl_df[(bedmethyl_df['mod_strand'] == '+') | (bedmethyl_df['mod_strand'] == '-')]\n",
    "    # Get all unique chromosome-strand combinations without headers in bedmethyl_df\n",
    "    unique_chrom_strand_combinations = bedmethyl_df[['chrom']].drop_duplicates().values #, 'mod_strand'\n",
    "    for chrom in unique_chrom_strand_combinations: #, strand\n",
    "        #print(\"Starting on:\", chrom, strand)\n",
    "        bed_subset = bed_df[(bed_df['chrom'] == chrom[0])] #& (bed_df['bed_strand'] == strand)]\n",
    "\n",
    "        if bed_subset.empty:\n",
    "            #print(\"No bed entries for {}\".format(chrom[0]))\n",
    "            # then skip this chromosome-strand combination\n",
    "            continue\n",
    "        #display(bed_subset.head(10))\n",
    "        # Subset data for the current chromosome\n",
    "        bedmethyl_subset = bedmethyl_df[(bedmethyl_df['chrom'] == chrom[0])]\n",
    "        #print(\"bedmethyl_subset:\",bedmethyl_subset.head(10))\n",
    "        #display(bedmethyl_subset.head(10))\n",
    "        # Explicitly cast to numeric data type\n",
    "        bedmethyl_subset['ref_position'] = bedmethyl_subset['ref_position'].astype('int64')\n",
    "        bed_subset['bed_start'] = bed_subset['bed_start'].astype('int64')\n",
    "\n",
    "        # Perform the merge for the current subset\n",
    "        merged_subset = pd.merge_asof(\n",
    "            bedmethyl_subset,\n",
    "            bed_subset[['bed_start', 'bed_end', 'bed_strand','chr_type','type']],  # Exclude 'chrom' from right DF\n",
    "            left_on='ref_position',\n",
    "            right_on='bed_start',\n",
    "            direction='nearest'\n",
    "        )\n",
    "\n",
    "        # Append the merged data to the overall result\n",
    "        merged_data = merged_data.append(merged_subset)\n",
    "\n",
    "    #print(\"bedmethyl_df:\")\n",
    "    #display(merged_data.head(10))\n",
    "    # Filter out rows where ref_position is not within bed_start and bed_end\n",
    "    filtered_df = merged_data.loc[\n",
    "        (merged_data['ref_position'] >= merged_data['bed_start']) &\n",
    "        (merged_data['ref_position'] <= merged_data['bed_end'])\n",
    "    ]\n",
    "    # Filter out rows where bed_start is not a number\n",
    "    filtered_df = filtered_df[filtered_df['bed_start'].notna()]\n",
    "    filtered_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Print number rows in filtered_df\n",
    "    print(\"Found {} rows in filtered_df\".format(len(filtered_df)))\n",
    "\n",
    "    #display(filtered_df)\n",
    "    return filtered_df\n",
    "\n",
    "# Initialize comb_readmethyl_df as an empty dataframe\n",
    "comb_readmethyl_df = pd.DataFrame()\n",
    "\n",
    "# Build combined bed df\n",
    "combined_bed_df=pd.DataFrame()\n",
    "for each_bed in new_bed_files:\n",
    "    bed_path = each_bed[:-3] # remove .gz\n",
    "    print(\"Starting on:\",bed_path)\n",
    "    # read bedpath and append to bed_df\n",
    "    combined_bed_df = combined_bed_df.append(pd.read_csv(bed_path, sep=\"\\t\", header=None))#skiprows=1))\n",
    "combined_bed_df.columns = ['chrom','bed_start','bed_end','bed_strand','type','chr_type']\n",
    "\n",
    "combined_bed_df.sort_values(['chrom','bed_start'], inplace=True)\n",
    "combined_bed_df.reset_index(drop=True, inplace=True)\n",
    "display(combined_bed_df.head(10))\n",
    "\n",
    "# Initialize comb_bedmethyl_plot_df\n",
    "comb_bedmethyl_plot_df = pd.DataFrame()\n",
    "\n",
    "# Create combined plotting dataframe\n",
    "for each_output,each_condition in zip(out_file_names,conditions):\n",
    "    print(\"Starting on:\",each_output)\n",
    "    # Define bed methyl columns and import bedmethyl file\n",
    "    bedmethyl_df = pd.DataFrame()\n",
    "    bedmethyl_cols = ['read_id',\n",
    "    'forward_read_position',\n",
    "    'ref_position',\n",
    "    'chrom',\n",
    "    'mod_strand',\n",
    "    'ref_strand',\n",
    "    'ref_mod_strand',\n",
    "    'fw_soft_clipped_start',\n",
    "    'fw_soft_clipped_end',\n",
    "    'read_length',\n",
    "    'mod_qual',\n",
    "    'mod_code',\n",
    "    'base_qual',\n",
    "    'ref_kmer',\n",
    "    'query_kmer',\n",
    "    'canonical_base',\n",
    "    'modified_primary_base',\n",
    "    'inferred']\n",
    "    bedmethyl_df=pd.read_csv(each_output, sep=\"\\t\", header=None, names=bedmethyl_cols,skiprows=1)\n",
    "\n",
    "    # sort bedmethyl_df by chrom and start_position, required for merging in bed matching.\n",
    "    bedmethyl_df.sort_values(['chrom','ref_position'], inplace=True)\n",
    "    bedmethyl_df.reset_index(drop=True, inplace=True)\n",
    "    #display(bedmethyl_df.head(10))\n",
    "\n",
    "    # Adding new columns to bedmethyl_df using condition matching\n",
    "    bedmethyl_df = add_bed_columns_no_loops(bedmethyl_df.copy(), combined_bed_df)\n",
    "    # Add rel_start and rel_end columns equal to start-bed_start and end-bed_start\n",
    "    bedmethyl_df['rel_pos'] = bedmethyl_df['ref_position'] - bedmethyl_df['bed_start'] -bed_window +1\n",
    "\n",
    "    # add condition column\n",
    "    bedmethyl_df['condition'] = each_condition\n",
    "\n",
    "    # if comb_bedmethyl_plot_df is null, set it equal to bedmethyl_plot\n",
    "    if comb_bedmethyl_plot_df.empty:\n",
    "        print(\"comb_bedmethyl_plot_df is empty, setting it equal to bedmethyl_plot...\")\n",
    "        comb_bedmethyl_plot_df = bedmethyl_df.copy()\n",
    "    # else append bedmethyl_plot to comb_bedmethyl_plot_df\n",
    "    else:\n",
    "        print(\"comb_bedmethyl_plot_df is not empty, appending bedmethyl_plot...\")\n",
    "        comb_bedmethyl_plot_df = comb_bedmethyl_plot_df.append(bedmethyl_df)\n",
    "\n",
    "if any(x in type_selected[0] for x in (\"TSS\", \"TES\", \"MEX\")):\n",
    "    print(\"Strand orientation sensitive type selected, multiplying rel_start by -1 for '-' strand genes...\")\n",
    "    mask = comb_bedmethyl_plot_df['bed_strand'] == '-'\n",
    "    comb_bedmethyl_plot_df.loc[mask, 'rel_pos'] *= -1\n",
    "    #comb_bedmethyl_plot_df.sort_values(['chrom','rel_pos'], inplace=True)\n",
    "\n",
    "# Set entire mod_qual column equal to 1 base on m6A thresh\n",
    "threshold = m6A_thresh / 255\n",
    "# convert comb_bedmethly_plot_df_final['mod_qual'] to a float\n",
    "comb_bedmethyl_plot_df['mod_qual'] = comb_bedmethyl_plot_df['mod_qual'].astype(float)\n",
    "comb_bedmethyl_plot_df['mod_qual_bin'] = np.where(comb_bedmethyl_plot_df['mod_qual'] > threshold, 1, 0)\n",
    "\n",
    "#add read_start and read_end columns to comb_bedmethyl_plot_df_final based on min and max ref_position for each read_id\n",
    "print(\"Calculating read start and end...\")\n",
    "# Reduce the DataFrame size by selecting only the columns you need\n",
    "small_df = comb_bedmethyl_plot_df[['read_id', 'rel_pos']]\n",
    "# Perform the grouping and aggregation in one step\n",
    "grouped_df = small_df.groupby('read_id')['rel_pos'].agg(['min', 'max']).reset_index()\n",
    "# Rename the columns\n",
    "grouped_df.rename(columns={'min': 'rel_read_start', 'max': 'rel_read_end'}, inplace=True)\n",
    "# Merge the aggregated results back to the original DataFrame\n",
    "comb_bedmethyl_plot_df = pd.merge(comb_bedmethyl_plot_df, grouped_df, on='read_id', how='left')\n",
    "# delete small_df and grouped_df\n",
    "del small_df\n",
    "del grouped_df\n",
    "\n",
    "#comb_bedmethyl_plot_df.sort_values(['chrom','rel_pos'], inplace=True)\n",
    "comb_bedmethyl_plot_df = comb_bedmethyl_plot_df[comb_bedmethyl_plot_df['bed_start'].notna()]\n",
    "comb_bedmethyl_plot_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# print count of unique read_ids for each condition\n",
    "print(\"All unique read_ids for each condition:\")\n",
    "print(comb_bedmethyl_plot_df.groupby(['condition'])['read_id'].nunique())\n",
    "\n",
    "#display(comb_bedmethyl_plot_df.head(10))\n",
    "# drop all rows where read_length is less than 1000\n",
    "comb_bedmethyl_plot_df = comb_bedmethyl_plot_df[comb_bedmethyl_plot_df['read_length'] >= 1000]\n",
    "\n",
    "\n",
    "# print count of unique read_ids for each condition\n",
    "print(\"All unique read_ids for each condition > read_len of 1000:\")\n",
    "print(comb_bedmethyl_plot_df.groupby(['condition'])['read_id'].nunique())\n",
    "\n",
    "# Define SQLite database file name\n",
    "#db_fn = \"temp_files/\" + \"plot_db_\" + \"-\".join([str(x)[:6] for x in type_selected]) + \"_\" + str(round(thresh_list[0],2)) + \"_\" + str(bam_fracs[0]) + \".db\"\n",
    "# Create a SQLite database connection\n",
    "#conn = sqlite3.connect(db_fn)\n",
    "# Save the DataFrame to SQLite database\n",
    "#comb_bedmethyl_plot_df.to_sql('bedmethyl_plot', conn, if_exists='replace', index=False)\n",
    "# Close the connection\n",
    "#conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ea8de8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T18:50:59.287581700Z",
     "start_time": "2023-11-15T18:50:29.099710100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Similar to other dataframes, define fn based on configurations. If it doesn't exist, create it, otherwise import it.\n",
    "plot_df_fn = \"temp_files/\" + \"plot_df_\" + \"-\".join([str(x)[0:12] for x in type_selected])+\"_\"+ str(thresh_list[0])+\"_\"+str(bam_fracs[0])+str(bed_window)+\".csv\"\n",
    "#comb_bedmethyl_plot_df.to_csv(plot_df_fn, index=False)\n",
    "if os.path.exists(plot_df_fn):\n",
    "    print(\"plot_df_fn exists, importing...\")\n",
    "    plot_df = pd.read_csv(plot_df_fn)\n",
    "else:\n",
    "    print(\"plot_df_fn does not exist, creating...\")\n",
    "    comb_bedmethyl_plot_df.to_csv(plot_df_fn, index=False)\n",
    "    plot_df = comb_bedmethyl_plot_df.copy()\n",
    "\n",
    "metadata_cols = ['chrom', 'chr_type', 'condition', 'bed_start','type', 'read_id', 'rel_read_start','rel_read_end']\n",
    "\n",
    "nanotools.display_sample_rows(plot_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e95f46b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-15T20:20:45.335363500Z"
    },
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "### PROCESSING AUTOCORRELATON\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from scipy.interpolate import interp1d\n",
    "metadata_cols = ['chrom', 'chr_type', 'condition', 'bed_start','type', 'read_id', 'rel_read_start','rel_read_end']\n",
    "#version that requires a nuc and linker region (does not allow nucleosomes to be stuck next to eachother)\n",
    "def process_raw_read(read_id, group, metadata_cols,gauss_std_dev, crr_length,corr_start):\n",
    "    global m6A_thresh\n",
    "    m6A_thresh_local = m6A_thresh - 0.2*m6A_thresh\n",
    "    # Calculate the total number of modified bases, total bases, and minimum base position in the read\n",
    "    BASE_NUM = max(group['rel_pos']) - min(group['rel_pos']) + 1\n",
    "    BASE_MIN = min(group['rel_pos'])\n",
    "\n",
    "    if np.mean(group['mod_qual']) > m6A_thresh_local/255:\n",
    "        return read_id, group.iloc[0][metadata_cols], np.zeros(1), None\n",
    "\n",
    "    # Initialize the calling_vec with -1 and populate it with mod_qual values based on relative position\n",
    "    calling_vec_raw = np.full(BASE_NUM+1, np.nan)\n",
    "    for i in range(len(group['rel_pos'])):\n",
    "        calling_vec_raw[group.iloc[i]['rel_pos'] - BASE_MIN] = group.iloc[i]['mod_qual']\n",
    "\n",
    "    #print(\"read_mean: \", read_mean)\n",
    "    # Impute -1 values with mean\n",
    "    #calling_vec[calling_vec == -1] = 0#read_mean\n",
    "    #print(\"calling_vec RAW: \", calling_vec[0:100])\n",
    "    # Interpolate to fill NaN values\n",
    "    not_nan = ~np.isnan(calling_vec_raw)\n",
    "    indices = np.arange(len(calling_vec_raw))\n",
    "\n",
    "    # set calling vec as a copy of calling_vec_raw\n",
    "    calling_vec = calling_vec_raw.copy()\n",
    "\n",
    "    # set all values in calling_vec > m6A_thresh to 1\n",
    "    calling_vec[calling_vec > m6A_thresh_local/255] = 1\n",
    "    # set all values in calling_vec <= m6A_thresh to 0\n",
    "    calling_vec[calling_vec <= m6A_thresh_local/255] = 0\n",
    "\n",
    "    interp_func = interp1d(indices[not_nan], calling_vec[not_nan], bounds_error=False, copy=False, fill_value=\"extrapolate\", kind='nearest')\n",
    "    calling_vec_filled = interp_func(indices)\n",
    "    #print(\"calling_vec INTERP: \", calling_vec_filled[0:100])\n",
    "\n",
    "    # set all values in calling_vec > m6A_thresh to 1\n",
    "    #calling_vec_filled[calling_vec_filled > m6A_thresh/255] = 1\n",
    "    # set all values in calling_vec <= m6A_thresh to 0\n",
    "    #calling_vec_filled[calling_vec_filled <= m6A_thresh/255] = 0\n",
    "    # Ensure no NaNs remain after smoothing\n",
    "    if np.isnan(calling_vec_filled).any():\n",
    "        # Handle remaining NaNs after smoothing if they exist\n",
    "        calling_vec_filled = np.nan_to_num(calling_vec_filled, nan=0.0)\n",
    "    #print(\"calling_vec THRESH: \", calling_vec_filled[0:100])\n",
    "    # Apply gaussian smoothing\n",
    "    calling_vec_smoothed = gaussian_filter(calling_vec_filled, sigma=gauss_std_dev)\n",
    "    #print(\"calling_vec GAUSS: \", calling_vec_smoothed[0:100])\n",
    "    read_mean = np.mean(calling_vec_smoothed[calling_vec != -1])\n",
    "    read_std = np.std(calling_vec_smoothed[calling_vec != -1])\n",
    "    if read_std == 0 or read_mean == 0:\n",
    "        return read_id, group.iloc[0][metadata_cols], np.zeros(1), None\n",
    "\n",
    "    # Calculate 1D autocorrelation\n",
    "    autocorr = np.correlate(calling_vec_smoothed-read_mean, calling_vec_smoothed-read_mean, mode='same')/(read_std * read_std * len(calling_vec_smoothed))\n",
    "    autocorr_centered = autocorr[autocorr.size // 2:]  # Taking one side as it's symmetric\n",
    "    #autocorr_normalized = autocorr_centered / autocorr_centered[0]\n",
    "\n",
    "    # Limit the autocorrelation calculation to a lag of crr_length\n",
    "\n",
    "    if len(autocorr_centered) < (crr_length+1): # np.isnan(autocorr_limited).any() or np.isinf(autocorr_limited).any() or np.max(autocorr_limited) > np.finfo(np.float32).max:\n",
    "        return read_id, group.iloc[0][metadata_cols], np.zeros(1), None\n",
    "\n",
    "    else:\n",
    "        autocorr_limited = autocorr_centered[corr_start:crr_length + 1]  # Include lag 0 to 500\n",
    "        # if autocorr_limited contains NaN, infinity or a value too large for dtype('float32'). return 0\n",
    "        # output scatter plot of first 500 values of calling_vec,calling_vec_filled, and calling_vec_smoothed\n",
    "        # Instead of creating a single figure, create a subplot figure\n",
    "        read_fig = make_subplots(rows=2, cols=1, subplot_titles=('Methylation', 'Autocorrelation'), shared_xaxes=False)# Set subfigure distribution:\n",
    "\n",
    "        colors_scheme = plotly.colors.qualitative.Prism\n",
    "        # define dictionary with one color for each of \"Raw\", \"Extrapolated\", \"Smoothed\" and \"Autocorrelation\"\n",
    "        colors_dict = dict(zip([\"Raw\", \"Extrapolated\", \"Smoothed\", \"Autocorrelation\"], colors_scheme))\n",
    "        # Add the methylation plot to the first subplot\n",
    "        read_fig.add_trace(go.Scatter(\n",
    "            x=np.arange(1000),\n",
    "            y=calling_vec_raw[100:1100],\n",
    "            mode='lines',\n",
    "            name=\"Raw\",\n",
    "            marker=dict(color=colors_dict[\"Raw\"])\n",
    "        ), row=1, col=1)\n",
    "        read_fig.add_trace(go.Scatter(\n",
    "            x=np.arange(1000),\n",
    "            y=calling_vec_filled[100:1100],\n",
    "            mode='lines',\n",
    "            name=\"Extrapolated\",\n",
    "            marker=dict(color=colors_dict[\"Extrapolated\"])\n",
    "        ), row=1, col=1)\n",
    "        read_fig.add_trace(go.Scatter(\n",
    "            x=np.arange(1000),\n",
    "            y=calling_vec_smoothed[100:1100],\n",
    "            mode='lines',\n",
    "            name=\"Smoothed\",\n",
    "            marker=dict(color=colors_dict[\"Smoothed\"])\n",
    "        ), row=1, col=1)\n",
    "\n",
    "        # Add the autocorrelation scatter plot to the second subplot\n",
    "        read_fig.add_trace(go.Scatter(\n",
    "            x=np.arange(1000),\n",
    "            y=autocorr_limited[100:1100],\n",
    "            mode='markers',\n",
    "            name=\"Autocorrelation\",\n",
    "            # reduce marker size\n",
    "            marker=dict(size=2.5,color=colors_dict[\"Autocorrelation\"])\n",
    "        ), row=2, col=1)\n",
    "\n",
    "        # Update the layout of the subplot figure\n",
    "        read_fig.update_layout(\n",
    "            title='Read Analysis',\n",
    "            width=800,\n",
    "            height=400,\n",
    "            template='plotly_white'\n",
    "        )\n",
    "        # Update xaxis and yaxis properties if needed\n",
    "        read_fig.update_xaxes(title_text='Genomic Position', row=1, col=1)\n",
    "        read_fig.update_yaxes(title_text='Mod Probability', row=1, col=1)\n",
    "        read_fig.update_xaxes(title_text='Lag (bp)', row=1, col=2)\n",
    "        read_fig.update_yaxes(title_text='Autocorrelation Value', row=2, col=1)\n",
    "        return read_id, group.iloc[0][metadata_cols], autocorr_limited, read_fig\n",
    "\n",
    "### CONFIGS\n",
    "#grouped = grouped_subset.groupby('read_id')\n",
    "crr_length = 2000\n",
    "gauss_std = 10\n",
    "corr_start = 100\n",
    "# show this many single read tracks:\n",
    "figures_shown = 0\n",
    "# Process this many reads\n",
    "reads_to_process = 0\n",
    "corr_buff = 500\n",
    "\n",
    "print(\"Grouping df...\")\n",
    "\n",
    "\n",
    "# grouped_auto = plot_df where (rel_read_end - rel_read_start) >= crr_length + 100\n",
    "grouped_auto = plot_df[plot_df['chr_type'] == 'X'].copy()\n",
    "#grouped_auto = grouped_auto[grouped_auto['condition'] == 'N2_fiber']\n",
    "grouped_auto = grouped_auto[plot_df['type'].str.contains('rex')]\n",
    "grouped_auto = grouped_auto[(grouped_auto['rel_read_end'] - grouped_auto['rel_read_start']) >= (2*crr_length + corr_buff)]\n",
    "\n",
    "grouped_auto.sort_values(by=[\"read_id\",\"rel_pos\"], inplace=True)\n",
    "grouped_auto.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# drop rows where read_id not in first 5 read_ids\n",
    "if reads_to_process > 0:\n",
    "    first_rows = grouped_auto['read_id'].unique()[:reads_to_process]#[plot_df['type'] == 'intergenic_control']\n",
    "    grouped_auto = grouped_auto[grouped_auto['read_id'].isin(first_rows)]\n",
    "    # reset index\n",
    "    grouped_auto.reset_index(inplace=True, drop=True)\n",
    "\n",
    "grouped_auto = grouped_auto.groupby('read_id')\n",
    "#display(grouped_auto.head(10))\n",
    "#nanotools.display_sample_rows(grouped_auto)\n",
    "\n",
    "print(\"Processing autocorrelations...\")\n",
    "grouped_data_with_constants = [(read_id,group,metadata_cols,gauss_std,crr_length,corr_start) for read_id,group in grouped_auto]\n",
    "\n",
    "#processes=multiprocessing.cpu_count()\n",
    "with multiprocessing.Pool() as pool:\n",
    "    # set results equal to pool.starmap() with the function and grouped_data_with_constants as arguments using tqdm to track progress\n",
    "    results = pool.starmap(process_raw_read, tqdm.tqdm(grouped_data_with_constants, total=len(grouped_data_with_constants)))\n",
    "\n",
    "# Clear grouped_auto dataframe\n",
    "grouped_auto = None\n",
    "\n",
    "# Extracting autocorrelations and their corresponding metadata\n",
    "grouped_autocorrelations = {}\n",
    "# Initialize lists to hold the filtered conditions, clusters, and chr_types\n",
    "conditions_list = []\n",
    "chr_types_list = []\n",
    "types_list = []\n",
    "\n",
    "for read_id, metadata, autocorr, read_fig in results:\n",
    "    #if autocorrs has nan values skip\n",
    "    if len(autocorr) >1:\n",
    "        # Create a unique key for each combination of type, chr_type, and condition\n",
    "        key = (metadata['type'], metadata['chr_type'], metadata['condition'])\n",
    "        if key not in grouped_autocorrelations:\n",
    "            grouped_autocorrelations[key] = []\n",
    "        grouped_autocorrelations[key].append(autocorr)\n",
    "\n",
    "        conditions_list.append(metadata['condition'])\n",
    "        chr_types_list.append(metadata['chr_type'])\n",
    "        types_list.append(metadata['type'])\n",
    "\n",
    "# Create a heatmap for each group\n",
    "fig = go.Figure()\n",
    "\n",
    "y_labels = []  # To store y-axis labels\n",
    "z_data = []  # To store autocorrelation data for heatmap\n",
    "\n",
    "for group_key, autocorrs in grouped_autocorrelations.items():\n",
    "    group_label = f\"{group_key[0]}, {group_key[1]}, {group_key[2]}\"\n",
    "    for i, autocorr in enumerate(autocorrs, start=1):\n",
    "        read_label = f\"{group_label} - Read {i}\"\n",
    "        y_labels.append(read_label)\n",
    "        z_data.append(autocorr)\n",
    "\n",
    "from scipy.signal import find_peaks, peak_prominences, peak_widths\n",
    "# Extracting autocorrelations and their corresponding metadata for clustering, only if results[2] does not contain any nan values\n",
    "autocorrelation_data = [result[2] for result in results if len(result[2])>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9980ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T22:41:38.487732400Z",
     "start_time": "2023-11-16T22:41:15.758569700Z"
    }
   },
   "outputs": [],
   "source": [
    "### PLOTTING AUTOCORRELATIONS\n",
    "leiden_res = 0.5\n",
    "\n",
    "def extract_peak_features(autocorr, num_peaks=4):\n",
    "    # Find peaks\n",
    "    peaks, _ = find_peaks(autocorr)\n",
    "\n",
    "    # Initialize a fixed-length array filled with placeholders\n",
    "    features = np.full(num_peaks * 4, -1.0) # 4 features per peak\n",
    "\n",
    "    # If there are peaks, extract their features\n",
    "    if len(peaks) > 0:\n",
    "        # Sort peaks by height and select the top ones\n",
    "        sorted_peaks = sorted(peaks, key=lambda x: autocorr[x], reverse=True)[:num_peaks]\n",
    "\n",
    "        # if any two peaks are < 50 apart then remove the one with the lower height\n",
    "        if len(sorted_peaks) > 1:\n",
    "            for i in range(len(sorted_peaks)-1):\n",
    "                if sorted_peaks[i+1] - sorted_peaks[i] < 50:\n",
    "                    if autocorr[sorted_peaks[i+1]] > autocorr[sorted_peaks[i]]:\n",
    "                        sorted_peaks[i] = -1\n",
    "                    else:\n",
    "                        sorted_peaks[i+1] = -1\n",
    "            sorted_peaks = [x for x in sorted_peaks if x != -1]\n",
    "\n",
    "\n",
    "        # Extract peak heights\n",
    "        peak_heights = autocorr[sorted_peaks]\n",
    "\n",
    "        # Extract peak prominences\n",
    "        prominences = peak_prominences(autocorr, sorted_peaks)[0]\n",
    "\n",
    "        # Extract peak widths\n",
    "        widths = peak_widths(autocorr, sorted_peaks)[0]\n",
    "\n",
    "        # Fill the features array with actual values\n",
    "        for i, peak in enumerate(sorted_peaks):\n",
    "            features[i * 4] = peak                    # Peak position\n",
    "            features[i * 4 + 1] = peak_heights[i]     # Height\n",
    "            features[i * 4 + 2] = prominences[i]      # Prominence\n",
    "            features[i * 4 + 3] = widths[i]           # Width\n",
    "\n",
    "    return features\n",
    "\n",
    "# Apply the function to all autocorrelograms\n",
    "expanded_peak_features = np.array([extract_peak_features(autocorr) for autocorr in autocorrelation_data])\n",
    "# Check the shape of the expanded_peak_features array\n",
    "import scanpy as sc\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "# Convert autocorrelation data to a DataFrame for ease of handling\n",
    "autocorr_df = pd.DataFrame(autocorrelation_data)\n",
    "# Compute the distance matrix on expanded peak features\n",
    "#distance_matrix_expanded = squareform(pdist(expanded_peak_features, metric='euclidean'))\n",
    "\n",
    "# Compute the distance matrix (Euclidean distance is used here, modify if needed)\n",
    "distance_matrix_expanded = squareform(pdist(autocorr_df, metric='correlation'))\n",
    "\n",
    "# Convert the distance matrix to a similarity matrix for expanded peak features\n",
    "similarity_matrix_expanded = 1 / (1 + distance_matrix_expanded)\n",
    "\n",
    "# Create an AnnData object with the expanded peak features similarity matrix\n",
    "adata_expanded = sc.AnnData(similarity_matrix_expanded)\n",
    "adata_expanded.obs_names = [f'Read_{i}' for i in range(adata_expanded.shape[0])]\n",
    "adata_expanded.var_names = adata_expanded.obs_names\n",
    "\n",
    "# Computing the neighborhood graph on the expanded peak features\n",
    "sc.pp.neighbors(adata_expanded, use_rep='X', metric='correlation')\n",
    "\n",
    "### DEFINE CLUSTERING RESOLUTION HERE: ###\n",
    "# Applying Leiden clustering on the expanded peak features\n",
    "sc.tl.leiden(adata_expanded, resolution=leiden_res)\n",
    "\n",
    "# Determine the cluster for each read\n",
    "# Reset the index of cluster_labels to align with autocorr_df\n",
    "cluster_labels = adata_expanded.obs['leiden'].astype(int).reset_index(drop=True)\n",
    "\n",
    "# Ensure that the lengths match\n",
    "if len(cluster_labels) != len(autocorr_df):\n",
    "    raise ValueError(\"Mismatch in length between autocorrelation data and cluster labels\")\n",
    "\n",
    "# Add all cluster labels to the DataFrame\n",
    "autocorr_df['cluster'] = cluster_labels\n",
    "autocorr_df_plotting = autocorr_df.copy()\n",
    "autocorr_df_plotting['read_id'] = [result[0] for result in results if len(result[2])>1]\n",
    "\n",
    "# Count the number of reads in each cluster\n",
    "cluster_counts = cluster_labels.value_counts()\n",
    "print(\"cluster_counts = \", cluster_counts)\n",
    "\n",
    "# Total number of reads\n",
    "total_reads = len(cluster_labels)\n",
    "\n",
    "# Calculate the threshold for 5% of the total dataset\n",
    "threshold_count = 0.05 * total_reads\n",
    "\n",
    "# Filter clusters that are less than 5% of total\n",
    "significant_clusters = cluster_counts[cluster_counts >= threshold_count].index\n",
    "\n",
    "print(\"Significant clusters:\", significant_clusters)\n",
    "\n",
    "# Filter autocorrelations based on significant clusters\n",
    "filtered_df = autocorr_df[autocorr_df['cluster'].isin(significant_clusters)]\n",
    "filtered_df_plotting = autocorr_df_plotting[autocorr_df_plotting['cluster'].isin(significant_clusters)]\n",
    "\n",
    "# Group the filtered autocorrelation data by cluster\n",
    "grouped_by_cluster = filtered_df.groupby('cluster')\n",
    "\n",
    "# Calculate the representative autocorrelogram for each cluster\n",
    "representative_autocorrs = grouped_by_cluster.mean()\n",
    "\n",
    "### Plot unique read plots:\n",
    "# Extract the unique cluster names\n",
    "unique_clusters = filtered_df_plotting['cluster'].unique()\n",
    "\n",
    "# representative_reads_tuple = autocorr_df_plotting.groupby('cluster')['read_id'].nth(1)\n",
    "# Plot one figure from one read for each unique cluster_name\n",
    "#for cluster, rep_read_id in representative_reads_tuple.iteritems():\n",
    "# choose 10 reads from cluster == 5 as representative_reads\n",
    "representative_reads_list = autocorr_df_plotting[autocorr_df_plotting['cluster'] == 3]['read_id'].unique()[25:35]\n",
    "for rep_read_id in representative_reads_list:\n",
    "    # Loop through the results to find the figure for the representative read\n",
    "    for result in results:\n",
    "        if result[0] == rep_read_id:\n",
    "            print(\"read_id\", rep_read_id)\n",
    "            # Display the figure for the representative read of this cluster\n",
    "            disp_fig = result[3]\n",
    "            # set x range for disp fig to 0-600\n",
    "            disp_fig.update_xaxes(range=[0, 1000])\n",
    "            # set width to 500\n",
    "            disp_fig.update_layout(width=800, height = 500)\n",
    "            #disp_fig.show()\n",
    "            #if rep_read_id == \"0b9e1dda-a4ae-4841-869b-533dd829136a\":\n",
    "                # save png and svg to images_11_14_23/raw_read_extrapolation_smoothing_strong_rex\n",
    "                #disp_fig.write_image(\"images_11_14_23/raw_read_extrap_smooth_strong_rex\" + rep_read_id[0:6] + \".png\")\n",
    "                #disp_fig.write_image(\"images_11_14_23/raw_read_extrap_smooth_strong_rex\" + rep_read_id[0:6] + \".svg\")\n",
    "            break\n",
    "\n",
    "\"\"\"def determine_cluster_name2(autocorr, cluster_id, prominence_threshold=0.1):\n",
    "    # Find peaks and their prominences\n",
    "    peaks, _ = find_peaks(autocorr, prominence=prominence_threshold,width=25)\n",
    "    prominences = peak_prominences(autocorr, peaks)[0]\n",
    "    print(\"peaks = \", peaks)\n",
    "    print(\"prominences = \", prominences)\n",
    "\n",
    "    # Check if there are any prominent peaks\n",
    "    if len(peaks) > 0 and any(prominences >= prominence_threshold):\n",
    "        # Find the position of the first prominent peak\n",
    "        first_prominent_peak = peaks[np.argmax(prominences >= prominence_threshold)]\n",
    "        return f'NRL-{first_prominent_peak + 100}-C{cluster_id}'\n",
    "    else:\n",
    "        # For clusters without prominent peaks, use 'NP' followed by the cluster id\n",
    "        return f'NP{cluster_id}'\"\"\"\n",
    "\n",
    "def determine_cluster_name(autocorr, cluster_id, prominence_threshold=0.06, distance_threshold=50):\n",
    "    global corr_start\n",
    "    # Find peaks in the autocorrelation signal\n",
    "    peaks, properties = find_peaks(autocorr, prominence=prominence_threshold, width=25)\n",
    "    prominences = properties[\"prominences\"]\n",
    "\n",
    "    # Filter peaks based on prominence to ensure they are significant\n",
    "    significant_peaks = peaks[prominences >= prominence_threshold]\n",
    "    print(\"peaks = \", peaks)\n",
    "    print(\"prominences = \", prominences)\n",
    "    print(\"significant_peaks = \", significant_peaks)\n",
    "\n",
    "    # If there are enough significant peaks, calculate peak-to-peak distances including the first peak\n",
    "    if len(significant_peaks) > 1:\n",
    "        # if any two peaks are < 50 apart then remove the one with the lower height\n",
    "        for i in range(len(significant_peaks)-1):\n",
    "            if significant_peaks[i+1] - significant_peaks[i] < distance_threshold:\n",
    "                if autocorr[significant_peaks[i+1]] > autocorr[significant_peaks[i]]:\n",
    "                    significant_peaks[i] = -1\n",
    "                else:\n",
    "                    significant_peaks[i+1] = -1\n",
    "        significant_peaks = [x for x in significant_peaks if x != -1]\n",
    "\n",
    "        # Calculate distances between consecutive significant peaks\n",
    "        peak_distances = np.diff(significant_peaks)\n",
    "\n",
    "        # Include the distance from start (0 + 100) to the first significant peak\n",
    "        #peak_distances_with_first = np.insert(peak_distances, 0, significant_peaks[0] + corr_start)\n",
    "        print(\"peak_distances_with_first = \", peak_distances)\n",
    "        if len(peak_distances) > 0:\n",
    "            average_distance = round(np.mean(peak_distances))\n",
    "            return f'NRL-{average_distance:.1f}-C{cluster_id}'\n",
    "        # Calculate the average distance if we have enough valid peak distances\n",
    "        \"\"\"if len(significant_peaks) == 1:\n",
    "            return f'NRL-{significant_peaks[0]:.1f}-C{cluster_id}'\"\"\"\n",
    "    else:\n",
    "        # If not enough valid peak distances, consider it as no pattern found\n",
    "        return f'NP{cluster_id}'\n",
    "\n",
    "# Apply the function to each cluster's average autocorrelogram to determine names\n",
    "cluster_name_mapping = {cluster_id: determine_cluster_name(autocorr, cluster_id)\n",
    "                        for cluster_id, autocorr in representative_autocorrs.iterrows()}\n",
    "\n",
    "\n",
    "\n",
    "# Update the index of representative_autocorrs with new cluster names\n",
    "representative_autocorrs.index = representative_autocorrs.index.map(cluster_name_mapping)\n",
    "\n",
    "# Sort representative_autocorrs by index (cluster names)\n",
    "#representative_autocorrs_sorted = representative_autocorrs.sort_index(ascending=False)\n",
    "# Calculate the range for each row\n",
    "representative_autocorrs['range'] = representative_autocorrs.apply(lambda row: row.iloc[220:].max() - row.iloc[220:].min(), axis=1)\n",
    "\n",
    "# Sort the DataFrame based on the range\n",
    "representative_autocorrs_sorted = representative_autocorrs.sort_values(by='range', ascending=True)\n",
    "\n",
    "# Drop the 'range' column if you want to revert back to the original columns\n",
    "representative_autocorrs_sorted.drop(columns=['range'], inplace=True)\n",
    "\n",
    "# Prepare data for the heatmap\n",
    "heatmap_data = representative_autocorrs_sorted.values\n",
    "heatmap_labels = representative_autocorrs_sorted.index.to_list()\n",
    "\n",
    "# Create the heatmap\n",
    "fig_heatmap = go.Figure(go.Heatmap(\n",
    "    z=heatmap_data,\n",
    "    x=list(range(len(heatmap_data[0]))),\n",
    "    y=heatmap_labels,\n",
    "    colorscale='Inferno'\n",
    "))\n",
    "fig_heatmap.update_layout(\n",
    "    title='Heatmap of Representative Autocorrelograms for Each Cluster',\n",
    "    xaxis_title='Lag',\n",
    "    yaxis_title='Cluster',\n",
    "    yaxis={'type': 'category'},\n",
    "    width=800,\n",
    "    height=600,\n",
    "    template='plotly_white'\n",
    ")\n",
    "fig_heatmap.update_xaxes(tickmode='array', tickvals=list(range(0, crr_length-corr_start, 100)), ticktext=list(range(corr_start, crr_length+1, 100)))\n",
    "\n",
    "\n",
    "#display(autocorr_df.head(10))\n",
    "\n",
    "### PRINT PER-READ HEATMAP\n",
    "print(\"Number of rows after filtering:\", len(filtered_df))\n",
    "# Sort the DataFrame based on the cluster labels\n",
    "sorted_df = filtered_df.sort_values(by='cluster')\n",
    "# Step 1: Map the index of representative_autocorrs_sorted to full cluster names\n",
    "unique_clusters = representative_autocorrs_sorted.index.map(cluster_name_mapping).unique()\n",
    "\n",
    "\n",
    "# Step 2: Create a dictionary for sorting order based on full cluster names\n",
    "sort_order = {name: i for i, name in enumerate(unique_clusters)}\n",
    "\n",
    "# Step 3: Map the 'cluster' column to the full cluster names using cluster_name_mapping\n",
    "sorted_df['full_cluster_name'] = sorted_df['cluster'].map(cluster_name_mapping)\n",
    "\n",
    "# Step 4: Sort 'sorted_df' based on the order in 'representative_autocorrs_sorted'\n",
    "sorted_df['sort_order'] = sorted_df['full_cluster_name'].map(sort_order)\n",
    "sorted_df = sorted_df.sort_values(by='sort_order')\n",
    "\n",
    "# Optional: Remove the 'sort_order' column if it's no longer needed\n",
    "sorted_df.drop(columns=['sort_order'], inplace=True)\n",
    "\n",
    "# Check if we have more than 500 rows\n",
    "if sorted_df.shape[0] > 500:\n",
    "    # Randomly sample 500 rows from sorted_df\n",
    "    sampled_df = sorted_df.sample(n=500, random_state=np.random.RandomState())\n",
    "    sampled_df = sampled_df.sort_values(by='full_cluster_name')\n",
    "    # reset index\n",
    "    sampled_df.reset_index(inplace=True, drop=True)\n",
    "else:\n",
    "    # If we have 500 rows or less, just use the entire dataframe\n",
    "    sampled_df = sorted_df\n",
    "\n",
    "# Use the 'read_id' as the y-axis labels for the sampled dataframe\n",
    "y_labels_sampled = [f'{row[\"full_cluster_name\"]}_{row.name}' for index, row in sampled_df.iterrows()]\n",
    "\n",
    "# Extract the autocorrelation data for the sampled dataframe\n",
    "z_data_sampled = sampled_df.drop(['cluster', 'full_cluster_name'], axis=1).values\n",
    "\n",
    "# Create the heatmap with the sampled data\n",
    "read_fig = go.Figure(go.Heatmap(\n",
    "    z=z_data_sampled,\n",
    "    x=list(range(z_data_sampled.shape[1])),  # Use the number of columns for the x-axis\n",
    "    y=y_labels_sampled,\n",
    "    colorscale='Inferno'\n",
    "))\n",
    "\n",
    "read_fig.update_layout(\n",
    "    title='Random Per-Read Autocorrelation Heatmaps Grouped by Full Cluster Name',\n",
    "    xaxis_title='Lag',\n",
    "    yaxis_title='Read ID with Cluster Name',\n",
    "    yaxis={'type': 'category'},\n",
    "    width=800,\n",
    "    height=800,\n",
    "    template='plotly_white',\n",
    ")\n",
    "# Relabel x-axis labels to start from 100\n",
    "read_fig.update_xaxes(tickmode='array', tickvals=list(range(0, z_data_sampled.shape[1], 100)), ticktext=list(range(corr_start, z_data_sampled.shape[1]+corr_start, 100)))\n",
    "\n",
    "\n",
    "# Assign colors from 'prism' scheme to each cluster_name\n",
    "colors_scheme = plotly.colors.qualitative.Prism\n",
    "cluster_colors = {cluster_name: colors_scheme[i % len(colors_scheme)] for i, cluster_name in enumerate(representative_autocorrs_sorted.index)}\n",
    "\n",
    "# Create scatter plots for each cluster with updated cluster names\n",
    "fig_scatter = go.Figure()\n",
    "\n",
    "for cluster_name, autocorr in representative_autocorrs_sorted.iterrows():\n",
    "    fig_scatter.add_trace(go.Scatter(\n",
    "        x=list(range(len(autocorr))),\n",
    "        y=autocorr,\n",
    "        mode='markers+lines',\n",
    "        # reduce line width\n",
    "        line=dict(width=3, color=cluster_colors[cluster_name]),\n",
    "        # reduce marker size\n",
    "        marker=dict(size=3, color=cluster_colors[cluster_name]),\n",
    "        name=cluster_name,  # Updated cluster name\n",
    "    ))\n",
    "\n",
    "fig_scatter.update_layout(\n",
    "    title='Scatter Plots of Representative Autocorrelograms for Each Cluster',\n",
    "    xaxis_title='Lag',\n",
    "    yaxis_title='Autocorrelation',\n",
    "    width=800,\n",
    "    height=600,\n",
    "    template='plotly_white'\n",
    ")\n",
    "fig_scatter.update_xaxes(tickmode='array', tickvals=list(range(0, crr_length-corr_start, 100)), ticktext=list(range(corr_start, crr_length+1, 100)))\n",
    "\n",
    "# Replace this with your actual DataFrame containing 'condition' and 'cluster' information\n",
    "# For example, if you have a DataFrame 'metadata_df' with these columns, use that\n",
    "# Extract conditions and clusters\n",
    "\"\"\"conditions = [metadata['condition'] for _, metadata, _, _ in results\n",
    "chr_types = [metadata['chr_type'] for _, metadata, _ , _ in results]\"\"\"\n",
    "clusters = adata_expanded.obs['leiden'].astype(int).tolist()\n",
    "\n",
    "\"\"\"for read_id, metadata, autocorr, read_fig in results:\n",
    "    #if autocorrs has nan values skip\n",
    "    if len(autocorr) < 100:\n",
    "        continue\"\"\"\n",
    "\n",
    "# Create DataFrame for plotting\n",
    "metadata_df = pd.DataFrame({\n",
    "    'condition': conditions_list,\n",
    "    'chr_type': chr_types_list,\n",
    "    'type': types_list,\n",
    "    'cluster': clusters\n",
    "})\n",
    "\n",
    "# Update the cluster names in metadata_df with new cluster names\n",
    "metadata_df['cluster'] = metadata_df['cluster'].map(cluster_name_mapping)\n",
    "metadata_df_sorted = metadata_df.sort_values(by=['type', 'condition','cluster'])\n",
    "\n",
    "# Create a new column that combines 'condition' and 'chr_type'\n",
    "metadata_df_sorted['combined'] = metadata_df_sorted['type'] + ', '+ metadata_df_sorted['condition']  +  ', ' + metadata_df_sorted['chr_type']\n",
    "\n",
    "# Now group by this new combined column and cluster, then count the number of reads\n",
    "cluster_counts_by_combined = metadata_df_sorted.groupby(['combined', 'cluster']).size().unstack(fill_value=0)\n",
    "\n",
    "# Sort the index of the resulting DataFrame to ensure the rows are ordered by 'type' and then by 'condition'\n",
    "cluster_counts_by_combined = cluster_counts_by_combined.sort_index(key=lambda x: [tuple(i.split(', ')[1:]) for i in x])\n",
    "\n",
    "\n",
    "# Calculate the percentage of reads in each cluster for each unique combination\n",
    "cluster_percentages_by_combined = cluster_counts_by_combined.div(cluster_counts_by_combined.sum(axis=1), axis=0) * 100\n",
    "\n",
    "# Sort the combined column to ensure the order of N2 Fiber, X, etc.\n",
    "sorted_combinations = sorted(cluster_percentages_by_combined.index, key=lambda x: (x.split(', ')[0], x.split(', ')[1]))\n",
    "\n",
    "nanotools.display_sample_rows(cluster_percentages_by_combined)\n",
    "\n",
    "# Plotting with sorted combinations\n",
    "fig_stacked = go.Figure()\n",
    "\n",
    "for cluster_name in cluster_percentages_by_combined.columns:\n",
    "    fig_stacked.add_trace(go.Bar(\n",
    "        x=sorted_combinations,\n",
    "        y=cluster_percentages_by_combined.loc[sorted_combinations, cluster_name],\n",
    "        name=cluster_name,\n",
    "        # set colorscheme to prism\n",
    "        marker=dict(color=cluster_colors[cluster_name]),\n",
    "        # Add % data labels\n",
    "        text=cluster_percentages_by_combined.loc[sorted_combinations, cluster_name].round(1),\n",
    "    ))\n",
    "\n",
    "fig_stacked.update_layout(\n",
    "    barmode='stack',\n",
    "    title='Percentage of Reads in Each Cluster by Condition and Chr_Type',\n",
    "    xaxis_title='Condition, Chr_Type',\n",
    "    yaxis_title='Percentage of Reads',\n",
    "    yaxis=dict(type='linear', ticksuffix='%'),\n",
    "    legend_title='Clusters',\n",
    "    template='plotly_white',\n",
    "    width=800,\n",
    "    height=600\n",
    ")\n",
    "fig_heatmap.show()\n",
    "fig_scatter.show()\n",
    "fig_stacked.show()\n",
    "read_fig.show()\n",
    "\n",
    "# Save all figures as svgs and pngs to images_11_14_23/\n",
    "\"\"\"fig_heatmap.write_image(\"images_11_14_23/average_autocorrelograms_heatmap_2000bp_10gs_0p5res.png\")\n",
    "fig_scatter.write_image(\"images_11_14_23/average_autocorrelograms_scatter_2000bp_10gs_0p5res.png\")\n",
    "fig_stacked.write_image(\"images_11_14_23/average_autocorrelograms_stacked_box_2000bp_10gs_0p5res.png\")\n",
    "read_fig.write_image(\"images_11_14_23/per_read_500_autocorrelograms_heatmap_2000bp_10gs_0p5res.png\")\n",
    "fig_heatmap.write_image(\"images_11_14_23/average_autocorrelograms_heatmap_2000bp_10gs_0p5res.svg\")\n",
    "fig_scatter.write_image(\"images_11_14_23/average_autocorrelograms_scatter_2000bp_10gs_0p5res.svg\")\n",
    "fig_stacked.write_image(\"images_11_14_23/average_autocorrelograms_stacked_box_2000bp_10gs_0p5res.svg\")\n",
    "read_fig.write_image(\"images_11_14_23/per_read_500_autocorrelograms_heatmap_2000bp_10gs_0p5res.svg\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f29526",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T22:41:55.829831700Z",
     "start_time": "2023-11-16T22:41:55.755306700Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Drop rows where 'combined' column contains 'weak_rex'\n",
    "cluster_percentages_by_combined = cluster_percentages_by_combined[~cluster_percentages_by_combined.index.to_series().str.contains('strong_rex')]\n",
    "\n",
    "# Create a bar plot\n",
    "fig = go.Figure()\n",
    "\n",
    "condition_colors = {\n",
    "    'N2_fiber': 'rgba(0, 77, 153, 0.8)',  # Darker Blue\n",
    "    'SDC2_degron_fiber': 'rgba(204, 0, 0, 0.8)'  # More Vivid Red\n",
    "}\n",
    "\n",
    "# Extract clusters\n",
    "clusters = cluster_percentages_by_combined.columns\n",
    "clusters_reversed = clusters[::-1]  # Reverse the cluster order\n",
    "\n",
    "# Add bars for each condition in each cluster\n",
    "for condition in ['N2_fiber', 'SDC2_degron_fiber']:\n",
    "    condition_df = cluster_percentages_by_combined[cluster_percentages_by_combined.index.to_series().str.contains(condition)]\n",
    "    fig.add_trace(go.Bar(\n",
    "        name=condition,\n",
    "        x=clusters_reversed,\n",
    "        y=[condition_df[cluster].mean() for cluster in clusters_reversed],  # Use mean percentage for each cluster\n",
    "        marker_color=condition_colors[condition]\n",
    "    ))\n",
    "\n",
    "# Update layout to group bars\n",
    "fig.update_layout(\n",
    "    title='Bar Plot of Cluster Percentages by Condition (STRONG REX)',\n",
    "    xaxis_title='Cluster',\n",
    "    yaxis_title='Percentage of Reads',\n",
    "    yaxis=dict(type='linear', ticksuffix='%'),\n",
    "    barmode='group',  # Group bars by cluster\n",
    "    template='plotly_white',\n",
    "    width=800,\n",
    "    height=600\n",
    ")\n",
    "fig.update_xaxes(showgrid=False)  # Remove gridlines from X-axis\n",
    "fig.update_yaxes(showgrid=False)  # Remove gridlines from Y-axis\n",
    "\n",
    "# Display the figure\n",
    "fig.show()\n",
    "\n",
    "fig.write_image(\"images_11_14_23/average_autocorrelograms_bar_plot_2000bp_10gs_0p5res_WEAKrex.png\")\n",
    "fig.write_image(\"images_11_14_23/average_autocorrelograms_bar_plot_2000bp_10gs_0p5res_WEAKrex.svg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5863a01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculating Nucleosome, Met Accessible Domain regions and midpoints\n",
    "# Simple Algo\n",
    "import multiprocessing\n",
    "metadata_cols = ['chrom', 'chr_type', 'condition', 'bed_start','type', 'read_id', 'rel_read_start','rel_read_end']\n",
    "MAD_dist_max = 65 # Distance below which m6A marks are combined into MAD\n",
    "NUC_max_width  = 170 # Distance below which m6A marks are combined into NUC\n",
    "NUC_min_width  = 110 # Distance above which m6A marks are combined into NUC\n",
    "\n",
    "def calculate_midpoints_for_group(read_id,group,MAD_dist_max,NUC_max_width ,NUC_min_width ,metadata_cols):\n",
    "    # if read_id %2000 == 0 then print progress message\n",
    "    #iter=0\n",
    "    #if iter % 5000 == 0:\n",
    "    #    print(\"Processing read:\", iter, sep='\\n')\n",
    "    #if group.empty:\n",
    "    #    print(f\"Warning: Empty group for read_id: {read_id}. Skipping.\")\n",
    "    #    return read_id, [], [], None  # Return None to indicate empty group for later checks\n",
    "\n",
    "    regions_MAD_list = []\n",
    "    regions_NUC_list = []\n",
    "    #print(\"Processing read:\", read_id, sep='\\n')\n",
    "    \"\"\"print(\"group:\",group.head(10))\"\"\"\n",
    "\n",
    "    min_position = min(group['rel_pos'])\n",
    "\n",
    "    MAD_start, MAD_end = min_position, min_position\n",
    "    NUC_start, NUC_end = min_position, min_position\n",
    "\n",
    "    for i in range(len(group['rel_pos'])):\n",
    "        x = group.iloc[i]['rel_pos']\n",
    "        '''if iter % 2000 == 0:\n",
    "            print(\"x: \", x)'''\n",
    "        \"\"\"if read_id == \"0037677b-c871-4a92-943a-2062ebecaf2f\" and x > 200:\n",
    "            print(\"MAD_start: \", MAD_start, \" | MAD_end: \", MAD_end, \" | dist from x:\", x - MAD_end)\n",
    "            print(\"NUC_start: \", NUC_start, \"NUC_end: \", NUC_end, \" | dist from x:\", x - NUC_end)\n",
    "            print(\"regions_NUC_list:\",regions_NUC_list)\n",
    "            print(\"regions_MAD_list:\",regions_MAD_list)\n",
    "            print(\"x: \", x)\"\"\"\n",
    "\n",
    "        # Initialize current state if no MAD_region\n",
    "        if MAD_start is None and NUC_start is None:\n",
    "            MAD_start, MAD_end = x, x\n",
    "            NUC_start, NUC_end = x, x\n",
    "\n",
    "        '''if iter % 2000 == 0:\n",
    "            print(\"MAD_start: \", MAD_start, \" | MAD_end: \", MAD_end, \" | MAD_length: \", MAD_end - MAD_start)\n",
    "            print(\"NUC_start: \", NUC_start, \"NUC_end: \", NUC_end, \" | NUC_length: \", NUC_end - NUC_start)'''\n",
    "\n",
    "        # if x is within MAD_dist_max of MAD_end, extend MAD by setting MAD_end equal to x\n",
    "        if (x - MAD_end) <= MAD_dist_max:\n",
    "            MAD_end = x\n",
    "            # if NUC meets NUC_min_width  and NUC_max_width , add midpoint to midpoints_NUC_list\n",
    "            if NUC_min_width  < (NUC_end - NUC_start) <= NUC_max_width :\n",
    "                regions_NUC_list.append((NUC_start,NUC_end))\n",
    "                '''if iter % 2000 == 0:\n",
    "                    print(\"Appending NUC:\", (NUC_end + NUC_start) / 2)'''\n",
    "            # Reset NUC_start and NUC_end to x regardless of whether it was appended or not.\n",
    "            NUC_start, NUC_end = x, x\n",
    "\n",
    "        # if x is greater than MAD_dist_max from MAD_end, add MAD region to regions_MAD_list and reset MAD_start and MAD_end to x\n",
    "        elif (x - MAD_end) > MAD_dist_max:\n",
    "            if (MAD_end - MAD_start) > 0:\n",
    "                regions_MAD_list.append((MAD_start, MAD_end))\n",
    "                '''if iter % 2000 == 0:\n",
    "                    print(\"Appending MAD:\", (MAD_start, MAD_end))'''\n",
    "                MAD_start, MAD_end = x, x\n",
    "            # if MAD is 0 update to new location\n",
    "            if (MAD_end - MAD_start) == 0:\n",
    "                MAD_start, MAD_end = x, x\n",
    "            # if extended nuc would not be greater than NUC_max_width , extend NUC by setting NUC_end equal to x\n",
    "            if (x - NUC_start) <= NUC_max_width :\n",
    "                NUC_end = x\n",
    "            # else if extended nuc would be greater than NUC_max_width , add midpoint to midpoints_NUC_list and reset NUC_start and NUC_end to x\n",
    "            if (x - NUC_start) > NUC_max_width :\n",
    "                if NUC_min_width  < (NUC_end - NUC_start) <= NUC_max_width :\n",
    "                    regions_NUC_list.append((NUC_start,NUC_end))\n",
    "                    '''if iter % 2000 == 0:\n",
    "                        print(\"Appending NUC:\", (NUC_end + NUC_start) / 2)'''\n",
    "                    # if new position is < MAD dist from end of nuc, update MAD\n",
    "                    if (x-NUC_end) <= MAD_dist_max:\n",
    "                        MAD_start, MAD_end = NUC_end, x\n",
    "                    NUC_start, NUC_end = x, x\n",
    "                MAD_start, MAD_end = x, x\n",
    "    # append last nucs / mads\n",
    "    if (MAD_end - MAD_start) > 0:\n",
    "        regions_MAD_list.append((MAD_start, MAD_end))\n",
    "    if NUC_min_width  < (NUC_end - NUC_start) <= NUC_max_width :\n",
    "        regions_NUC_list.append((NUC_start,NUC_end))\n",
    "    return read_id, regions_MAD_list, regions_NUC_list, group.iloc[0][metadata_cols]\n",
    "\n",
    "def create_dataframe_from_results(results, kind='NUC', metadata_cols=metadata_cols):\n",
    "    # if \"rel_start\" in res[3] == -21 then print res[0:3]\n",
    "    if kind == 'NUC':\n",
    "        df = pd.DataFrame.from_dict({res[0]: [(x[0] + x[1]) / 2 for x in res[2]] for res in results}, orient='index')\n",
    "    elif kind == 'MAD':\n",
    "        df = pd.DataFrame.from_dict({res[0]: [(x[0] + x[1]) / 2 for x in res[1]] for res in results}, orient='index')\n",
    "    elif kind == 'MAD_region':\n",
    "        df = pd.DataFrame.from_dict({res[0]: res[1] for res in results}, orient='index')\n",
    "    elif kind == 'NUC_region':\n",
    "        df = pd.DataFrame.from_dict({res[0]: res[2] for res in results}, orient='index')\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid kind: {kind}\")\n",
    "\n",
    "    # Reset the index to make it a new column\n",
    "    df.reset_index(inplace=True)\n",
    "\n",
    "    # Rename the new column to something meaningful\n",
    "    df.rename(columns={'index': 'read_id'}, inplace=True)\n",
    "    # Adding additional metadata columns to each df.\n",
    "    # Create a new DataFrame for the metadata_cols\n",
    "    metadata_df = pd.DataFrame({cols: [res[3][cols] if res[3] is not None else None for res in results] for cols in metadata_cols})\n",
    "    # Concatenate the new DataFrame and the original DataFrame along axis 1 (columns)\n",
    "    df = pd.merge(metadata_df, df, on='read_id', how='left')\n",
    "\n",
    "    return df\n",
    "\n",
    "# Using multiprocessing to parallelize the calculations\n",
    "print(\"Grouping df...\")\n",
    "# Sort plot_df by read_id then by rel_pos\n",
    "#print(\"Plot_df:\")\n",
    "#display(plot_df.head(10))\n",
    "plot_df.sort_values(by=[\"read_id\",\"rel_pos\"], inplace=True)\n",
    "plot_df.reset_index(inplace=True, drop=True)\n",
    "# grouped = plot_df dropping all rows where mod_qual != 1, then grouped by read_id\n",
    "plot_df_m6a_ony = plot_df[plot_df['mod_qual_bin'] == 1]\n",
    "\n",
    "grouped = plot_df_m6a_ony.groupby('read_id')\n",
    "print(\"Grouped_df...\")\n",
    "display(grouped.head(10))\n",
    "print(\"Calculating nucleosome and MAD positions...\")\n",
    "\n",
    "grouped_data_with_constants = [(read_id,group,MAD_dist_max, NUC_max_width , NUC_min_width ,metadata_cols) for read_id,group in grouped]\n",
    "\n",
    "#processes=multiprocessing.cpu_count()\n",
    "with multiprocessing.Pool(processes=multiprocessing.cpu_count()) as pool:\n",
    "    results = pool.starmap(calculate_midpoints_for_group, grouped_data_with_constants)\n",
    "\n",
    "print(\"creating result dfs...\")\n",
    "midpoint_NUC = create_dataframe_from_results(results, kind='NUC', metadata_cols=metadata_cols)\n",
    "midpoint_MAD = create_dataframe_from_results(results, kind='MAD', metadata_cols=metadata_cols)\n",
    "region_MAD = create_dataframe_from_results(results, kind='MAD_region', metadata_cols=metadata_cols)\n",
    "region_NUC = create_dataframe_from_results(results, kind='NUC_region', metadata_cols=metadata_cols)\n",
    "\n",
    "# drop duplicates based on read_id from all dfs\n",
    "midpoint_NUC.drop_duplicates(subset=['read_id'], keep='first', inplace=True)\n",
    "midpoint_MAD.drop_duplicates(subset=['read_id'], keep='first', inplace=True)\n",
    "region_MAD.drop_duplicates(subset=['read_id'], keep='first', inplace=True)\n",
    "region_NUC.drop_duplicates(subset=['read_id'], keep='first', inplace=True)\n",
    "\n",
    "#print column names\n",
    "print(\"midpoint_MAD columns:\")\n",
    "display(midpoint_MAD.head(10))\n",
    "print(\"midpoint_NUC columns:\")\n",
    "display(midpoint_NUC.head(3))\n",
    "print(\"region_MAD columns:\")\n",
    "display(region_MAD.head(10))\n",
    "print(\"region_NUC columns:\")\n",
    "display(region_NUC.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0f1cd5",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-10T22:50:23.405763100Z"
    }
   },
   "outputs": [],
   "source": [
    "## Calculate positive and negative controls\n",
    "# Define percentile\n",
    "percentile = 0.95\n",
    "\n",
    "# Filter condition\n",
    "filtered_df = plot_df.copy()#[plot_df['condition'] == \"N2_bg\"]\n",
    "filtered_df = filtered_df[filtered_df['condition'] == 'N2_fiber']\n",
    "# Add column for average_mod_qual for each read_id\n",
    "filtered_df['avg_mod_qual'] = filtered_df.groupby('read_id')['mod_qual'].transform('mean')\n",
    "percentile_99 = filtered_df['avg_mod_qual'].quantile(percentile)\n",
    "high_methylation_read_ids = filtered_df[filtered_df['avg_mod_qual'] <= percentile_99]['read_id']\n",
    "plot_df_filtered_99 = plot_df[plot_df['read_id'].isin(high_methylation_read_ids)]\n",
    "print(\"reads above threshold:\", len(high_methylation_read_ids),\" | bases above threshold:\", len(plot_df_filtered_99))\n",
    "\n",
    "x_values = np.linspace(0, 1, 1000)\n",
    "kde = gaussian_kde(plot_df_filtered_99[\"mod_qual\"])\n",
    "y_values = kde(x_values)/1000\n",
    "\n",
    "# Create the KDE plot using Plotly\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x_values, y=y_values, mode='lines', name='KDE'))\n",
    "fig.update_layout(title='KDE plot of mod_qual',\n",
    "                  xaxis_title='mod_qual',\n",
    "                  yaxis_title='Density')\n",
    "# set theme to plotly white\n",
    "fig.update_layout(template=\"plotly_white\")\n",
    "fig.show()\n",
    "\n",
    "output_fn = f\"temp_files/N2-intergenic_0p1-neg-ctrl-{str(percentile)}.txt\"\n",
    "# output y_values list to txt file\n",
    "with open(output_fn, 'w') as f:\n",
    "    for item in y_values:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87981f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T08:18:52.925867300Z",
     "start_time": "2023-11-15T08:09:54.504767500Z"
    }
   },
   "outputs": [],
   "source": [
    "### MATRIX BASED NUCLEOSOME POSITIONS\n",
    "import multiprocessing\n",
    "output_file = \"temp_files/nucleosomes.csv\"\n",
    "NUC_width  = 147 # Distance below which m6A marks are combined into NUC\n",
    "\n",
    "emission_NEG_fn = \"temp_files/N2-intergenic_0p1-neg-ctrl-0.01.txt\"\n",
    "emission_PGC_fn = \"temp_files/N2-sdc2-intergenic_0p1-pos-ctrl-0.95.txt\"\n",
    "\n",
    "# Assume emission_NEG_array and emission_PGC_array are defined\n",
    "if emission_NEG_fn is None:\n",
    "    print(\"Using default Negative Control array...\")\n",
    "    emission_NEG_array = np.array([0.00149399264776291,\n",
    "\t0.00152416739428112,\n",
    "\t0.00155428407897689,\n",
    "\t0.00158366982352413,\n",
    "\t0.00161263012922332,\n",
    "\t0.00164116736093195,\n",
    "\t0.00166881741930283,\n",
    "\t0.00169644480709635,\n",
    "\t0.00172243991332119,\n",
    "\t0.00174843501954603,\n",
    "\t0.00177314930158316,\n",
    "\t0.00179732634903045,\n",
    "\t0.0018208293765571,\n",
    "\t0.00184330135380909,\n",
    "\t0.00186562176698358,\n",
    "\t0.00188633353415362,\n",
    "\t0.00190704530132367,\n",
    "\t0.00192636133771002,\n",
    "\t0.00194527658490408,\n",
    "\t0.00196334324282506,\n",
    "\t0.0019804441422428,\n",
    "\t0.00199721573026683,\n",
    "\t0.00201223912200322,\n",
    "\t0.00202726251373961,\n",
    "\t0.00204062362531173,\n",
    "\t0.00205368981620955,\n",
    "\t0.00206583124459128,\n",
    "\t0.00207718528459883,\n",
    "\t0.00208815849360906,\n",
    "\t0.00209786334175532,\n",
    "\t0.00210756818990159,\n",
    "\t0.00211581913836465,\n",
    "\t0.00212394661339389,\n",
    "\t0.00213115648509963,\n",
    "\t0.00213778589208315,\n",
    "\t0.00214392113051369,\n",
    "\t0.00214892137536451,\n",
    "\t0.00215392162021533,\n",
    "\t0.00215755004456053,\n",
    "\t0.00216117036760352,\n",
    "\t0.00216398797335146,\n",
    "\t0.00216643632914367,\n",
    "\t0.00216847696309653,\n",
    "\t0.00216984018420171,\n",
    "\t0.00217113675922387,\n",
    "\t0.00217149832890489,\n",
    "\t0.0021718598985859,\n",
    "\t0.00217152301775043,\n",
    "\t0.00217096234398462,\n",
    "\t0.00216995779524117,\n",
    "\t0.00216840664063006,\n",
    "\t0.00216674524343967,\n",
    "\t0.00216440065280018,\n",
    "\t0.00216205606216068,\n",
    "\t0.00215914191711877,\n",
    "\t0.00215611090596126,\n",
    "\t0.00215274778308761,\n",
    "\t0.00214907876784382,\n",
    "\t0.00214528404933982,\n",
    "\t0.00214102027144815,\n",
    "\t0.00213675649355648,\n",
    "\t0.00213199079662317,\n",
    "\t0.00212717066161593,\n",
    "\t0.00212198067237151,\n",
    "\t0.00211653678978936,\n",
    "\t0.00211095201976393,\n",
    "\t0.00210501163039892,\n",
    "\t0.00209907124103391,\n",
    "\t0.00209269337479204,\n",
    "\t0.00208630408847105,\n",
    "\t0.00207963303317977,\n",
    "\t0.00207282026467108,\n",
    "\t0.00206586580125198,\n",
    "\t0.00205865519703331,\n",
    "\t0.00205142714861267,\n",
    "\t0.00204384512826049,\n",
    "\t0.00203626310790831,\n",
    "\t0.00202837761686597,\n",
    "\t0.00202038426090988,\n",
    "\t0.00201226433018618,\n",
    "\t0.00200397563310567,\n",
    "\t0.00199565703906218,\n",
    "\t0.00198711789365238,\n",
    "\t0.00197857874824259,\n",
    "\t0.00196987018727312,\n",
    "\t0.00196112193231988,\n",
    "\t0.0019522914291573,\n",
    "\t0.00194337899906746,\n",
    "\t0.00193444440342173,\n",
    "\t0.00192541639579044,\n",
    "\t0.00191638838815915,\n",
    "\t0.00190731276271764,\n",
    "\t0.00189823080231246,\n",
    "\t0.00188916972807923,\n",
    "\t0.0018801241937282,\n",
    "\t0.0018711050764496,\n",
    "\t0.00186215953560976,\n",
    "\t0.00185321399476991,\n",
    "\t0.00184442692402003,\n",
    "\t0.00183564732214734,\n",
    "\t0.00182701881316729,\n",
    "\t0.00181847317634955,\n",
    "\t0.00181003266704129,\n",
    "\t0.00180179935671974,\n",
    "\t0.00179357963248821,\n",
    "\t0.00178584221445295,\n",
    "\t0.00177810479641768,\n",
    "\t0.00177072403798445,\n",
    "\t0.00176348322228335,\n",
    "\t0.00175647482276216,\n",
    "\t0.00174980225906073,\n",
    "\t0.0017431934656268,\n",
    "\t0.00173715985499785,\n",
    "\t0.0017311262443689,\n",
    "\t0.00172565269607944,\n",
    "\t0.00172032748113237,\n",
    "\t0.00171539371441344,\n",
    "\t0.00171088162972888,\n",
    "\t0.00170655046389971,\n",
    "\t0.00170308894211409,\n",
    "\t0.00169962742032846,\n",
    "\t0.0016969800459273,\n",
    "\t0.0016944619012697,\n",
    "\t0.00169249675040448,\n",
    "\t0.00169097712103983,\n",
    "\t0.00168971428709704,\n",
    "\t0.00168924443741698,\n",
    "\t0.00168877458773692,\n",
    "\t0.00168933081369971,\n",
    "\t0.00168995787752069,\n",
    "\t0.0016913461472447,\n",
    "\t0.00169318876174207,\n",
    "\t0.00169548047964238,\n",
    "\t0.00169873971042928,\n",
    "\t0.0017020085047235,\n",
    "\t0.00170648947357022,\n",
    "\t0.00171097044241694,\n",
    "\t0.00171631727885466,\n",
    "\t0.00172203762680224,\n",
    "\t0.00172824495561917,\n",
    "\t0.00173521579213078,\n",
    "\t0.00174228730888025,\n",
    "\t0.00175051296494183,\n",
    "\t0.00175873862100341,\n",
    "\t0.00176801038609194,\n",
    "\t0.00177759279776968,\n",
    "\t0.00178783967310848,\n",
    "\t0.00179886081754385,\n",
    "\t0.00181006443736322,\n",
    "\t0.00182228102950662,\n",
    "\t0.00183449762165001,\n",
    "\t0.00184768975197826,\n",
    "\t0.00186106295667957,\n",
    "\t0.00187502883980271,\n",
    "\t0.00188951141693461,\n",
    "\t0.00190423112833026,\n",
    "\t0.00191976740640802,\n",
    "\t0.00193530368448579,\n",
    "\t0.00195183130849742,\n",
    "\t0.00196845001771239,\n",
    "\t0.00198567721797348,\n",
    "\t0.00200329895597738,\n",
    "\t0.00202114813398883,\n",
    "\t0.00203953448938654,\n",
    "\t0.00205792084478426,\n",
    "\t0.00207696436002779,\n",
    "\t0.00209601568311108,\n",
    "\t0.00211544651033297,\n",
    "\t0.00213505669974141,\n",
    "\t0.0021548321436499,\n",
    "\t0.00217488913226217,\n",
    "\t0.00219496718641124,\n",
    "\t0.00221537141661406,\n",
    "\t0.00223577564681688,\n",
    "\t0.002256299317718,\n",
    "\t0.00227686249114174,\n",
    "\t0.00229742175345622,\n",
    "\t0.00231797608361963,\n",
    "\t0.00233851210455439,\n",
    "\t0.00235892884624954,\n",
    "\t0.00237934558794468,\n",
    "\t0.00239954336575385,\n",
    "\t0.00241969433418121,\n",
    "\t0.00243964324843439,\n",
    "\t0.00245940163251569,\n",
    "\t0.00247903240997627,\n",
    "\t0.00249816977517,\n",
    "\t0.00251730714036373,\n",
    "\t0.00253575320236407,\n",
    "\t0.00255411926639662,\n",
    "\t0.00257200383160063,\n",
    "\t0.00258954974262857,\n",
    "\t0.0026068402979811,\n",
    "\t0.00262346728823345,\n",
    "\t0.00264009427848579,\n",
    "\t0.00265574328491529,\n",
    "\t0.00267136067974647,\n",
    "\t0.00268625812992732,\n",
    "\t0.00270078385794448,\n",
    "\t0.00271482710676313,\n",
    "\t0.00272797547796219,\n",
    "\t0.00274106811104646,\n",
    "\t0.00275286018811893,\n",
    "\t0.00276465226519139,\n",
    "\t0.00277549804233091,\n",
    "\t0.00278599718019019,\n",
    "\t0.00279593860426456,\n",
    "\t0.00280511833590122,\n",
    "\t0.00281414646296176,\n",
    "\t0.00282199050600528,\n",
    "\t0.00282983454904881,\n",
    "\t0.00283659926587619,\n",
    "\t0.00284310137291861,\n",
    "\t0.00284881712314759,\n",
    "\t0.00285373097591462,\n",
    "\t0.0028583866666396,\n",
    "\t0.00286191187936988,\n",
    "\t0.00286543709210015,\n",
    "\t0.00286784905596031,\n",
    "\t0.00287010446919811,\n",
    "\t0.00287166224046996,\n",
    "\t0.00287268836045052,\n",
    "\t0.00287340899169051,\n",
    "\t0.00287325249995569,\n",
    "\t0.00287309600822087,\n",
    "\t0.00287186670592455,\n",
    "\t0.00287057989213421,\n",
    "\t0.00286847444583565,\n",
    "\t0.00286590836228474,\n",
    "\t0.00286300801757572,\n",
    "\t0.00285943123904376,\n",
    "\t0.00285583510643775,\n",
    "\t0.00285135924319256,\n",
    "\t0.00284688337994738,\n",
    "\t0.00284181504428998,\n",
    "\t0.00283650744092764,\n",
    "\t0.00283089171678661,\n",
    "\t0.00282481979440526,\n",
    "\t0.00281868236584968,\n",
    "\t0.00281191307565515,\n",
    "\t0.00280514378546061,\n",
    "\t0.00279778159178072,\n",
    "\t0.00279025675624698,\n",
    "\t0.00278247440790956,\n",
    "\t0.00277440805368165,\n",
    "\t0.00276626310802112,\n",
    "\t0.00275772428059235,\n",
    "\t0.00274918545316358,\n",
    "\t0.0027402863069457,\n",
    "\t0.00273132710759631,\n",
    "\t0.00272216384304706,\n",
    "\t0.00271283222470792,\n",
    "\t0.00270342209370996,\n",
    "\t0.00269376162316871,\n",
    "\t0.00268410115262745,\n",
    "\t0.00267411397844846,\n",
    "\t0.00266410204357272,\n",
    "\t0.0026539405715072,\n",
    "\t0.00264368758456844,\n",
    "\t0.00263336631929775,\n",
    "\t0.00262289389696513,\n",
    "\t0.00261242107840481,\n",
    "\t0.00260174618371062,\n",
    "\t0.00259107128901644,\n",
    "\t0.00258026477300697,\n",
    "\t0.00256939988257235,\n",
    "\t0.00255846378400356,\n",
    "\t0.00254741324379054,\n",
    "\t0.00253634629093877,\n",
    "\t0.00252507504392566,\n",
    "\t0.00251380379691255,\n",
    "\t0.0025023930007641,\n",
    "\t0.00249093937623652,\n",
    "\t0.00247939968750435,\n",
    "\t0.00246775731272549,\n",
    "\t0.0024560860411578,\n",
    "\t0.00244424678292431,\n",
    "\t0.00243240752469082,\n",
    "\t0.00242039539324634,\n",
    "\t0.00240834973732403,\n",
    "\t0.00239618425532223,\n",
    "\t0.00238391181744946,\n",
    "\t0.00237157772447086,\n",
    "\t0.00235902398517773,\n",
    "\t0.00234647024588459,\n",
    "\t0.0023336939501436,\n",
    "\t0.00232089563806583,\n",
    "\t0.00230794559982686,\n",
    "\t0.00229489474029509,\n",
    "\t0.00228176844467743,\n",
    "\t0.00226845887481082,\n",
    "\t0.00225514930494422,\n",
    "\t0.0022415815317423,\n",
    "\t0.00222800912939691,\n",
    "\t0.00221424733004706,\n",
    "\t0.0022003935850584,\n",
    "\t0.00218642661016831,\n",
    "\t0.0021722617873322,\n",
    "\t0.00215808312729033,\n",
    "\t0.00214366577544926,\n",
    "\t0.00212924842360819,\n",
    "\t0.00211465082044989,\n",
    "\t0.00209999171421747,\n",
    "\t0.00208523301609069,\n",
    "\t0.00207034569704204,\n",
    "\t0.00205543141202063,\n",
    "\t0.00204033209893997,\n",
    "\t0.00202523278585931,\n",
    "\t0.00200996428509849,\n",
    "\t0.0019946581421026,\n",
    "\t0.00197925654419814,\n",
    "\t0.00196376279559351,\n",
    "\t0.00194824325308297,\n",
    "\t0.0019326202820674,\n",
    "\t0.00191699731105183,\n",
    "\t0.00190128344073455,\n",
    "\t0.00188555838281092,\n",
    "\t0.00186979031776118,\n",
    "\t0.00185399126441182,\n",
    "\t0.00183817992156937,\n",
    "\t0.00182233568886051,\n",
    "\t0.00180649145615164,\n",
    "\t0.00179063475186534,\n",
    "\t0.00177477756595309,\n",
    "\t0.00175894100514134,\n",
    "\t0.00174311537439783,\n",
    "\t0.0017273109446994,\n",
    "\t0.00171154686926291,\n",
    "\t0.0016957859228771,\n",
    "\t0.00168011067771277,\n",
    "\t0.00166443543254843,\n",
    "\t0.00164884368014338,\n",
    "\t0.00163328343443994,\n",
    "\t0.00161778151875516,\n",
    "\t0.00160236121032654,\n",
    "\t0.00158696118087024,\n",
    "\t0.0015717292825299,\n",
    "\t0.00155649738418957,\n",
    "\t0.00154144106989098,\n",
    "\t0.00152642908195586,\n",
    "\t0.00151152299297459,\n",
    "\t0.00149672746250112,\n",
    "\t0.0014819738214615,\n",
    "\t0.00146741095947384,\n",
    "\t0.00145284809748619,\n",
    "\t0.00143850054783584,\n",
    "\t0.00142418493215013,\n",
    "\t0.00141001546650868,\n",
    "\t0.00139596006919445,\n",
    "\t0.00138198396196036,\n",
    "\t0.00136824265170775,\n",
    "\t0.00135450134145514,\n",
    "\t0.00134105764081501,\n",
    "\t0.00132763184614742,\n",
    "\t0.00131439181724396,\n",
    "\t0.00130125900500067,\n",
    "\t0.00128822313563168,\n",
    "\t0.00127538874392315,\n",
    "\t0.001262559093001,\n",
    "\t0.00125002751902218,\n",
    "\t0.00123749594504335,\n",
    "\t0.00122518071851831,\n",
    "\t0.00121295538705716,\n",
    "\t0.00120087371641448,\n",
    "\t0.00118901001391015,\n",
    "\t0.00117717598865721,\n",
    "\t0.00116564928782962,\n",
    "\t0.00115412258700203,\n",
    "\t0.00114283794089724,\n",
    "\t0.00113162201887202,\n",
    "\t0.00112055184536474,\n",
    "\t0.00110964624622561,\n",
    "\t0.00109879029004084,\n",
    "\t0.00108819405126365,\n",
    "\t0.00107759781248645,\n",
    "\t0.00106726369661379,\n",
    "\t0.00105697537692674,\n",
    "\t0.00104688426775375,\n",
    "\t0.00103695975518832,\n",
    "\t0.0010271099969022,\n",
    "\t0.00101750648800675,\n",
    "\t0.00100790297911129,\n",
    "\t0.000998574902724826,\n",
    "\t0.000989269584532959,\n",
    "\t0.000980145224832513,\n",
    "\t0.000971134396414901,\n",
    "\t0.000962212227150721,\n",
    "\t0.000953491814293388,\n",
    "\t0.000944771401436055,\n",
    "\t0.000936335842153333,\n",
    "\t0.000927901402138094,\n",
    "\t0.000919699878608294,\n",
    "\t0.000911604527835387,\n",
    "\t0.000903616376577226,\n",
    "\t0.000895804854571337,\n",
    "\t0.000888011547514706,\n",
    "\t0.000880465963767979,\n",
    "\t0.000872920380021253,\n",
    "\t0.000865571953731993,\n",
    "\t0.000858286028389301,\n",
    "\t0.0008511140273685,\n",
    "\t0.000844081212149661,\n",
    "\t0.000837083102316501,\n",
    "\t0.000830296598932136,\n",
    "\t0.00082351009554777,\n",
    "\t0.000816966812248717,\n",
    "\t0.00081047274524928,\n",
    "\t0.000804099870408216,\n",
    "\t0.000797837747951584,\n",
    "\t0.000791622424112246,\n",
    "\t0.00078557969700825,\n",
    "\t0.000779536969904255,\n",
    "\t0.000773685769414869,\n",
    "\t0.000767854882354304,\n",
    "\t0.000762145512890653,\n",
    "\t0.000756518887317006,\n",
    "\t0.000750948457352519,\n",
    "\t0.000745518516410936,\n",
    "\t0.000740088575469351,\n",
    "\t0.000734887391392363,\n",
    "\t0.000729691708482459,\n",
    "\t0.000724613960722743,\n",
    "\t0.000719595007612682,\n",
    "\t0.000714636779849552,\n",
    "\t0.000709787393145405,\n",
    "\t0.000704945959474296,\n",
    "\t0.000700259132765475,\n",
    "\t0.000695572306056653,\n",
    "\t0.000691000669700496,\n",
    "\t0.00068646956328324,\n",
    "\t0.000682003915709478,\n",
    "\t0.000677624852333247,\n",
    "\t0.000673266799812099,\n",
    "\t0.000669060906547919,\n",
    "\t0.00066485501328374,\n",
    "\t0.000660756608283642,\n",
    "\t0.000656683068038609,\n",
    "\t0.000652673186708866,\n",
    "\t0.000648726221194235,\n",
    "\t0.000644802695524232,\n",
    "\t0.000640976717490456,\n",
    "\t0.00063715073945668,\n",
    "\t0.000633427003179422,\n",
    "\t0.000629716612618168,\n",
    "\t0.000626072256016885,\n",
    "\t0.000622476638767007,\n",
    "\t0.000618914414036679,\n",
    "\t0.000615444264268344,\n",
    "\t0.000611974114500009,\n",
    "\t0.000608597190210564,\n",
    "\t0.000605224460114442,\n",
    "\t0.000601911931742057,\n",
    "\t0.000598632141467172,\n",
    "\t0.000595382346985485,\n",
    "\t0.00059219115700149,\n",
    "\t0.000589002444417624,\n",
    "\t0.00058589565119799,\n",
    "\t0.000582788857978357,\n",
    "\t0.000579743349561872,\n",
    "\t0.000576721655620523,\n",
    "\t0.000573736587747257,\n",
    "\t0.000570804017238244,\n",
    "\t0.000567878586911705,\n",
    "\t0.000565016182426628,\n",
    "\t0.00056215377794155,\n",
    "\t0.000559344191342501,\n",
    "\t0.000556548428683992,\n",
    "\t0.00055378323086729,\n",
    "\t0.000551050701545474,\n",
    "\t0.0005483286120001,\n",
    "\t0.000545656023417182,\n",
    "\t0.000542983434834264,\n",
    "\t0.000540363805116811,\n",
    "\t0.000537752442733559,\n",
    "\t0.00053517517832031,\n",
    "\t0.000532625168270409,\n",
    "\t0.000530087240315178,\n",
    "\t0.000527586229871436,\n",
    "\t0.000525085219427695,\n",
    "\t0.000522627681576383,\n",
    "\t0.000520173047948365,\n",
    "\t0.000517745979727691,\n",
    "\t0.000515335227480211,\n",
    "\t0.000512937717693019,\n",
    "\t0.000510568480152332,\n",
    "\t0.000508199664723461,\n",
    "\t0.000505873567010415,\n",
    "\t0.000503547469297369,\n",
    "\t0.000501250589220506,\n",
    "\t0.000498966196010502,\n",
    "\t0.000496695162962353,\n",
    "\t0.000494444904965888,\n",
    "\t0.00049219731869335,\n",
    "\t0.000489979566671334,\n",
    "\t0.000487761814649318,\n",
    "\t0.000485568031671005,\n",
    "\t0.000483381287702995,\n",
    "\t0.0004812082906857,\n",
    "\t0.000479051186767545,\n",
    "\t0.000476899003524202,\n",
    "\t0.000474773728274767,\n",
    "\t0.000472648453025332,\n",
    "\t0.000470548777374678,\n",
    "\t0.000468453783132135,\n",
    "\t0.000466372655635534,\n",
    "\t0.000464303522368087,\n",
    "\t0.000462240098070154,\n",
    "\t0.000460196113797724,\n",
    "\t0.000458152129525293,\n",
    "\t0.000456130665525532,\n",
    "\t0.000454111218266608,\n",
    "\t0.000452106390033155,\n",
    "\t0.000450110963102256,\n",
    "\t0.000448123632972984,\n",
    "\t0.000446155248301112,\n",
    "\t0.000444186863629241,\n",
    "\t0.000442243842531968,\n",
    "\t0.000440301072062903,\n",
    "\t0.000438373921295375,\n",
    "\t0.000436454086652417,\n",
    "\t0.000434542734007763,\n",
    "\t0.000432645711476032,\n",
    "\t0.000430750112587146,\n",
    "\t0.000428875823852097,\n",
    "\t0.000427001535117047,\n",
    "\t0.000425144345785697,\n",
    "\t0.00042329275262283,\n",
    "\t0.000421452959951471,\n",
    "\t0.000419627930890632,\n",
    "\t0.000417806234963531,\n",
    "\t0.000416005890414868,\n",
    "\t0.000414205545866205,\n",
    "\t0.000412424037570675,\n",
    "\t0.000410646501849858,\n",
    "\t0.000408880780328978,\n",
    "\t0.000407126112245158,\n",
    "\t0.000405376199837444,\n",
    "\t0.000403644457701639,\n",
    "\t0.000401912715565835,\n",
    "\t0.000400201615548831,\n",
    "\t0.000398492854072518,\n",
    "\t0.000396800259167048,\n",
    "\t0.000395118943264493,\n",
    "\t0.000393444494351909,\n",
    "\t0.000391787717133798,\n",
    "\t0.000390130939915688,\n",
    "\t0.000388496551523359,\n",
    "\t0.000386862840212455,\n",
    "\t0.000385244372316799,\n",
    "\t0.000383633706524213,\n",
    "\t0.000382031144945843,\n",
    "\t0.000380443486116947,\n",
    "\t0.000378856815347661,\n",
    "\t0.000377292106448783,\n",
    "\t0.000375727397549905,\n",
    "\t0.000374183018515518,\n",
    "\t0.000372646012445319,\n",
    "\t0.000371119054035018,\n",
    "\t0.000369605708583288,\n",
    "\t0.000368094967065863,\n",
    "\t0.000366604174333278,\n",
    "\t0.000365113381600693,\n",
    "\t0.000363640651466014,\n",
    "\t0.000362172261615784,\n",
    "\t0.000360714926551738,\n",
    "\t0.000359268776842218,\n",
    "\t0.000357826772517435,\n",
    "\t0.000356402688345427,\n",
    "\t0.000354978604173419,\n",
    "\t0.000353578176775674,\n",
    "\t0.000352181016014909,\n",
    "\t0.000350796387518862,\n",
    "\t0.000349421233587207,\n",
    "\t0.000348051632342441,\n",
    "\t0.000346697812418305,\n",
    "\t0.000345343992494169,\n",
    "\t0.000344010261141992,\n",
    "\t0.00034267756315257,\n",
    "\t0.000341358284997707,\n",
    "\t0.000340046494128588,\n",
    "\t0.000338741586524437,\n",
    "\t0.000337450485940016,\n",
    "\t0.000336159986650359,\n",
    "\t0.000334894491201272,\n",
    "\t0.000333628995752185,\n",
    "\t0.000332377860385073,\n",
    "\t0.000331132469050751,\n",
    "\t0.000329895133393405,\n",
    "\t0.000328669628295482,\n",
    "\t0.000327446009242417,\n",
    "\t0.000326240172898006,\n",
    "\t0.000325034336553596,\n",
    "\t0.000323843802632659,\n",
    "\t0.000322657417876296,\n",
    "\t0.000321480409969967,\n",
    "\t0.000320313662797077,\n",
    "\t0.000319150848565898,\n",
    "\t0.000318007470383873,\n",
    "\t0.000316864092201848,\n",
    "\t0.000315736736668329,\n",
    "\t0.000314612008995157,\n",
    "\t0.000313497416614113,\n",
    "\t0.000312391120023778,\n",
    "\t0.000311289206730967,\n",
    "\t0.00031020112042547,\n",
    "\t0.000309113034119972,\n",
    "\t0.000308041703754552,\n",
    "\t0.000306971605443548,\n",
    "\t0.000305913023897305,\n",
    "\t0.000304861432085648,\n",
    "\t0.000303816465382589,\n",
    "\t0.000302786032511515,\n",
    "\t0.000301755666774007,\n",
    "\t0.000300742386528938,\n",
    "\t0.000299729106283869,\n",
    "\t0.000298727582240846,\n",
    "\t0.000297731224303511,\n",
    "\t0.000296741301278296,\n",
    "\t0.000295761634915037,\n",
    "\t0.000294783224857443,\n",
    "\t0.000293820019319691,\n",
    "\t0.000292856813781938,\n",
    "\t0.000291906837801576,\n",
    "\t0.000290960877936851,\n",
    "\t0.000290023596044622,\n",
    "\t0.00028909658679505,\n",
    "\t0.000288171901203492,\n",
    "\t0.000287260515496621,\n",
    "\t0.000286349129789751,\n",
    "\t0.000285450676845293,\n",
    "\t0.00028455469589738,\n",
    "\t0.00028366677093643,\n",
    "\t0.000282785980428877,\n",
    "\t0.000281908502735774,\n",
    "\t0.000281042693185597,\n",
    "\t0.000280176883635421,\n",
    "\t0.00027932567435724,\n",
    "\t0.00027847587497657,\n",
    "\t0.000277636298150485,\n",
    "\t0.000276803458917196,\n",
    "\t0.000275974807062331,\n",
    "\t0.000275156232831541,\n",
    "\t0.000274337658600752,\n",
    "\t0.000273532950404955,\n",
    "\t0.000272728462742518,\n",
    "\t0.000271933368354912,\n",
    "\t0.000271142793629978,\n",
    "\t0.000270357248373163,\n",
    "\t0.000269580417275012,\n",
    "\t0.000268804437281335,\n",
    "\t0.000268042603230979,\n",
    "\t0.000267280769180623,\n",
    "\t0.000266530390658594,\n",
    "\t0.000265783880626498,\n",
    "\t0.000265043132268183,\n",
    "\t0.000264309766054401,\n",
    "\t0.000263578076364993,\n",
    "\t0.000262857690514166,\n",
    "\t0.000262137304663338,\n",
    "\t0.000261427425761381,\n",
    "\t0.000260719853874449,\n",
    "\t0.000260018739978868,\n",
    "\t0.000259323811898758,\n",
    "\t0.000258631710753275,\n",
    "\t0.000257950807562433,\n",
    "\t0.000257269904371592,\n",
    "\t0.000256601265399393,\n",
    "\t0.000255934105664093,\n",
    "\t0.000255273919170365,\n",
    "\t0.000254618716906352,\n",
    "\t0.000253966734398879,\n",
    "\t0.000253323284246238,\n",
    "\t0.000252679834093597,\n",
    "\t0.000252047517582553,\n",
    "\t0.000251415607573636,\n",
    "\t0.000250791120036053,\n",
    "\t0.000250170532065499,\n",
    "\t0.000249554342610917,\n",
    "\t0.000248946453180823,\n",
    "\t0.000248339005770367,\n",
    "\t0.000247743004342121,\n",
    "\t0.000247147002913874,\n",
    "\t0.000246558633187281,\n",
    "\t0.000245973115091144,\n",
    "\t0.000245391893239115,\n",
    "\t0.00024481663393148,\n",
    "\t0.000244242474958107,\n",
    "\t0.000243677256200615,\n",
    "\t0.000243112037443122,\n",
    "\t0.000242554686957331,\n",
    "\t0.000241999298730009,\n",
    "\t0.000241449423858154,\n",
    "\t0.000240905260031805,\n",
    "\t0.00024036294523246,\n",
    "\t0.000239828941113632,\n",
    "\t0.000239294936994804,\n",
    "\t0.000238768967787017,\n",
    "\t0.000238244169586444,\n",
    "\t0.000237724495420592,\n",
    "\t0.000237208788823291,\n",
    "\t0.000236695372210134,\n",
    "\t0.000236188667012042,\n",
    "\t0.00023568196181395,\n",
    "\t0.000235183703228886,\n",
    "\t0.000234685934302548,\n",
    "\t0.000234194909884835,\n",
    "\t0.00023370774540129,\n",
    "\t0.000233223680435969,\n",
    "\t0.00023274600010699,\n",
    "\t0.000232268477733303,\n",
    "\t0.000231799765754802,\n",
    "\t0.000231331053776302,\n",
    "\t0.000230868747713796,\n",
    "\t0.00023040907834047,\n",
    "\t0.000229953056537195,\n",
    "\t0.000229502523969241,\n",
    "\t0.000229052823992294,\n",
    "\t0.00022861154042444,\n",
    "\t0.000228170256856586,\n",
    "\t0.000227737891724723,\n",
    "\t0.00022730803001349,\n",
    "\t0.000226882893428852,\n",
    "\t0.000226463050554506,\n",
    "\t0.000226044793367349,\n",
    "\t0.000225634712976775,\n",
    "\t0.000225224632586202,\n",
    "\t0.000224822999457194,\n",
    "\t0.000224422819412171,\n",
    "\t0.000224028095108463,\n",
    "\t0.000223637943422475,\n",
    "\t0.000223250173711301,\n",
    "\t0.000222870165268062,\n",
    "\t0.000222490156824824,\n",
    "\t0.000222121864192542,\n",
    "\t0.000221754512788202,\n",
    "\t0.000221393658863558,\n",
    "\t0.000221036847815168,\n",
    "\t0.000220683208351109,\n",
    "\t0.000220336720102671,\n",
    "\t0.000219990231854234,\n",
    "\t0.000219654013032453,\n",
    "\t0.000219317814346803,\n",
    "\t0.000218988656646756,\n",
    "\t0.00021866267939191,\n",
    "\t0.000218340543167296,\n",
    "\t0.000218024683265277,\n",
    "\t0.000217709699142918,\n",
    "\t0.00021740627044663,\n",
    "\t0.000217102841750341,\n",
    "\t0.000216806732872727,\n",
    "\t0.000216512919670892,\n",
    "\t0.00021622333370687,\n",
    "\t0.00021593887166747,\n",
    "\t0.000215655703589276,\n",
    "\t0.000215380299278321,\n",
    "\t0.000215104894967367,\n",
    "\t0.000214836774936073,\n",
    "\t0.000214570108340863,\n",
    "\t0.000214307931846458,\n",
    "\t0.000214049826600171,\n",
    "\t0.000213793815724045,\n",
    "\t0.000213545439779141,\n",
    "\t0.000213297063834237,\n",
    "\t0.000213055370049964,\n",
    "\t0.000212814369016686,\n",
    "\t0.00021257753598141,\n",
    "\t0.00021234351805298,\n",
    "\t0.000212111397471346,\n",
    "\t0.000211883975081779,\n",
    "\t0.000211656552692212,\n",
    "\t0.000211435212188537,\n",
    "\t0.000211214005486352,\n",
    "\t0.000210996865091232,\n",
    "\t0.000210781734070072,\n",
    "\t0.000210568949130645,\n",
    "\t0.00021036033358647,\n",
    "\t0.000210151971603062,\n",
    "\t0.000209948339503172,\n",
    "\t0.000209744707403282,\n",
    "\t0.00020954457722148,\n",
    "\t0.00020934566670508,\n",
    "\t0.000209148698349941,\n",
    "\t0.000208954278532204,\n",
    "\t0.000208760387808217,\n",
    "\t0.000208570259528677,\n",
    "\t0.000208380131249136,\n",
    "\t0.000208193566946129,\n",
    "\t0.000208007816531995,\n",
    "\t0.000207824449975262,\n",
    "\t0.000207643421077145,\n",
    "\t0.000207463150993792,\n",
    "\t0.000207285999639121,\n",
    "\t0.00020710884828445,\n",
    "\t0.000206935107171597,\n",
    "\t0.000206761802690145,\n",
    "\t0.000206590714329373,\n",
    "\t0.000206421248619673,\n",
    "\t0.000206252815287235,\n",
    "\t0.000206087200269366,\n",
    "\t0.000205921585251497,\n",
    "\t0.000205760013058788,\n",
    "\t0.000205598614130014,\n",
    "\t0.000205440217173112,\n",
    "\t0.000205283438749239,\n",
    "\t0.000205128027190769,\n",
    "\t0.000204975262952075,\n",
    "\t0.000204822626027799,\n",
    "\t0.000204673927893311,\n",
    "\t0.000204525229758824,\n",
    "\t0.000204379501751994,\n",
    "\t0.00020423491672112,\n",
    "\t0.000204092046723699,\n",
    "\t0.000203951615162467,\n",
    "\t0.000203811664116433,\n",
    "\t0.000203675865447009,\n",
    "\t0.000203540066777586,\n",
    "\t0.000203408108477148,\n",
    "\t0.000203277143375585,\n",
    "\t0.000203148220553568,\n",
    "\t0.000203021463536072,\n",
    "\t0.000202895446517695,\n",
    "\t0.000202772891050758,\n",
    "\t0.000202650335583821,\n",
    "\t0.000202531415746654,\n",
    "\t0.000202413053974553,\n",
    "\t0.000202297027843048,\n",
    "\t0.000202182853833558,\n",
    "\t0.000202069854713078,\n",
    "\t0.000201960408012907,\n",
    "\t0.000201850961312736,\n",
    "\t0.00020174599791707,\n",
    "\t0.000201641324068154,\n",
    "\t0.000201539371328492,\n",
    "\t0.000201439015761654,\n",
    "\t0.000201340085857401,\n",
    "\t0.000201244172446545,\n",
    "\t0.000201148313224282,\n",
    "\t0.000201057014875246,\n",
    "\t0.00020096571652621,\n",
    "\t0.000200877820309825,\n",
    "\t0.000200791364550719,\n",
    "\t0.000200707265618541,\n",
    "\t0.000200626801593565,\n",
    "\t0.000200546875878303,\n",
    "\t0.000200472808975743,\n",
    "\t0.000200398742073183,\n",
    "\t0.000200329639676169,\n",
    "\t0.000200261978991624,\n",
    "\t0.000200197605597393,\n",
    "\t0.000200137002918519,\n",
    "\t0.000200077621070956,\n",
    "\t0.000200024816452072,\n",
    "\t0.000199972011833189,\n",
    "\t0.000199926517229558,\n",
    "\t0.000199882339441608,\n",
    "\t0.000199844327192172,\n",
    "\t0.000199811606095788,\n",
    "\t0.00019978167545251,\n",
    "\t0.000199761141719694,\n",
    "\t0.000199740607986877,\n",
    "\t0.000199731621329111,\n",
    "\t0.000199723641969381,\n",
    "\t0.000199724098440952,\n",
    "\t0.000199729935458768,\n",
    "\t0.000199740333157121,\n",
    "\t0.000199761303342173,\n",
    "\t0.000199782273527224,\n",
    "\t0.000199819612513777,\n",
    "\t0.000199857080642749,\n",
    "\t0.000199909419283246,\n",
    "\t0.000199968660568062,\n",
    "\t0.000200035936954253,\n",
    "\t0.000200116675290393,\n",
    "\t0.000200198819625938,\n",
    "\t0.000200301329649831,\n",
    "\t0.000200403839673724,\n",
    "\t0.00020052387080789,\n",
    "\t0.000200649575876729,\n",
    "\t0.000200786254254552,\n",
    "\t0.00020093655301151,\n",
    "\t0.000201090408014908,\n",
    "\t0.000201266667370873,\n",
    "\t0.000201442926726838,\n",
    "\t0.00020164693178497,\n",
    "\t0.000201856708998635,\n",
    "\t0.000202082315666143,\n",
    "\t0.000202322616845521,\n",
    "\t0.000202569298123725,\n",
    "\t0.00020284006874703,\n",
    "\t0.000203110839370335,\n",
    "\t0.000203409986717346,\n",
    "\t0.000203712280179377,\n",
    "\t0.000204033785043592,\n",
    "\t0.000204368585215941,\n",
    "\t0.000204712803032829,\n",
    "\t0.000205081022763226,\n",
    "\t0.000205449242493624,\n",
    "\t0.000205858843705319,\n",
    "\t0.000206269610592543,\n",
    "\t0.000206704366648947,\n",
    "\t0.000207151294201218,\n",
    "\t0.000207610972814619,\n",
    "\t0.000208093899219027,\n",
    "\t0.000208578467946045,\n",
    "\t0.000209097882474504,\n",
    "\t0.000209617297002964,\n",
    "\t0.000210163855596826,\n",
    "\t0.000210720160065236,\n",
    "\t0.000211292334392631,\n",
    "\t0.000211885838392886,\n",
    "\t0.000212484853947511,\n",
    "\t0.000213125298019156,\n",
    "\t0.000213765742090802,\n",
    "\t0.000214436342965348,\n",
    "\t0.00021511409969143,\n",
    "\t0.000215810317775301,\n",
    "\t0.000216525069614443,\n",
    "\t0.000217246742629908,\n",
    "\t0.000217997955511329,\n",
    "\t0.00021874916839275,\n",
    "\t0.000219531803703943,\n",
    "\t0.000220318698500062,\n",
    "\t0.000221125776118586,\n",
    "\t0.000221947990853914,\n",
    "\t0.000222780702994833,\n",
    "\t0.000223642948881333,\n",
    "\t0.000224505194767833,\n",
    "\t0.000225395531828983,\n",
    "\t0.000226287253260136,\n",
    "\t0.000227196000586105,\n",
    "\t0.000228114166492185,\n",
    "\t0.000229039960001378,\n",
    "\t0.000229980919505062,\n",
    "\t0.000230922349020108,\n",
    "\t0.00023188178358581,\n",
    "\t0.000232841218151512,\n",
    "\t0.000233810417976696,\n",
    "\t0.00023478348655222,\n",
    "\t0.000235758799559412,\n",
    "\t0.000236737382099275,\n",
    "\t0.000237715437531063,\n",
    "\t0.000238688633026399,\n",
    "\t0.000239661828521735,\n",
    "\t0.00024062359010255,\n",
    "\t0.000241582287507762,\n",
    "\t0.000242529187859978,\n",
    "\t0.000243463279983227,\n",
    "\t0.000244391292284552,\n",
    "\t0.000245289674189373,\n",
    "\t0.000246188056094193,\n",
    "\t0.000247041563206147,\n",
    "\t0.000247887829158342,\n",
    "\t0.000248689811368131,\n",
    "\t0.000249455832666534,\n",
    "\t0.000250200947164118,\n",
    "\t0.00025088081221076,\n",
    "\t0.000251560677257402,\n",
    "\t0.000252145254830345,\n",
    "\t0.00025272304042403,\n",
    "\t0.000253226844667207,\n",
    "\t0.000253686120950204,\n",
    "\t0.00025410276062841,\n",
    "\t0.000254426712035328,\n",
    "\t0.000254749680232945,\n",
    "\t0.000254906158322218,\n",
    "\t0.000255062636411492,\n",
    "\t0.000255074786952388,\n",
    "\t0.000255024098251716,\n",
    "\t0.000254897104726594,\n",
    "\t0.000254649487918376,\n",
    "\t0.00025438527813475,\n",
    "\t0.000253925686065691,\n",
    "\t0.000253466093996631,\n",
    "\t0.000252832809555784,\n",
    "\t0.000252147373206868,\n",
    "\t0.000251352324908476,\n",
    "\t0.000250428540873678,\n",
    "\t0.000249463017922746,\n",
    "\t0.000248262239262703,\n",
    "\t0.00024706146060266,\n",
    "\t0.000245608930612002,\n",
    "\t0.000244108977696273,\n",
    "\t0.000242467896104711,\n",
    "\t0.000240702808066369,\n",
    "\t0.000238878433365831,\n",
    "\t0.000236847595464494,\n",
    "\t0.000234816757563156,\n",
    "\t0.00023254494552298,\n",
    "\t0.000230250429281286,\n",
    "\t0.000227799376482782,\n",
    "\t0.000225245992342934,\n",
    "\t0.000222609597455605,\n",
    "\t0.000219775296150444,\n",
    "\t0.000216940994845282,\n",
    "\t0.000213848306558107,\n",
    "\t0.000210752029562849,\n",
    "\t0.00020751151176366,\n",
    "\t0.000204202208609996,\n",
    "\t0.000200821681705679,\n",
    "\t0.000197318786336146,\n",
    "\t0.000193805789483209])\n",
    "else:\n",
    "    # read file into np.array. File has one element on each new line.\n",
    "    print(\"Loading Negative Control array from file:\", emission_NEG_fn)\n",
    "    emission_NEG_array = np.loadtxt(emission_NEG_fn, delimiter='\\n')\n",
    "print(\"emission_NEG_array: \",emission_NEG_array[0:3])\n",
    "if emission_PGC_fn is None:\n",
    "    print(\"Using default Positive Control array...\")\n",
    "    mission_PGC_array = np.array([0.000218394723023875,\n",
    "\t0.000222978465201427,\n",
    "\t0.000227555235612708,\n",
    "\t0.000232042256740602,\n",
    "\t0.000236477039895504,\n",
    "\t0.000240859072758921,\n",
    "\t0.000245130489859589,\n",
    "\t0.000249399045315033,\n",
    "\t0.00025346156231431,\n",
    "\t0.000257524079313588,\n",
    "\t0.000261423437111359,\n",
    "\t0.000265254358688498,\n",
    "\t0.000268998994625546,\n",
    "\t0.000272611639162654,\n",
    "\t0.000276204832147616,\n",
    "\t0.000279591573431396,\n",
    "\t0.000282978314715176,\n",
    "\t0.000286185689607641,\n",
    "\t0.000289341558785724,\n",
    "\t0.000292388346455785,\n",
    "\t0.000295310991154374,\n",
    "\t0.000298191360734618,\n",
    "\t0.000300847306847099,\n",
    "\t0.000303503252959581,\n",
    "\t0.000305946590457699,\n",
    "\t0.000308352207072622,\n",
    "\t0.000310640154361108,\n",
    "\t0.000312827912186866,\n",
    "\t0.000314967507226035,\n",
    "\t0.00031694669569631,\n",
    "\t0.000318925884166584,\n",
    "\t0.000320722603463631,\n",
    "\t0.000322503826440235,\n",
    "\t0.000324170972597796,\n",
    "\t0.000325765955144589,\n",
    "\t0.000327300402987551,\n",
    "\t0.000328695816284937,\n",
    "\t0.000330091229582323,\n",
    "\t0.000331322025577324,\n",
    "\t0.000332551849422903,\n",
    "\t0.000333687359540695,\n",
    "\t0.000334779485343705,\n",
    "\t0.000335824760758471,\n",
    "\t0.000336792196205269,\n",
    "\t0.000337752168342873,\n",
    "\t0.000338607434642682,\n",
    "\t0.000339462700942492,\n",
    "\t0.000340242041926293,\n",
    "\t0.00034099705541866,\n",
    "\t0.000341705810617947,\n",
    "\t0.000342357601456322,\n",
    "\t0.000342998463121428,\n",
    "\t0.000343571594698672,\n",
    "\t0.000344144726275916,\n",
    "\t0.000344663695707856,\n",
    "\t0.000345171551680735,\n",
    "\t0.000345649015912825,\n",
    "\t0.000346098487752084,\n",
    "\t0.000346536881064369,\n",
    "\t0.000346933935058412,\n",
    "\t0.000347330989052454,\n",
    "\t0.000347685332563613,\n",
    "\t0.000348035043701356,\n",
    "\t0.000348354352914425,\n",
    "\t0.0003486527921594,\n",
    "\t0.000348939900488314,\n",
    "\t0.000349198408022205,\n",
    "\t0.000349456915556097,\n",
    "\t0.000349680405036328,\n",
    "\t0.00034990298039066,\n",
    "\t0.000350102932050512,\n",
    "\t0.000350291505322845,\n",
    "\t0.000350468591594087,\n",
    "\t0.000350624912901818,\n",
    "\t0.000350779797406728,\n",
    "\t0.000350905526787732,\n",
    "\t0.000351031256168735,\n",
    "\t0.0003511312685659,\n",
    "\t0.000351222140178359,\n",
    "\t0.000351301922414389,\n",
    "\t0.000351366918815182,\n",
    "\t0.000351429231461231,\n",
    "\t0.00035147174591655,\n",
    "\t0.000351514260371869,\n",
    "\t0.000351541249832706,\n",
    "\t0.000351564601794836,\n",
    "\t0.000351580303151066,\n",
    "\t0.000351588383786576,\n",
    "\t0.000351594400336041,\n",
    "\t0.00035159171823718,\n",
    "\t0.000351589036138319,\n",
    "\t0.000351582688323641,\n",
    "\t0.000351575852830583,\n",
    "\t0.00035157224358863,\n",
    "\t0.000351571034765586,\n",
    "\t0.000351573367225251,\n",
    "\t0.00035158556281305,\n",
    "\t0.000351597758400849,\n",
    "\t0.000351630877968515,\n",
    "\t0.000351664983707363,\n",
    "\t0.000351719192014218,\n",
    "\t0.00035178442627504,\n",
    "\t0.000351863877322186,\n",
    "\t0.000351971348663306,\n",
    "\t0.000352080714295238,\n",
    "\t0.000352257327250995,\n",
    "\t0.000352433940206751,\n",
    "\t0.000352661934264006,\n",
    "\t0.000352910088753456,\n",
    "\t0.000353192630472559,\n",
    "\t0.000353524860915848,\n",
    "\t0.000353866773759944,\n",
    "\t0.000354296018062288,\n",
    "\t0.000354725262364633,\n",
    "\t0.000355241714232557,\n",
    "\t0.000355781263153741,\n",
    "\t0.000356383393728729,\n",
    "\t0.000357052939499887,\n",
    "\t0.00035775241829841,\n",
    "\t0.000358595779717108,\n",
    "\t0.000359439141135807,\n",
    "\t0.000360421662612702,\n",
    "\t0.000361426272987725,\n",
    "\t0.000362527833131959,\n",
    "\t0.000363707501217466,\n",
    "\t0.000364933313022451,\n",
    "\t0.000366301616633184,\n",
    "\t0.000367669920243917,\n",
    "\t0.000369227091459414,\n",
    "\t0.000370797301651808,\n",
    "\t0.00037251128922475,\n",
    "\t0.000374311093921705,\n",
    "\t0.00037619847690961,\n",
    "\t0.000378274531647771,\n",
    "\t0.000380352507520218,\n",
    "\t0.000382673987163385,\n",
    "\t0.000384995466806552,\n",
    "\t0.000387495041689219,\n",
    "\t0.000390071441969319,\n",
    "\t0.000392750388190677,\n",
    "\t0.000395590109958127,\n",
    "\t0.000398451538494382,\n",
    "\t0.000401561800721822,\n",
    "\t0.000404672062949262,\n",
    "\t0.000408013982270395,\n",
    "\t0.000411424693165034,\n",
    "\t0.00041498755249337,\n",
    "\t0.000418727703428767,\n",
    "\t0.000422511009486048,\n",
    "\t0.000426533881796863,\n",
    "\t0.000430556754107679,\n",
    "\t0.000434816908899611,\n",
    "\t0.00043912110684349,\n",
    "\t0.000443573871068032,\n",
    "\t0.000448156154614176,\n",
    "\t0.000452799868440732,\n",
    "\t0.000457655116102447,\n",
    "\t0.000462510363764161,\n",
    "\t0.000467633555107455,\n",
    "\t0.000472781365207816,\n",
    "\t0.000478103726356244,\n",
    "\t0.000483539264151966,\n",
    "\t0.000489044531073141,\n",
    "\t0.000494714487178773,\n",
    "\t0.000500384443284405,\n",
    "\t0.000506270514427137,\n",
    "\t0.00051215915327328,\n",
    "\t0.000518184431539964,\n",
    "\t0.000524274288668231,\n",
    "\t0.000530431498930391,\n",
    "\t0.000536703458976223,\n",
    "\t0.000542986191924638,\n",
    "\t0.000549435731106588,\n",
    "\t0.000555885270288539,\n",
    "\t0.00056244211671957,\n",
    "\t0.000569034452787666,\n",
    "\t0.000575670102389663,\n",
    "\t0.000582360373041047,\n",
    "\t0.00058906029933304,\n",
    "\t0.000595823129283702,\n",
    "\t0.000602585959234365,\n",
    "\t0.000609387289704112,\n",
    "\t0.000616196850688627,\n",
    "\t0.000623017018977834,\n",
    "\t0.000629847189592377,\n",
    "\t0.000636674815598702,\n",
    "\t0.000643492602453251,\n",
    "\t0.000650310389307799,\n",
    "\t0.000657083288111348,\n",
    "\t0.000663850992446114,\n",
    "\t0.000670575539320763,\n",
    "\t0.000677269732115129,\n",
    "\t0.00068393694657477,\n",
    "\t0.000690534055361778,\n",
    "\t0.000697131164148786,\n",
    "\t0.000703612482952332,\n",
    "\t0.000710090059049341,\n",
    "\t0.000716474820420568,\n",
    "\t0.000722811659648513,\n",
    "\t0.000729081101137085,\n",
    "\t0.000735225536762686,\n",
    "\t0.000741361632973395,\n",
    "\t0.000747303142836627,\n",
    "\t0.00075324465269986,\n",
    "\t0.000759037728611016,\n",
    "\t0.000764776431657374,\n",
    "\t0.0007704243176846,\n",
    "\t0.000775948171208844,\n",
    "\t0.000781446480169015,\n",
    "\t0.000786745277275307,\n",
    "\t0.0007920440743816,\n",
    "\t0.000797155130133698,\n",
    "\t0.000802220506724191,\n",
    "\t0.000807143865374689,\n",
    "\t0.000811922399406534,\n",
    "\t0.000816652484961936,\n",
    "\t0.000821170417188913,\n",
    "\t0.000825688349415891,\n",
    "\t0.000829990924892564,\n",
    "\t0.000834263215826226,\n",
    "\t0.000838396723650937,\n",
    "\t0.000842424469175378,\n",
    "\t0.000846389693983552,\n",
    "\t0.000850175408553351,\n",
    "\t0.000853961123123149,\n",
    "\t0.00085752077684141,\n",
    "\t0.000861068311833609,\n",
    "\t0.000864436510307483,\n",
    "\t0.000867703797712943,\n",
    "\t0.000870894638391669,\n",
    "\t0.000873930776226943,\n",
    "\t0.000876962325502774,\n",
    "\t0.000879785303894813,\n",
    "\t0.000882608282286852,\n",
    "\t0.000885285577662803,\n",
    "\t0.000887904039513025,\n",
    "\t0.000890443689823824,\n",
    "\t0.00089286665314664,\n",
    "\t0.000895272124559082,\n",
    "\t0.000897508871919366,\n",
    "\t0.00089974561927965,\n",
    "\t0.000901813936052142,\n",
    "\t0.000903836049920251,\n",
    "\t0.000905779619472603,\n",
    "\t0.000907636564018275,\n",
    "\t0.000909468061593912,\n",
    "\t0.000911172024943254,\n",
    "\t0.000912875988292595,\n",
    "\t0.00091445560642861,\n",
    "\t0.000916014500362404,\n",
    "\t0.000917497882505659,\n",
    "\t0.00091891896742172,\n",
    "\t0.000920308710335504,\n",
    "\t0.000921598518504327,\n",
    "\t0.00092288832667315,\n",
    "\t0.000924033142749313,\n",
    "\t0.000925166969951085,\n",
    "\t0.000926226866763281,\n",
    "\t0.000927241519109932,\n",
    "\t0.00092821979320619,\n",
    "\t0.000929117531804726,\n",
    "\t0.000930015044059764,\n",
    "\t0.000930797121131362,\n",
    "\t0.000931579198202959,\n",
    "\t0.000932281330170592,\n",
    "\t0.000932948006258782,\n",
    "\t0.000933569173117553,\n",
    "\t0.000934117200143328,\n",
    "\t0.000934654305182335,\n",
    "\t0.000935055460228136,\n",
    "\t0.000935456615273937,\n",
    "\t0.000935763390724809,\n",
    "\t0.000936041200571098,\n",
    "\t0.00093626084924554,\n",
    "\t0.000936411103903786,\n",
    "\t0.000936541953841058,\n",
    "\t0.000936559997667074,\n",
    "\t0.000936578041493089,\n",
    "\t0.00093648115659902,\n",
    "\t0.000936361984126056,\n",
    "\t0.00093616416073995,\n",
    "\t0.000935896134131372,\n",
    "\t0.000935588264732996,\n",
    "\t0.000935138455395966,\n",
    "\t0.000934688646058935,\n",
    "\t0.000934096579935242,\n",
    "\t0.000933490441097169,\n",
    "\t0.000932787712683358,\n",
    "\t0.000932020800968407,\n",
    "\t0.000931205878566927,\n",
    "\t0.000930274312752543,\n",
    "\t0.000929342746938158,\n",
    "\t0.000928246243065466,\n",
    "\t0.000927146782135951,\n",
    "\t0.000925925208964196,\n",
    "\t0.000924644354558562,\n",
    "\t0.000923288912725867,\n",
    "\t0.000921803143399651,\n",
    "\t0.000920307974493325,\n",
    "\t0.000918650662830106,\n",
    "\t0.000916993351166887,\n",
    "\t0.0009152094354125,\n",
    "\t0.000913382321411783,\n",
    "\t0.000911482304643578,\n",
    "\t0.000909488135422295,\n",
    "\t0.000907473192949486,\n",
    "\t0.000905315714012363,\n",
    "\t0.00090315823507524,\n",
    "\t0.000900860641103677,\n",
    "\t0.000898531873212969,\n",
    "\t0.000896114147362088,\n",
    "\t0.000893610542865042,\n",
    "\t0.000891078966862419,\n",
    "\t0.000888435230607041,\n",
    "\t0.000885791494351664,\n",
    "\t0.00088303055955132,\n",
    "\t0.000880255200314673,\n",
    "\t0.000877408637360903,\n",
    "\t0.0008745107693719,\n",
    "\t0.000871582203438364,\n",
    "\t0.00086857148185471,\n",
    "\t0.000865560760271057,\n",
    "\t0.000862442235678448,\n",
    "\t0.000859319547961511,\n",
    "\t0.000856131612852902,\n",
    "\t0.000852909100533736,\n",
    "\t0.000849661175678016,\n",
    "\t0.000846364880369195,\n",
    "\t0.000843066372575779,\n",
    "\t0.000839707267287603,\n",
    "\t0.000836348161999428,\n",
    "\t0.000832951387900238,\n",
    "\t0.000829540399155383,\n",
    "\t0.00082611229807238,\n",
    "\t0.000822660255783891,\n",
    "\t0.000819204740267805,\n",
    "\t0.000815720428537459,\n",
    "\t0.000812236116807113,\n",
    "\t0.000808738418615214,\n",
    "\t0.000805237340997972,\n",
    "\t0.00080173458505071,\n",
    "\t0.000798230076926907,\n",
    "\t0.00079472662769909,\n",
    "\t0.000791228001051902,\n",
    "\t0.000787729374404713,\n",
    "\t0.000784243646654687,\n",
    "\t0.00078075983199952,\n",
    "\t0.000777289130977434,\n",
    "\t0.000773828664986046,\n",
    "\t0.000770377756776348,\n",
    "\t0.000766955151455066,\n",
    "\t0.000763532546133784,\n",
    "\t0.000760154654254514,\n",
    "\t0.000756779452603083,\n",
    "\t0.000753436291569518,\n",
    "\t0.000750111623114781,\n",
    "\t0.000746805389943305,\n",
    "\t0.000743537471065353,\n",
    "\t0.000740270531434388,\n",
    "\t0.000737065161957817,\n",
    "\t0.000733859792481245,\n",
    "\t0.000730702415869147,\n",
    "\t0.000727564980890488,\n",
    "\t0.000724461886173006,\n",
    "\t0.00072141089392076,\n",
    "\t0.000718367500589473,\n",
    "\t0.000715402798306326,\n",
    "\t0.00071243809602318,\n",
    "\t0.000709538448109523,\n",
    "\t0.0007066572704063,\n",
    "\t0.000703816785435171,\n",
    "\t0.000701022249340699,\n",
    "\t0.00069824205199385,\n",
    "\t0.000695536870777856,\n",
    "\t0.000692831689561863,\n",
    "\t0.000690204540288889,\n",
    "\t0.000687591024182971,\n",
    "\t0.000685038135088265,\n",
    "\t0.000682536461591478,\n",
    "\t0.000680058425289435,\n",
    "\t0.000677658252687724,\n",
    "\t0.000675258080086013,\n",
    "\t0.000672946596691343,\n",
    "\t0.000670642441430305,\n",
    "\t0.000668397370152596,\n",
    "\t0.000666189367488631,\n",
    "\t0.000664010649210049,\n",
    "\t0.000661898571680254,\n",
    "\t0.000659786494150459,\n",
    "\t0.000657769388226167,\n",
    "\t0.000655752655471249,\n",
    "\t0.00065381422052602,\n",
    "\t0.000651911476890052,\n",
    "\t0.000650044947545228,\n",
    "\t0.000648238087343326,\n",
    "\t0.000646437388905121,\n",
    "\t0.000644720490453201,\n",
    "\t0.000643003592001282,\n",
    "\t0.000641353380258228,\n",
    "\t0.000639724308889376,\n",
    "\t0.00063813372187888,\n",
    "\t0.000636590152714898,\n",
    "\t0.000635058279575077,\n",
    "\t0.000633597719693693,\n",
    "\t0.000632137159812308,\n",
    "\t0.000630758190380054,\n",
    "\t0.000629395731015154,\n",
    "\t0.000628073694572396,\n",
    "\t0.0006267885989274,\n",
    "\t0.000625519040278574,\n",
    "\t0.000624306783211952,\n",
    "\t0.000623094526145331,\n",
    "\t0.00062194557330668,\n",
    "\t0.000620803334552815,\n",
    "\t0.000619701090730542,\n",
    "\t0.000618626080299188,\n",
    "\t0.000617569494336162,\n",
    "\t0.000616558969543957,\n",
    "\t0.000615548444751751,\n",
    "\t0.000614612644878304,\n",
    "\t0.00061367864199689,\n",
    "\t0.000612783096373646,\n",
    "\t0.000611906722990546,\n",
    "\t0.000611050160545483,\n",
    "\t0.000610229106229797,\n",
    "\t0.000609410650973877,\n",
    "\t0.000608642721439824,\n",
    "\t0.00060787479190577,\n",
    "\t0.000607144622295878,\n",
    "\t0.000606427738585229,\n",
    "\t0.000605732412931554,\n",
    "\t0.000605065602707787,\n",
    "\t0.000604405772218546,\n",
    "\t0.000603796488516437,\n",
    "\t0.000603187204814327,\n",
    "\t0.000602614012411541,\n",
    "\t0.000602049168839442,\n",
    "\t0.000601505899700324,\n",
    "\t0.000600983953152558,\n",
    "\t0.000600470029609514,\n",
    "\t0.000599989494732583,\n",
    "\t0.000599508959855652,\n",
    "\t0.000599063783974608,\n",
    "\t0.000598623223537716,\n",
    "\t0.000598205754717613,\n",
    "\t0.0005978053297099,\n",
    "\t0.000597416736030119,\n",
    "\t0.000597060765497211,\n",
    "\t0.000596704794964303,\n",
    "\t0.000596382212131776,\n",
    "\t0.000596061131404379,\n",
    "\t0.000595761745640372,\n",
    "\t0.000595474157741653,\n",
    "\t0.000595197426671371,\n",
    "\t0.000594941907208094,\n",
    "\t0.000594687286349716,\n",
    "\t0.000594462379360029,\n",
    "\t0.000594237472370342,\n",
    "\t0.000594034773035221,\n",
    "\t0.000593840703305,\n",
    "\t0.000593659802593801,\n",
    "\t0.000593497777476533,\n",
    "\t0.00059333828213923,\n",
    "\t0.000593201116975081,\n",
    "\t0.000593063951810931,\n",
    "\t0.000592945168400905,\n",
    "\t0.000592831196017883,\n",
    "\t0.000592727614919201,\n",
    "\t0.000592635140294471,\n",
    "\t0.000592546110701244,\n",
    "\t0.000592473415976495,\n",
    "\t0.000592400721251746,\n",
    "\t0.000592344776058365,\n",
    "\t0.000592291445610832,\n",
    "\t0.000592248202997888,\n",
    "\t0.000592213023548508,\n",
    "\t0.000592181148339416,\n",
    "\t0.000592159369420095,\n",
    "\t0.000592137590500773,\n",
    "\t0.000592126800707829,\n",
    "\t0.000592116745052764,\n",
    "\t0.000592113027440863,\n",
    "\t0.000592113061318685,\n",
    "\t0.000592115808770647,\n",
    "\t0.00059212434962016,\n",
    "\t0.000592132963919072,\n",
    "\t0.000592149011297151,\n",
    "\t0.000592165058675231,\n",
    "\t0.00059218506429714,\n",
    "\t0.000592206761570742,\n",
    "\t0.00059222979406909,\n",
    "\t0.000592254902841916,\n",
    "\t0.000592280199499509,\n",
    "\t0.000592307594203675,\n",
    "\t0.00059233498890784,\n",
    "\t0.000592363361839817,\n",
    "\t0.000592392022048822,\n",
    "\t0.00059242084602359,\n",
    "\t0.000592449859330927,\n",
    "\t0.000592478776580631,\n",
    "\t0.000592507168553147,\n",
    "\t0.000592535560525664,\n",
    "\t0.000592562660181378,\n",
    "\t0.000592589523510639,\n",
    "\t0.000592615391265963,\n",
    "\t0.000592640397886167,\n",
    "\t0.000592664917198194,\n",
    "\t0.000592687777141864,\n",
    "\t0.000592710637085533,\n",
    "\t0.000592731357244762,\n",
    "\t0.000592751885781504,\n",
    "\t0.00059277094518019,\n",
    "\t0.00059278905979556,\n",
    "\t0.000592806361907693,\n",
    "\t0.000592821762868463,\n",
    "\t0.000592837163829234,\n",
    "\t0.000592850237690147,\n",
    "\t0.000592863288556002,\n",
    "\t0.000592875123394168,\n",
    "\t0.000592886388656147,\n",
    "\t0.000592897131039091,\n",
    "\t0.000592906990031666,\n",
    "\t0.000592916788553247,\n",
    "\t0.000592925681899632,\n",
    "\t0.000592934575246018,\n",
    "\t0.000592943115622039,\n",
    "\t0.000592951540480486,\n",
    "\t0.000592960081058767,\n",
    "\t0.000592968766414283,\n",
    "\t0.000592977606857003,\n",
    "\t0.000592987440756889,\n",
    "\t0.000592997274656774,\n",
    "\t0.000593008599023902,\n",
    "\t0.000593020237731278,\n",
    "\t0.000593033169367441,\n",
    "\t0.000593047310675612,\n",
    "\t0.000593062119970463,\n",
    "\t0.000593079481478572,\n",
    "\t0.000593096842986681,\n",
    "\t0.00059311775683079,\n",
    "\t0.000593139073118193,\n",
    "\t0.000593163828504855,\n",
    "\t0.000593190983263093,\n",
    "\t0.000593219902557034,\n",
    "\t0.000593253362754043,\n",
    "\t0.000593286822951052,\n",
    "\t0.00059332685047335,\n",
    "\t0.000593367076604277,\n",
    "\t0.000593412291057477,\n",
    "\t0.000593460058705214,\n",
    "\t0.000593510756564866,\n",
    "\t0.000593566842758649,\n",
    "\t0.000593623320565975,\n",
    "\t0.000593688502874333,\n",
    "\t0.000593753685182692,\n",
    "\t0.000593827851214409,\n",
    "\t0.00059390527534313,\n",
    "\t0.000593987627763663,\n",
    "\t0.000594076657224713,\n",
    "\t0.000594167065531452,\n",
    "\t0.000594268037198389,\n",
    "\t0.000594369008865326,\n",
    "\t0.000594480222438743,\n",
    "\t0.000594593897052795,\n",
    "\t0.000594714260684274,\n",
    "\t0.000594841392337322,\n",
    "\t0.00059497119236053,\n",
    "\t0.000595112527525574,\n",
    "\t0.000595253862690618,\n",
    "\t0.000595411641843845,\n",
    "\t0.000595571691659137,\n",
    "\t0.000595741117782164,\n",
    "\t0.000595917632522721,\n",
    "\t0.000596098527511738,\n",
    "\t0.000596291871627953,\n",
    "\t0.000596485215744169,\n",
    "\t0.000596695200324178,\n",
    "\t0.000596906040895123,\n",
    "\t0.000597128526129575,\n",
    "\t0.000597357508234215,\n",
    "\t0.000597592732321052,\n",
    "\t0.000597840477089824,\n",
    "\t0.000598088797854742,\n",
    "\t0.000598361070459372,\n",
    "\t0.000598633343064003,\n",
    "\t0.000598920079429399,\n",
    "\t0.000599212601299102,\n",
    "\t0.000599513517632457,\n",
    "\t0.000599826762067023,\n",
    "\t0.000600142035138305,\n",
    "\t0.000600476435355776,\n",
    "\t0.000600810835573247,\n",
    "\t0.000601162187075376,\n",
    "\t0.000601518134821354,\n",
    "\t0.000601884764312424,\n",
    "\t0.000602263082434395,\n",
    "\t0.000602646027435377,\n",
    "\t0.000603051837826823,\n",
    "\t0.000603457648218269,\n",
    "\t0.000603882833388092,\n",
    "\t0.000604311196198104,\n",
    "\t0.000604752038390955,\n",
    "\t0.000605203095025277,\n",
    "\t0.000605659637664174,\n",
    "\t0.0006061334857484,\n",
    "\t0.000606607333832627,\n",
    "\t0.000607102463829317,\n",
    "\t0.000607599158672513,\n",
    "\t0.000608110684005642,\n",
    "\t0.000608631210233605,\n",
    "\t0.000609160394462458,\n",
    "\t0.00060970857218076,\n",
    "\t0.00061025683868796,\n",
    "\t0.000610827701969662,\n",
    "\t0.000611398565251363,\n",
    "\t0.000611985103359496,\n",
    "\t0.000612578529560371,\n",
    "\t0.000613180600633589,\n",
    "\t0.000613796450843435,\n",
    "\t0.000614414000924118,\n",
    "\t0.000615052123800565,\n",
    "\t0.000615690246677013,\n",
    "\t0.000616346404411806,\n",
    "\t0.000617008037014311,\n",
    "\t0.000617681610837895,\n",
    "\t0.000618369320209508,\n",
    "\t0.000619060257335116,\n",
    "\t0.000619769669105301,\n",
    "\t0.000620479080875487,\n",
    "\t0.000621206608104756,\n",
    "\t0.000621937597962568,\n",
    "\t0.000622679976443195,\n",
    "\t0.000623432440789044,\n",
    "\t0.000624189636083787,\n",
    "\t0.000624963494366673,\n",
    "\t0.000625737352649559,\n",
    "\t0.000626532326089027,\n",
    "\t0.000627329338545763,\n",
    "\t0.000628141395567509,\n",
    "\t0.000628963368325284,\n",
    "\t0.000629791616267602,\n",
    "\t0.000630634966487383,\n",
    "\t0.000631478316707164,\n",
    "\t0.00063234278601301,\n",
    "\t0.000633207591208892,\n",
    "\t0.000634086951766792,\n",
    "\t0.000634973315774242,\n",
    "\t0.000635867616157656,\n",
    "\t0.000636775667267338,\n",
    "\t0.000637685091154005,\n",
    "\t0.000638617331540917,\n",
    "\t0.000639549571927829,\n",
    "\t0.000640500795643636,\n",
    "\t0.000641458429960038,\n",
    "\t0.000642425853336763,\n",
    "\t0.000643405818947027,\n",
    "\t0.000644388693738128,\n",
    "\t0.000645391183460628,\n",
    "\t0.000646393673183129,\n",
    "\t0.00064741478645544,\n",
    "\t0.00064843998890814,\n",
    "\t0.000649476884714488,\n",
    "\t0.00065052498105115,\n",
    "\t0.000651578325213818,\n",
    "\t0.000652652456881438,\n",
    "\t0.000653726588549059,\n",
    "\t0.000654824140865433,\n",
    "\t0.000655924518040757,\n",
    "\t0.000657038540734793,\n",
    "\t0.000658162316769386,\n",
    "\t0.000659292524636586,\n",
    "\t0.000660439776860197,\n",
    "\t0.000661587029083808,\n",
    "\t0.000662756983452414,\n",
    "\t0.000663927766702582,\n",
    "\t0.000665113998121805,\n",
    "\t0.000666308345594024,\n",
    "\t0.00066751207476046,\n",
    "\t0.000668733507236887,\n",
    "\t0.00066995590878459,\n",
    "\t0.000671203404177973,\n",
    "\t0.000672450899571356,\n",
    "\t0.000673715525707908,\n",
    "\t0.000674986552847955,\n",
    "\t0.000676267427286205,\n",
    "\t0.000677561968301867,\n",
    "\t0.000678859085408484,\n",
    "\t0.000680177133254108,\n",
    "\t0.000681495181099732,\n",
    "\t0.000682832050822501,\n",
    "\t0.000684173614509742,\n",
    "\t0.000685528725696515,\n",
    "\t0.000686897870149738,\n",
    "\t0.000688271695422548,\n",
    "\t0.000689666559217806,\n",
    "\t0.000691061423013064,\n",
    "\t0.000692477140231675,\n",
    "\t0.000693895896626334,\n",
    "\t0.000695328252400697,\n",
    "\t0.000696771138250316,\n",
    "\t0.00069822023839242,\n",
    "\t0.00069968755119173,\n",
    "\t0.00070115486399104,\n",
    "\t0.000702645606567474,\n",
    "\t0.000704137707391858,\n",
    "\t0.000705649025642396,\n",
    "\t0.000707171342173749,\n",
    "\t0.000708702736467397,\n",
    "\t0.000710252829864218,\n",
    "\t0.00071180339575871,\n",
    "\t0.000713380316523326,\n",
    "\t0.000714957237287942,\n",
    "\t0.00071655368033604,\n",
    "\t0.000718158158799163,\n",
    "\t0.000719773953098194,\n",
    "\t0.000721406776620874,\n",
    "\t0.000723042227410737,\n",
    "\t0.00072470423644495,\n",
    "\t0.000726366245479162,\n",
    "\t0.000728056991530223,\n",
    "\t0.000729755804112328,\n",
    "\t0.000731470166148265,\n",
    "\t0.000733201948734139,\n",
    "\t0.000734939036080134,\n",
    "\t0.000736703478092782,\n",
    "\t0.00073846792010543,\n",
    "\t0.000740261067766464,\n",
    "\t0.000742059153325729,\n",
    "\t0.000743876075764799,\n",
    "\t0.000745708785948454,\n",
    "\t0.000747549855339688,\n",
    "\t0.000749418161815614,\n",
    "\t0.000751286468291541,\n",
    "\t0.000753196847242768,\n",
    "\t0.000755110606223725,\n",
    "\t0.000757048307346835,\n",
    "\t0.000759000905802841,\n",
    "\t0.000760965451607507,\n",
    "\t0.000762956936019976,\n",
    "\t0.000764948420432445,\n",
    "\t0.000766979498383766,\n",
    "\t0.000769010653969476,\n",
    "\t0.000771069646335534,\n",
    "\t0.000773141212701806,\n",
    "\t0.000775228384091265,\n",
    "\t0.000777341054410367,\n",
    "\t0.000779457434111872,\n",
    "\t0.000781622757053423,\n",
    "\t0.000783788079994974,\n",
    "\t0.000785985910860532,\n",
    "\t0.00078819393701331,\n",
    "\t0.000790421502281194,\n",
    "\t0.000792672751324963,\n",
    "\t0.000794930246744883,\n",
    "\t0.000797225220421711,\n",
    "\t0.000799520194098538,\n",
    "\t0.000801852028630546,\n",
    "\t0.000804191218028024,\n",
    "\t0.000806554359270955,\n",
    "\t0.000808939218045397,\n",
    "\t0.000811336138235996,\n",
    "\t0.000813777027770949,\n",
    "\t0.000816217917305902,\n",
    "\t0.000818700617387576,\n",
    "\t0.00082118765203997,\n",
    "\t0.0008237026146183,\n",
    "\t0.000826236439992509,\n",
    "\t0.000828783933222733,\n",
    "\t0.000831365270667848,\n",
    "\t0.000833946608112963,\n",
    "\t0.00083657522461193,\n",
    "\t0.000839204881250082,\n",
    "\t0.000841868926497476,\n",
    "\t0.000844549964946513,\n",
    "\t0.000847253165795237,\n",
    "\t0.000849995753082534,\n",
    "\t0.000852741007488514,\n",
    "\t0.000855536013916083,\n",
    "\t0.000858331020343653,\n",
    "\t0.000861166027508755,\n",
    "\t0.000864014966329145,\n",
    "\t0.000866887954210278,\n",
    "\t0.000869792499682431,\n",
    "\t0.000872704127538948,\n",
    "\t0.000875666119017602,\n",
    "\t0.000878628110496256,\n",
    "\t0.000881641763355293,\n",
    "\t0.000884667213885331,\n",
    "\t0.000887730403543006,\n",
    "\t0.000890830600949875,\n",
    "\t0.000893943704542551,\n",
    "\t0.000897109852558889,\n",
    "\t0.000900276000575227,\n",
    "\t0.000903503089578598,\n",
    "\t0.000906737981180751,\n",
    "\t0.000910014263986748,\n",
    "\t0.000913320853572508,\n",
    "\t0.000916647499598504,\n",
    "\t0.000920028898242369,\n",
    "\t0.000923410296886234,\n",
    "\t0.000926873346205817,\n",
    "\t0.000930339894840074,\n",
    "\t0.000933869674649324,\n",
    "\t0.000937433545965573,\n",
    "\t0.000941027278840493,\n",
    "\t0.000944678847033066,\n",
    "\t0.000948333281424669,\n",
    "\t0.000952076388848754,\n",
    "\t0.000955819496272838,\n",
    "\t0.000959631558690689,\n",
    "\t0.000963470156634597,\n",
    "\t0.000967349859834764,\n",
    "\t0.000971288006527242,\n",
    "\t0.000975238125670542,\n",
    "\t0.000979291704860575,\n",
    "\t0.000983345284050607,\n",
    "\t0.000987499440167837,\n",
    "\t0.000991679607559342,\n",
    "\t0.000995915889907207,\n",
    "\t0.00100021168126121,\n",
    "\t0.00100452866883133,\n",
    "\t0.00100894480759017,\n",
    "\t0.00101336094634901,\n",
    "\t0.00101788565644797,\n",
    "\t0.00102242703212508,\n",
    "\t0.00102704108531194,\n",
    "\t0.00103171277048897,\n",
    "\t0.00103642272951668,\n",
    "\t0.00104124841420309,\n",
    "\t0.00104607409888951,\n",
    "\t0.00105105260344727,\n",
    "\t0.00105604097762172,\n",
    "\t0.00106112485716333,\n",
    "\t0.00106626479420305,\n",
    "\t0.00107145552644442,\n",
    "\t0.00107675373389904,\n",
    "\t0.00108205388358284,\n",
    "\t0.00108751750422307,\n",
    "\t0.0010929811248633,\n",
    "\t0.0010985663035624,\n",
    "\t0.00110420294974046,\n",
    "\t0.00110992229276355,\n",
    "\t0.00111576917818447,\n",
    "\t0.00112163425235141,\n",
    "\t0.00112769728775417,\n",
    "\t0.00113376032315694,\n",
    "\t0.00113998384159905,\n",
    "\t0.0011462539649642,\n",
    "\t0.00115262574783751,\n",
    "\t0.00115911414014664,\n",
    "\t0.00116563858159329,\n",
    "\t0.0011723572377683,\n",
    "\t0.0011790758939433,\n",
    "\t0.00118600055168519,\n",
    "\t0.00119296231825438,\n",
    "\t0.00120008808260684,\n",
    "\t0.00120735458687513,\n",
    "\t0.00121469099641726,\n",
    "\t0.0012222628134627,\n",
    "\t0.00122983463050815,\n",
    "\t0.00123768292890811,\n",
    "\t0.00124555534589431,\n",
    "\t0.0012536226889269,\n",
    "\t0.00126181435979035,\n",
    "\t0.00127010824587836,\n",
    "\t0.00127863908544147,\n",
    "\t0.00128716992500457,\n",
    "\t0.00129605834477527,\n",
    "\t0.00130494958569159,\n",
    "\t0.00131415786849641,\n",
    "\t0.00132351331687698,\n",
    "\t0.0013330373841918,\n",
    "\t0.00134284395443312,\n",
    "\t0.00135267989943836,\n",
    "\t0.00136294133344842,\n",
    "\t0.00137320276745847,\n",
    "\t0.00138383095663463,\n",
    "\t0.00139457791367547,\n",
    "\t0.00140555615760454,\n",
    "\t0.00141682148131154,\n",
    "\t0.00142816263080447,\n",
    "\t0.00143998148274867,\n",
    "\t0.00145180033469287,\n",
    "\t0.00146422508859409,\n",
    "\t0.00147677589302065,\n",
    "\t0.00148968451343801,\n",
    "\t0.00150292529511478,\n",
    "\t0.00151631538733128,\n",
    "\t0.00153026923149225,\n",
    "\t0.00154422307565322,\n",
    "\t0.0015588661124482,\n",
    "\t0.00157358555973087,\n",
    "\t0.00158879086911441,\n",
    "\t0.00160433242081279,\n",
    "\t0.00162012273267228,\n",
    "\t0.00163654703744233,\n",
    "\t0.00165297134221239,\n",
    "\t0.00167056129536625,\n",
    "\t0.00168818408368585,\n",
    "\t0.00170653269115108,\n",
    "\t0.00172524956083767,\n",
    "\t0.00174437577825596,\n",
    "\t0.00176424832026792,\n",
    "\t0.00178417680133417,\n",
    "\t0.00180529216320438,\n",
    "\t0.00182640752507459,\n",
    "\t0.00184850498483911,\n",
    "\t0.00187095505953909,\n",
    "\t0.00189401571114457,\n",
    "\t0.00191789700051755,\n",
    "\t0.00194200954848064,\n",
    "\t0.00196786039017938,\n",
    "\t0.00199371123187812,\n",
    "\t0.00202094638583134,\n",
    "\t0.00204851002065852,\n",
    "\t0.00207698294190708,\n",
    "\t0.00210636871540614,\n",
    "\t0.00213612102092552,\n",
    "\t0.00216743770022235,\n",
    "\t0.00219875437951919,\n",
    "\t0.00223186562287322,\n",
    "\t0.002265220129355,\n",
    "\t0.00229982604316085,\n",
    "\t0.00233537051245975,\n",
    "\t0.00237164959623964,\n",
    "\t0.00240999546852203,\n",
    "\t0.00244834134080443,\n",
    "\t0.00248894817437497,\n",
    "\t0.00252966643108909,\n",
    "\t0.00257194806075135,\n",
    "\t0.00261509453502322,\n",
    "\t0.00265906570390398,\n",
    "\t0.00270467661645154,\n",
    "\t0.00275035058369873,\n",
    "\t0.00279844003097806,\n",
    "\t0.0028465294782574,\n",
    "\t0.00289645928223499,\n",
    "\t0.00294711818927611,\n",
    "\t0.00299895420706446,\n",
    "\t0.0030525049582971,\n",
    "\t0.00310627467351356,\n",
    "\t0.00316206323666071,\n",
    "\t0.00321785179980787,\n",
    "\t0.00327526927706371,\n",
    "\t0.0033331232871328,\n",
    "\t0.00339186369933182,\n",
    "\t0.00345156649098618,\n",
    "\t0.00351153936333938,\n",
    "\t0.00357282849105249,\n",
    "\t0.0036341176187656,\n",
    "\t0.00369654540506517,\n",
    "\t0.00375915692945482,\n",
    "\t0.00382224091084585,\n",
    "\t0.00388570855412807,\n",
    "\t0.00394920891838826,\n",
    "\t0.00401281140376521,\n",
    "\t0.00407641388914217,\n",
    "\t0.00413964933120217,\n",
    "\t0.00420285861084546,\n",
    "\t0.00426546685055729,\n",
    "\t0.00432771333582761,\n",
    "\t0.00438946493209569,\n",
    "\t0.00445014068270667,\n",
    "\t0.00451080183169063,\n",
    "\t0.0045689904384967,\n",
    "\t0.00462717904530277,\n",
    "\t0.0046827504948057,\n",
    "\t0.0047371824516626,\n",
    "\t0.00479003628663365,\n",
    "\t0.00484039541377503,\n",
    "\t0.00489038093094737,\n",
    "\t0.00493596719073419,\n",
    "\t0.00498155345052102,\n",
    "\t0.00502294128211577,\n",
    "\t0.00506306851695057,\n",
    "\t0.00510037945365653,\n",
    "\t0.00513438273809201,\n",
    "\t0.00516724582301342,\n",
    "\t0.00519368232885554,\n",
    "\t0.00522011883469766,\n",
    "\t0.00523922479227554,\n",
    "\t0.00525694987913387,\n",
    "\t0.0052703464034534,\n",
    "\t0.00527993952171863,\n",
    "\t0.00528763827575224,\n",
    "\t0.00528873998943537,\n",
    "\t0.0052898417031185,\n",
    "\t0.00528293164700858,\n",
    "\t0.00527526673464406,\n",
    "\t0.00526218784906826,\n",
    "\t0.00524556973181389,\n",
    "\t0.00522594188059144,\n",
    "\t0.00519913850467686,\n",
    "\t0.00517233512876227,\n",
    "\t0.00513561008799325,\n",
    "\t0.00509874724632348,\n",
    "\t0.00505603020799238,\n",
    "\t0.00501052142847885,\n",
    "\t0.00496196266345832,\n",
    "\t0.0049081637637635,\n",
    "\t0.00485390439145447])\n",
    "else:\n",
    "    # read file into np.array. File has one element on each new line.\n",
    "    print(\"Loading Positive Control array from file:\", emission_PGC_fn)\n",
    "    emission_PGC_array = np.loadtxt(emission_PGC_fn, delimiter='\\n')\n",
    "print(\"emission_PGC_array: \",emission_PGC_array[0:3])\n",
    "\n",
    "def single_fiber_nuc(read_id, group, LINK_width,NUC_width, metadata_cols, emission_NEG_array, emission_PGC_array):\n",
    "    # Initialize lists to store information about linker and nucleosome regions and their midpoints\n",
    "    regions_LINK_list = []\n",
    "    regions_NUC_list = []\n",
    "    mid_NUC_list = []\n",
    "    mid_LINK_list = []\n",
    "\n",
    "    # Calculate the total number of modified bases, total bases, and minimum base position in the read\n",
    "    MOD_BASE_NUM = len(group['rel_pos'])\n",
    "    BASE_NUM = max(group['rel_pos']) - min(group['rel_pos']) + 1\n",
    "    BASE_MIN = min(group['rel_pos'])\n",
    "\n",
    "    # Initialize the calling_vec with -1 and populate it with mod_qual values based on relative position\n",
    "    calling_vec = np.full(BASE_NUM+1, -1.0)\n",
    "    for i in range(len(group['rel_pos'])):\n",
    "        calling_vec[group.iloc[i]['rel_pos'] - BASE_MIN] = group.iloc[i]['mod_qual']\n",
    "\n",
    "    # Initialize probability matrix (prob_mat) and pointer matrix (ptr_mat)\n",
    "    # prob_mat stores log probabilities, and ptr_mat stores the previous state index for backtracking\n",
    "    prob_mat = np.zeros((BASE_NUM+1, 148))\n",
    "    ptr_mat = np.full((BASE_NUM+1, 148), -1, dtype=int)\n",
    "\n",
    "    # Initialization of first row in prob_mat and ptr_mat\n",
    "    initial_rate = 1 / 148.0\n",
    "    log_initial_rate = np.log(initial_rate)\n",
    "    prob_mat[1, :] = log_initial_rate\n",
    "    ptr_mat[1, :] = 0\n",
    "\n",
    "    # Dynamic Programming Step: Fill the prob_mat and ptr_mat\n",
    "    for i in range(2, BASE_NUM+1):\n",
    "        within_linker = 0.0\n",
    "        back_frm_ncls = 0.0\n",
    "\n",
    "        # Compute probabilities for staying within linker and going back to linker from nucleosome\n",
    "        if calling_vec[i] == -1:\n",
    "            within_linker = prob_mat[i-1, 0]\n",
    "            if prob_mat[i-1, 147] != 0:\n",
    "                back_frm_ncls = prob_mat[i-1, 147]\n",
    "        else:\n",
    "            k = int(calling_vec[i] * 1000)\n",
    "            within_linker = np.log(emission_PGC_array[k]) + prob_mat[i-1, 0]\n",
    "            if prob_mat[i-1, 147] != 0:\n",
    "                back_frm_ncls = np.log(emission_PGC_array[k]) + prob_mat[i-1, 147]\n",
    "\n",
    "        # Update first column of prob_mat and ptr_mat based on calculated probabilities\n",
    "        if back_frm_ncls != 0 and back_frm_ncls > within_linker:\n",
    "            prob_mat[i, 0] = back_frm_ncls\n",
    "            ptr_mat[i, 0] = 147\n",
    "        else:\n",
    "            prob_mat[i, 0] = within_linker\n",
    "            ptr_mat[i, 0] = 0\n",
    "\n",
    "        # Update second column of prob_mat and ptr_mat\n",
    "        if calling_vec[i] == -1:\n",
    "            prob_mat[i, 1] = prob_mat[i-1, 0]\n",
    "        else:\n",
    "            k = int(calling_vec[i] * 1000)\n",
    "            prob_mat[i, 1] = np.log(emission_NEG_array[k]) + prob_mat[i-1, 0]\n",
    "        ptr_mat[i, 1] = 0\n",
    "\n",
    "        # Update the remaining columns of prob_mat and ptr_mat\n",
    "        for j in range(2, 148):\n",
    "            if calling_vec[i] == -1:\n",
    "                if prob_mat[i-1, j-1] != 0:\n",
    "                    prob_mat[i, j] = prob_mat[i-1, j-1]\n",
    "            else:\n",
    "                k = int(calling_vec[i] * 1000)\n",
    "                if prob_mat[i-1, j-1] != 0:\n",
    "                    prob_mat[i, j] = np.log(emission_NEG_array[k]) + prob_mat[i-1, j-1]\n",
    "            if prob_mat[i, j] != 0:\n",
    "                ptr_mat[i, j] = j-1\n",
    "\n",
    "    # Backtrack to identify most probable states\n",
    "    max_index = np.argmax(prob_mat[BASE_NUM, :])\n",
    "    backtrack_vec = []\n",
    "    for i in range(BASE_NUM, 1, -1):\n",
    "        backtrack_vec.append(max_index)\n",
    "        max_index = ptr_mat[i, max_index]\n",
    "    backtrack_vec.reverse()\n",
    "\n",
    "    # Identify nucleosome and linker regions based on backtracking results\n",
    "    ncls_start = 0\n",
    "    ncls_end = 0\n",
    "    shift = min(group['rel_pos']) - 1\n",
    "    InNucleosome = False\n",
    "    for i, val in enumerate(backtrack_vec):\n",
    "        if val > 0:\n",
    "            if not InNucleosome:\n",
    "                ncls_start = i + 1 + shift\n",
    "                InNucleosome = True\n",
    "        else:\n",
    "            if InNucleosome:\n",
    "                ncls_end = i + 1 + shift\n",
    "                ncls_mid = round((ncls_end-ncls_start)/2 + ncls_start, 0)\n",
    "                if ncls_end - ncls_start < NUC_width:\n",
    "                    ncls_start = ncls_end - NUC_width\n",
    "                    ncls_mid = round((ncls_end-ncls_start)/2 + ncls_start, 0)\n",
    "                regions_NUC_list.append((ncls_start, ncls_end))\n",
    "                mid_NUC_list.append(ncls_mid)\n",
    "                InNucleosome = False\n",
    "\n",
    "    # Check if the nucleosome extends to the end of the read\n",
    "    if InNucleosome:\n",
    "        ncls_end = ncls_start + NUC_width\n",
    "        ncls_mid = round((ncls_end-ncls_start)/2 + ncls_start, 0)\n",
    "        regions_NUC_list.append((ncls_start, ncls_end))\n",
    "        mid_NUC_list.append(ncls_mid)\n",
    "\n",
    "    # Infer linker regions and their midpoints\n",
    "    for i in range(len(regions_NUC_list)-1):\n",
    "        regions_LINK_list.append((regions_NUC_list[i][1], regions_NUC_list[i+1][0]))\n",
    "        mid_LINK_list.append(round((regions_NUC_list[i][1] + regions_NUC_list[i+1][0])/2, 0))\n",
    "\n",
    "    return read_id, mid_NUC_list, mid_LINK_list, regions_LINK_list, regions_NUC_list, group.iloc[0][metadata_cols]\n",
    "\n",
    "#version that requires a nuc and linker region (does not allow nucleosomes to be stuck next to eachother)\n",
    "def single_fiber_nuc_linker(read_id, group, LINK_width,NUC_width, metadata_cols, emission_NEG_array, emission_PGC_array):\n",
    "    # Initialize lists to store information about linker and nucleosome regions and their midpoints\n",
    "    regions_LINK_list = []\n",
    "    regions_NUC_list = []\n",
    "    mid_NUC_list = []\n",
    "    mid_LINK_list = []\n",
    "    mod_qual_LINK_list = []  # List to store mod_qual values for linker regions\n",
    "    mod_qual_NUC_list = []  # List to store mod_qual values for nucleosome region\n",
    "    width = LINK_width + NUC_width + 1\n",
    "\n",
    "    # Calculate the total number of modified bases, total bases, and minimum base position in the read\n",
    "    BASE_NUM = max(group['rel_pos']) - min(group['rel_pos']) + 1\n",
    "    BASE_MIN = min(group['rel_pos'])\n",
    "\n",
    "    # Initialize the calling_vec with -1 and populate it with mod_qual values based on relative position\n",
    "    calling_vec = np.full(BASE_NUM+1, -1.0)\n",
    "    for i in range(len(group['rel_pos'])):\n",
    "        calling_vec[group.iloc[i]['rel_pos'] - BASE_MIN] = group.iloc[i]['mod_qual']\n",
    "\n",
    "\n",
    "    # Initialize probability matrix (prob_mat) and pointer matrix (ptr_mat)\n",
    "    # prob_mat stores log probabilities, and ptr_mat stores the previous state index for backtracking\n",
    "    prob_mat = np.zeros((BASE_NUM+1, width))\n",
    "    ptr_mat = np.full((BASE_NUM+1, width), -1, dtype=int)\n",
    "\n",
    "    # Initialization of first row in prob_mat and ptr_mat\n",
    "    initial_rate = 1 / width\n",
    "    log_initial_rate = np.log(initial_rate)\n",
    "    prob_mat[1, :] = log_initial_rate\n",
    "    ptr_mat[1, :] = 0\n",
    "\n",
    "    # Dynamic Programming Step: Fill the prob_mat and ptr_mat\n",
    "    #high_mod_qual_indices = [i for i, val in enumerate(calling_vec) if val > 0.8]  # Store indices of high mod_qual\n",
    "\n",
    "    #distance_threshold = 20\n",
    "    for i in range(2, BASE_NUM+1):\n",
    "        within_linker = 0.0\n",
    "        back_frm_ncls = 0.0\n",
    "\n",
    "        # Compute probabilities for staying within linker and going back to linker from nucleosome\n",
    "        if calling_vec[i] == -1:\n",
    "            within_linker = prob_mat[i-1, 0]\n",
    "            if prob_mat[i-1, width-1] != 0:\n",
    "                back_frm_ncls = prob_mat[i-1, width-1]\n",
    "        else:\n",
    "            k = int(calling_vec[i] * 1000)\n",
    "            within_linker = np.log(emission_PGC_array[k]) + prob_mat[i-1, 0]\n",
    "            if prob_mat[i-1, width-1] != 0:\n",
    "                back_frm_ncls = np.log(emission_PGC_array[k]) + prob_mat[i-1, width-1]\n",
    "        \"\"\"# Special threshold condition for linker region\n",
    "                if i in high_mod_qual_indices:\n",
    "                    for idx in high_mod_qual_indices:\n",
    "                        if abs(i - idx) < distance_threshold:  # Check distance criterion\n",
    "                            within_linker += 100  # Boost the probability value\n",
    "                            break\"\"\"\n",
    "\n",
    "        # Update first column of prob_mat and ptr_mat based on calculated probabilities\n",
    "        if back_frm_ncls != 0 and back_frm_ncls > within_linker:\n",
    "            prob_mat[i, 0] = back_frm_ncls\n",
    "            ptr_mat[i, 0] = width-1\n",
    "        else:\n",
    "            prob_mat[i, 0] = within_linker\n",
    "            ptr_mat[i, 0] = 0\n",
    "\n",
    "        # Update columns for high methylation (1-30) in the nucleosome\n",
    "        for j in range(1, LINK_width+1):\n",
    "            if calling_vec[i] == -1:\n",
    "                prob_mat[i, j] = prob_mat[i-1, j-1]\n",
    "            else:\n",
    "                k = int(calling_vec[i] * 1000)\n",
    "                prob_mat[i, j] = np.log(emission_PGC_array[k]) + prob_mat[i-1, j-1]\n",
    "                \"\"\"# Special threshold condition for linker region\n",
    "                if i in high_mod_qual_indices:\n",
    "                    for idx in high_mod_qual_indices:\n",
    "                        if abs(i - idx) < distance_threshold:  # Check distance criterion\n",
    "                            prob_mat[i, j] += 100  # Boost the probability value\n",
    "                            break\"\"\"\n",
    "            ptr_mat[i, j] = j - 1\n",
    "\n",
    "        # Update columns for low methylation (31-177) in the nucleosome\n",
    "        for j in range(LINK_width+1, width):\n",
    "            if calling_vec[i] == -1:\n",
    "                if prob_mat[i-1, j-1] != 0:\n",
    "                    prob_mat[i, j] = prob_mat[i-1, j-1]\n",
    "            else:\n",
    "                k = int(calling_vec[i] * 1000)\n",
    "                if prob_mat[i-1, j-1] != 0:\n",
    "                    prob_mat[i, j] = np.log(emission_NEG_array[k]) + prob_mat[i-1, j-1]\n",
    "            if prob_mat[i, j] != 0:\n",
    "                ptr_mat[i, j] = j-1\n",
    "\n",
    "    # Backtrack to identify most probable states\n",
    "    current_qual_LINK = []\n",
    "    current_qual_NUC = []\n",
    "    max_index = np.argmax(prob_mat[BASE_NUM, :])\n",
    "    backtrack_vec = []\n",
    "    for i in range(BASE_NUM, 1, -1):\n",
    "        backtrack_vec.append(max_index)\n",
    "        ### STORE MOD-qual values for nuc and link regions\n",
    "        if max_index > 0:  # In nucleosome\n",
    "            if calling_vec[i-1] != -1:\n",
    "                current_qual_NUC.append(calling_vec[i-1])\n",
    "        else:  # In linker\n",
    "            if calling_vec[i-1] != -1:\n",
    "                current_qual_LINK.append(calling_vec[i-1])\n",
    "\n",
    "        if len(current_qual_NUC) > 0 and (max_index == 0 or i == 2):\n",
    "            mod_qual_NUC_list.append(np.mean(current_qual_NUC))\n",
    "            current_qual_NUC = []\n",
    "\n",
    "        if len(current_qual_LINK) > 0 and (max_index > 0 or i == 2):\n",
    "            mod_qual_LINK_list.append(np.mean(current_qual_LINK))\n",
    "            current_qual_LINK = []\n",
    "        max_index = ptr_mat[i, max_index]\n",
    "    backtrack_vec.reverse()\n",
    "\n",
    "    # Identify nucleosome and linker regions based on backtracking results\n",
    "    ncls_start = 0\n",
    "    ncls_end = 0\n",
    "    shift = min(group['rel_pos']) - 1\n",
    "    InNucleosome = False\n",
    "    for i, val in enumerate(backtrack_vec):\n",
    "        if val > 0:\n",
    "            if not InNucleosome:\n",
    "                ncls_start = i + 1 + shift\n",
    "                InNucleosome = True\n",
    "        else:\n",
    "            if InNucleosome:\n",
    "                ncls_end = i + 1 + shift\n",
    "                ncls_mid = round((ncls_end-ncls_start)/2 + ncls_start, 0)\n",
    "                if ncls_end - ncls_start < NUC_width:\n",
    "                    ncls_start = ncls_end - NUC_width\n",
    "                    ncls_mid = round((ncls_end-ncls_start)/2 + ncls_start, 0)\n",
    "                regions_NUC_list.append((ncls_start, ncls_end))\n",
    "                mid_NUC_list.append(ncls_mid)\n",
    "                InNucleosome = False\n",
    "\n",
    "    # Check if the nucleosome extends to the end of the read\n",
    "    if InNucleosome:\n",
    "        ncls_end = ncls_start + NUC_width\n",
    "        ncls_mid = round((ncls_end-ncls_start)/2 + ncls_start, 0)\n",
    "        regions_NUC_list.append((ncls_start, ncls_end))\n",
    "        mid_NUC_list.append(ncls_mid)\n",
    "\n",
    "    # Infer linker regions and their midpoints\n",
    "    for i in range(len(regions_NUC_list)-1):\n",
    "        regions_LINK_list.append((regions_NUC_list[i][1], regions_NUC_list[i+1][0]))\n",
    "        mid_LINK_list.append(round((regions_NUC_list[i][1] + regions_NUC_list[i+1][0])/2, 0))\n",
    "\n",
    "    return read_id, mid_NUC_list, mid_LINK_list, regions_LINK_list, regions_NUC_list, group.iloc[0][metadata_cols], mod_qual_LINK_list, mod_qual_NUC_list\n",
    "\n",
    "def create_dataframe_from_results(results, kind='NUC', metadata_cols=metadata_cols):\n",
    "    # if \"rel_start\" in res[3] == -21 then print res[0:3]\n",
    "    if kind == 'NUC_mid':\n",
    "        df = pd.DataFrame.from_dict({res[0]: res[1] for res in results}, orient='index')\n",
    "    elif kind == 'LINK_mid':\n",
    "        df = pd.DataFrame.from_dict({res[0]: res[2] for res in results}, orient='index')\n",
    "    elif kind == 'LINK_region':\n",
    "        df = pd.DataFrame.from_dict({res[0]: res[3] for res in results}, orient='index')\n",
    "    elif kind == 'NUC_region':\n",
    "        df = pd.DataFrame.from_dict({res[0]: res[4] for res in results}, orient='index')\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid kind: {kind}\")\n",
    "\n",
    "    # Reset the index to make it a new column\n",
    "    df.reset_index(inplace=True)\n",
    "\n",
    "    # Rename the new column to something meaningful\n",
    "    df.rename(columns={'index': 'read_id'}, inplace=True)\n",
    "    # Adding additional metadata columns to each df.\n",
    "    # Create a new DataFrame for the metadata_cols\n",
    "    metadata_df = pd.DataFrame({cols: [res[5][cols] if res[5] is not None else None for res in results] for cols in metadata_cols})\n",
    "    # Concatenate the new DataFrame and the original DataFrame along axis 1 (columns)\n",
    "    df = pd.merge(metadata_df, df, on='read_id', how='left')\n",
    "\n",
    "    return df\n",
    "\n",
    "def create_dataframe_from_mod_qual(results, kind='mod_qual_NUC', metadata_cols=metadata_cols):\n",
    "    df = pd.DataFrame.from_dict({res[0]: res[7 if kind == 'mod_qual_NUC' else 6] for res in results}, orient='index')\n",
    "    df.reset_index(inplace=True)\n",
    "    df.rename(columns={'index': 'read_id'}, inplace=True)\n",
    "    metadata_df = pd.DataFrame({cols: [res[5][cols] if res[5] is not None else None for res in results] for cols in metadata_cols})\n",
    "    df = pd.merge(metadata_df, df, on='read_id', how='left')\n",
    "    return df\n",
    "\n",
    "# Using multiprocessing to parallelize the calculations\n",
    "print(\"Grouping df...\")\n",
    "# Sort plot_df by read_id then by rel_pos\n",
    "#print(\"Plot_df:\")\n",
    "#display(plot_df.head(10))\n",
    "plot_df.sort_values(by=[\"read_id\",\"rel_pos\"], inplace=True)\n",
    "plot_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# drop rows where read_id not in first 5 read_ids\n",
    "first_rows = plot_df['read_id'].unique()#[plot_df['type'] == 'intergenic_control'] [:2000]\n",
    "#print(\"first_five: \", first_five)\n",
    "grouped = plot_df[plot_df['read_id'].isin(first_rows)]\n",
    "grouped = grouped.groupby('read_id')\n",
    "#grouped = plot_df.groupby('read_id')\n",
    "\n",
    "#grouped = grouped_subset.groupby('read_id')\n",
    "\n",
    "print(\"Calculating nucleosome positions...\")\n",
    "LINK_width = 1\n",
    "NUC_width = 146\n",
    "grouped_data_with_constants = [(read_id,group,LINK_width,NUC_width ,metadata_cols,emission_NEG_array,emission_PGC_array) for read_id,group in grouped]\n",
    "\n",
    "#processes=multiprocessing.cpu_count()\n",
    "with multiprocessing.Pool(processes=multiprocessing.cpu_count()) as pool:\n",
    "    # set results equal to pool.starmap() with the function and grouped_data_with_constants as arguments using tqdm to track progress\n",
    "    results = pool.starmap(single_fiber_nuc_linker, tqdm.tqdm(grouped_data_with_constants, total=len(grouped_data_with_constants)))\n",
    "\n",
    "#print(\"creating result dfs...\")\n",
    "midpoint_NUC = create_dataframe_from_results(results, kind='NUC_mid', metadata_cols=metadata_cols)\n",
    "midpoint_MAD = create_dataframe_from_results(results, kind='LINK_mid', metadata_cols=metadata_cols)\n",
    "region_MAD = create_dataframe_from_results(results, kind='LINK_region', metadata_cols=metadata_cols)\n",
    "region_NUC = create_dataframe_from_results(results, kind='NUC_region', metadata_cols=metadata_cols)\n",
    "mod_qual_LINK = create_dataframe_from_mod_qual(results, kind='mod_qual_LINK', metadata_cols=metadata_cols)\n",
    "mod_qual_NUC = create_dataframe_from_mod_qual(results, kind='mod_qual_NUC', metadata_cols=metadata_cols)\n",
    "\n",
    "\n",
    "# drop duplicates based on read_id from all dfs\n",
    "midpoint_NUC.drop_duplicates(subset=['read_id'], keep='first', inplace=True)\n",
    "midpoint_MAD.drop_duplicates(subset=['read_id'], keep='first', inplace=True)\n",
    "region_MAD.drop_duplicates(subset=['read_id'], keep='first', inplace=True)\n",
    "region_NUC.drop_duplicates(subset=['read_id'], keep='first', inplace=True)\n",
    "mod_qual_LINK.drop_duplicates(subset=['read_id'], keep='first', inplace=True)\n",
    "mod_qual_NUC.drop_duplicates(subset=['read_id'], keep='first', inplace=True)\n",
    "\n",
    "nanotools.display_sample_rows(midpoint_NUC)\n",
    "nanotools.display_sample_rows(region_NUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4309065",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T08:22:53.322031100Z",
     "start_time": "2023-11-15T08:18:52.832204Z"
    }
   },
   "outputs": [],
   "source": [
    "### Process to add per-read statistics\n",
    "#define minimum and maximum positions for nucs / MADs\n",
    "min_pos = -bed_window\n",
    "max_pos = bed_window\n",
    "\n",
    "def find_closest_tuple(row,min_pos,max_pos,metadata_cols=metadata_cols, min_size=35,max_bound=1000):\n",
    "    #print(\"row:\",row)\n",
    "    closest_distance = max_bound\n",
    "    closest_tuple = None\n",
    "    for col_name, tup in zip(row.index, row):\n",
    "        # Skip metadata columns\n",
    "        if col_name in metadata_cols:\n",
    "            continue\n",
    "        if tup is not None and tup is not np.nan:\n",
    "            # Check if the tuple is entirely contained within desired region\n",
    "            if tup[0] >= min_pos and tup[1] <= max_pos and abs(tup[0] - tup[1]) >= min_size:\n",
    "                distance_1 = abs(tup[0] - 0)\n",
    "                distance_2 = abs(tup[1] - 0)\n",
    "\n",
    "                min_distance = min(distance_1, distance_2)\n",
    "\n",
    "                if min_distance < closest_distance:\n",
    "                    closest_distance = min_distance\n",
    "                    closest_tuple = tup\n",
    "\n",
    "        if closest_tuple is None:\n",
    "            if tup is not None and tup is not np.nan:\n",
    "                # Check if the tuple is entirely contained within desired region\n",
    "                if tup[0] >= min_pos and tup[1] <= max_pos:\n",
    "                    distance_1 = abs(tup[0] - 0)\n",
    "                    distance_2 = abs(tup[1] - 0)\n",
    "\n",
    "                    min_distance = min(distance_1, distance_2)\n",
    "\n",
    "                    if min_distance < closest_distance:\n",
    "                        closest_distance = min_distance\n",
    "                        closest_tuple = tup\n",
    "    return closest_tuple\n",
    "\n",
    "### Function to calculate percent_MAD\n",
    "def calc_percent_region(group, min_pos, max_pos, metadata_cols):\n",
    "    total_MAD = 0\n",
    "    read_length = 0  # To store the total length for all reads\n",
    "    for index, row in group.iterrows():\n",
    "        read_start = row['rel_read_start']\n",
    "        read_start = max(min_pos, read_start)\n",
    "        read_end = row['rel_read_end']\n",
    "        read_end = min(max_pos, read_end)\n",
    "        read_length = (read_end - read_start)\n",
    "\n",
    "\n",
    "        for col in group.columns:\n",
    "            if col in metadata_cols:  # Skip metadata columns\n",
    "                continue\n",
    "            mad_tup = row[col]\n",
    "            if pd.notna(mad_tup):\n",
    "                # if at least one of mad_tup[0] and mad_tup[1] is within [min_pos, max_pos]\n",
    "\n",
    "                # Only include the part of the tuple that is within [min_pos, max_pos]\n",
    "                min_val = max(read_start, mad_tup[0])\n",
    "                max_val = min(read_end, mad_tup[1])\n",
    "\n",
    "                if (min_pos <= mad_tup[0] <= max_pos) or (min_pos <= mad_tup[1] <= max_pos):\n",
    "                    try:\n",
    "                        total_MAD += (max_val - min_val)\n",
    "                    except:\n",
    "                        print(\"col:\", col)\n",
    "                        print(\"Warning: mad_tup is None or NaN:\", mad_tup)\n",
    "\n",
    "    percent_MAD = total_MAD / read_length if read_length > 0 else None\n",
    "    if percent_MAD is not None:\n",
    "        if (percent_MAD < 0 or percent_MAD >1):\n",
    "            print(\"row:\")\n",
    "            print(row)\n",
    "            print(\"total_MAD:\", total_MAD)\n",
    "            print(\"read_length:\", read_length)\n",
    "            print(\"percent_MAD:\", percent_MAD)\n",
    "    return percent_MAD\n",
    "\n",
    "def calculate_fiber_NRL_list(row, metadata_cols,sign_filter=None):\n",
    "    row_values = row.drop(metadata_cols).dropna().values  # Drop metadata columns and NaNs\n",
    "    # If row_values is empty, return an empty list\n",
    "    if len(row_values) == 0:\n",
    "        return []\n",
    "\n",
    "    # Flatten the array, taking into account that some elements may be lists\n",
    "    n_values = np.concatenate([np.array(x) if isinstance(x, list) else np.array([x]) for x in row_values])\n",
    "\n",
    "    if n_values.size == 0:\n",
    "        return []\n",
    "\n",
    "    #print(\"n_values:\", n_values)\n",
    "\n",
    "    if sign_filter == '+':\n",
    "        filter_func = lambda x: x > 0\n",
    "    elif sign_filter == '-':\n",
    "        filter_func = lambda x: x < 0\n",
    "    else:\n",
    "        filter_func = lambda x: True\n",
    "\n",
    "    # Filter the n_values\n",
    "    n_values = np.array([x for x in n_values if filter_func(x)])\n",
    "\n",
    "    # Check if n_values is empty after filtering or originally\n",
    "    if n_values.size == 0:\n",
    "        return []\n",
    "\n",
    "    # Calculate the list of differences (each item from the next\n",
    "    #fiber_NRL_list = [n_values[j] - n_values[i]\n",
    "    #                  for i in range(len(n_values))\n",
    "    #                  for j in range(i + 1, len(n_values))]\n",
    "\n",
    "    # Calculate the list of differences for n_values greater than each item\n",
    "    fiber_NRL_list = [n_values[j] - n_values[i]\n",
    "                      for i in range(len(n_values))\n",
    "                      for j in range(i + 1, len(n_values))\n",
    "                      if n_values[j] > n_values[i]]\n",
    "\n",
    "    return fiber_NRL_list\n",
    "\n",
    "def closest_nuc(row):\n",
    "    smallest = row['smallest_positive_nuc_midpoint']\n",
    "    greatest = row['greatest_negative_nuc_midpoint']\n",
    "\n",
    "    if np.isnan(smallest) and np.isnan(greatest):\n",
    "        return np.nan\n",
    "\n",
    "    if np.isnan(smallest):\n",
    "        return greatest\n",
    "\n",
    "    if np.isnan(greatest):\n",
    "        return smallest\n",
    "\n",
    "    return smallest if abs(smallest) < abs(greatest) else greatest\n",
    "\n",
    "merged_df = pd.DataFrame()\n",
    "\n",
    "### Calculate smallest positive and greatest negative midpoints\n",
    "midpoint_NUC['smallest_positive_nuc_midpoint'] = midpoint_NUC.apply(lambda row: min([x for x in row[len(metadata_cols):] if (max_pos > x >= 0)], default=None), axis=1)\n",
    "midpoint_NUC['greatest_negative_nuc_midpoint'] = midpoint_NUC.apply(lambda row: max([x for x in row[len(metadata_cols):] if (min_pos < x < 0)], default=None), axis=1)\n",
    "midpoint_NUC['closest_nuc'] = midpoint_NUC.apply(closest_nuc, axis=1)\n",
    "midpoint_NUC['inter_nuc_dist'] = midpoint_NUC['smallest_positive_nuc_midpoint'] - midpoint_NUC['greatest_negative_nuc_midpoint']\n",
    "# set closest_region in region_MAD to be qual to the tupple who's element is closest to 0 in ros 0-19\n",
    "region_MAD['closest_MAD_region'] = region_MAD.apply(lambda row: find_closest_tuple(row, min_pos=min_pos, max_pos=max_pos, metadata_cols=metadata_cols, min_size=35,max_bound = max_pos), axis=1)\n",
    "region_MAD['MAD_size'] = region_MAD['closest_MAD_region'].apply(lambda x: x[1] - x[0] if x is not None else None)\n",
    "region_MAD['closest_MAD_midpoint'] = region_MAD.apply(lambda row: (row['closest_MAD_region'][0] + row['closest_MAD_region'][1]) / 2 if row['closest_MAD_region'] is not None else None, axis=1)\n",
    "merged_df = pd.merge(plot_df, midpoint_NUC[['read_id', 'smallest_positive_nuc_midpoint', 'greatest_negative_nuc_midpoint','inter_nuc_dist','closest_nuc']], on='read_id', how='left')\n",
    "merged_df = pd.merge(merged_df, region_MAD[['read_id', 'closest_MAD_region','MAD_size','closest_MAD_midpoint']], on='read_id', how='left')\n",
    "# drop merged columns from orginal dfs\n",
    "midpoint_NUC.drop(columns=['smallest_positive_nuc_midpoint', 'greatest_negative_nuc_midpoint','inter_nuc_dist','closest_nuc'], inplace=True)\n",
    "region_MAD.drop(columns=['closest_MAD_region','MAD_size','closest_MAD_midpoint'], inplace=True)\n",
    "\n",
    "# 1. Merge the DataFrames\n",
    "### Calculate % MAD and % NUC\n",
    "min_calc_region = -500\n",
    "max_calc_region = 100\n",
    "print(\"Status: Calculating percent_MAD...\")\n",
    "grouped_mad = region_MAD.groupby('read_id').apply(calc_percent_region, min_pos=min_calc_region, max_pos=max_calc_region, metadata_cols=metadata_cols).reset_index(name='percent_MAD')\n",
    "print(\"Status: Calculating percent_NUC...\")\n",
    "grouped_nuc = region_NUC.groupby('read_id').apply(calc_percent_region, min_pos=min_calc_region, max_pos=max_calc_region, metadata_cols=metadata_cols).reset_index(name='percent_NUC')\n",
    "merged_df = pd.merge(merged_df, grouped_mad, on='read_id', how='left')\n",
    "merged_df = pd.merge(merged_df, grouped_nuc, on='read_id', how='left')\n",
    "merged_df['percent_OTHER'] = 1 - merged_df['percent_MAD'] - merged_df['percent_NUC']\n",
    "\n",
    "### Create 'fiber_NRL_list' column in the midpoint_NUC DataFrame\n",
    "midpoint_NUC['fiber_NRL_list'] = midpoint_NUC.apply(lambda row: calculate_fiber_NRL_list(row, metadata_cols,sign_filter=None), axis=1)\n",
    "midpoint_NUC['fiber_NRL_list_pos'] = midpoint_NUC.apply(lambda row: calculate_fiber_NRL_list(row, metadata_cols,sign_filter=\"+\"), axis=1)\n",
    "midpoint_NUC['fiber_NRL_list_neg'] = midpoint_NUC.apply(lambda row: calculate_fiber_NRL_list(row, metadata_cols,sign_filter=\"-\"), axis=1)\n",
    "# Merge the new 'fiber_NRL_list' column into the 'merged_df' DataFrame\n",
    "merged_df = pd.merge(merged_df, midpoint_NUC[['read_id', 'fiber_NRL_list','fiber_NRL_list_pos','fiber_NRL_list_neg']], on='read_id', how='left')\n",
    "# Restore midpoint_NUC to it's original state\n",
    "midpoint_NUC.drop(columns=['fiber_NRL_list','fiber_NRL_list_pos','fiber_NRL_list_neg'], inplace=True)\n",
    "\n",
    "# 2. Sort the merged DataFrame\n",
    "merged_df = merged_df.sort_values(by=['percent_MAD'], ascending=True)\n",
    "# reset index\n",
    "merged_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Group by read_id and display 10 unique read_id rows\n",
    "grouped = merged_df.copy()\n",
    "# drop dupicate rows by read_id\n",
    "grouped.drop_duplicates(subset=['read_id'], inplace=True)\n",
    "# sort by read_id\n",
    "grouped.sort_values(by=['read_id'], inplace=True)\n",
    "# Set grouped column \"rel_read_length\" equal to rel_read_end - rel_read_start\n",
    "grouped.reset_index(inplace=True, drop=True)\n",
    "\n",
    "#set combined_nucs_df equal to columns 'read_id' and all columns with integer column names.\n",
    "combined_nucs_df = midpoint_NUC[['read_id'] + [col for col in midpoint_NUC.columns if isinstance(col, int)]]\n",
    "# add a new column that creates a list of all the nucleosome positions for each read that are not nan\n",
    "combined_nucs_df['nucs_list'] = combined_nucs_df.apply(lambda row: [x for x in row[1:] if pd.notna(x)], axis=1)\n",
    "grouped = pd.merge(grouped,combined_nucs_df[['read_id','nucs_list']], on='read_id', how='left')\n",
    "\n",
    "print(\"Creating aligned nuc and mad lists...\")\n",
    "def align_nuc_list(row, column_name):\n",
    "    subtract_val = row[column_name]\n",
    "    if isinstance(subtract_val, tuple):\n",
    "        midpoint = (subtract_val[0] + subtract_val[1]) / 2.0\n",
    "    else:\n",
    "        midpoint = subtract_val\n",
    "\n",
    "    if isinstance(midpoint, (int, float)) and not np.isnan(midpoint):\n",
    "        return [x - midpoint for x in row['nucs_list']]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def calculate_internuc(nuc_list):\n",
    "    return [j - i for i, j in zip(nuc_list[:-1], nuc_list[1:])]\n",
    "\n",
    "def align_nuc_list_internuc(row):\n",
    "    smallest_positive = row['smallest_positive_nuc_midpoint']\n",
    "    greatest_negative = row['greatest_negative_nuc_midpoint']\n",
    "\n",
    "    # Check if both are NaN\n",
    "    if np.isnan(smallest_positive) or np.isnan(greatest_negative):\n",
    "        return []\n",
    "    else:\n",
    "        midpoint = (smallest_positive + greatest_negative) / 2.0\n",
    "\n",
    "    return [x - midpoint for x in row['nucs_list']]\n",
    "\n",
    "def largest_gap_size(nucs_list, nuc_width=147):\n",
    "    if len(nucs_list) < 2:\n",
    "        return np.nan\n",
    "    sorted_list = sorted(nucs_list)\n",
    "    max_gap = max(np.diff(sorted_list)) - nuc_width\n",
    "    return max_gap\n",
    "\n",
    "def largest_gap_pos(nucs_list, nuc_width=147):\n",
    "    if len(nucs_list) < 2:\n",
    "        return np.nan\n",
    "    sorted_list = sorted(nucs_list)\n",
    "    max_gap = max(np.diff(sorted_list))\n",
    "    # Find the index of the largest gap\n",
    "    max_gap_index = np.argmax(np.diff(sorted_list))\n",
    "    # Calculate the center position of the largest gap\n",
    "    center_position = (sorted_list[max_gap_index] + sorted_list[max_gap_index + 1]) / 2\n",
    "    return center_position\n",
    "\n",
    "grouped = grouped.dropna(subset=['nucs_list'])\n",
    "\n",
    "grouped['inter_nuc_sub'] = grouped['nucs_list'].apply(calculate_internuc)\n",
    "grouped['largest_nfr_size'] = grouped['nucs_list'].apply(largest_gap_size, NUC_width)\n",
    "grouped['largest_nfr_pos'] = grouped['nucs_list'].apply(largest_gap_pos, NUC_width)\n",
    "grouped['nuc_list_internuc_aligned'] = grouped.apply(align_nuc_list_internuc, axis=1)\n",
    "grouped['nucs_list_closest_aligned'] = grouped.apply(align_nuc_list, args=('closest_nuc',), axis=1)\n",
    "grouped['nucs_list_largest_nfr_aligned'] = grouped.apply(align_nuc_list, args=('largest_nfr_pos',), axis=1)\n",
    "\n",
    "\n",
    "if 'exp_id' not in grouped.columns:\n",
    "    import pysam\n",
    "    # Check that all lists have the same length\n",
    "    if not (len(new_bam_files) == len(exp_ids)):\n",
    "        raise ValueError(\"The lists new_bam_files, exp_ids, and conditions must have the same length.\")\n",
    "\n",
    "    # Initialize an empty list to store the data\n",
    "    data = []\n",
    "\n",
    "    print(\"Status: Looping through bam files to append exp_ids\")\n",
    "    # Loop over bam files, their corresponding experiment ids, and conditions\n",
    "    for bam_file, exp_id in zip(new_bam_files, exp_ids):\n",
    "        # Open the bam file\n",
    "        with pysam.AlignmentFile(bam_file, \"rb\") as bam:\n",
    "            # Fetch the first 3 read ids as an example\n",
    "            # In practice, you would iterate over all reads as needed\n",
    "            for read in bam.fetch(): # Adjust the number as needed for actual use\n",
    "                data.append({\n",
    "                    \"bam_file_name\": bam_file,\n",
    "                    \"read_id\": read.query_name,\n",
    "                    \"exp_id\": exp_id,\n",
    "                })\n",
    "\n",
    "    # Convert the list of dictionaries to a DataFrame\n",
    "    read_exp_ids_df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "\n",
    "    # Add exp_id column to grouped dataframe by merging on read_id\n",
    "    # if exp_id column does not exist in grouped then:\n",
    "\n",
    "    grouped = pd.merge(grouped, read_exp_ids_df[['read_id', 'exp_id']], on='read_id', how='left')\n",
    "\n",
    "nanotools.display_sample_rows(grouped, 5)\n",
    "nanotools.display_sample_rows(merged_df, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc56f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Calculate positioning statistics\n",
    "# Function to compute the midpoint\n",
    "\"\"\"def compute_midpoint(row):\n",
    "    pos_values = [v for v in row[integer_columns] if v > 0]\n",
    "    neg_values = [v for v in row[integer_columns] if v < 0]\n",
    "\n",
    "    # If either list is empty, return NaN as the midpoint\n",
    "    if not pos_values or not neg_values:\n",
    "        return float('nan')\n",
    "\n",
    "    least_pos = min(pos_values)\n",
    "    greatest_neg = max(neg_values)\n",
    "\n",
    "    return (least_pos + greatest_neg) / 2\"\"\"\n",
    "\n",
    "def calculate_feature(args):\n",
    "    occ_cutoff, midpoint_NUC, NUC_max_width, metadata_cols, align_to, condition, chr_type, dtype  = args\n",
    "\n",
    "    # Filter the dataframe based on condition, chr_type, and dtype\n",
    "    print(\"Processing with following filters:\", condition, chr_type, dtype, sep=\"\\n\")\n",
    "    midpoint_NUC_filtered = midpoint_NUC.copy(deep=True)\n",
    "    midpoint_NUC_filtered = midpoint_NUC_filtered[\n",
    "        (midpoint_NUC_filtered['condition'] == condition) &\n",
    "        (midpoint_NUC_filtered['chr_type'] == chr_type) &\n",
    "        (midpoint_NUC_filtered['type'] == dtype) &\n",
    "        (midpoint_NUC_filtered[align_to] is not np.nan)\n",
    "    ]\n",
    "\n",
    "    # Identify columns with integer names\n",
    "    integer_columns = [col for col in midpoint_NUC_filtered.columns if isinstance(col, int)]\n",
    "\n",
    "    # Shift columns by the midpoint\n",
    "    #for col in integer_columns:\n",
    "    #    midpoint_NUC_filtered[col] = midpoint_NUC_filtered[col] - midpoint_NUC_filtered[align_to]\n",
    "\n",
    "    midpoint_NUC_filtered.set_index('read_id', inplace=True)\n",
    "    #print(\"midpoint_NUC_filtered:\")\n",
    "    #display(midpoint_NUC_filtered.head(20))\n",
    "    results = {\n",
    "        'mean_nuc_pos': [],\n",
    "        'total_reads': [],\n",
    "        'total_nucs': [],\n",
    "        'percent_occ': [],\n",
    "        'nucs_list': [],\n",
    "        'subs_list': []\n",
    "    }\n",
    "\n",
    "    used_nucleosomes = set()\n",
    "\n",
    "    print(f\"Looping through  {len(midpoint_NUC_filtered)} reads...\")\n",
    "    iter=0\n",
    "    for idx, row in midpoint_NUC_filtered.iterrows():\n",
    "        # print progress whenever idx % 2000 == 0\n",
    "        iter += 1\n",
    "        if iter % 200 == 0:\n",
    "            print(\"Processing read:\", iter, sep='\\n')\n",
    "        nuc_positions = row[row.index.difference(metadata_cols)].dropna()\n",
    "        for nuc in nuc_positions:\n",
    "            if (idx,nuc) in used_nucleosomes:\n",
    "                #print(f\"Warning: Duplicate nucleosome found for read_id: {idx}, nuc: {nuc}\")\n",
    "                continue\n",
    "\n",
    "            temp_used_nucleosomes = set()\n",
    "            temp_subs_list = []\n",
    "            temp_mean_subs_list = []\n",
    "            #print(\"idx:\", idx, \" | nuc:\", nuc)\n",
    "\n",
    "            temp_subtractions = abs(midpoint_NUC_filtered[row.index.difference(metadata_cols)] - nuc)\n",
    "            #Drop the current row from the subtractions table\n",
    "            temp_subtractions.drop(index=idx, inplace=True)\n",
    "            #temp_subtractions = temp_subtractions.apply(lambda row: row.where(row == row.min(), np.nan), axis=1)\n",
    "            temp_subtractions = temp_subtractions.apply(lambda row: row.nsmallest(2), axis=1)\n",
    "            temp_subtractions = temp_subtractions.where(temp_subtractions <= ((NUC_max_width / 2 + 12)), np.nan)\n",
    "            #print(\"temp_subtractions:\")\n",
    "            #display(temp_subtractions.head(20))\n",
    "\n",
    "            non_nan_rows = temp_subtractions.dropna(how='all')\n",
    "            # add current nucleosome to temp_used_nucleosomes\n",
    "            temp_used_nucleosomes.add((idx, nuc))\n",
    "\n",
    "            for used_row_idx in non_nan_rows.index:\n",
    "                for col in non_nan_rows.columns:\n",
    "                    if not pd.isna(non_nan_rows.at[used_row_idx, col]) and (used_row_idx, midpoint_NUC_filtered.at[used_row_idx, col]) not in used_nucleosomes:\n",
    "                        original_nuc_value = midpoint_NUC_filtered.at[used_row_idx, col]\n",
    "                        temp_used_nucleosomes.add((used_row_idx, original_nuc_value))\n",
    "                        temp_subs_list.append(abs(original_nuc_value - nuc))\n",
    "\n",
    "\n",
    "            used_nucleosomes.update(temp_used_nucleosomes)\n",
    "            #print(f\"Used nucleosomes: {used_nucleosomes}\")\n",
    "\n",
    "            # Calculate the results\n",
    "            mean_nuc = np.mean([x[1] for x in temp_used_nucleosomes])\n",
    "            total_reads = len(midpoint_NUC_filtered)\n",
    "            total_nucs = len(temp_used_nucleosomes)\n",
    "            percent_occ = total_nucs / total_reads if total_reads else 0\n",
    "            nucs_list = [x[1] for x in temp_used_nucleosomes]\n",
    "\n",
    "            # Append results\n",
    "            results['mean_nuc_pos'].append(mean_nuc)\n",
    "            results['total_reads'].append(total_reads)\n",
    "            results['total_nucs'].append(total_nucs)\n",
    "            results['percent_occ'].append(percent_occ)\n",
    "            results['nucs_list'].append(nucs_list)\n",
    "            results['subs_list'].append(temp_subs_list)\n",
    "            #print(f\"Results: {results}\")\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df['condition'] = condition\n",
    "    results_df['chr_type'] = chr_type\n",
    "    results_df['type'] = dtype\n",
    "    # Sort the results_df by mean_nuc_pos\n",
    "    # drop all rows where percent_occ is < occ_cutoff\n",
    "    results_df = results_df[results_df['percent_occ'] >= occ_cutoff]\n",
    "    results_df.sort_values('mean_nuc_pos', inplace=True)\n",
    "    results_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Add the \"nuc_id\" column\n",
    "    # Count negative and positive values to get the maximum n- and n+ steps\n",
    "    neg_count = sum(results_df['mean_nuc_pos'] < 0)\n",
    "    pos_count = 1\n",
    "\n",
    "    # Initialize nuc_id list\n",
    "    nuc_id_list = []\n",
    "\n",
    "    # Loop through DataFrame and assign nuc_id\n",
    "    for idx, row in results_df.iterrows():\n",
    "        if row['mean_nuc_pos'] < 0:\n",
    "            nuc_id = f'n-{neg_count}'\n",
    "            neg_count -= 1\n",
    "        elif row['mean_nuc_pos'] > 0:\n",
    "            nuc_id = f'n+{pos_count}'\n",
    "            pos_count += 1\n",
    "        else:\n",
    "            nuc_id = 'n'\n",
    "        nuc_id_list.append(nuc_id)\n",
    "\n",
    "    results_df['nuc_id'] = nuc_id_list\n",
    "    return results_df\n",
    "\n",
    "def calculate_feature2(args):\n",
    "    bin_width, occ_cutoff, midpoint_NUC, NUC_max_width, metadata_cols, align_to, condition, chr_type, dtype  = args\n",
    "\n",
    "    # Filter the dataframe based on condition, chr_type, and dtype\n",
    "    print(\"Processing with following filters:\", condition, chr_type, dtype, sep=\"\\n\")\n",
    "    midpoint_NUC_filtered = midpoint_NUC.copy(deep=True)\n",
    "    midpoint_NUC_filtered = midpoint_NUC_filtered[\n",
    "        (midpoint_NUC_filtered['condition'] == condition) &\n",
    "        (midpoint_NUC_filtered['chr_type'] == chr_type) &\n",
    "        (midpoint_NUC_filtered['type'] == dtype)\n",
    "    ]\n",
    "\n",
    "    # Identify columns with integer names\n",
    "    integer_columns = [col for col in midpoint_NUC_filtered.columns if isinstance(col, int)]\n",
    "\n",
    "\n",
    "    # Shift columns by the midpoint\n",
    "    if align_to is not None:\n",
    "        # drop rows where from midpoint_NUC_filtered where align_to column is not np.nan\n",
    "        print(\"Dropping this many rows where align_to column is np.nan:\", len(midpoint_NUC_filtered[midpoint_NUC_filtered[align_to].isna()]))\n",
    "        midpoint_NUC_filtered = midpoint_NUC_filtered[midpoint_NUC_filtered[align_to].notna()]\n",
    "        for col in integer_columns:\n",
    "            midpoint_NUC_filtered[col] = midpoint_NUC_filtered[col] - midpoint_NUC_filtered[align_to]\n",
    "        # drop align_to column\n",
    "        midpoint_NUC_filtered.drop(columns=[align_to], inplace=True)\n",
    "\n",
    "    # drop rows with duplicate read_ids\n",
    "    # print rows with duplicate read_ids in midpoint_NUC_filtered\n",
    "    midpoint_NUC_filtered.drop_duplicates(subset=['read_id'], keep='first', inplace=True)\n",
    "    #reset index\n",
    "    midpoint_NUC_filtered.reset_index(drop=True, inplace=True)\n",
    "    midpoint_NUC_filtered.set_index('read_id', inplace=True)\n",
    "    #print(\"midpoint_NUC_filtered:\")\n",
    "    #display(midpoint_NUC_filtered.head(20))\n",
    "    results = {\n",
    "        'mean_nuc_pos': [],\n",
    "        'std_nuc_pos': [],\n",
    "        'total_reads': [],\n",
    "        'total_nucs': [],\n",
    "        'percent_occ': [],\n",
    "        'nucs_list': [],\n",
    "        'subs_list': []\n",
    "    }\n",
    "\n",
    "    used_nucleosomes = set()\n",
    "\n",
    "    # Initialize list to hold mean nucleosome positions\n",
    "    mean_nuc_positions = []\n",
    "\n",
    "    print(f\"Looping through  {len(midpoint_NUC_filtered)} reads...\")\n",
    "    iter=0\n",
    "    for idx, row in midpoint_NUC_filtered.iterrows():\n",
    "        # print progress whenever idx % 2000 == 0\n",
    "        iter += 1\n",
    "        if iter % 200 == 0:\n",
    "            print(\"Processing read:\", iter, sep='\\n')\n",
    "\n",
    "        nuc_positions = row[row.index.difference(metadata_cols)].dropna()\n",
    "        # drop nuc_positions that are within bin_width of any mean_nuc_positions\n",
    "        #nuc_positions = [x for x in nuc_positions if not any(abs(mean_nuc - x) <= bin_width for mean_nuc in mean_nuc_positions)]\n",
    "\n",
    "        # drop nuc_positions that are within |5| of any value in results['std_nuc_pos']\n",
    "        nuc_positions = [x for x in nuc_positions if not any(abs(x - std_nuc) <= 5 for std_nuc in results['std_nuc_pos'])]\n",
    "\n",
    "        #nuc_positions = [x for x in nuc_positions if x not in results['std_nuc_pos']]\n",
    "\n",
    "        #print(nuc_positions)\n",
    "        for nuc in nuc_positions:\n",
    "            #if (idx,nuc) in used_nucleosomes:\n",
    "                #print(f\"Warning: Duplicate nucleosome found for read_id: {idx}, nuc: {nuc}\")\n",
    "            #    continue\n",
    "\n",
    "            temp_used_nucleosomes = set()\n",
    "            temp_subs_list = []\n",
    "            temp_mean_subs_list = []\n",
    "            #print(\"idx:\", idx, \" | nuc:\", nuc)\n",
    "\n",
    "            temp_subtractions = abs(midpoint_NUC_filtered[row.index.difference(metadata_cols)] - nuc)\n",
    "            #Drop the current row from the subtractions table\n",
    "            temp_subtractions.drop(index=idx, inplace=True)\n",
    "            #temp_subtractions = temp_subtractions.apply(lambda row: row.where(row == row.min(), np.nan), axis=1)\n",
    "            temp_subtractions = temp_subtractions.apply(lambda row: row.nsmallest(2), axis=1)\n",
    "            temp_subtractions = temp_subtractions.where(temp_subtractions <= ((NUC_max_width / 2 + 12)), np.nan)\n",
    "            #print(\"temp_subtractions:\")\n",
    "            #display(temp_subtractions.head(20))\n",
    "\n",
    "            non_nan_rows = temp_subtractions.dropna(how='all')\n",
    "            # add current nucleosome to temp_used_nucleosomes\n",
    "            temp_used_nucleosomes.add((idx, nuc))\n",
    "\n",
    "            for used_row_idx in non_nan_rows.index:\n",
    "                for col in non_nan_rows.columns:\n",
    "                    # check if non_nan_rows.at[used_row_idx, col] is a series\n",
    "                    if isinstance(non_nan_rows.at[used_row_idx, col], pd.Series):\n",
    "                        print(\"Warning: non_nan_rows.at[used_row_idx, col] is a series\")\n",
    "                        print(\"non_nan_rows.at[used_row_idx, col]:\", non_nan_rows.at[used_row_idx, col])\n",
    "                        print(\"non_nan_rows.at[used_row_idx, col].values:\", non_nan_rows.at[used_row_idx, col].values)\n",
    "                    if not pd.isna(non_nan_rows.at[used_row_idx, col]) and (used_row_idx, midpoint_NUC_filtered.at[used_row_idx, col]) not in used_nucleosomes:\n",
    "                        original_nuc_value = midpoint_NUC_filtered.at[used_row_idx, col]\n",
    "                        temp_used_nucleosomes.add((used_row_idx, original_nuc_value))\n",
    "                        temp_subs_list.append(abs(original_nuc_value - nuc))\n",
    "\n",
    "            mean_nuc = np.mean([x[1] for x in temp_used_nucleosomes])\n",
    "            mean_nuc_positions.append(mean_nuc)\n",
    "\n",
    "            used_nucleosomes.update(temp_used_nucleosomes)\n",
    "            #print(f\"Used nucleosomes: {used_nucleosomes}\")\n",
    "\n",
    "            # Calculate the results\n",
    "            total_reads = len(midpoint_NUC_filtered)\n",
    "            total_nucs = len(temp_used_nucleosomes)\n",
    "            percent_occ = total_nucs / total_reads if total_reads else 0\n",
    "            nucs_list = [x[1] for x in temp_used_nucleosomes]\n",
    "\n",
    "            # Append results\n",
    "            results['mean_nuc_pos'].append(mean_nuc)\n",
    "            results['std_nuc_pos'].append(nuc)\n",
    "            results['total_reads'].append(total_reads)\n",
    "            results['total_nucs'].append(total_nucs)\n",
    "            results['percent_occ'].append(percent_occ)\n",
    "            results['nucs_list'].append(nucs_list)\n",
    "            results['subs_list'].append(temp_subs_list)\n",
    "            #print(f\"Results: {results}\")\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df['condition'] = condition\n",
    "    results_df['chr_type'] = chr_type\n",
    "    results_df['type'] = dtype\n",
    "    # Sort the results_df by mean_nuc_pos\n",
    "    # drop all rows where percent_occ is < occ_cutoff\n",
    "    results_df = results_df[results_df['percent_occ'] >= occ_cutoff]\n",
    "    results_df.sort_values('mean_nuc_pos', inplace=True)\n",
    "    results_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Initialize an empty dataframe to hold the final result\n",
    "    final_df = pd.DataFrame()\n",
    "    # Find the min and max std_nuc_pos\n",
    "    min_std_nuc_pos = -bed_window #results_df['std_nuc_pos'].min()\n",
    "    max_std_nuc_pos = bed_window #results_df['std_nuc_pos'].max()\n",
    "\n",
    "    results_df = results_df.drop_duplicates(subset=['std_nuc_pos'])\n",
    "    # Create bins\n",
    "    bins = np.arange(min_std_nuc_pos, max_std_nuc_pos, bin_width)\n",
    "\n",
    "    # Initialize a list to hold the rows for each bin\n",
    "    bin_rows = []\n",
    "\n",
    "    for lower_bound in bins:\n",
    "        upper_bound = lower_bound + bin_width\n",
    "\n",
    "        lower_bound = lower_bound + bin_width/4\n",
    "        upper_bound = upper_bound - bin_width/4\n",
    "\n",
    "        # Filter rows where mean_nuc_pos falls within the bin\n",
    "        bin_data = results_df[(results_df['std_nuc_pos'] >= lower_bound) & (results_df['std_nuc_pos'] < upper_bound)]\n",
    "\n",
    "        if not bin_data.empty:\n",
    "            # Find the row with the lowest average of all elements in subs_list\n",
    "            #bin_data['avg_subs'] = bin_data['subs_list'].apply(np.mean)\n",
    "            #min_avg_subs_row = bin_data[bin_data['avg_subs'] == bin_data['avg_subs'].min()].copy()\n",
    "\n",
    "            # Find the row with the max occupancy\n",
    "            min_avg_subs_row = bin_data[bin_data['percent_occ'] == bin_data['percent_occ'].max()].copy()\n",
    "\n",
    "            # Add bin_start and bin_end columns\n",
    "            min_avg_subs_row['bin_start'] = lower_bound\n",
    "            min_avg_subs_row['bin_end'] = upper_bound\n",
    "            min_avg_subs_row['bin_pos'] = lower_bound + (upper_bound - lower_bound)/2\n",
    "\n",
    "\n",
    "            bin_rows.append(min_avg_subs_row)\n",
    "\n",
    "    # Concatenate all rows for each bin into a single dataframe\n",
    "    final_df = pd.concat(bin_rows)\n",
    "    # sort by std_nuc_pos then reset index\n",
    "    final_df.sort_values(by=['std_nuc_pos'], inplace=True)\n",
    "    final_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    # Drop the temporary column used for calculation\n",
    "    #final_df.drop(columns=['avg_subs'], inplace=True)\n",
    "\n",
    "    # Add the \"nuc_id\" column\n",
    "    # Count negative and positive values to get the maximum n- and n+ steps\n",
    "    neg_count = sum(results_df['std_nuc_pos'] < 0)\n",
    "    pos_count = 1\n",
    "\n",
    "    # Initialize nuc_id list\n",
    "    nuc_id_list = []\n",
    "\n",
    "    # Loop through DataFrame and assign nuc_id\n",
    "    for idx, row in final_df.iterrows():\n",
    "        if row['mean_nuc_pos'] < 0:\n",
    "            nuc_id = f'n-{neg_count}'\n",
    "            neg_count -= 1\n",
    "        elif row['mean_nuc_pos'] > 0:\n",
    "            nuc_id = f'n+{pos_count}'\n",
    "            pos_count += 1\n",
    "        else:\n",
    "            nuc_id = 'n'\n",
    "        nuc_id_list.append(nuc_id)\n",
    "\n",
    "    final_df['nuc_id'] = nuc_id_list\n",
    "    return final_df\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    specific_comps = [\n",
    "    ('N2_fiber', 'X', 'xol-1_TSS'),\n",
    "    #('N2_fiber', 'X', 'intergenic_control'),\n",
    "    ('SDC2_degron_fiber', 'X', 'xol-1_TSS')]\n",
    "    #('SDC2_degron_fiber', 'X', 'intergenic_control')]\n",
    "    # set occupancy cutoff, (ignore nucleosomes with occupancy below cutoff)\n",
    "    occ_cutoff = 0\n",
    "    bin_width = 40\n",
    "\n",
    "    # Align to inter-nuc distance?\n",
    "    #align_to = None # \"smallest_positive_nuc_midpoint\", \"greatest_negative_nuc_midpoint\", \"closest_nuc\", \"inter_nuc_dist\" or None \"closest_nuc\"\n",
    "    #align_to = \"closest_nuc\"\n",
    "    align_to = \"closest_MAD_midpoint\"\n",
    "\n",
    "    # create copy of midpoint_NUC and merge with grouped on read_id, keeping align_to column\n",
    "    midpoint_NUC_merged = midpoint_NUC.copy(deep=True)\n",
    "    if align_to is not None:\n",
    "        print(\"Merging...\")\n",
    "        midpoint_NUC_merged = pd.merge(midpoint_NUC, grouped[['read_id', align_to]], on='read_id', how='left')\n",
    "\n",
    "    # keep first 100 rows of midpoint_NUC_merged\n",
    "    #midpoint_NUC_merged = midpoint_NUC_merged.sample(n=100)\n",
    "    args_list = [(bin_width, occ_cutoff,midpoint_NUC_merged, NUC_width, metadata_cols, align_to, c, ch, d) for c, ch, d in specific_comps]\n",
    "\n",
    "    #processes=multiprocessing.cpu_count()\n",
    "    with Pool(processes=multiprocessing.cpu_count()) as pool:\n",
    "        results = pool.map(calculate_feature2, args_list)\n",
    "\n",
    "    combined_res_df = pd.concat(results)\n",
    "\n",
    "    # Display first 100 rows\n",
    "    nanotools.display_sample_rows(combined_res_df, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a9bae2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T10:32:48.461603100Z",
     "start_time": "2023-11-09T10:32:44.245017400Z"
    }
   },
   "outputs": [],
   "source": [
    "### PLOT NRL STATISTICS\n",
    "import seaborn as sns\n",
    "importlib.reload(nanotools)\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "# Your DataFrame is assumed to be in a variable named `df`\n",
    "\n",
    "# Create an empty list to store the data for the distplot\n",
    "distplot_df = pd.DataFrame()\n",
    "#grouped_plot = grouped[grouped['fiber_NRL_list'].apply(lambda x: len(x) > 0)]\n",
    "grouped_plot = grouped.copy()\n",
    "grouped_plot[\"rel_read_len\"] = grouped_plot[\"rel_read_end\"] - grouped_plot[\"rel_read_start\"]\n",
    "# drop all rows where rel_read_end < bed_window * 2 -100\n",
    "#display(grouped_plot.sample(n=10))\n",
    "#grouped_plot = grouped_plot[grouped_plot[\"rel_read_len\"] >= (bed_window * 2 - 100)]\n",
    "#display(grouped_plot.sample(n=10))\n",
    "# sample_num = minimumum counts of each unique combination of 'type' and 'condition'\n",
    "#sample_num = grouped_plot[['type', 'condition','read_id']].drop_duplicates().groupby(['type', 'condition']).size().min()\n",
    "#print(\"sample_num:\", sample_num)\n",
    "#grouped_plot = grouped_plot.groupby(['condition','type']).apply(lambda x: x.sample(sample_num)).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Iterate over each unique combination of 'type' and 'condition'\n",
    "for unique_comb in grouped_plot[['type', 'condition','chr_type']].drop_duplicates().values:\n",
    "    type_val, condition_val, chr_type_val = unique_comb\n",
    "    print(\"condition_val:\", condition_val, sep=\"\\n\")\n",
    "    # Filter data based on the unique combination of 'type' and 'condition'\n",
    "    sub_df = grouped_plot[(grouped['type'] == type_val) & (grouped_plot['condition'] == condition_val) & (grouped_plot['chr_type'] == chr_type_val)]\n",
    "    # explod sub_df['fiber_NRL_list'] and drop NaN values and convert to column in fiber_data called \"dist\"\n",
    "    fiber_data = sub_df['fiber_NRL_list'].explode().dropna().to_frame()\n",
    "    fiber_data.rename(columns={'fiber_NRL_list': 'dist'}, inplace=True)\n",
    "    # Add a column called \"type\" with the value of type_val and condition_val\n",
    "    fiber_data['condition'] = f\"{condition_val}\" #{type_val}_\n",
    "    fiber_data['type'] = f\"{type_val}\"\n",
    "    fiber_data['chr_type'] = f\"{chr_type_val}\"\n",
    "    # reset index\n",
    "    fiber_data.reset_index(inplace=True, drop=True)\n",
    "    #print length of fiber_data\n",
    "    print(\"len(fiber_data):\", len(fiber_data['condition']))\n",
    "    print(\"len(sub_df):\", len(sub_df['type']))\n",
    "    #display(fiber_data.head(10))\n",
    "    # if distplot_df is empty, set distplot_df equal to fiber_data\n",
    "    if distplot_df.empty:\n",
    "        distplot_df = fiber_data\n",
    "    # else append fiber_data to distplot_df\n",
    "    else:\n",
    "        distplot_df = distplot_df.append(fiber_data)\n",
    "\n",
    "display(distplot_df.sample(n=20))\n",
    "peaks1, fig1 = nanotools.plot_NRL_dist(distplot_df[(distplot_df['condition'] == \"N2_fiber\") & (distplot_df['type'] == type_selected[4])] ,\n",
    "                                       \"X\",\"#16415e\",\"N2\",smoothing_val=0.2)\n",
    "display(peaks1.head(10))\n",
    "peaks2, fig2 = nanotools.plot_NRL_dist(distplot_df[(distplot_df['condition'] == \"SDC2_degron_fiber\") & (distplot_df['type'] == type_selected[4])],\n",
    "                                       \"X\",\"#16415e\",\"SDC2_degron\",smoothing_val=0.2)\n",
    "display(peaks2.head(10))\n",
    "\n",
    "fig = nanotools.plot_NRL_dist_compare(distplot_df[(distplot_df['type'] == type_selected[4])] ,\n",
    "                                      \"X\",\"N2 and SDC2_degron\",smoothing_val=0.2,norm_bool=False,hue='condition')\n",
    "\n",
    "\n",
    "fig.savefig('images/dpy27_sdc2_sdc3_fiber_comparison_NRL.png', dpi=300, bbox_inches='tight')\n",
    "fig.savefig('images/dpy27_sdc2_sdc3_fiber_comparison_NRL.svg', bbox_inches='tight')\n",
    "\n",
    "fig1.savefig('images/dpy27_sdc2_sdc3_fiber_NRL_N2.png', dpi=300, bbox_inches='tight')\n",
    "fig1.savefig('images/dpy27_sdc2_sdc3_fiber_NRL_N2.svg', bbox_inches='tight')\n",
    "\n",
    "fig2.savefig('images/dpy27_sdc2_sdc3_fiber_NRL_SDC2_degron.png', dpi=300, bbox_inches='tight')\n",
    "fig2.savefig('images/dpy27_sdc2_sdc3_fiber_NRL_SDC2_degron.svg', bbox_inches='tight')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98952f50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T03:18:36.533977100Z",
     "start_time": "2023-11-10T03:18:36.435334200Z"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "importlib.reload(nanotools)\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "# Your DataFrame is assumed to be in a variable named `df`\n",
    "\n",
    "# Create an empty list to store the data for the distplot\n",
    "distplot_df = pd.DataFrame()\n",
    "\n",
    "align_on = 'nuc_list_largest_nfr_aligned'\n",
    "    #\"nucs_list\"\n",
    "    # 'nuc_list_internuc_aligned'\n",
    "    # \"nucs_list\"\n",
    "    # \"nucs_list_MAD_aligned\"\n",
    "    # 'nucs_list_n1_aligned'\n",
    "\n",
    "# drop rows from grouped where align_on column is a float\n",
    "grouped_plot = grouped[grouped[align_on].apply(lambda x: isinstance(x, list))]\n",
    "# drop rows where align_on column == []\n",
    "grouped_plot = grouped_plot[grouped_plot[align_on].apply(lambda x: len(x) > 0)]\n",
    "display(grouped_plot.head(10))\n",
    "grouped_plot[\"rel_read_len\"] = grouped_plot[\"rel_read_end\"] - grouped_plot[\"rel_read_start\"]\n",
    "# drop all rows where rel_read_end < bed_window * 2 -100\n",
    "#grouped_plot = grouped_plot[grouped_plot[\"rel_read_len\"] >= (bed_window * 2 - 400)]\n",
    "#sample_num = min(grouped_plot['condition'].value_counts())\n",
    "#grouped_plot = grouped.groupby('condition').apply(lambda x: x.sample(sample_num)).reset_index(drop=True)\n",
    "#display(grouped_plot.sample(n=10))\n",
    "# print number of N2_fiber reads and number of SDC2_degron_fiber reads\n",
    "print(\"N2_fiber reads:\", len(grouped_plot[grouped_plot['condition'] == \"N2_fiber\"]))\n",
    "print(\"SDC2_degron_fiber reads:\", len(grouped_plot[grouped_plot['condition'] == \"SDC2_degron_fiber\"]))\n",
    "\n",
    "# select type for plotting\n",
    "grouped_plot = grouped_plot[grouped_plot['type'] == type_selected[0]]\n",
    "\"\"\"# find the number of reads in each condition, and set num_reads to the max\n",
    "num_reads = min(grouped_plot['condition'].value_counts())\n",
    "# drop reads from condition with most reads to match reads from condition with least reads\n",
    "grouped_plot = grouped_plot.groupby('condition').apply(lambda x: x.sample(num_reads)).reset_index(drop=True)\"\"\"\n",
    "\n",
    "\n",
    "# Iterate over each unique combination of 'type' and 'condition'\n",
    "for unique_comb in grouped_plot[['type', 'condition']].drop_duplicates().values:\n",
    "    type_val, condition_val = unique_comb\n",
    "    print(\"condition_val:\", condition_val, sep=\"\\n\")\n",
    "    # Filter data based on the unique combination of 'type' and 'condition'\n",
    "    sub_df = grouped_plot[(grouped_plot['type'] == type_val) & (grouped_plot['condition'] == condition_val)]\n",
    "    # explod sub_df['fiber_NRL_list'] and drop NaN values and convert to column in fiber_data called \"dist\"\n",
    "    fiber_data = sub_df[align_on].explode().dropna().to_frame()\n",
    "    fiber_data.rename(columns={align_on: 'dist'}, inplace=True)\n",
    "    # Add a column called \"type\" with the value of type_val and condition_val\n",
    "    fiber_data['condition'] = f\"{condition_val}\" #{type_val}_\n",
    "    # reset index\n",
    "    fiber_data.reset_index(inplace=True, drop=True)\n",
    "    # print length of fiber_data\n",
    "    print(\"len(fiber_data):\", len(fiber_data['condition']))\n",
    "    #display(fiber_data.head(10))\n",
    "    # if distplot_df is empty, set distplot_df equal to fiber_data\n",
    "    if distplot_df.empty:\n",
    "        distplot_df = fiber_data\n",
    "    # else append fiber_data to distplot_df\n",
    "    else:\n",
    "        distplot_df = distplot_df.append(fiber_data)\n",
    "\n",
    "nanotools.display_sample_rows(distplot_df, 5)\n",
    "#plot_title = concatenate \"N2\" and align_on\n",
    "plot_title = \"N2_\" + align_on\n",
    "\n",
    "#display(distplot_df.head(10))\n",
    "peaks1,fig1 = nanotools.plot_NRL_dist(distplot_df[(distplot_df['condition'] == \"N2_fiber\") ],\"X\",\"#16415e\",str(\"N2_\"+align_on),smoothing_val=0.2)\n",
    "display(peaks1.head(10))\n",
    "peaks2,fig2 = nanotools.plot_NRL_dist(distplot_df[(distplot_df['condition'] == \"SDC2_degron_fiber\") ],\"X\",\"#16415e\",str(\"SDC2_degron_\"+align_on),smoothing_val=0.2)\n",
    "display(peaks2.head(10))\n",
    "\n",
    "fig = nanotools.plot_NRL_dist_compare(distplot_df,\"X\",\"N2 and SDC2_degron\",smoothing_val=0.2,norm_bool=False,window=bed_window)\n",
    "fig.savefig('images/dpy_27_aligned_on_internuc_KDE_COMB.png', dpi=300, bbox_inches='tight')\n",
    "fig.savefig('images/dpy_27_aligned_on_internuc_KDE_COMB.svg', bbox_inches='tight')\n",
    "#fig.show()\n",
    "\n",
    "fig1.savefig('images/dpy_27_aligned_on_internuc_KDE_N2.png', dpi=300, bbox_inches='tight')\n",
    "fig1.savefig('images/dpy_27_aligned_on_internuc_KDE_N2.svg', bbox_inches='tight')\n",
    "\n",
    "fig2.savefig('images/dpy_27_aligned_on_internuc_KDE_SDC2_degron.png', dpi=300, bbox_inches='tight')\n",
    "fig2.savefig('images/dpy_27_aligned_on_internuc_KDE_SDC2_degron.svg', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007e7c13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T10:11:43.846987100Z",
     "start_time": "2023-11-09T10:11:41.609661600Z"
    }
   },
   "outputs": [],
   "source": [
    "### PLOT PERCENT MAD, NUC, OTHER\n",
    "# Initialize the Plotly figure\n",
    "# Create subplot\n",
    "\n",
    "# Create a temporary helper column that combines 'condition' and 'type'\n",
    "grouped['condition_type'] = grouped['condition'].astype(str) + \"_\" + grouped['type'].astype(str) + \"_\" + grouped['chr_type'].astype(str)\n",
    "\n",
    "# Create subplot with 1 row and 3 columns\n",
    "fig = make_subplots(rows=1, cols=3, subplot_titles=('Percent MAD', 'Percent NUC', 'Percent OTHER'))\n",
    "\n",
    "# Unique metrics\n",
    "metrics = ['percent_MAD', 'percent_NUC', 'percent_OTHER']\n",
    "\n",
    "# Loop over metrics\n",
    "for col, metric in enumerate(metrics, start=1):\n",
    "    fig.add_trace(\n",
    "        go.Box(\n",
    "            y=grouped[metric],\n",
    "            x=grouped['condition_type'],\n",
    "            name=metric,\n",
    "            legendgroup=metric,\n",
    "        ),\n",
    "        row=1, col=col\n",
    "    )\n",
    "\n",
    "# Drop the temporary helper column\n",
    "grouped.drop('condition_type', axis=1, inplace=True)\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Box Plots for Metrics by Condition and Type',\n",
    "    template='plotly_white'\n",
    ")\n",
    "\n",
    "#fig.show()\n",
    "\n",
    "# Extract integer column names\n",
    "int_columns = [col for col in mod_qual_LINK.columns if str(col).isdigit()]\n",
    "\n",
    "# Concatenate all integer columns for LINK and drop NaN values\n",
    "all_values_LINK = pd.concat([mod_qual_LINK[col] for col in int_columns]).dropna()\n",
    "\n",
    "# Concatenate all integer columns for NUC and drop NaN values\n",
    "all_values_NUC = pd.concat([mod_qual_NUC[col] for col in int_columns]).dropna()\n",
    "\n",
    "# Create a subplot with 1 row and 2 columns\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Boxplots\", \"Counts\"))\n",
    "\n",
    "# Add boxplot for LINK to the first subplot\n",
    "fig.add_trace(\n",
    "    go.Box(\n",
    "        y=all_values_LINK,\n",
    "        name='LINK',\n",
    "        marker_color='blue'\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Add boxplot for NUC to the first subplot\n",
    "fig.add_trace(\n",
    "    go.Box(\n",
    "        y=all_values_NUC,\n",
    "        name='NUC',\n",
    "        marker_color='red'\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Add bar chart for counts to the second subplot\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=['LINK', 'NUC'],\n",
    "        y=[len(all_values_LINK), len(all_values_NUC)],\n",
    "        name='Counts',\n",
    "        marker_color=['blue', 'red']\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Box Plots for NUCs and LINKs',\n",
    "    template='plotly_white'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48afdb8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T00:14:43.769736100Z",
     "start_time": "2023-11-09T00:14:43.627751900Z"
    }
   },
   "outputs": [],
   "source": [
    "### Plot n - n+1 nucleosome positioning variance\n",
    "import plotly.graph_objects as go\n",
    "# Assuming combined_res_df is the DataFrame you got after running calculate_feature for all combinations\n",
    "# Process combined_df similar to how you processed results_df\n",
    "df = combined_res_df.copy()\n",
    "\n",
    "# Create an empty list to store the data for the plot\n",
    "data = []\n",
    "\n",
    "# Loop through each unique combination of ['condition','chr_type','type']\n",
    "for comb in df[['condition', 'chr_type', 'type']].drop_duplicates().values:\n",
    "    condition, chr_type, dtype = comb\n",
    "    subset_df = df[(df['condition'] == condition) & (df['chr_type'] == chr_type) & (df['type'] == dtype)]\n",
    "\n",
    "    subs_list = []\n",
    "    nuc_ids = []\n",
    "    # Loop through each unique nuc_id\n",
    "    for nuc_id in subset_df['nuc_id'].unique():\n",
    "        individual_subs_list = subset_df[subset_df['nuc_id'] == nuc_id]['subs_list'].explode().dropna()\n",
    "        subs_list.extend(individual_subs_list)\n",
    "        nuc_ids.extend([nuc_id] * len(individual_subs_list))\n",
    "\n",
    "    trace_name = f\"{condition}_{chr_type}_{dtype}\"  # Name based on the unique combination\n",
    "    trace = go.Box(y=subs_list, name=trace_name, x=nuc_ids,\n",
    "                   #offset boxes from eachother\n",
    "                   offsetgroup=trace_name\n",
    "                    )  # x-axis is nuc_id\n",
    "    data.append(trace)\n",
    "\n",
    "# Create layout\n",
    "layout = go.Layout(\n",
    "    title=\"Boxplot of subs_list for each nuc_id\",\n",
    "    xaxis_title=\"Nucleotide ID\",\n",
    "    yaxis_title=\"Subs List Value\",\n",
    "    template='plotly_white',\n",
    "    boxmode='group'\n",
    ")\n",
    "\n",
    "# Create figure and add data and layout\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "\n",
    "\n",
    "# Show the figure\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f18d26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T02:51:07.483923100Z",
     "start_time": "2023-11-09T02:51:05.844752600Z"
    }
   },
   "outputs": [],
   "source": [
    "### Plot singe fiber nucleosome positioing and distribution\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Assuming combined_res_df is the DataFrame you got after running calculate_feature for all combinations\n",
    "df = combined_res_df.copy()\n",
    "\n",
    "# Drop all rows where abs(mean_nuc_pos) is > cutoff\n",
    "df = df[df['mean_nuc_pos'].abs() <= 750]\n",
    "\n",
    "# Create subplots with 2 rows and 1 column, sharing the same x-axis\n",
    "fig = make_subplots(rows=2, cols=1, shared_xaxes=True)\n",
    "\n",
    "# Color map to store unique colors for each trace_name\n",
    "color_map = {}\n",
    "\n",
    "# Loop through each unique combination of ['condition','chr_type','type']\n",
    "for idx, comb in enumerate(df[['condition', 'chr_type', 'type']].drop_duplicates().values):\n",
    "    condition, chr_type, dtype = comb\n",
    "    subset_df = df[(df['condition'] == condition) & (df['chr_type'] == chr_type) & (df['type'] == dtype)]\n",
    "\n",
    "    subs_list = []\n",
    "    mean_nuc_pos_list = []\n",
    "    percent_occ_list = []\n",
    "\n",
    "    # Loop through each unique nuc_id\n",
    "    for nuc_id in subset_df['nuc_id'].unique():\n",
    "        nuc_subset = subset_df[subset_df['nuc_id'] == nuc_id]\n",
    "        individual_subs_list = nuc_subset['subs_list'].explode().dropna()\n",
    "        subs_list.extend(individual_subs_list)\n",
    "\n",
    "        mean_nuc_pos_values = nuc_subset['mean_nuc_pos'].unique()\n",
    "        #mean_nuc_pos_values = nuc_subset['std_nuc_pos'].unique()\n",
    "        mean_nuc_pos_list.extend([mean_nuc_pos_values[0]] * len(individual_subs_list))\n",
    "\n",
    "        percent_occ_values = nuc_subset['percent_occ'].unique()\n",
    "        percent_occ_list.append((mean_nuc_pos_values[0], percent_occ_values[0]))\n",
    "\n",
    "    # Generate a unique color for each trace_name\n",
    "    unique_color = f'rgba({50+idx*500},{100+idx*5},{150+idx*5},0.8)'\n",
    "    color_map[tuple(comb)] = unique_color  # Convert numpy array to tuple\n",
    "\n",
    "\n",
    "    # Boxplot Trace\n",
    "    trace_name = f\"{condition}_{chr_type}_{dtype}\"\n",
    "    box_trace = go.Box(y=subs_list, name=trace_name, x=mean_nuc_pos_list,\n",
    "                       width=10, line=dict(color=unique_color))\n",
    "    fig.add_trace(box_trace, row=1, col=1)\n",
    "\n",
    "    # Bar Trace for percent_occ\n",
    "    bar_x, bar_y = zip(*percent_occ_list)\n",
    "    bar_trace = go.Bar(x=bar_x, y=bar_y, name=f\"{trace_name}_percent_occ\",\n",
    "                       marker=dict(color=unique_color), width=10)\n",
    "    fig.add_trace(bar_trace, row=2, col=1)\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title=\"Boxplot of subs_list and Barplot of percent_occ\",\n",
    "    xaxis_title=\"Mean Nucleotide Position\",\n",
    "    yaxis_title=\"Nuceosome offset\",\n",
    "    yaxis2_title=\"Percent Occupancy\",\n",
    "    template='plotly_white'\n",
    ")\n",
    "\n",
    "# Add a dashed vertical line at x=0\n",
    "fig.add_shape(type=\"line\", x0=0, y0=0, x1=0, y1=0.5, line=dict(color=\"Black\", width=1, dash=\"dash\"), row=2, col=1)\n",
    "fig.add_shape(type=\"line\", x0=0, y0=0, x1=0, y1=120, line=dict(color=\"Black\", width=1, dash=\"dash\"), row=1, col=1)\n",
    "\n",
    "# Change x axis tick marks to every 250\n",
    "fig.update_xaxes(dtick=250, row=1, col=1)\n",
    "fig.update_xaxes(dtick=250, row=2, col=1)\n",
    "\n",
    "# Show figure\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9205016",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T08:25:29.189872100Z",
     "start_time": "2023-11-15T08:25:28.385942300Z"
    }
   },
   "outputs": [],
   "source": [
    "# reimport nanotools\n",
    "importlib.reload(nanotools)\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'grouped' is your DataFrame\n",
    "grouped_clean = grouped.copy(deep=True)\n",
    "grouped_clean=grouped_clean.dropna(subset=['inter_nuc_dist'])\n",
    "# keep only rows where type == \"TSS_q4\"\n",
    "grouped_clean = grouped_clean[grouped_clean['type'] == \"strong_rex\"]\n",
    "# sort by type in alphabetical order\n",
    "grouped_clean = grouped_clean.sort_values(by=['condition','chr_type'])\n",
    "\n",
    "# Define subplot structure: 1 row, 2 columns\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=grouped_clean['chr_type'].unique())\n",
    "\n",
    "colors = {\"SDC2_degron_fiber\": 'red', \"N2_fiber\": 'blue', \"X\": 'red', \"Autosome\": 'blue'}\n",
    "\n",
    "# Add traces, specifying the correct subplot for each 'type'\n",
    "for type_value in grouped_clean['chr_type'].unique():\n",
    "    for i, condition_value in enumerate(grouped_clean['condition'].unique()):\n",
    "        df_filtered = grouped_clean[(grouped_clean['condition'] == condition_value) & (grouped_clean['chr_type'] == type_value)]\n",
    "        fig.add_trace(go.Box(\n",
    "            y=df_filtered['inter_nuc_dist'], #- NUC_width,\n",
    "            name=condition_value,\n",
    "            marker_color=colors.get(condition_value, 'black'),  # Default color if condition not in colors dict\n",
    "            fillcolor='rgba(0,0,0,0)',\n",
    "            boxmean=True\n",
    "        ), row=1, col=grouped_clean['chr_type'].unique().tolist().index(type_value) + 1)\n",
    "\n",
    "        # label the mean, centered\n",
    "        fig.add_annotation(\n",
    "            x=grouped_clean['condition'].unique().tolist().index(condition_value),\n",
    "            y=df_filtered['inter_nuc_dist'].mean(), #- NUC_width-50,\n",
    "            text=f\"{df_filtered['inter_nuc_dist'].mean():.2f}\",\n",
    "            showarrow=False,\n",
    "            yshift=8,\n",
    "            font=dict(\n",
    "                size=11,\n",
    "                color=\"black\"\n",
    "            ),\n",
    "            row=1, col=grouped_clean['chr_type'].unique().tolist().index(type_value) + 1\n",
    "        )\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title=\"Nucleosome N+1 - N Distance by Condition and Type\",\n",
    "    yaxis_title=\"Inter-nucleosome Distance (bp)\",\n",
    "    template=\"plotly_white\",\n",
    "    width = 600\n",
    ")\n",
    "\n",
    "# Set y-axis range\n",
    "fig.update_yaxes(range=[(grouped_clean['inter_nuc_dist']-NUC_width).quantile(0.05) * 0.8, (grouped_clean['inter_nuc_dist']-NUC_width).quantile(0.95) * 1.2], row=1, col=1)\n",
    "fig.update_yaxes(range=[(grouped_clean['inter_nuc_dist']-NUC_width).quantile(0.05) * 0.8, (grouped_clean['inter_nuc_dist']-NUC_width).quantile(0.95) * 1.2], row=1, col=2)\n",
    "\n",
    "try:\n",
    "    fig = nanotools.add_p_value_annotation(fig,[[0,1]],1)\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    fig = nanotools.add_p_value_annotation(fig,[[0,1]],2)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "# Define subplot structure: 1 row, 2 columns\n",
    "fig0 = make_subplots(rows=1, cols=2, subplot_titles=grouped_clean['chr_type'].unique())\n",
    "# Assuming 'grouped' is your DataFrame\n",
    "grouped_clean = grouped_clean.dropna(subset=['largest_nfr_size'])\n",
    "\n",
    "# Add traces, specifying the correct subplot for each 'type'\n",
    "for type_value in grouped_clean['chr_type'].unique():\n",
    "    for i, condition_value in enumerate(grouped_clean['condition'].unique()):\n",
    "        df_filtered = grouped_clean[(grouped_clean['condition'] == condition_value) & (grouped_clean['chr_type'] == type_value)]\n",
    "        fig0.add_trace(go.Box(\n",
    "            y=df_filtered['largest_nfr_size'],\n",
    "            name=condition_value,\n",
    "            marker_color=colors.get(condition_value, 'black'),  # Default color if condition not in colors dict\n",
    "            fillcolor='rgba(0,0,0,0)'\n",
    "        ), row=1, col=grouped_clean['chr_type'].unique().tolist().index(type_value) + 1)\n",
    "\n",
    "# Update layout\n",
    "fig0.update_layout(\n",
    "    title=\"Largest NFR Distance by Condition and Type\",\n",
    "    yaxis_title=\"Inter-nucleosome distance (bp)\",\n",
    "    template=\"plotly_white\",\n",
    "    width = 600\n",
    ")\n",
    "\n",
    "# Set y-axis range\n",
    "fig0.update_yaxes(range=[0, grouped_clean['largest_nfr_size'].quantile(0.95) * 1.1], row=1, col=1)\n",
    "fig0.update_yaxes(range=[0, grouped_clean['largest_nfr_size'].quantile(0.95) * 1.1], row=1, col=2)\n",
    "\n",
    "try:\n",
    "    fig0 = nanotools.add_p_value_annotation(fig0,[[0,1]],1)\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    fig0 = nanotools.add_p_value_annotation(fig0,[[0,1]],2)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Step 1: Expand the lists in 'inter_nuc_sub' into separate rows\n",
    "grouped_exploded = grouped_clean.explode('inter_nuc_sub')\n",
    "\n",
    "# Step 2: Drop rows with NaNs (which were originally empty lists)\n",
    "grouped_exploded = grouped_exploded.dropna(subset=['inter_nuc_sub'])\n",
    "\n",
    "# Define subplot structure: 1 row, 2 columns\n",
    "fig2 = make_subplots(rows=1, cols=2, subplot_titles=grouped_exploded['condition'].unique())\n",
    "\n",
    "# Generate a unique color for each 'condition'\n",
    "conditions_selec = grouped_exploded['condition'].unique()\n",
    "# reverse conditions\n",
    "conditions_selec = conditions_selec[::-1]\n",
    "\n",
    "# Add traces, specifying the correct subplot for each 'type'\n",
    "for type_value in grouped_exploded['chr_type'].unique():\n",
    "    for condition in conditions_selec:\n",
    "        df_filtered = grouped_exploded[(grouped_exploded['condition'] == condition) & (grouped_exploded['chr_type'] == type_value)]\n",
    "        fig2.add_trace(go.Box(\n",
    "            y=df_filtered['inter_nuc_sub'],\n",
    "            name=condition,\n",
    "            marker_color=colors.get(condition, 'black'),  # Default color if condition not in colors dict\n",
    "            fillcolor='rgba(0,0,0,0)'\n",
    "        ), row=1, col=grouped_exploded['chr_type'].unique().tolist().index(type_value) + 1)\n",
    "\n",
    "# Update layout\n",
    "fig2.update_layout(\n",
    "    title=\"Per-fiber nucleosome subtraction values\",\n",
    "    yaxis_title=\"Ni+1 - Ni (bp)\",\n",
    "    template=\"plotly_white\",\n",
    "    width = 600\n",
    ")\n",
    "\n",
    "# Set y-axis range\n",
    "fig2.update_yaxes(range=[120, 250], row=1, col=1)\n",
    "fig2.update_yaxes(range=[120, 250], row=1, col=2)\n",
    "\n",
    "# Optional: Add p-value annotation\n",
    "# This depends on the function `nanotools.add_p_value_annotation2`\n",
    "# fig2 = nanotools.add_p_value_annotation2(fig2, [[0,1]])\n",
    "\n",
    "try:\n",
    "    fig2 = nanotools.add_p_value_annotation(fig2,[[0,1]],1)\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    fig2 = nanotools.add_p_value_annotation(fig2,[[0,1]],2)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Reshape the DataFrame for percent_NUC and percent_MAD plotting\n",
    "nanotools.display_sample_rows(grouped_clean)\n",
    "melted_df = grouped_clean.melt(id_vars=['condition','chr_type'], value_vars=['percent_NUC', 'percent_MAD'], var_name='stat', value_name='percent')\n",
    "melted_df = melted_df.dropna(subset=['percent'])\n",
    "# reset index\n",
    "melted_df.reset_index(inplace=True, drop=True)\n",
    "conditions_list = melted_df['condition'].unique()\n",
    "fig3 = make_subplots(rows=2, cols=2, subplot_titles=melted_df['condition'].unique(),shared_yaxes=True)\n",
    "\n",
    "# Adding plots for percent_NUC and percent_MAD by condition\n",
    "for each_cond in conditions_list:\n",
    "    for each_type in melted_df['chr_type'].unique():\n",
    "        df_filtered = melted_df[(melted_df['condition'] == each_cond) & (melted_df['chr_type'] == each_type)]\n",
    "        #display(df_filtered.head(10))\n",
    "        fig3.add_trace(go.Box(y=df_filtered[df_filtered['stat'] == 'percent_NUC']['percent'],\n",
    "                              name=each_type,\n",
    "                              marker_color=colors.get(each_cond, 'black'),  # Default color if condition not in colors dict\n",
    "                              fillcolor='rgba(0,0,0,0)'  # Transparent fill\n",
    "                              ), row=1, col=melted_df['condition'].unique().tolist().index(each_cond) + 1)\n",
    "        fig3.add_trace(go.Box(y=df_filtered[df_filtered['stat'] == 'percent_MAD']['percent'],\n",
    "                              name=each_type,\n",
    "                              marker_color=colors.get(each_cond, 'black'),  # Default color if condition not in colors dict\n",
    "                              fillcolor='rgba(0,0,0,0)'  # Transparent fill\n",
    "                              ), row=2, col=melted_df['condition'].unique().tolist().index(each_cond) + 1)\n",
    "\n",
    "\n",
    "# Update layout for the fifth row's y-axis to show percentage\n",
    "fig3.update_yaxes(title_text=\"Percentage\", tickformat='.0%', row=1, col=1)\n",
    "fig3.update_yaxes(title_text=\"Percentage\", tickformat='.0%', row=2, col=1)\n",
    "# update y range of row=2 col=1\n",
    "\n",
    "# Update layout\n",
    "fig3.update_layout(\n",
    "    title=\"Boxplot of % Nucleosome and % NFR\",\n",
    "    yaxis_title=\"% Occupied\",\n",
    "    template=\"plotly_white\",\n",
    "    width = 600,\n",
    "    height = 800\n",
    "    # group box plots of the same subplot together\n",
    ")\n",
    "\n",
    "# set y axis range between 20% and 110%\n",
    "fig3.update_yaxes(range=[0.2, 1.1], row=1, col=1)\n",
    "fig3.update_yaxes(range=[0.2, 1.1], row=1, col=2)\n",
    "# set y axis range between 20% and 110%\n",
    "fig3.update_yaxes(range=[0, 0.95], row=2, col=1)\n",
    "fig3.update_yaxes(range=[0, 0.95], row=2, col=2)\n",
    "\n",
    "try:\n",
    "    fig3 = nanotools.add_p_value_annotation(fig3,[[0,1]],1)\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    fig3 = nanotools.add_p_value_annotation(fig3,[[0,1]],2)\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    fig3 = nanotools.add_p_value_annotation(fig3,[[0,1]],3)\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    fig3 = nanotools.add_p_value_annotation(fig3,[[0,1]],4)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Show plot\n",
    "fig.show()\n",
    "fig0.show()\n",
    "fig2.show()\n",
    "fig3.show()\n",
    "# save fig to images_11_14_23/inter_nuc_dist_boxplot_strong_rex.png and .svg\n",
    "fig.write_image(\"images_11_14_23/inter_nuc_dist_boxplot_strong_rex.png\")\n",
    "fig.write_image(\"images_11_14_23/inter_nuc_dist_boxplot_strong_rex.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d99ced",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T08:31:23.392478100Z",
     "start_time": "2023-11-15T08:31:19.889730200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Assuming 'grouped' is your DataFrame\n",
    "grouped_clean = grouped.copy(deep=True)\n",
    "grouped_clean=grouped_clean.dropna(subset=['inter_nuc_dist'])\n",
    "# keep only rows where type == \"TSS_q4\" or == \"TSS_q3\"\n",
    "grouped_clean = grouped_clean[grouped_clean['type'].isin([\"strong_rex\"])]\n",
    "#grouped_clean = grouped_clean[grouped_clean['chr_type'] == \"Autosome\"]\n",
    "nanotools.display_sample_rows(grouped_clean)\n",
    "\n",
    "colors = {\"SDC2_degron_fiber\": 'red', \"N2_fiber\": 'blue', \"X\": 'red', \"Autosome\": 'blue'}\n",
    "\n",
    "### Plot dist of internuc dist\n",
    "# Parameters\n",
    "bin_width = 1\n",
    "bin_range = range(147-NUC_width, 700, bin_width)  # Bin range from 170 to 600\n",
    "\n",
    "# Grouping the data by 'condition', 'type', and 'chr_type'\n",
    "grouped_clean_grouped = grouped_clean.groupby(['condition', 'type', 'chr_type'])\n",
    "\n",
    "# Updated plot data preparation with cumulative percentages\n",
    "cumulative_percentage_plot_data = []\n",
    "\n",
    "for group_name, group_df in grouped_clean_grouped:\n",
    "    group_df['inter_nuc_dist_adjusted'] = group_df['inter_nuc_dist'].clip(lower=147, upper=600+NUC_width) -NUC_width\n",
    "    # Total number of reads in the group\n",
    "    total_reads = group_df.shape[0]\n",
    "    # Create bins and calculate cumulative count for each bin\n",
    "    bin_counts = [group_df[group_df['inter_nuc_dist_adjusted'] >= bin_edge].shape[0] for bin_edge in bin_range]\n",
    "\n",
    "    # Convert counts to a cumulative count\n",
    "    #cumulative_counts = np.cumsum(bin_counts)\n",
    "\n",
    "    # Convert cumulative counts to percentages (excluding the first bin)\n",
    "    bin_percentages = (np.array(bin_counts[1:]) / total_reads)\n",
    "\n",
    "    # Bin centers for plotting (excluding the first bin)\n",
    "    bin_centers = np.array(bin_range[1:]) + (bin_width / 2)\n",
    "\n",
    "    # Adding to cumulative percentage plot data\n",
    "    cumulative_percentage_plot_data.append(go.Scatter(x=bin_centers,\n",
    "                                                      y=bin_percentages,\n",
    "                                                      mode='lines',\n",
    "                                                      name=str(group_name[0]+' '+group_name[2]+' '+group_name[1])))\n",
    "                                                      #marker_color=colors.get(group_name[0], 'black')))\n",
    "\n",
    "# Create the updated plot layout for cumulative percentages\n",
    "cumulative_percentage_layout = go.Layout(\n",
    "    title='Cumulative Percentage Distribution of inter_nuc_dist',\n",
    "    xaxis=dict(title='Inter-nucleosome distance (bp)'),\n",
    "    yaxis=dict(title='Cumulative Percentage of Reads (%)'),\n",
    "    template='plotly_white',\n",
    "    width = 600\n",
    ")\n",
    "\n",
    "# Set y axis to %\n",
    "cumulative_percentage_layout['yaxis']['tickformat'] = '.0%'\n",
    "\n",
    "# Create the updated figure for cumulative percentages\n",
    "cumulative_percentage_fig = go.Figure(data=cumulative_percentage_plot_data, layout=cumulative_percentage_layout)\n",
    "\n",
    "# Grouping the data by 'bed_start', 'chrom', 'condition', 'type', and 'chr_type'\n",
    "grouped_genes = grouped_clean.groupby(['bed_start', 'chrom', 'condition', 'type', 'chr_type'])\n",
    "\n",
    "### Plot difference plot\n",
    "# Assuming the first two groups are the ones you want to compare\n",
    "first_group_data = None\n",
    "second_group_data = None\n",
    "\n",
    "for i, (group_name, group_df) in enumerate(grouped_clean_grouped):\n",
    "    group_df['inter_nuc_dist_adjusted'] = group_df['inter_nuc_dist'].clip(lower=147, upper=600+NUC_width) - NUC_width\n",
    "    total_reads = group_df.shape[0]\n",
    "    bin_counts = [group_df[group_df['inter_nuc_dist_adjusted'] >= bin_edge].shape[0] for bin_edge in bin_range]\n",
    "    bin_percentages = (np.array(bin_counts[1:]) / total_reads)\n",
    "    bin_centers = np.array(bin_range[1:]) + (bin_width / 2)\n",
    "\n",
    "    if i == 0:\n",
    "        first_group_data = bin_percentages\n",
    "    elif i == 1:\n",
    "        second_group_data = bin_percentages\n",
    "        break\n",
    "\n",
    "# Calculate the difference between the first and second group\n",
    "difference = first_group_data - second_group_data\n",
    "\n",
    "# Plot for the difference\n",
    "difference_plot_data = go.Scatter(x=bin_centers, y=difference, mode='lines', name='Difference between first and second group')\n",
    "difference_layout = go.Layout(\n",
    "    title='Difference in Cumulative Percentage Distribution between First and Second Group',\n",
    "    xaxis=dict(title='Inter-nucleosome distance (bp)'),\n",
    "    yaxis=dict(title='Difference in Cumulative Percentage (%)'),\n",
    "    template='plotly_white',\n",
    "    width = 600,\n",
    "    #yaxis tickformat\n",
    "    yaxis_tickformat = '.0%'\n",
    ")\n",
    "\n",
    "difference_fig = go.Figure(data=[difference_plot_data], layout=difference_layout)\n",
    "\n",
    "\n",
    "# Preparing data for the box plot\n",
    "box_plot_data = []\n",
    "\n",
    "for group_name, group_df in grouped_genes:\n",
    "    # Calculate the percentage of reads in open configuration (inter_nuc_dist >= 180)\n",
    "    percent_open_reads = (group_df['inter_nuc_dist'] >= 190).sum() / group_df.shape[0]\n",
    "    #print(\"Starting on:\", group_name,\"with this many reads:\", len(group_df),\"with % open:\", percent_open_reads,sep=\"\\n\")\n",
    "\n",
    "    # Append the result with the condition and chr_type as categories\n",
    "    condition, chr_type = group_name[2], group_name[4]\n",
    "    box_plot_data.append({'condition': condition, 'chr_type': chr_type, 'percent_open_reads': percent_open_reads})\n",
    "\n",
    "\n",
    "# Convert to DataFrame for easier plotting\n",
    "box_plot_df = pd.DataFrame(box_plot_data)\n",
    "nanotools.display_sample_rows(box_plot_df)\n",
    "\n",
    "# Create the box plot\n",
    "box_fig = go.Figure()\n",
    "\n",
    "# Adding box plots for each chr_type\n",
    "chr_types = box_plot_df['chr_type'].unique()\n",
    "for chr_type in chr_types:\n",
    "    filtered_df = box_plot_df[box_plot_df['chr_type'] == chr_type]\n",
    "    box_fig.add_trace(go.Box(y=filtered_df['percent_open_reads'],\n",
    "                             x=filtered_df['condition'],\n",
    "                             name=chr_type,\n",
    "                             boxpoints='all',\n",
    "                             jitter=0.3))\n",
    "\n",
    "\n",
    "# Update layout\n",
    "box_fig.update_layout(\n",
    "    title='Percentage of Reads in Open Configuration by Condition and Chr_Type',\n",
    "    xaxis_title='Condition',\n",
    "    yaxis_title='Percentage of Reads in Open Configuration (%)',\n",
    "    template='plotly_white',\n",
    "    # group box plots\n",
    "    boxmode='group',\n",
    "    width = 300\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Create a box plot\n",
    "n1_fig = go.Figure()\n",
    "\n",
    "# Adding box plots for each chr_type\n",
    "chr_types = grouped_clean['condition'].unique()\n",
    "# flip the order of chr_types\n",
    "#chr_types = chr_types[::-1]\n",
    "for chr_type in chr_types:\n",
    "    filtered_df = grouped_clean[grouped_clean['condition'] == chr_type]\n",
    "    n1_fig.add_trace(go.Box(y=filtered_df['smallest_positive_nuc_midpoint'],\n",
    "                            #x=filtered_df['condition'],\n",
    "                            name=chr_type,\n",
    "                            marker_color=colors.get(chr_type, 'black'),\n",
    "                            fillcolor='rgba(0,0,0,0)'))  # Transparent fill\n",
    "\n",
    "n1_fig.update_layout(\n",
    "    title=\"Distribution of N+1 nucleosome position\",\n",
    "    xaxis_title=\"Type\",\n",
    "    yaxis_title=\"Smallest Positive Nucleotide Midpoint\",\n",
    "    template=\"plotly_white\",\n",
    "    width = 300,\n",
    "    #do not show legend\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "# Calculating and printing the variance\n",
    "print(\"\\nVariance of 'smallest_positive_nuc_midpoint' by 'condition':\")\n",
    "conditions = grouped_clean['condition'].unique()\n",
    "for condition in conditions:\n",
    "    subset_df = grouped_clean[grouped_clean['condition'] == condition]\n",
    "    variance = subset_df['smallest_positive_nuc_midpoint'].var()\n",
    "    print(f\"Condition: {condition}, Variance: {variance:.2f}\")\n",
    "\n",
    "# Show the updated plot\n",
    "cumulative_percentage_fig.show()\n",
    "\n",
    "difference_fig.show()\n",
    "# Show plot\n",
    "box_fig.show()\n",
    "# Distribution of N+1 nucleosome\n",
    "n1_fig.show()\n",
    "\n",
    "# save cumulative_percentage_fig and difference_fig to png and svg files in images_11_14_23/\n",
    "cumulative_percentage_fig.write_image(\"images_11_14_23/cumulative_percentage_dist_strong_rex.png\")\n",
    "cumulative_percentage_fig.write_image(\"images_11_14_23/cumulative_percentage_dist_strong_rex.svg\")\n",
    "difference_fig.write_image(\"images_11_14_23/difference_cumulative_percentage_dist_strong_rex.png\")\n",
    "difference_fig.write_image(\"images_11_14_23/difference_cumulative_percentage_dist_strong_rex.svg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce0ebd8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-07T00:16:54.005174800Z",
     "start_time": "2023-11-07T00:16:48.033040800Z"
    }
   },
   "outputs": [],
   "source": [
    "### Plot accessibility pileups based on single fiber alignments\n",
    "print(\"plot_df\")\n",
    "display(plot_df.head(3))\n",
    "print(\"grouped\")\n",
    "display(grouped.head(3))\n",
    "print(\"coverage_df\")\n",
    "display(coverage_df.head(3))\n",
    "# merge comb_bedmethyl_plot_df with grouped on read_id addig grouped's 'closest_nuc' column\n",
    "print(\"Merging plot_df and grouped...\")\n",
    "merged_df_access = pd.merge(plot_df, grouped[['read_id', 'closest_nuc','closest_MAD_midpoint','smallest_positive_nuc_midpoint','greatest_negative_nuc_midpoint']], on='read_id', how='left')\n",
    "# drop all rows where closest_nuc is NaN\n",
    "#merged_df_access.dropna(subset=['closest_nuc'], inplace=True)\n",
    "\n",
    "#subtract closest_nuc from rel_pos\n",
    "#merged_df_access['rel_pos'] -= merged_df_access['closest_MAD_midpoint']\n",
    "#merged_df_access['rel_pos'] -= ((merged_df_access['smallest_positive_nuc_midpoint']-merged_df_access['greatest_negative_nuc_midpoint'])/2+merged_df_access['greatest_negative_nuc_midpoint'])\n",
    "\n",
    "# drop rows such that each unique combination of condition, type and chr_type has the same number of records. Match the combination that has the fewest.\n",
    "\n",
    "# Group by 'condition', 'type', 'chr-type' and 'rel_pos', adding sum and count of 'mod_qual' column\n",
    "group_merge = merged_df_access.groupby(['condition', 'type', 'chr_type', 'rel_pos'])['mod_qual_bin'].agg(['sum', 'count']).reset_index()\n",
    "group_merge['raw_meth_frac'] = group_merge['sum'] / group_merge['count']\n",
    "\n",
    "# group coverage_df by 'condition' summing total_m6A and total_A\n",
    "coverage_group = coverage_df.groupby(['condition'])['total_m6a', 'total_A_m6A'].sum().reset_index()\n",
    "# set condition_m6A_frac to total_m6A / total_A\n",
    "coverage_group['condition_m6A_frac'] = coverage_group['total_m6a'] / coverage_group['total_A_m6A']\n",
    "\n",
    "#merge group_merge with coverage_group on 'condition'\n",
    "group_merge = pd.merge(group_merge, coverage_group[['condition', 'condition_m6A_frac']], on='condition', how='left')\n",
    "#Add weighted_m6A_frac column\n",
    "group_merge['weighted_norm_mod_frac'] = group_merge['raw_meth_frac'] / group_merge['condition_m6A_frac']\n",
    "\n",
    "#Rename rel_pos column to rel_start\n",
    "group_merge.rename(columns={'rel_pos': 'rel_start'}, inplace=True)\n",
    "display(group_merge.sample(n=10))\n",
    "\n",
    "\n",
    "region_fig = plot_bedmethyl(group_merge, conditions, chr_types=[\"Autosome\"], types=[\"her-1_TSS\"], strands=[\"all\"], window_size=50, selection_indices=[1,8], bed_window=1500)\n",
    "\n",
    "# save region_fig to temp folder\n",
    "region_fig[0].write_image(\"images/dpy27_sdc2_sdc3_aligned_nearest_nuc_n2vsSDC2.svg\")\n",
    "region_fig[0].write_image(\"images/dpy27_sdc2_sdc3_aligned_nearest_nuc_n2vsSDC2.png\",width=1600,height=1300)\n",
    "# merge grouped with coverage_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888ee2c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T05:49:51.601404900Z",
     "start_time": "2023-11-11T05:49:05.220420800Z"
    }
   },
   "outputs": [],
   "source": [
    "## Downsamplin for read plotting\n",
    "n_read_ids = 200  # max reads / condition for plotting\n",
    "# Function to downsample each group\n",
    "def downsample_group(group):\n",
    "    global bed_window\n",
    "    print(\"\\nProcessing group:\", group.name)  # Display group name (combination of 'condition', 'chr_type', 'type')\n",
    "    unique_read_ids = group['read_id'].unique()\n",
    "\n",
    "    # Check if downsampling is needed\n",
    "    if len(unique_read_ids) > n_read_ids:\n",
    "        # Filter read_ids based on length requirement\n",
    "        sampled_read_ids_long = group[\n",
    "            (group['rel_read_end'] - group['rel_read_start']) > (3/4 * bed_window)\n",
    "        ]['read_id'].unique()\n",
    "\n",
    "        # Sample read_ids based on the number required\n",
    "        if len(sampled_read_ids_long) > n_read_ids:\n",
    "            sampled_read_ids = pd.Series(sampled_read_ids_long).sample(n=n_read_ids).tolist()\n",
    "        else:\n",
    "            # Include additional read_ids if long ones are not enough\n",
    "            sampled_read_ids = sampled_read_ids_long.tolist()\n",
    "            remaining_ids = group[~group['read_id'].isin(sampled_read_ids)]['read_id'].unique()\n",
    "            additional_sampled_ids = pd.Series(remaining_ids).sample(n=(n_read_ids - len(sampled_read_ids))).tolist()\n",
    "            sampled_read_ids.extend(additional_sampled_ids)\n",
    "\n",
    "        downsampled_group = group[group['read_id'].isin(sampled_read_ids)]\n",
    "        return downsampled_group\n",
    "    else:\n",
    "        return group\n",
    "\n",
    "# print number of unique read_ids in merged_df\n",
    "print(\"Number of unique read_ids in merged_df:\", len(merged_df['read_id'].unique()))\n",
    "\n",
    "# Apply downsampling for each unique combination of 'condition', 'chr_type', 'type'\n",
    "down_sampled_plot_df = merged_df.groupby(['condition', 'chr_type', 'type']).apply(downsample_group).reset_index(drop=True)\n",
    "# print number of groups\n",
    "print(\"Number of groups:\", len(down_sampled_plot_df.groupby(['condition', 'chr_type', 'type'])))\n",
    "\n",
    "# Further processing on the downsampled DataFrame\n",
    "down_sampled_plot_df = down_sampled_plot_df.sort_values(by=['smallest_positive_nuc_midpoint', 'greatest_negative_nuc_midpoint'], ascending=[True, True])\n",
    "down_sampled_plot_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Assuming 'grouped' is another DataFrame you want to filter based on the downsampled read_ids\n",
    "nanotools.display_sample_rows(grouped)\n",
    "down_sampled_group_df = grouped[grouped['read_id'].isin(down_sampled_plot_df['read_id'])]\n",
    "#down_sampled_group_df = down_sampled_group_df[down_sampled_group_df['nucs_list'].apply(lambda x: isinstance(x, list))]\n",
    "# add read_count column to down_sampled_group_df by merging with down_sampled_plot_df on read_id\n",
    "\n",
    "# Subtract closest_nuc if not na otherwise subtract greatest_negative_nuc_midpoint to existing rel_pos\n",
    "#down_sampled_plot_df['rel_pos'] = down_sampled_plot_df['rel_pos'] - down_sampled_plot_df['closest_nuc']\n",
    "#down_sampled_plot_df['rel_pos'] = down_sampled_plot_df['rel_pos'] - ((down_sampled_plot_df['smallest_positive_nuc_midpoint']-down_sampled_plot_df['greatest_negative_nuc_midpoint'])/2 + down_sampled_plot_df['greatest_negative_nuc_midpoint'])\n",
    "\n",
    "# subtract closest_nuc from each item in nuc_list array in down_sampled_group_df\n",
    "#down_sampled_group_df['nucs_list_aligned'] = down_sampled_group_df.apply(lambda row: [item - row['closest_nuc'] for item in row['nucs_list']], axis=1)\n",
    "#nucs_list_aligned\n",
    "#nuc_list_internuc_aligned\n",
    "\n",
    "\n",
    "# Display all rows grouped by read_id, dropping all duplicate read_id rows\n",
    "nanotools.display_sample_rows(down_sampled_plot_df)\n",
    "nanotools.display_sample_rows(down_sampled_group_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4b9080",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T06:07:06.655357900Z",
     "start_time": "2023-11-11T06:06:55.505784900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reimport nanotools\n",
    "importlib.reload(nanotools)\n",
    "from scipy.signal import find_peaks\n",
    "### READ PLOT + NUCLEOSOME PLOT\n",
    "def create_plot(plot_df, group_df, condition, chr_type, data_type, plot_window):\n",
    "    print(\"Creating dataframes...\")\n",
    "    plot_df_copy = plot_df.copy(deep=True)\n",
    "    plot_df_copy = plot_df_copy[(plot_df_copy['condition'] == condition) &\n",
    "                                (plot_df_copy['chr_type'] == chr_type) &\n",
    "                                (plot_df_copy['type'] == data_type) &\n",
    "                                (plot_df_copy['rel_pos'] > -plot_window) &\n",
    "                                (plot_df_copy['rel_pos'] < plot_window)]\n",
    "\n",
    "    # drop rows where both smallest_positive_nuc_midpoint and greatest_negative_nuc_midpoint are NaN\n",
    "    plot_df_copy = plot_df_copy[~(plot_df_copy['smallest_positive_nuc_midpoint'].isna() & plot_df_copy['greatest_negative_nuc_midpoint'].isna())]\n",
    "    plot_df_copy = plot_df_copy.sort_values(by=['smallest_positive_nuc_midpoint', 'greatest_negative_nuc_midpoint'],ascending=[True, False])\n",
    "    plot_df_copy_nodups = plot_df_copy.drop_duplicates(subset=['read_id'])[['read_id','smallest_positive_nuc_midpoint', 'greatest_negative_nuc_midpoint']]\n",
    "    plot_df_copy.reset_index(inplace=True, drop=True)\n",
    "    plot_df_copy_nodups.reset_index(inplace=True, drop=True)\n",
    "    # use ngroup to create a incrementing column in ascending order\n",
    "    plot_df_copy_nodups['read_count'] = range(1, len(plot_df_copy_nodups) + 1)\n",
    "\n",
    "    #merge the read_count column back into plot_df_copy\n",
    "    plot_df_copy = pd.merge(plot_df_copy, plot_df_copy_nodups[['read_id', 'read_count']], on='read_id', how='left')\n",
    "\n",
    "    # drop rows from down_sampled_group_df_copy where read_id not in plot_df_copy read_ids\n",
    "    down_sampled_group_df_copy = group_df.copy(deep=True)\n",
    "    down_sampled_group_df_copy = down_sampled_group_df_copy[down_sampled_group_df_copy['read_id'].isin(plot_df_copy_nodups['read_id'])]\n",
    "    # merge read_count column from plot_df_copy_no_dups with down_sampled_group_df_copy on read_id\n",
    "    down_sampled_group_df_copy = pd.merge(down_sampled_group_df_copy, plot_df_copy_nodups[['read_id', 'read_count']], on='read_id', how='left')\n",
    "    #drop rows where nucs_list is nan\n",
    "    down_sampled_group_df_copy.dropna(subset=['nucs_list'], inplace=True)\n",
    "    nanotools.display_sample_rows(down_sampled_group_df_copy,10)\n",
    "\n",
    "    #display(plot_df_copy.head(100))\n",
    "\n",
    "    # Create a subplot with 3 rows and 1 column\n",
    "    fig = make_subplots(rows=3,\n",
    "                        cols=1,\n",
    "                        shared_xaxes=True,\n",
    "                        vertical_spacing=0.02,\n",
    "                        specs=[[{}], [{}], [{\"secondary_y\": True}]],# [{}]],\n",
    "                        row_heights=[0.7, 0.15, 0.15])\n",
    "\n",
    "    # Update xaxes for all subplots\n",
    "    fig.update_xaxes(range=[-plot_window, plot_window])\n",
    "\n",
    "\n",
    "    #print(\"plot_df_copy\")\n",
    "    #display(plot_df_copy.head(10))\n",
    "    # Calculate sum and count of mod_qual at each rel_pos\n",
    "    agg_df = plot_df_copy.groupby('rel_pos')['mod_qual_bin'].agg(['sum', 'count']).reset_index()\n",
    "    agg_df['ratio'] = agg_df['sum'] / agg_df['count']\n",
    "    # Calculate the moving average of the ratio with a centered window of 20\n",
    "    rolling_window_size=25\n",
    "    agg_df['moving_avg'] = agg_df['ratio'].rolling(window=rolling_window_size, center=True).mean()\n",
    "    #drop nan values\n",
    "    agg_df.dropna(inplace=True)\n",
    "    #print(\"agg_df:\",agg_df)\n",
    "    #display(agg_df.head(100))\n",
    "\n",
    "    # create occupancy_df where columns are read_id and\n",
    "    # Assuming genome_size is known\n",
    "    genome_size = 2 * plot_window  # Replace with your actual genome size\n",
    "\n",
    "    # Initialize a numpy array with zeros for each base pair in the genome region\n",
    "    read_counts = np.zeros(genome_size)\n",
    "\n",
    "    print(\"Adding m6a line traces...\")\n",
    "    # Add line traces for each unique read_id\n",
    "    for read_id in plot_df_copy['read_count'].unique():\n",
    "        read_data = plot_df_copy[plot_df_copy['read_count'] == read_id]\n",
    "        min_rel_pos = read_data['rel_pos'].min()\n",
    "        max_rel_pos = read_data['rel_pos'].max()\n",
    "\n",
    "        ## FOR CALCULATING OCCUPANCY\n",
    "        # Loop through the range of positions between min and max positions\n",
    "        for pos in range(int(min_rel_pos + plot_window), int(max_rel_pos + plot_window + 1)):\n",
    "            if 0 <= pos < genome_size:  # Check if pos is within the range\n",
    "                read_counts[pos] += 1\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=[min_rel_pos, max_rel_pos], y=[read_data['read_count'].iloc[0],read_data['read_count'].iloc[0]],\n",
    "                       mode='lines', line=dict(color='#000000', width=0.2),showlegend=False),row=1, col=1\n",
    "        )\n",
    "        # set y range\n",
    "\n",
    "    # drop rows where mod_qual == 0\n",
    "    #plot_df_dropped = plot_df_copy[plot_df_copy['mod_qual'] != 0]\n",
    "\n",
    "    print(\"Plotting nucleosomes...\")\n",
    "    ### PLOT NUCLEOSOMES\n",
    "    midpoints_list = []\n",
    "    x_coords = []\n",
    "    y_coords = []\n",
    "    # add a blue line for each read_count in down_sampled_group_df using read_count as y value and for each value in nuc_list (value-nuc_width/2),(value-nuc_width/2) as x values\n",
    "    for read_id in down_sampled_group_df_copy['read_count']:\n",
    "\n",
    "        read_data = down_sampled_group_df_copy[down_sampled_group_df_copy['read_count'] == read_id]\n",
    "        #print(read_data['nucs_list'])\n",
    "        # for each value in nuc_list column\n",
    "        # Initialize an empty list to store the x and y coordinates for the scatter plot\n",
    "\n",
    "        read_height = read_data['read_count'].iloc[0]\n",
    "        # drop nucs from nucs_list that are outside of plot_window\n",
    "        read_data['nucs_list'] = read_data['nucs_list'].apply(lambda x: [nuc for nuc in x if nuc >= -plot_window and nuc <= plot_window])\n",
    "\n",
    "        # Loop through the nucleotides and populate x_coords and y_coords\n",
    "        for nuc in read_data['nucs_list'].iloc[0]:\n",
    "            midpoints_list.append(nuc)  # Assuming midpoints_list is already defined\n",
    "            min_rel_pos = nuc - NUC_width / 2\n",
    "            max_rel_pos = nuc + NUC_width / 2\n",
    "\n",
    "            x_coords.extend([min_rel_pos, max_rel_pos, None])  # Use None to separate individual line segments\n",
    "            y_coords.extend([read_height, read_height, None])\n",
    "\n",
    "    # Upper scatter plot\n",
    "    scatter_trace = go.Scatter(x=plot_df_copy['rel_pos'], y=plot_df_copy['read_count'], mode='markers',\n",
    "                               marker=dict(size=2, color=plot_df_copy['mod_qual'], colorscale=[[0, '#33B8FF'], [1, '#FF5733']]))\n",
    "    fig.add_trace(scatter_trace, row=1, col=1)\n",
    "\n",
    "    # Add a single trace for all line segments\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=x_coords,\n",
    "            y=y_coords,\n",
    "            mode='lines',\n",
    "            line=dict(color='#33B8FF', width=2),\n",
    "            opacity=0.75,\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1\n",
    "        )\n",
    "\n",
    "    # Lower line plot for moving average of the ratio\n",
    "    line_trace = go.Scatter(x=agg_df['rel_pos'], y=agg_df['moving_avg'], mode='lines',\n",
    "                            #set color to match blue\n",
    "                            line=dict(color='#FF5733', width=2),\n",
    "                            # smooth line\n",
    "                            line_shape='spline')\n",
    "    fig.add_trace(line_trace, row=2, col=1)\n",
    "\n",
    "    print(\"Plotting histogram...\")\n",
    "    #midpoints_df = pd.DataFrame.from_dict(midpoints_dict, orient='index')\n",
    "    #display(midpoints_list)\n",
    "    # Add midpoint plot\n",
    "    rolling_window_size_hist = 20\n",
    "    hist_bins = int(round(2*plot_window/10)+1)\n",
    "    midpoint_histogram = go.Histogram(x=midpoints_list,\n",
    "                                      #histnorm='density',\n",
    "                                      nbinsx=hist_bins,\n",
    "                                      marker=dict(color='#33B8FF',opacity=0.8)\n",
    "                                      ) #\n",
    "    fig.add_trace(midpoint_histogram, row=3, col=1,secondary_y=False)\n",
    "\n",
    "    ### OVERLAY GAUSSIAN SMOOTHED PLOT ON TOP OF HISTOGRAM\n",
    "\n",
    "    print(\"Plotting gaussian smoothed plot...\")\n",
    "\n",
    "    # Assume midpoints_list contains midpoints of nucleosomes for the current plot\n",
    "    genome_size = 2 * plot_window  # Define the genome size based on the plot_window\n",
    "\n",
    "    # Initialize a numpy array with zeros for each base pair in the genome region\n",
    "    nucleosome_array = np.zeros(genome_size)\n",
    "\n",
    "    # Populate the nucleosome_array based on the midpoints\n",
    "    for midpoint in midpoints_list:\n",
    "        # Convert midpoint to an integer index\n",
    "        position_index = int(midpoint + plot_window)  # Shift by plot_window to handle negative positions\n",
    "        if 0 <= position_index < genome_size:  # Check if position_index is within the range\n",
    "            nucleosome_array[position_index] += 1\n",
    "\n",
    "    \"\"\"# Calculate the mean nucleosome density, avoiding division by zero\n",
    "    mean_density = np.mean(nucleosome_array[nucleosome_array > 0])\n",
    "    if mean_density == 0:\n",
    "        raise ValueError(\"Mean nucleosome density is zero. Check your midpoint values.\")\n",
    "\n",
    "    # Scale by 1/(mean nucleosome density)\n",
    "    scaled_nucleosome_array = nucleosome_array / mean_density\"\"\"\n",
    "\n",
    "    nucleosome_normalized = np.divide(nucleosome_array, read_counts, where=read_counts != 0)\n",
    "\n",
    "    # Apply Gaussian smoothing with a standard deviation of 20 base pairs\n",
    "    smoothed_nucleosome_array = gaussian_filter1d(nucleosome_normalized, 10)\n",
    "\n",
    "    # Generate x values for the smoothed density plot, shifting back by plot_window to align with the original coordinates\n",
    "    x_values = np.arange(-plot_window, plot_window, 1)\n",
    "\n",
    "    # Add the smoothed nucleosome density as a line trace to the third subplot\n",
    "    smoothed_trace = go.Scatter(\n",
    "        x=x_values,\n",
    "        y=smoothed_nucleosome_array,\n",
    "        mode='lines',\n",
    "        name='Smoothed Nucleosome Density',\n",
    "        line=dict(color='#007dfa', width=2),  # Adjust color and width as desired\n",
    "    )\n",
    "\n",
    "    print(\"Plotting peaks...\")\n",
    "    # Find indices of peaks in the smoothed nucleosome array\n",
    "    peaks, _ = find_peaks(smoothed_nucleosome_array)\n",
    "\n",
    "    # The y-range for the vertical lines\n",
    "    y_range = [smoothed_nucleosome_array.min(), smoothed_nucleosome_array.max()]\n",
    "\n",
    "    # Add vertical lines for each peak\n",
    "    for peak_idx in peaks:\n",
    "        # Convert index to x-coordinate\n",
    "        peak_pos = x_values[peak_idx]\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=[peak_pos, peak_pos],\n",
    "                y=y_range,\n",
    "                mode='lines',\n",
    "                line=dict(color='grey', width=0.5, dash='dash'),\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=3, col=1, secondary_y=True\n",
    "        )\n",
    "\n",
    "    # Add the new trace to the subplot\n",
    "    fig.add_trace(smoothed_trace, row=3, col=1,secondary_y=True)\n",
    "    ###\n",
    "\n",
    "    print(\"Plotting bigwig...\")\n",
    "    ## MNASE\n",
    "    \"\"\"bigwig_trace = nanotools.create_bigwig_trace(\"/Data1/reference/lieb_mnase_2017/GSM2098437_RT_rep1_MNaseTC_30m_smoothDyads_ce11.bw\", plot_df_copy)\"\"\"\n",
    "    ## GRO MINUS\n",
    "    #bigwig_trace = nanotools.create_bigwig_trace(\"/Data1/reference/lieb_gro_2013/GSM1056279_GRO-seq_N2_Emb_replicateAVG_WS230_RPKM_minus_ce11.bw\", plot_df_copy)\n",
    "    ## GRO PLUS\n",
    "    #bigwig_trace = nanotools.create_bigwig_trace(\"/Data1/reference/lieb_gro_2013/GSM1056279_GRO-seq_N2_Emb_replicateAVG_WS230_RPKM_plus_ce11.bw\", plot_df_copy)\n",
    "    # Now iterate through the list of traces and add them to the figure\n",
    "    \"\"\"for trace in bigwig_trace:\n",
    "        fig.add_trace(trace, row=4, col=1)\"\"\"\n",
    "\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(template=\"simple_white\",\n",
    "                      height=800,\n",
    "                      width=1100,\n",
    "                      )\n",
    "    fig.update_yaxes(title_text=\"Read_ID\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"% m6A\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Nucleosome Probability\", row=3, col=1)\n",
    "    fig.update_xaxes(title_text=\"Genomic location (bp)\", row=3, col=1)\n",
    "    # set y max to 60\n",
    "\n",
    "# Add Rex Line\n",
    "    fig.add_shape(\n",
    "        go.layout.Shape(\n",
    "            type=\"line\",\n",
    "            x0=0,\n",
    "            x1=0,\n",
    "            y0=0,\n",
    "            y1=1,\n",
    "            yref=\"paper\",\n",
    "            line=dict(\n",
    "                color=\"grey\",\n",
    "                width=1,\n",
    "                dash=\"dash\",\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    #fig.add_annotation(\n",
    "    #    x=0,\n",
    "    #    y=1,\n",
    "    #    yref=\"paper\",\n",
    "    #    text=\"rex\",\n",
    "    #    showarrow=False,\n",
    "    #    font=dict(\n",
    "    #        size=15,\n",
    "    #        color=\"grey\"\n",
    "    #    )\n",
    "    #)\n",
    "\n",
    "    return fig\n",
    "\n",
    "# Sample usage of the function\n",
    "selec_cond = \"SDC2_degron_fiber\"\n",
    "selec_type = \"intergenic_control\"\n",
    "selected_chr_type = \"Autosome\"\n",
    "# set title to path: \"images/\"+selec_cond+\"_t\"+selec_type+\"_r\"+str(n_read_ids)+\"_b\"+str(bed_window)+\"png\"\n",
    "fig_title = \"images/\"+selec_cond+\"_t_\"+selec_type+\"_r\"+str(n_read_ids)+\"_b\"+str(bed_window)+\"_chr\"+selected_chr_type+\"_gro_11-9-2013.png\"\n",
    "print(\"saving image...\")\n",
    "N2_fig = create_plot(down_sampled_plot_df, down_sampled_group_df,selec_cond, selected_chr_type, selec_type, int(round(bed_window,0)))\n",
    "N2_fig.write_image(fig_title,width=1600)\n",
    "#N2_fig.show(renderer='plotly_mimetype+notebook')\n",
    "#SDC2_fig = create_plot(down_sampled_plot_df, \"SDC2_degron_fiber\", \"X\", \"TSS_q4\", plot_window)\n",
    "#SDC2_fig.show(renderer='plotly_mimetype+notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45737977",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T23:20:01.915540900Z",
     "start_time": "2023-11-08T23:19:59.221518100Z"
    }
   },
   "outputs": [],
   "source": [
    "import pyBigWig\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "def calculate_correlations(grouped_df, bigwig_paths, bin_size):\n",
    "    exp_data = {}\n",
    "    correlations = []  # To store correlation results\n",
    "    bw_objects = [pyBigWig.open(path) for path in bigwig_paths]  # Open all bigwig files\n",
    "    grouped_df_copy = grouped_df.copy(deep=True)\n",
    "    ### Temporarily replace \"CHROMOSOME_\" with \"chr\" in chrom column in grouped_df\n",
    "    grouped_df_copy['chrom'] = grouped_df_copy['chrom'].apply(lambda x: x.replace(\"CHROMOSOME_\", \"chr\"))\n",
    "        # Adjust positions in the 'nucs_list' column\n",
    "\n",
    "    def min_max_normalize(array):\n",
    "        return (array - array.min()) / (array.max() - array.min())\n",
    "\n",
    "    def adjust_positions(row):\n",
    "        mid_point = row['bed_start'] + (row['bed_end'] - row['bed_start']) // 2\n",
    "        # Adjust each position in the list\n",
    "        return [int(pos + mid_point - 1) for pos in row['nucs_list']]\n",
    "\n",
    "    # Apply the adjustment to each row\n",
    "    grouped_df_copy['adjusted_nucs_list'] = grouped_df_copy.apply(adjust_positions, axis=1)\n",
    "\n",
    "    # Drop rows where 'adjusted_nucs_list' is empty or not a list\n",
    "    grouped_df_copy = grouped_df_copy[grouped_df_copy['adjusted_nucs_list'].map(lambda d: isinstance(d, list) and len(d) > 0)]\n",
    "\n",
    "    # Reset index after dropping rows\n",
    "    grouped_df_copy.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "    # Iterate over each experiment and chromosome\n",
    "    for (exp_id, chrom, bed_start, bed_end, cond), group in grouped_df_copy.groupby(['exp_id','chrom', 'bed_start', 'bed_end', 'condition']):\n",
    "        print(f\"Processing {exp_id}, {chrom},bed:,{bed_start}-{bed_end}\")\n",
    "        # Get nucleosome positions and bin them for each chromosome\n",
    "        all_positions = np.concatenate(group['nucs_list'].values)\n",
    "        # Subtract bed_end-bed_start/2 from each position in all_positions\n",
    "        all_positions = all_positions + (bed_end - bed_start) // 2\n",
    "        # drop all positions that are outside of the bed window\n",
    "        all_positions = all_positions[(all_positions >= 0) & (all_positions <= (bed_end - bed_start))]\n",
    "        binned_positions = all_positions // bin_size\n",
    "        # initialize binned_nucleosome_counts as a series of 0s between 0 and bed_end-bed_start\n",
    "        binned_nucleosome_counts = pd.Series(0, index=np.arange(0, (bed_end - bed_start) // bin_size))\n",
    "        # Count the number of nucleosomes in each bin\n",
    "        for position in binned_positions:\n",
    "            binned_nucleosome_counts[position] += 1\n",
    "\n",
    "        # if nucleosome_array is not the same length as binned_nucleosome_counts, initialize nucleosome_array with 0s\n",
    "        #if len(nucleosome_array) != len(binned_nucleosome_counts):\n",
    "        nucleosome_array = np.zeros(len(binned_nucleosome_counts))\n",
    "\n",
    "        # Populate the nucleosome_array based on the binned counts\n",
    "        for bin_start, count in binned_nucleosome_counts.items():\n",
    "            position_index = int(bin_start * bin_size)\n",
    "            nucleosome_array[position_index:int(position_index + bin_size)] += count\n",
    "\n",
    "        # Calculate the smoothed nucleosome array\n",
    "        smoothed_nucleosome_array = gaussian_filter1d(nucleosome_array, 20)\n",
    "        smoothed_nucleosome_array = min_max_normalize(smoothed_nucleosome_array)\n",
    "\n",
    "        # Store the smoothed array with its exp_id and condition\n",
    "        if cond not in exp_data:\n",
    "            exp_data[cond] = {}\n",
    "        if exp_id not in exp_data[cond]:\n",
    "            exp_data[cond][exp_id] = []\n",
    "\n",
    "        exp_data[cond][exp_id].append(smoothed_nucleosome_array)\n",
    "\n",
    "    # make list of unique chrom ,bed_start and bed_end in grouped_df\n",
    "    chrom_bed_start_bed_end_list = grouped_df_copy[['chrom', 'bed_start', 'bed_end']].drop_duplicates().values.tolist()\n",
    "    print(\"chrom_bed_start_bed_end_list:\",chrom_bed_start_bed_end_list)\n",
    "    # For each bigwig replicate\n",
    "    #for chrom, bed_start, bed_end in chrom_bed_start_bed_end_list:\n",
    "    for chrom, bed_start, bed_end in chrom_bed_start_bed_end_list:\n",
    "        print(f\"Processing {chrom},bed:,{bed_start}-{bed_end}\")\n",
    "        for i, bw in enumerate(bw_objects):\n",
    "            cond = \"N2-MNase\"\n",
    "            exp_id = \"MNase-seq-rep\" + str(i + 1)\n",
    "            # Get bigwig values for the entire chromosome and then trim to match the length of smoothed nucleosome array\n",
    "            bigwig_values = bw.values(chrom, bed_start, bed_end)\n",
    "\n",
    "            # Convert to numpy array and handle None values\n",
    "            bigwig_values = np.nan_to_num(bigwig_values)\n",
    "            bigwig_values = min_max_normalize(bigwig_values)\n",
    "            # Store the smoothed array with its exp_id and condition\n",
    "            if cond not in exp_data:\n",
    "                exp_data[cond] = {}\n",
    "            if exp_id not in exp_data[cond]:\n",
    "                exp_data[cond][exp_id] = []\n",
    "            exp_data[cond][exp_id].append(bigwig_values)\n",
    "\n",
    "    # Now calculate pairwise correlations for each condition\n",
    "    pairwise_correlations = []\n",
    "    all_exp_ids = [exp_id for exps in exp_data.values() for exp_id in exps]\n",
    "\n",
    "    # Ensure you have a list of unique experiment IDs if they can repeat across conditions\n",
    "    all_exp_ids = list(set(all_exp_ids))\n",
    "\n",
    "    for i in range(len(all_exp_ids)):\n",
    "        for j in range(len(all_exp_ids)):\n",
    "            exp_id1 = all_exp_ids[i]\n",
    "            exp_id2 = all_exp_ids[j]\n",
    "\n",
    "            # Find the condition for each experiment ID\n",
    "            condition1 = [cond for cond, exps in exp_data.items() if exp_id1 in exps][0]\n",
    "            condition2 = [cond for cond, exps in exp_data.items() if exp_id2 in exps][0]\n",
    "\n",
    "            # Now perform the correlation check for each k\n",
    "            for k in range(len(exp_data[condition1][exp_id1])):\n",
    "                # When exp_id1 is the same as exp_id2, we are comparing the same arrays\n",
    "                if exp_id1 == exp_id2:\n",
    "                    correlation = 1.0\n",
    "                else:\n",
    "                    array1 = exp_data[condition1][exp_id1][k]\n",
    "                    array2 = exp_data[condition2][exp_id2][k]\n",
    "                    if array1.any() and array2.any():\n",
    "                        correlation = spearmanr(array1, array2)[0]\n",
    "                    else:\n",
    "                        correlation = np.nan  # Assign NaN if either array is empty\n",
    "\n",
    "                pairwise_correlations.append((exp_id1, exp_id2, condition1, condition2, correlation))\n",
    "\n",
    "\n",
    "    # Close bigwig files\n",
    "    for bw in bw_objects:\n",
    "        bw.close()\n",
    "\n",
    "    # Convert results to dataframe and pivot to wide format for heatmap\n",
    "    # Construct a DataFrame from the pairwise correlations\n",
    "    pairwise_correlation_df = pd.DataFrame(pairwise_correlations, columns=['exp_id1', 'exp_id2', 'condition1', 'condition2', 'correlation'])\n",
    "    # Step 1 & 2: Combine the 'exp_id' and 'condition' columns\n",
    "\n",
    "    pairwise_correlation_df['exp_condition1'] = pairwise_correlation_df['condition1'] + '-' + pairwise_correlation_df['exp_id1']\n",
    "    pairwise_correlation_df['exp_condition2'] = pairwise_correlation_df['condition2'] + '-' + pairwise_correlation_df['exp_id2']\n",
    "\n",
    "    # Step 3: Pivot the DataFrame\n",
    "    pivot_df = pairwise_correlation_df.pivot(index='exp_condition1', columns='exp_condition2', values='correlation')\n",
    "\n",
    "    # Display the first 3 rows of the pivoted DataFrame\n",
    "    display(pivot_df.head(3))\n",
    "\n",
    "    return pivot_df\n",
    "\n",
    "# Define bigwig paths and bin size\n",
    "# Replace with your actual bigwig paths\n",
    "bigwig_paths = [\"/Data1/reference/lieb_mnase_2017/GSM2098437_RT_rep1_MNaseTC_30m_smoothDyads_ce11.bw\", \"/Data1/reference/lieb_mnase_2017/GSM2098437_RT_rep2_MNaseTC_30m_smoothDyads_ce11.bw\"]  # Replace with your actual bigwig file paths\n",
    "\n",
    "bin_size = 1  # Define your bin size accordingly\n",
    "\n",
    "nanotools.display_sample_rows(grouped,10)\n",
    "\n",
    "# Calculate correlations\n",
    "correlation_df = calculate_correlations(grouped, bigwig_paths, bin_size)\n",
    "\n",
    "# Display the correlation table (first 3 rows)\n",
    "nanotools.display_sample_rows(correlation_df)\n",
    "\n",
    "def plot_heatmap(correlation_matrix):\n",
    "    heatmap = go.Figure(data=go.Heatmap(\n",
    "        z=correlation_matrix.values,  # Correlation values\n",
    "        x=correlation_matrix.columns,  # exp_id as x-axis\n",
    "        y=correlation_matrix.index,  # exp_id as y-axis\n",
    "        colorscale='Viridis'\n",
    "    ))\n",
    "\n",
    "    heatmap.update_layout(\n",
    "        title='Heatmap of Pearson Correlation Coefficients',\n",
    "        xaxis_title=\"exp_id (X-axis)\",\n",
    "        yaxis_title=\"exp_id (Y-axis)\",\n",
    "        template=\"simple_white\"\n",
    "    )\n",
    "\n",
    "    heatmap.show()\n",
    "\n",
    "# Assuming correlation_df is a correlation matrix with exp_ids as index and columns\n",
    "plot_heatmap(correlation_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ebcc60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-07T02:47:27.111285100Z",
     "start_time": "2023-11-07T02:47:26.892432400Z"
    }
   },
   "outputs": [],
   "source": [
    "### STANDARD READ PLOT\n",
    "def create_plot(plot_df, condition, chr_type, data_type, bed_window):\n",
    "    plot_df_copy = plot_df.copy()\n",
    "    plot_df_copy.reset_index(inplace=True, drop=True)\n",
    "    # Filter the DataFrame based on the specified condition, chr_type, and data_type\n",
    "    plot_df_copy = plot_df_copy[(plot_df_copy['condition'] == condition) & (plot_df_copy['chr_type'] == chr_type) & (plot_df_copy['type'] == data_type)]\n",
    "    plot_df_copy = plot_df_copy.sort_values(by=['smallest_positive_nuc_midpoint', 'greatest_negative_nuc_midpoint','rel_pos'])\n",
    "    plot_df_copy.reset_index(inplace=True, drop=True)\n",
    "    # Create a lookup table of unique read_ids and read_count\n",
    "    read_id_lookup = plot_df_copy[['read_id', 'read_count','smallest_positive_nuc_midpoint', 'greatest_negative_nuc_midpoint']].drop_duplicates().reset_index(drop=True)\n",
    "    # reset read_count column to increment by 1 for each row\n",
    "    read_id_lookup['read_count'] = read_id_lookup.index + 1\n",
    "    #print(\"read_id_lookup:\")\n",
    "    #display(read_id_lookup.head(100))\n",
    "    # Create a new column 'read_count' in plot_df_copy by mapping the read_id_lookup\n",
    "    plot_df_copy['read_count'] = plot_df_copy['read_id'].map(read_id_lookup.set_index('read_id')['read_count'])\n",
    "\n",
    "    #display(plot_df_copy.head(100))\n",
    "\n",
    "    # Create a subplot with 3 rows and 1 column\n",
    "    fig = make_subplots(rows=3, cols=1, shared_xaxes=True, vertical_spacing=0.02, row_heights=[0.5, 0.2, 0.2])\n",
    "\n",
    "    # Update xaxes for all subplots\n",
    "    fig.update_xaxes(range=[-bed_window, bed_window])\n",
    "\n",
    "\n",
    "    #print(\"plot_df_copy\")\n",
    "    #display(plot_df_copy.head(10))\n",
    "    # Calculate sum and count of mod_qual at each rel_pos\n",
    "    agg_df = plot_df_copy.groupby('rel_pos')['mod_qual'].agg(['sum', 'count']).reset_index()\n",
    "    agg_df['ratio'] = agg_df['sum'] / agg_df['count']\n",
    "    # Calculate the moving average of the ratio with a centered window of 20\n",
    "    agg_df['moving_avg'] = agg_df['ratio'].rolling(window=50, center=True).mean()\n",
    "    #drop nan values\n",
    "    agg_df.dropna(inplace=True)\n",
    "    #print(\"agg_df:\",agg_df)\n",
    "    #display(agg_df.head(100))\n",
    "\n",
    "    # Add line traces for each unique read_id\n",
    "    for read_id in plot_df_copy['read_count'].unique():\n",
    "        read_data = plot_df_copy[plot_df_copy['read_count'] == read_id]\n",
    "        min_rel_pos = read_data['rel_pos'].min()\n",
    "        max_rel_pos = read_data['rel_pos'].max()\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=[min_rel_pos, max_rel_pos], y=[read_data['read_count'].iloc[0]] * 2,\n",
    "                       mode='lines', line=dict(color='#000000', width=0.2),showlegend=False),row=1, col=1\n",
    "        )\n",
    "    # drop rows where mod_qual == 0\n",
    "    plot_df_dropped = plot_df_copy[plot_df_copy['mod_qual'] != 0]\n",
    "\n",
    "    # Upper scatter plot\n",
    "    scatter_trace = go.Scatter(x=plot_df_dropped['rel_pos'], y=plot_df_dropped['read_count'], mode='markers',\n",
    "                               marker=dict(size=2, color=plot_df_dropped['mod_qual'], colorscale=[[0, '#FFFFFF'], [1, '#0000FF']]))\n",
    "    fig.add_trace(scatter_trace, row=1, col=1)\n",
    "\n",
    "\n",
    "\n",
    "    # Lower line plot for moving average of the ratio\n",
    "    line_trace = go.Scatter(x=agg_df['rel_pos'], y=agg_df['moving_avg'], mode='lines',\n",
    "                            #set color to match blue\n",
    "                            line=dict(color='#0000FF', width=1),\n",
    "                            # smooth line\n",
    "                            line_shape='spline')\n",
    "    fig.add_trace(line_trace, row=2, col=1)\n",
    "\n",
    "    ### PLOT NUCLEOSOMES\n",
    "    #NUC_max_width  = 160 # Distance below which m6A marks are combined into NUC\n",
    "    #NUC_min_width  = 100 # Distance above which m6A marks are combined into NUC\n",
    "    # Group by 'read_count'\n",
    "    grouped = plot_df_dropped.groupby('read_count')\n",
    "    midpoints_dict = {}\n",
    "    for read_count, group in grouped:\n",
    "        # Sort by 'rel_pos'\n",
    "        group = group.sort_values(by='rel_pos')\n",
    "        # Initialize an empty list to hold midpoints for this read_count\n",
    "        midpoints_list = []\n",
    "        for i in range(len(group) - 1):\n",
    "            x1 = group.iloc[i]['rel_pos']\n",
    "            x2 = group.iloc[i + 1]['rel_pos']\n",
    "            y = read_count\n",
    "            # Check if the x-values are < MAD_dist_max apart\n",
    "            if x2 - x1 > NUC_min_width  and x2 - x1 < NUC_max_width :\n",
    "                # Check if there are no other points between x1 and x2\n",
    "                in_between = group[(group['rel_pos'] > x1) & (group['rel_pos'] < x2)]\n",
    "                if in_between.empty:\n",
    "                    fig.add_trace(\n",
    "                        go.Scatter(x=[x1, x2], y=[y, y], mode='lines',\n",
    "                                   line=dict(color='#FF9999', width=1), showlegend=False),\n",
    "                        row=1, col=1\n",
    "                    )\n",
    "                    # Calculate the midpoint and add to list\n",
    "                    midpoint = (x1 + x2) / 2\n",
    "                    midpoints_list.append(midpoint)\n",
    "\n",
    "            # Add methylase accessible DNA sequences\n",
    "            if x2 - x1 <MAD_dist_max:\n",
    "                # Check if there are no other points between x1 and x2\n",
    "                in_between = group[(group['rel_pos'] > x1) & (group['rel_pos'] < x2)]\n",
    "                if in_between.empty:\n",
    "                    fig.add_trace(\n",
    "                        go.Scatter(x=[x1, x2], y=[y, y], mode='lines',\n",
    "                                   line=dict(color='#0000FF', width=1), showlegend=False,opacity=0.5),\n",
    "                        row=1, col=1\n",
    "                    )\n",
    "\n",
    "        # Add the list of midpoints to the dictionary\n",
    "        midpoints_dict[read_count] = midpoints_list\n",
    "\n",
    "    midpoints_df = pd.DataFrame.from_dict(midpoints_dict, orient='index')\n",
    "    midpoint_series = [midpoint for sublist in midpoints_dict.values() for midpoint in sublist]\n",
    "\n",
    "    # Add midpoint plot\n",
    "    midpoint_trace = go.Histogram(x=midpoint_series, histnorm='probability', nbinsx=100, marker=dict(color='#FF9999'))\n",
    "    fig.add_trace(midpoint_trace, row=3, col=1)\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(template=\"simple_white\")\n",
    "    fig.update_yaxes(title_text=\"Read_ID\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"% m6A\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Nucleosome Probability\", row=3, col=1)\n",
    "    fig.update_xaxes(title_text=\"Genomic location (bp)\", row=3, col=1)\n",
    "    fig.update_layout(height=800)\n",
    "    fig.update_layout(width=1100)\n",
    "    # set y max to 60\n",
    "\n",
    "# Add Rex Line\n",
    "    fig.add_shape(\n",
    "        go.layout.Shape(\n",
    "            type=\"line\",\n",
    "            x0=0,\n",
    "            x1=0,\n",
    "            y0=0,\n",
    "            y1=1,\n",
    "            yref=\"paper\",\n",
    "            line=dict(\n",
    "                color=\"grey\",\n",
    "                width=1,\n",
    "                dash=\"dash\",\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    #fig.add_annotation(\n",
    "    #    x=0,\n",
    "    #    y=1,\n",
    "    #    yref=\"paper\",\n",
    "    #    text=\"rex\",\n",
    "    #    showarrow=False,\n",
    "    #    font=dict(\n",
    "    #        size=15,\n",
    "    #        color=\"grey\"\n",
    "    #    )\n",
    "    #)\n",
    "\n",
    "    return fig\n",
    "\n",
    "# Sample usage of the function\n",
    "N2_fig = create_plot(down_sampled_plot_df, \"N2_fiber\", \"X\", \"center_DPY27_chip_albretton;SDC2_ol1000;SDC3_ol1000\", bed_window)\n",
    "N2_fig.write_image(\"images/N2_fiber_sdc3_sdc2_dpy27-fibers.png\",width=1600,height=1300)\n",
    "#N2_fig.show(renderer='plotly_mimetype+notebook')\n",
    "#SDC2_fig = create_plot(down_sampled_plot_df, \"SDC2_degron_fiber\", \"X\", \"TSS_q4\", bed_window)\n",
    "#SDC2_fig.show(renderer='plotly_mimetype+notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbac2881",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T23:34:15.545710Z",
     "start_time": "2023-10-17T23:33:40.608709400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save each fig to /images folder as svg, with the filename incorporating the condition and chr_type and type\n",
    "N2_fig.write_image(\"images/SDC2-degron_fiber_sdc3_sdc2_dpy27-fibers.svg\")\n",
    "#SDC2_fig.write_image(\"images/SDC2_degron_fiber_X_TSS_q4.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921d3e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plotting nucleosome offset\n",
    "# Convert the dictionary to a DataFrame\n",
    "midpoints_df = pd.DataFrame.from_dict(midpoints_dict, orient='index')\n",
    "#display(midpoints_dict)\n",
    "\n",
    "print(\"midpoints_df:\")\n",
    "display(midpoints_df.head(100))\n",
    "\n",
    "# Find the least positive value for each row\n",
    "least_positive_per_row = midpoints_df[midpoints_df > 0].min(axis=1)\n",
    "#least_positive_per_row = least_positive_per_row[least_positive_per_row<480]\n",
    "\n",
    "# Find the least negative value for each row\n",
    "least_negative_per_row = midpoints_df[midpoints_df < 0].max(axis=1)\n",
    "#least_negative_per_row = least_negative_per_row[least_negative_per_row > -480]\n",
    "\n",
    "# Calculate the difference between the least positive and least negative nucleosome position for each read\n",
    "differences = least_positive_per_row -least_negative_per_row\n",
    "\n",
    "# Plot the distribution of the differences using a histogram\n",
    "fig_diff = go.Figure()\n",
    "fig_diff.add_trace(go.Histogram(x=differences, marker=dict(color='#FF9999'),nbinsx=100))\n",
    "fig_diff.update_layout(title=\"Distribution of Nucleosome Position Differences\",\n",
    "                       xaxis_title=\"Difference between Least Positive and Least Negative Nucleosome Position\",\n",
    "                       yaxis_title=\"Frequency\",\n",
    "                       template=\"simple_white\")\n",
    "fig_diff.show()\n",
    "\n",
    "# Identify the least negative and the second least negative nucleosome positions for each read\n",
    "least_negative_per_row_sorted = midpoints_df[midpoints_df < 0].apply(lambda x: sorted(x.dropna()), axis=1)\n",
    "second_least_negative_per_row = least_negative_per_row_sorted.apply(lambda x: x[-2] if len(x) > 1 else np.nan)\n",
    "\n",
    "# Calculate the difference between the least negative and the second least negative nucleosome position for each read\n",
    "diff_second_least = least_negative_per_row - second_least_negative_per_row\n",
    "\n",
    "# Remove NaN values (reads that might not have a second least negative position)\n",
    "diff_second_least = diff_second_least.dropna()\n",
    "\n",
    "# Plot the distribution of the differences using a histogram\n",
    "fig_diff_second = go.Figure()\n",
    "fig_diff_second.add_trace(go.Histogram(x=diff_second_least, marker=dict(color='#FF9999'),nbinsx=100))\n",
    "fig_diff_second.update_layout(title=\"Distribution of Differences between Least Negative and Second Least Negative Nucleosome Positions\",\n",
    "                              xaxis_title=\"Difference between Least Negative and Second Least Negative Nucleosome Position\",\n",
    "                              yaxis_title=\"Frequency\",\n",
    "                              template=\"simple_white\")\n",
    "fig_diff_second.show()\n",
    "\n",
    "# Compute the averages\n",
    "avg_least_positive = least_positive_per_row.mean()\n",
    "avg_least_negative = least_negative_per_row.mean()\n",
    "print(\"avg_least_positive:\",avg_least_positive)\n",
    "print(\"avg_least_negative:\",avg_least_negative)\n",
    "\n",
    "# Calculate the mean of each column, ignoring NaN values\n",
    "range_n = 9\n",
    "assigned_pos = [avg_least_positive + (160 * i) for i in range(range_n)]\n",
    "assigned_neg = [avg_least_negative - ((range_n-1)*160) + (160*i) for i in range(range_n)]\n",
    "\n",
    "assigned_col = assigned_neg + assigned_pos\n",
    "print(\"assigned_col:\")\n",
    "print(assigned_col)\n",
    "\n",
    "# Create an empty DataFrame to store the rearranged values\n",
    "# Make it large enough to accommodate shifts; you can adjust the size as needed\n",
    "max_cols = midpoints_df.shape[1] * 2  # Example size, adjust as needed\n",
    "rearranged_df = pd.DataFrame(index=midpoints_df.index, columns=range(max_cols))\n",
    "# Create an empty DataFrame to store the rearranged values\n",
    "rearranged_df = pd.DataFrame(index=midpoints_df.index, columns=range(len(assigned_col)))\n",
    "\n",
    "\n",
    "# Iterate through each row to find the closest column mean and rearrange\n",
    "for idx, row in midpoints_df.iterrows():\n",
    "    row_values = row.dropna().values  # Drop NaN values\n",
    "    if len(row_values) == 0:  # Skip empty rows\n",
    "        continue\n",
    "\n",
    "    # For each value in row, find the closest column mean\n",
    "    for value in row_values:\n",
    "        closest_column = np.argmin(np.abs(assigned_col - value))\n",
    "        rearranged_df.at[idx, closest_column] = value  # Place the value in the closest column\n",
    "\n",
    "# Drop columns that are entirely NaN, if desired\n",
    "rearranged_df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "display(rearranged_df.head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c6decc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# find average value of each column and save as a list\n",
    "mean_list = []\n",
    "for col in rearranged_df.columns:\n",
    "    mean_list.append(rearranged_df[col].mean())\n",
    "print(\"mean_list:\")\n",
    "print(mean_list)\n",
    "\n",
    "for index, row in rearranged_df.iterrows():\n",
    "    non_nan_indices = row.dropna().index.tolist()\n",
    "\n",
    "    if non_nan_indices:  # Check if there are any non-NaN values in the row\n",
    "        start, end = non_nan_indices[0], non_nan_indices[-1]\n",
    "        # Use the column average for filling NaNs\n",
    "        rearranged_df.loc[index, start+1:end] = rearranged_df.loc[index, start+1:end].fillna(100000)\n",
    "\n",
    "# For each column, calculate the % of non-NaN values == 100000\n",
    "percent_100000 = (rearranged_df == 100000).sum() / len(rearranged_df)\n",
    "print(\"percent_100000:\")\n",
    "print(percent_100000)\n",
    "# Plot a go bar plot of percent_100000 with\n",
    "fig = go.Figure(data=go.Bar(x=[\"n-9\",\"n-8\",\"n-7\",\"n-6\",\"n-5\",\"n-4\",\"n-3\",\"n-2\",\"n-1\",\"n+1\",\"n+2\",\"n+3\",\"n+4\",\"n+5\",\"n+6\",\"n+7\",\"n+8\",\"n+9\"], y=percent_100000.values))\n",
    "\n",
    "# Create an empty DataFrame with the same shape as rearranged_df to store the mean differences\n",
    "mean_diff_df = pd.DataFrame(index=rearranged_df.index, columns=rearranged_df.columns)\n",
    "print(\"Rearranged df:\")\n",
    "display(rearranged_df.head(100))\n",
    "# Iterate through each row of rearranged_df\n",
    "for idx, row in rearranged_df.iterrows():\n",
    "    for col in rearranged_df.columns:\n",
    "        current_value = row[col]\n",
    "\n",
    "        # Check if the value is NaN; if so, continue to the next iteration\n",
    "        if pd.isna(current_value):\n",
    "            continue\n",
    "\n",
    "        if current_value == 100000.0:\n",
    "            #mean_diff_df.at[idx, col] = 80\n",
    "            continue\n",
    "\n",
    "        mean_diff_df.at[idx, col] = abs(current_value - mean_list[col])\n",
    "         # Calculate the differences between the current value and all other values in the row\n",
    "        differences = rearranged_df.subtract(current_value, axis=1)\n",
    "        #differences.at[idx, col] = np.nan\n",
    "        if idx == 1:\n",
    "            print(\"differences:\")\n",
    "            print(differences)\n",
    "\n",
    "        # For the current row, filter differences with absolute values less than 80\n",
    "        #valid_diffs = differences.loc[idx][differences.loc[idx].abs() < 80].abs()\n",
    "\n",
    "        # Take all differences with absolute values less than 80 and convert to a list\n",
    "        valid_diffs = differences[abs(differences) < 160].values.flatten().tolist()\n",
    "\n",
    "        # drop all nan values\n",
    "        valid_diffs = [x for x in valid_diffs if str(x) != 'nan']\n",
    "\n",
    "        # Take absolute value\n",
    "        valid_diffs = [abs(x) for x in valid_diffs]\n",
    "\n",
    "        # Count number of 100000 values in current column in rearranged_df and add this many \"80\"s to valid diff list\n",
    "        #valid_diffs.extend([80] * (rearranged_df[col] == 100000).sum())\n",
    "\n",
    "\n",
    "        # if first iteration, print valid diffs:\n",
    "        if idx == 1:\n",
    "            print(\"valid_diffs:\")\n",
    "            print(valid_diffs)\n",
    "\n",
    "        # Get the smallest difference value for the row\n",
    "        #smallest_diff = valid_diffs.min() if not valid_diffs.empty else np.nan  # Set to NaN if there are no valid differences\n",
    "\n",
    "        #get the mean of valid_diffs list\n",
    "        mean_diff_df.at[idx, col] = np.mean(valid_diffs) if valid_diffs else np.nan\n",
    "        # Store the smallest difference in the mean_diff_df\n",
    "        #mean_diff_df.at[idx, col] = smallest_diff\n",
    "\n",
    "print(\"mean_diff_df:\")\n",
    "display(mean_diff_df.head(10))\n",
    "\n",
    "rearranged_df_abs_diff = mean_diff_df.abs()\n",
    "#set col names to \"n-7\",\"n-6\",...,\"n+7\"\n",
    "rearranged_df_abs_diff.columns = [\"n-9\",\"n-8\",\"n-7\",\"n-6\",\"n-5\",\"n-4\",\"n-3\",\"n-2\",\"n-1\",\"n+1\",\"n+2\",\"n+3\",\"n+4\",\"n+5\",\"n+6\",\"n+7\",\"n+8\",\"n+9\"]\n",
    "#display(rearranged_df_abs_diff)\n",
    "\n",
    "# If you want to combine n- and n+ nucleosomes:\n",
    "combine_nucleosomes=0\n",
    "if combine_nucleosomes==1:\n",
    "    # Separate the columns into two sets: 'n-x' and 'n+x'\n",
    "    cols_n_minus = [\"n-9\",\"n-8\",\"n-7\", \"n-6\", \"n-5\", \"n-4\", \"n-3\", \"n-2\", \"n-1\"]\n",
    "    cols_n_plus = [\"n+1\", \"n+2\", \"n+3\", \"n+4\", \"n+5\", \"n+6\", \"n+7\",\"n+8\",\"n+9\"]\n",
    "\n",
    "    # Extract columns for 'n-x' and 'n+x'\n",
    "    n_minus_df = rearranged_df_abs_diff[cols_n_minus]\n",
    "    n_plus_df = rearranged_df_abs_diff[cols_n_plus]\n",
    "\n",
    "    # Flip the 'n-x' columns in reverse order\n",
    "    flipped_n_minus_df = n_minus_df[cols_n_minus[::-1]]\n",
    "\n",
    "    # Convert both DataFrames to NumPy arrays\n",
    "    n_plus_np = n_plus_df.to_numpy()\n",
    "    flipped_n_minus_np = flipped_n_minus_df.to_numpy()\n",
    "\n",
    "    # Vertically concatenate the NumPy arrays\n",
    "    result_np = np.vstack([n_plus_np, flipped_n_minus_np])\n",
    "\n",
    "    # Convert the result back to a DataFrame with the original 'n+x' column names\n",
    "    result_df = pd.DataFrame(result_np, columns=cols_n_plus)\n",
    "\n",
    "else:\n",
    "    result_df = rearranged_df_abs_diff\n",
    "\n",
    "#print 100 rows of rearranged_df\n",
    "print(\"result_df:\")\n",
    "display(result_df.head(10))\n",
    "\n",
    "#print(result_df)\n",
    "# Prepare data for box plot\n",
    "data = []\n",
    "for col in result_df.columns:\n",
    "    col_data = result_df[col].dropna()  # Remove NaN values\n",
    "    trace = go.Box(\n",
    "        y=col_data,\n",
    "        name=str(col),\n",
    "        boxpoints='all',  # Show all points\n",
    "        jitter=0.3,  # Add some jitter for visibility\n",
    "        pointpos=-1.8  # Position of the points\n",
    "    )\n",
    "    data.append(trace)\n",
    "\n",
    "# Create layout\n",
    "layout = go.Layout(\n",
    "    title=\"Box Plot of Rearranged DataFrame\",\n",
    "    xaxis=dict(title=\"Index\"),\n",
    "    yaxis=dict(title=\"Values\")\n",
    ")\n",
    "\n",
    "# Create the figure\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "#set theme to plotly_white\n",
    "fig.update_layout(template=\"simple_white\")\n",
    "\n",
    "# Add label for average values to each box plot\n",
    "for i, col in enumerate(result_df.columns):\n",
    "    col_data = result_df[col].dropna()  # Remove NaN values\n",
    "    print(col_data.mean())\n",
    "    fig.add_annotation(\n",
    "        x=i ,\n",
    "        y=col_data.mean(),\n",
    "        text=f\"{col_data.mean():.2f}\",\n",
    "        showarrow=False,\n",
    "        font=dict(\n",
    "            size=10,\n",
    "            color=\"black\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Show the figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3aea50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-14T17:53:41.366052400Z",
     "start_time": "2023-10-14T17:53:41.312028700Z"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "\n",
    "# Function to plot data for each Cluster_ID\n",
    "def plot_cluster(df_cluster):\n",
    "    fig = make_subplots(rows=2, cols=1, shared_xaxes=True, vertical_spacing=0.02, row_heights=[0.7, 0.3])\n",
    "\n",
    "    # Aggregate mod_qual by rel_pos\n",
    "    agg_df = df_cluster.groupby('rel_pos')['mod_qual'].agg(['sum', 'count']).reset_index()\n",
    "    agg_df['ratio'] = agg_df['sum'] / agg_df['count']\n",
    "    agg_df['moving_avg'] = agg_df['ratio'].rolling(window=50, center=True).mean()\n",
    "    agg_df.dropna(inplace=True)\n",
    "\n",
    "    # Add line traces for each unique read_id\n",
    "    for read_id in df_cluster['read_id'].unique():\n",
    "        read_data = df_cluster[df_cluster['read_id'] == read_id]\n",
    "        min_rel_pos = read_data['rel_pos'].min()\n",
    "        max_rel_pos = read_data['rel_pos'].max()\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=[min_rel_pos, max_rel_pos], y=[read_data['read_count'].iloc[0]] * 2,\n",
    "                       mode='lines', line=dict(color='gray', width=0.1), showlegend=False),\n",
    "            row=1, col=1\n",
    "        )\n",
    "\n",
    "    # Drop rows where mod_qual == 0\n",
    "    df_cluster_dropped = df_cluster[df_cluster['mod_qual'] != 0]\n",
    "\n",
    "    # Upper scatter plot\n",
    "    scatter_trace = go.Scatter(x=df_cluster_dropped['rel_pos'], y=df_cluster_dropped['read_count'], mode='markers',\n",
    "                               marker=dict(size=3, color=df_cluster_dropped['mod_qual'], colorscale=[[0, '#FFFFFF'], [1, '#0000FF']]))\n",
    "    fig.add_trace(scatter_trace, row=1, col=1)\n",
    "\n",
    "    # Lower line plot for moving average of the ratio\n",
    "    line_trace = go.Scatter(x=agg_df['rel_pos'], y=agg_df['moving_avg'], mode='lines',\n",
    "                            line=dict(color='#0000FF', width=1),\n",
    "                            line_shape='spline')\n",
    "    fig.add_trace(line_trace, row=2, col=1)\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(template=\"simple_white\", height=800)\n",
    "\n",
    "    # Show figure\n",
    "    fig.show()\n",
    "\n",
    "# Assuming plot_df is already loaded\n",
    "# Group by Cluster_ID and plot each group\n",
    "for cluster_id, group_df in plot_df.groupby('Cluster_ID'):\n",
    "    print(f\"Plotting for Cluster_ID: {cluster_id}\")\n",
    "    plot_cluster(group_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663d925f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Achieve the same as above using modbampy\n",
    "# NOTE This is unable to define methylation threshold\n",
    "# print(\"Kicking off loop for:\",new_bed_files,\"and\",new_bam_files)\n",
    "'''import modbampy\n",
    "importlib.reload(modbampy)\n",
    "from modbampy import ModBam\n",
    "\n",
    "# Assuming bedwindow is already defined in your code\n",
    "columns = [int(i) for i in range(1, 2*bed_window + 1)]\n",
    "index_desc = ['a', 'c', 'g', 't', 'A', 'C', 'G', 'T', 'd', 'D', 'm', 'M', 'f', 'F', 'n', 'N','Unk1','Unk2']\n",
    "'''A, C, G, T are the usual DNA bases,\n",
    "D indicates deletion counts,\n",
    "M modified base counts,\n",
    "F filtered counts - bases in reads with a modified-base record but which were filtered according to the thresholds provided.\n",
    "N no call base counts.'''\n",
    "counts_df = pd.DataFrame(columns=columns,index=index_desc)\n",
    "#Set all values to 0\n",
    "counts_df = counts_df.fillna(0)\n",
    "\n",
    "# Load the BED file\n",
    "print(\"Kicking off loop for:\",new_bed_files,\"and\",new_bam_files)\n",
    "for bed_file in new_bed_files:\n",
    "    print(\"Starting on bed_file:\",bed_file)\n",
    "    regions = pysam.TabixFile(bed_file)\n",
    "    # Iterate over the regions in the BED file\n",
    "    for region in regions.fetch(multiple_iterators=True):\n",
    "        #print(\"Region:\",region)\n",
    "        # Split the region string into the chromosome, start, and end positions\n",
    "        chromosome, start, end, strand, region_type, chr_type = region.split()\n",
    "        start = int(start)\n",
    "        end = int(end)\n",
    "        for bam_file in new_bam_files[0]:\n",
    "            with ModBam(bam_file) as bam:\n",
    "                positions, counts = bam.pileup(chromosome, start, end,threshold=0.1,mod_base=\"a\")\n",
    "            positions_reset = positions - start\n",
    "            #convert positions from np array to list\n",
    "            positions_list = positions_reset.tolist()\n",
    "\n",
    "            # Create temp_counts_df from numpy array counts\n",
    "            #print(\"counts shape\",counts.shape)\n",
    "            #print(\"counts\",counts)\n",
    "\n",
    "            #print(\"counts shape\",counts.T.shape)\n",
    "            #print(\"counts.T\",counts.T)\n",
    "            temp_counts_df = pd.DataFrame(counts.T,index=index_desc,columns=positions_list)\n",
    "            # set row index to be the same as counts_df\n",
    "            #print(\"print(temp_counts_df.shape)\",temp_counts_df.shape)\n",
    "            #print(\"print(counts_df.shape)\",counts_df.shape)\n",
    "            #print(\"Temp_count_df:\",temp_counts_df)\n",
    "            counts_df = counts_df.astype(float)\n",
    "            temp_counts_df = temp_counts_df.astype(float)\n",
    "\n",
    "            # Sum temp_counts_df to counts_df\n",
    "            counts_df = counts_df.add(temp_counts_df, fill_value=0)\n",
    "\n",
    "\n",
    "print(\"COUNTS_DF:\",counts_df)\n",
    "print(\"shape of counts_df:\",counts_df.shape)\n",
    "'''\n",
    "\n",
    "'''counts_df_plot = counts_df.copy()\n",
    "# Merge rows A and a, C and c, G and g, T and t, M and m, D and d, F and f, N and n, Unk1 and Unk2\n",
    "counts_df_plot.loc['A'] = counts_df_plot.loc['A'] + counts_df_plot.loc['a']\n",
    "counts_df_plot.loc['C'] = counts_df_plot.loc['C'] + counts_df_plot.loc['c']\n",
    "counts_df_plot.loc['G'] = counts_df_plot.loc['G'] + counts_df_plot.loc['g']\n",
    "counts_df_plot.loc['T'] = counts_df_plot.loc['T'] + counts_df_plot.loc['t']\n",
    "counts_df_plot.loc['M'] = counts_df_plot.loc['M'] + counts_df_plot.loc['m']\n",
    "counts_df_plot.loc['D'] = counts_df_plot.loc['D'] + counts_df_plot.loc['d']\n",
    "counts_df_plot.loc['F'] = counts_df_plot.loc['F'] + counts_df_plot.loc['f']\n",
    "counts_df_plot.loc['N'] = counts_df_plot.loc['N'] + counts_df_plot.loc['n']\n",
    "counts_df_plot.loc['Unk1'] = counts_df_plot.loc['Unk1'] + counts_df_plot.loc['Unk2']\n",
    "#drop merged rows\n",
    "counts_df_plot = counts_df_plot.drop(['a','c','g','t','m','d','f','n','Unk2'])\n",
    "# Add m6A_frac row\n",
    "counts_df_plot.loc['m6A_frac'] = counts_df_plot.loc['M'] / (counts_df_plot.loc['A'])\n",
    "print(counts_df_plot.index)\n",
    "#Sort dataframe by columns in ascending order\n",
    "counts_df_plot = counts_df_plot.sort_index(axis=1)'''\n",
    "\n",
    "'''# Plot m6A_frac in a line plot using plotly\n",
    "# Plotting with Plotly\n",
    "# Set plotly renderer to notebook\n",
    "\n",
    "# Compute the moving average for smoothing\n",
    "window_size = 25  # This defines the number of data points to use for each average value\n",
    "m6A_data = counts_df_plot.loc['m6A_frac']\n",
    "smoothed_data = m6A_data.rolling(window=window_size).mean()\n",
    "\n",
    "# Plotting with Plotly\n",
    "fig = go.Figure()\n",
    "\n",
    "# Original data\n",
    "fig.add_trace(go.Scatter(x=m6A_data.index, y=m6A_data.values,\n",
    "                         mode='lines',\n",
    "                         name='Original'))\n",
    "\n",
    "# Smoothed data\n",
    "fig.add_trace(go.Scatter(x=smoothed_data.index, y=smoothed_data.values,\n",
    "                         mode='lines',\n",
    "                         name=f'Smoothed (window size: {window_size})'))\n",
    "\n",
    "fig.update_layout(title='m6A Fraction vs Genomic Position',\n",
    "                  xaxis_title='Genomic Position',\n",
    "                  yaxis_title='m6A Fraction')\n",
    "\n",
    "# Set to plotly white theme\n",
    "fig.update_layout(template=\"plotly_white\")\n",
    "\n",
    "fig.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70070eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extract m6A frac by region\n",
    "importlib.reload(nanotools)\n",
    "result_list=[]\n",
    "result_df=pd.DataFrame()\n",
    "# Parallelize for each bam file:\n",
    "args_list = [(bam_file, condition, bam_frac,file_prefix, selection, m6A_thresh, output_stem,new_bed_files) for bam_file, condition, bam_frac in zip(new_bam_files,conditions,bam_fracs)]\n",
    "print(\"Args list:\",args_list)\n",
    "if __name__ == \"__main__\":\n",
    "    with Pool(processes=10) as pool: #processes=1\n",
    "        # append results to pandas df 'result'\n",
    "        result_list = pool.starmap(nanotools.extract_m6A_per_region_parellized, args_list)\n",
    "        for result in result_list:\n",
    "            result_df=pd.concat([result_df,result])\n",
    "    print(\"Program finished!\")\n",
    "\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e2dfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Build dataframe for plotting\n",
    "def reindex_df(df, weight_col):\n",
    "    \"\"\"expand the dataframe to prepare for resampling\n",
    "    result is 1 row per count per sample\"\"\"\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df = df.reindex(df.index.repeat(np.ceil(df[weight_col])/100000))\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return(df)\n",
    "\n",
    "'''# If combined regions file already exists, read dataframe from csv\n",
    "if os.path.exists(output_stem  + file_prefix + \"weighted_combined_regions_\"  + str(m6A_thresh) +\".csv\"):\n",
    "    weighted_combined_regions = pd.read_csv(output_stem + file_prefix + \"weighted_combined_regions_\"  + str(m6A_thresh) +\".csv\")\n",
    "    print(\"File: \",\n",
    "          output_stem +  file_prefix+\"combined_regions_\"  + str(m6A_thresh) +\".csv\",\n",
    "          \"already exists! Imported directly:\")\n",
    "    print(weighted_combined_regions)\n",
    "\n",
    "else:'''\n",
    "print(\"Building combined regions file...\")\n",
    "# Initialize variables\n",
    "filenames = []\n",
    "df_list = []\n",
    "combined_regions = []\n",
    "\n",
    "# Create \"filenames\" list that includes the name of each file to be read\n",
    "for each_type in selection:\n",
    "    for each_cond, each_frac in zip(conditions,bam_fracs):\n",
    "        filenames.append(output_stem + file_prefix+\"m6A_frac_\" + each_cond + \"_\"  + str(m6A_thresh)+\"_\"+each_type+\".csv\")\n",
    "\n",
    "# Loop through the list of file names\n",
    "for filename in filenames:\n",
    "    # Read each file into a dataframe\n",
    "    df = pd.read_csv(filename)\n",
    "    # Add the dataframe to the list of dataframes\n",
    "    df_list.append(df)\n",
    "\n",
    "# Concatenate the list of dataframes into a single dataframe\n",
    "combined_regions = pd.concat(df_list)\n",
    "\n",
    "# Reindex the dataframe to have the number repeated rows based on total bases in the region\n",
    "# This helps ensure plots are weighted correctly.\n",
    "weighted_combined_regions = reindex_df(combined_regions,'total_bases')\n",
    "\n",
    "# Add column equal to average of autosome m6A_frac column for each condition\n",
    "weighted_combined_regions['mean_autosome_m6A_frac'] = weighted_combined_regions.groupby('condition')['m6A_frac'].transform('mean')\n",
    "\n",
    "# Add column equal to m6A normalized by the condition's mean_autosome_m6A_frac\n",
    "weighted_combined_regions['norm_m6A_frac'] = weighted_combined_regions['m6A_frac']/weighted_combined_regions['mean_autosome_m6A_frac']\n",
    "\n",
    "# Save final dataframe to .csv file\n",
    "print(\"Weighted combined:\",weighted_combined_regions)\n",
    "print(\"Outputting file:\",output_stem  + file_prefix+\"combined_regions_\"  + str(m6A_thresh) +\".csv\")\n",
    "weighted_combined_regions.to_csv(output_stem  + file_prefix+\"weighted_combined_regions_\"  + str(m6A_thresh) +\".csv\", index=False, mode='w')\n",
    "\n",
    "# Extract average m6A/A across each chromosome for each condition from weighted_combined_regions\n",
    "# This is used for plotting the average m6A/A across the chromosome\n",
    "chromosome_m6A_frac = weighted_combined_regions.groupby(['condition','condition_min','chr_type'])['m6A_frac'].median().reset_index()\n",
    "# split condition column with character \"-\" and keep only first column\n",
    "chromosome_m6A_frac['genotype'] = chromosome_m6A_frac['condition'].str.split('-').str[0]\n",
    "\n",
    "# sort by genotype, chr_type and condition_min\n",
    "chromosome_m6A_frac.sort_values(by=['genotype','chr_type','condition_min'], inplace=True)\n",
    "\n",
    "#Add column for increase in methylation from previous timepoint for each condition and each chr_type, where the first timepoint is 0\n",
    "chromosome_m6A_frac['m6A_frac_diff'] = chromosome_m6A_frac.groupby(['genotype','chr_type'])['m6A_frac'].diff()\n",
    "# Set all Nan values in m6A_frac_diff to 0\n",
    "chromosome_m6A_frac['m6A_frac_diff'].fillna(0, inplace=True)\n",
    "\n",
    "#reset index\n",
    "chromosome_m6A_frac.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# normalize m6A_frac_diff by the first m6A_frac value for each genotype and chr_type\n",
    "print(\"chromosome_m6A_frac.groupby(['genotype','chr_type'])['m6A_frac'].transform(lambda x: x/x.iloc[0]):\",chromosome_m6A_frac.groupby(['genotype','chr_type'])['m6A_frac'].transform(lambda x: x.iloc[0]))\n",
    "\n",
    "chromosome_m6A_frac['norm_m6A_frac_diff'] = chromosome_m6A_frac['m6A_frac_diff']/chromosome_m6A_frac.groupby(['genotype','chr_type'])['m6A_frac'].transform(lambda x: x.iloc[0])\n",
    "\n",
    "chromosome_m6A_frac['m6A_frac_diff_from_first'] = chromosome_m6A_frac['m6A_frac']-chromosome_m6A_frac.groupby(['genotype','chr_type'])['m6A_frac'].transform(lambda x: x.iloc[0])\n",
    "\n",
    "print(chromosome_m6A_frac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87806902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot average m6A/A across the chromosome for each condition in a time course\n",
    "# Set px background to white\n",
    "px.defaults.template = \"plotly_white\"\n",
    "\n",
    "# list of samples to consider\n",
    "considered_samples = [0]\n",
    "\n",
    "# Plot title\n",
    "#plot_title = \"AID::SDC-2 + Auxin; 2uM Hia5 Timecourse; m6A thresh = 75%\"\n",
    "plot_title = \"Mean m6A/A across entire chromosomes; m6A Threshold = \" + str(round(m6A_thresh/254*100-1)) + \"%\"\n",
    "\n",
    "# plot boxplot of norm_m6A_frac by chromosome\n",
    "fig = px.box(result_df, x=\"condition\", y=\"m6A_frac\", color=\"chromosome\", title=plot_title, points=\"all\")\n",
    "#Update background to white\n",
    "fig.update_layout(plot_bgcolor='white')\n",
    "fig.show()\n",
    "\n",
    "# plot boxplot of norm_m6A_frac by chromosome\n",
    "fig = px.box(result_df, x=\"condition\", y=\"m6A_frac\", color=\"chr_type\", title=plot_title, points=\"all\")\n",
    "fig.update_layout(plot_bgcolor='white')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485e9723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot average m6A/A across the chromosome for each condition in a time course\n",
    "\n",
    "\n",
    "# list of samples to consider\n",
    "considered_samples = [0,1,2]\n",
    "\n",
    "# Plot title\n",
    "#plot_title = \"AID::SDC-2 + Auxin; 2uM Hia5 Timecourse; m6A thresh = 75%\"\n",
    "plot_title = \"Mean m6A/A on X Chromosome 200kb Regions;<br>3min 2uM Hia5 treatment; m6A thresh = \" + str(m6A_thresh/254) + \"%\"\n",
    "\n",
    "# Plot the boxplot\n",
    "marker_colors =[\"#c45746\",\"#16415e\"]\n",
    "\n",
    "plotly_conditions = conditions\n",
    "#plotly_conditions = [\"N2<br>No-Met\",\"N2<br>3-min\",\"N2<br>10-min\",\"N2<br>30-min\", \"N2<br>120-min\",\n",
    "#\"#021+Aux<br>No-Met\",\"#021+Aux<br>3-min\",\"#021+Aux<br>10-min\",\"#021+Aux<br>30-min\", \"#021+Aux<br>120-min\"]\n",
    "\n",
    "fig = make_subplots(rows=1, cols=len(considered_samples),\n",
    "                y_title = \"m6A/A\",\n",
    "                shared_yaxes=True,\n",
    "                subplot_titles=(list( plotly_conditions[i] for i in considered_samples )))\n",
    "\n",
    "plot_iter=0\n",
    "print(\"weighted_combined_regions \",weighted_combined_regions)\n",
    "for i in considered_samples:\n",
    "    tube_df = weighted_combined_regions.loc[weighted_combined_regions['condition']==conditions[i]]\n",
    "    chr_type = \"Autosome\"\n",
    "    df_plot=tube_df.loc[tube_df['chr_type']==chr_type]\n",
    "    #df_plot=tube_df.sample(frac=17/100,replace=False,random_state=1)\n",
    "    trace0 = go.Box(x=df_plot['condition']+\" \", y=df_plot['m6A_frac'], #+ \" \" makes box plots not overlap\n",
    "                         name=chr_type, marker_color =marker_colors[1],)\n",
    "    chr_type = \"X\"\n",
    "    df_plot=tube_df.loc[tube_df['chr_type']==chr_type]\n",
    "    trace1 = go.Box(x=df_plot['condition'], y=df_plot['m6A_frac'],\n",
    "                         name=chr_type, marker_color=marker_colors[0])#, #add scatter points\n",
    "                            #boxpoints='all', jitter=0.4, pointpos=0) #jitter for SDC-2 degron and N2 only for 3min\n",
    "    plot_iter += 1\n",
    "    fig.append_trace(trace0, row = 1, col = plot_iter)\n",
    "    fig.append_trace(trace1, row = 1, col = plot_iter)\n",
    "\n",
    "# remove boxplot fill color\n",
    "fig.update_traces(fillcolor='rgba(0,0,0,0)')\n",
    "fig['layout'].update(height = 600,width = 1000)\n",
    "fig.update_layout(template=\"plotly_white\",title=plot_title)\n",
    "fig.update_xaxes(showticklabels=False)\n",
    "fig.update_annotations(font_size=12)\n",
    "fig.update_traces(marker=dict(size=3))\n",
    "'''fig = add_p_value_annotation(fig, [[0,1]], 1, _format=dict(interline=0.07, text_height=1.07, color='black'))\n",
    "fig = add_p_value_annotation(fig, [[0,1]], 2, _format=dict(interline=0.07, text_height=1.07, color='black'))\n",
    "fig = add_p_value_annotation(fig, [[0,1]], 3, _format=dict(interline=0.07, text_height=1.07, color='black'))\n",
    "fig = add_p_value_annotation(fig, [[0,1]], 4, _format=dict(interline=0.07, text_height=1.07, color='black'))\n",
    "fig = add_p_value_annotation(fig, [[0,1]], 5, _format=dict(interline=0.07, text_height=1.07, color='black'))'''\n",
    "#fig.update_layout(boxmode='group', xaxis_tickangle=0)\n",
    "\n",
    "for i in range(0,len([0,10])):\n",
    "    fig.layout.annotations[i].update(y=-0.1)\n",
    "fig.update_yaxes(tickformat=\"1%\")\n",
    "fig.show()\n",
    "#Export plotly figure to .svg\n",
    "fig.write_image(output_stem + \"combined_regions_\"  + str(m6A_thresh) +\".svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e958b3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the boxplot\n",
    "marker_colors =[\"#fde725\",\"#a0da39\",\"#4ac16d\"]#,\"#1fa187\",\"#277f8e\",\"#365c8d\",\"#46327e\",\"#440154\",\"#c45746\",\"#16415e\"]\n",
    "\n",
    "plotly_conditions = conditions\n",
    "\n",
    "fig = make_subplots(rows=1, cols=len(conditions),\n",
    "                y_title = \"Coverage\",\n",
    "                shared_yaxes=True,\n",
    "                subplot_titles=(plotly_conditions))\n",
    "\n",
    "print(\"Total MB aligned for ALL conditons: \",int(combined_regions['total_bases'].sum()/1000000),\n",
    "     \" | across \", int(combined_regions['overlapping_reads'].sum()),\" reads with avg. length of: \",\n",
    "     int(combined_regions['total_bases'].sum()/combined_regions['overlapping_reads'].sum()))\n",
    "for i in range(0,len(conditions)):\n",
    "    tube_df = combined_regions.loc[combined_regions['condition']==conditions[i]]\n",
    "    m6A_frac_tube = [tube_df['total_bases'].sum()/100000000*3.125] #3.125 is the scaling factor for adenosines in c elegans genome.\n",
    "    print(\"Total MB aligned for \",conditions[i],\n",
    "          \": \",int(tube_df['total_bases'].sum()/1000000), \n",
    "          \" | across \", int(tube_df['overlapping_reads'].sum()),\n",
    "          \" reads with avg. length of: \",\n",
    "          int(tube_df['total_bases'].sum()/tube_df['overlapping_reads'].sum()))\n",
    "    trace0 = go.Bar(x=tube_df['condition']+\" \", y=m6A_frac_tube,\n",
    "                         name=plotly_conditions[i], marker_color =marker_colors[i])\n",
    "\n",
    "    fig.append_trace(trace0, row = 1, col = i+1)\n",
    "    \n",
    "fig['layout'].update(height = 800)\n",
    "fig.update_layout(template=\"plotly_white\")\n",
    "fig.update_xaxes(showticklabels=False)\n",
    "#fig.update_yaxes(range=[0.7, 1.3])\n",
    "    \n",
    "#fig.update_layout(boxmode='group', xaxis_tickangle=0)\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
